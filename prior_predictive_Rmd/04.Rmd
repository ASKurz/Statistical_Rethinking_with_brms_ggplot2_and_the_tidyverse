---
title: "Ch. 4 Linear Models"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  html_document
---

```{r set-options, echo = F, cache = F}
options(width = 100)
```

### The data.

Let's get the data from McElreath's [rethinking package](http://xcelab.net/rm/statistical-rethinking/).

```{r, message = F}
library(rethinking)
data(Howell1)
d <- Howell1
```

```{r, message = F}
rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)
```

We can use `filter()` to make an adults-only data frame.

```{r}
library(tidyverse)

d2 <- 
  d %>%
  filter(age >= 18)
```

### The model. 

The likelihood for model `b4.1` is 

$$
\begin{eqnarray}
\text{height}_{\text{data = d2}_i} & \sim & \text{Normal}(\mu, \sigma) \\
\mu & \sim & \text{Normal}(178, 20) \\
\sigma & \sim & \text{Uniform}(0, 50)
\end{eqnarray}
$$


```{r, fig.width = 8, fig.height = 2}
d2 %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height), index = letters[1:4]) %>% 
  mutate(mu         = rnorm(n = n(), mean = 178, sd = 20),
         sigma      = runif(n = n(), 0, 50)) %>% 
  mutate(sim_height = rnorm(n = n(), mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```

The likelihood for model `b4.1_half_cauchy` is

$$
\begin{eqnarray}
\text{height}_{\text{data = d2}_i} & \sim & \text{Normal}(\mu, \sigma) \\
\mu & \sim & \text{Normal}(178, 20) \\
\sigma & \sim & \text{HalfCauchy}(0, 1)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.1)
d2 %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height), index = letters[1:4]) %>% 
  mutate(mu         = rnorm(n = n(), mean = 178, sd = 20),
         sigma      = rcauchy(n = n(), location = 0, scale = 1) %>% abs()) %>%  # note the `%>% abs()` portion
  mutate(sim_height = rnorm(n = n(), mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```

The likelihood for model `b4.2` is 

$$
\begin{eqnarray}
\text{height}_{\text{data = d2}_i} & \sim & \text{Normal}(\mu, \sigma) \\
\mu & \sim & \text{Normal}(178, 0.1) \\
\sigma & \sim & \text{Uniform}(0, 50)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.2)
d2 %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height), index = letters[1:4]) %>% 
  mutate(mu         = rnorm(n = n(), mean = 178, sd = 0.1),
         sigma      = runif(n = n(), 0, 50)) %>% 
  mutate(sim_height = rnorm(n = n(), mean = mu, sd = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```

If you look closely, you'll notice each plot has a strong central tendency toward 178 on the y-axis. That's the result of the prior corresponding to `mu = rnorm(n = n(), mean = 178, sd = 0.1)`. Our liberal prior for $\sigma$ (i.e., `sigma = runif(n = n(), 0, 50))`) still resulted in quite a bit of spread around 178. However, we were very certain the mean was 178. 


The likelihood for our new univariable model, `b4.3`, is

$$
\begin{eqnarray}
\text{height}_{\text{data = d2}_i} & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta \text{weight}_i \\
\alpha & \sim & \text{Normal}(178, 100) \\
\beta & \sim & \text{Normal}(0, 10) \\
\sigma & \sim & \text{Uniform}(0, 50)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.3)
d2 %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height, weight), index = letters[1:4]) %>% 
  mutate(alpha      = rnorm(n = n(), mean = 178, sd = 100),
         beta       = rnorm(n = n(), mean = 0, sd = 10),
         sigma      = runif(n = n(), 0, 50)) %>% 
  mutate(sim_height = rnorm(n = n(), 
                            mean = alpha + beta * weight, 
                            sd   = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```


With centering, we can reduce the correlations among the parameters.

```{r}
d2 <- 
  d2 %>%
  mutate(weight_c = weight - mean(weight))
```

The likelihood for our `weight_c` univariable model, `b4.4`, is

$$
\begin{eqnarray}
\text{height}_{\text{data = d2}_i} & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta \text{weight_c}_i \\
\alpha & \sim & \text{Normal}(178, 100) \\
\beta & \sim & \text{Normal}(0, 10) \\
\sigma & \sim & \text{Uniform}(0, 50)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.4)
d2 %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height, weight_c), index = letters[1:4]) %>% 
  mutate(alpha      = rnorm(n = n(), mean = 178, sd = 100),
         beta       = rnorm(n = n(), mean = 0, sd = 10),
         sigma      = runif(n = n(), 0, 50)) %>% 
  mutate(sim_height = rnorm(n = n(), 
                            mean = alpha + beta * weight_c, 
                            sd   = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```

## Polynomial regression

Remember `d`?

```{r}
d %>%
  glimpse()
```


McElreath warned: "Fitting these models to data is easy. Interpreting them can be hard" (p. 111). Standardizing will help `brm()` fit the model. We might standardize our `weight` variable like so.

```{r}
d <-
  d %>%
  mutate(weight_s = (weight - mean(weight))/sd(weight))
```


The likelihood for the quadratic model is

$$
\begin{eqnarray}
\text{height}_{\text{data = d}_i} & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta_1 \text{weight_s}_i + \beta_2 \text{weight_s}_i^2 \\
\alpha & \sim & \text{Normal}(178, 100) \\
\beta_1 & \sim & \text{Normal}(0, 10) \\
\beta_2 & \sim & \text{Normal}(0, 10) \\
\sigma & \sim & \text{HalfCauchy}(0, 1)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.5)
d %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height, weight_s), index = letters[1:4]) %>% 
  mutate(alpha      = rnorm(n = n(), mean = 178, sd = 100),
         beta_1     = rnorm(n = n(), mean = 0, sd = 10),
         beta_2     = rnorm(n = n(), mean = 0, sd = 10),
         sigma      = rcauchy(n = n(), location = 0, scale = 1) %>% abs()) %>% 
  mutate(sim_height = rnorm(n = n(), 
                            mean = alpha + beta_1 * weight_s + beta_2 * weight_s^2, 
                            sd   = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```


The likelihood for the cubic model is

$$
\begin{eqnarray}
\text{height}_{\text{data = d}_i} & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta_1 \text{weight_s}_i + \beta_2 \text{weight_s}_i^2 + \beta_3 \text{weight_s}_i^3 \\
\alpha & \sim & \text{Normal}(178, 100) \\
\beta_1 & \sim & \text{Normal}(0, 10) \\
\beta_2 & \sim & \text{Normal}(0, 10) \\
\beta_3 & \sim & \text{Normal}(0, 10) \\
\sigma & \sim & \text{HalfCauchy}(0, 1)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.6)
d %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height, weight_s), index = letters[1:4]) %>% 
  mutate(alpha      = rnorm(n = n(), mean = 178, sd = 100),
         beta_1     = rnorm(n = n(), mean = 0, sd = 10),
         beta_2     = rnorm(n = n(), mean = 0, sd = 10),
         beta_3     = rnorm(n = n(), mean = 0, sd = 10),
         sigma      = rcauchy(n = n(), location = 0, scale = 1) %>% abs()) %>% 
  mutate(sim_height = rnorm(n = n(), 
                            mean = alpha + beta_1 * weight_s + beta_2 * weight_s^2 + beta_3 * weight_s^3, 
                            sd   = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```



The likelihood for the simple standardized `weight` linear model is

$$
\begin{eqnarray}
\text{height}_{\text{data = d}_i} & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + \beta \text{weight_s}_i \\
\alpha & \sim & \text{Normal}(178, 100) \\
\beta & \sim & \text{Normal}(0, 10) \\
\sigma & \sim & \text{HalfCauchy}(0, 1)
\end{eqnarray}
$$

```{r, fig.width = 8, fig.height = 2}
set.seed(4.7)
d %>% 
  mutate(row = 1:n()) %>% 
  expand(nesting(row, height, weight_s), index = letters[1:4]) %>% 
  mutate(alpha      = rnorm(n = n(), mean = 178, sd = 100),
         beta       = rnorm(n = n(), mean = 0, sd = 10),
         sigma      = rcauchy(n = n(), location = 0, scale = 1) %>% abs()) %>% 
  mutate(sim_height = rnorm(n = n(), 
                            mean = alpha + beta * weight_s, 
                            sd   = sigma)) %>% 
  
  ggplot(aes(x = height, y = sim_height)) +
  geom_abline(color = "white") +
  geom_point(alpha = 1/2, size = 1/2) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~index, scales = "free_y", ncol = 4)
```






## Reference {-}

[McElreath, R. (2016). *Statistical rethinking: A Bayesian course with examples in R and Stan.* Chapman & Hall/CRC Press.](https://xcelab.net/rm/statistical-rethinking/)

## Session info {-}

```{r}
sessionInfo()
```

```{r, warning = F, echo = F}
rm(d, d2)
```

