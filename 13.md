Ch. 13 Adventures in Covariance
================
A Solomon Kurz
2018-09-24

Adventures in Covariance
========================

> In this chapter, you’ll see how to… specify varying slopes in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. It will also improve estimates of intercepts, by borrowing information across parameter types. Essentially, varying slopes models are massive interaction machines. They allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling. When the variation in slopes is large, the average slope is of less interest. Sometimes, the pattern of variation in slopes provides hints about omitted variables that explain why some units respond more or less. We’ll see an example in this chapter.
>
> The machinery that makes such complex varying effects possible will be used later in the chapter to extend the varying effects strategy to more subtle model types, including the use of continuous categories, using Gaussian process. (p. 388)

Varying slopes by construction
------------------------------

> How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 389)

In the **Rethinking: Why Gaussian?** box, McElreath discussed how researchers might use other multivariate distributions to model multiple random effects. The only one he named as an alternative to the Gaussian was the multivariate Student’s *t*. As it turns out, brms does currently allow users to use multivariate Student’s *t* in this way. For details, check out [this discussion in the brms GitHub page](https://github.com/paul-buerkner/brms/issues/231). Bürkner's exemplar syntax from his comment on May 13, 2018, was `y ~ x + (x | gr(g, dist = "student"))`. I haven't experimented with this, but if you do, do consider [commenting on how it went](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/issues).

### Simulate the population.

If you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done [some nice work along these lines](https://gitlab.com/vuorre/bayesplan), too.

``` r
a       <-  3.5  # average morning wait time
b       <- -1    # average difference afternoon wait time
sigma_a <-  1    # std dev in intercepts
sigma_b <-  0.5  # std dev in slopes
rho     <- -.7   # correlation between intercepts and slopes

# The next three lines of code simply combine the terms, above
mu <- c(a, b)

cov_ab <- sigma_a * sigma_b * rho
sigma  <- matrix(c(sigma_a^2, cov_ab, 
                   cov_ab, sigma_b^2), ncol = 2)
```

If you haven't used `matirx()` before, you might get a sense of the elements like so.

``` r
matrix(c(1, 2, 
         3, 4), nrow = 2, ncol = 2)
```

    ##      [,1] [,2]
    ## [1,]    1    3
    ## [2,]    2    4

This next block of code will finally yield our café data.

``` r
library(tidyverse)

sigmas <- c(sigma_a, sigma_b)          # standard deviations
rho    <- matrix(c(1, rho,             # correlation matrix
                   rho, 1), nrow = 2)

# now matrix multiply to get covariance matrix
sigma <- diag(sigmas) %*% rho %*% diag(sigmas)

# how many cafes would you like?
n_cafes <- 20

set.seed(5)  # used to replicate example
vary_effects <- 
  MASS::mvrnorm(n_cafes, mu, sigma)

vary_effects <-
  vary_effects %>% 
  as_tibble() %>% 
  rename(a_cafe = V1,
         b_cafe = V2)

head(vary_effects)
```

    ## # A tibble: 6 x 2
    ##   a_cafe b_cafe
    ##    <dbl>  <dbl>
    ## 1   4.22 -1.61 
    ## 2   2.01 -0.752
    ## 3   4.57 -1.95 
    ## 4   3.34 -1.19 
    ## 5   1.70 -0.586
    ## 6   4.13 -1.14

Let’s make sure we’re keeping this all straight. `a_cafe` = our café-specific intercepts; `b_cafe` = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask *What's the distribution of `a_cafe` and `b_cafe`?* Our variant of Figure 13.2 contains the answer.

For our plots in this chapter, we'll use a custom theme. The color palette will come from the "pearl\_earring" palette of the [dutchmasters package](https://github.com/EdwinTh/dutchmasters). You can learn more about the original painting, Vermeer's *Girl with a Pearl Earring*, [here](https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring).

``` r
# devtools::install_github("EdwinTh/dutchmasters")
library(dutchmasters)

dutchmasters$pearl_earring
```

    ##         red(lips)              skin      blue(scarf1)      blue(scarf2)      white(colar) 
    ##         "#A65141"         "#E7CDC2"         "#80A0C7"         "#394165"         "#FCF9F0" 
    ##       gold(dress)      gold(dress2) black(background)      grey(scarf3)    yellow(scarf4) 
    ##         "#B1934A"         "#DCA258"         "#100F14"         "#8B9DAF"         "#EEDA9D" 
    ##                   
    ##         "#E8DCCF"

We'll name our custom theme `theme_pearl_earring`.

``` r
theme_pearl_earring <-
  theme(text       = element_text(color = "#E8DCCF", family = "Courier"),
        strip.text = element_text(color = "#E8DCCF", family = "Courier"),
        axis.text  = element_text(color = "#E8DCCF"),
        axis.ticks = element_line(color = "#E8DCCF"),
        line       = element_line(color = "#E8DCCF"),
        plot.background   = element_rect(fill = "#100F14", color = "transparent"),
        panel.background  = element_rect(fill = "#100F14", color = "#E8DCCF"),
        strip.background  = element_rect(fill = "#100F14", color = "transparent"),
        panel.grid = element_blank(),
        legend.background = element_rect(fill = "#100F14", color = "transparent"),
        legend.key        = element_rect(fill = "#100F14", color = "transparent"),
        axis.line = element_blank())
```

Now we're ready to plot Figure 13.2.

``` r
vary_effects %>% 
  ggplot(aes(x = a_cafe, y = b_cafe)) +
  geom_point(color = "#80A0C7") +
  geom_rug(color = "#8B9DAF", size = 1/7) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-6-1.png)

Again, these are not "data." This is a distribution of *parameters*.

### Simulate observations.

Here we put those simulated parameters to use.

``` r
n_visits <- 10
sigma    <-  0.5  # std dev within cafes

set.seed(5)  # used to replicate example
d <-
  vary_effects %>% 
  mutate(cafe      = 1:n_cafes) %>% 
  expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %>% 
  mutate(afternoon = rep(0:1, times = n() / 2)) %>% 
  mutate(mu        = a_cafe + b_cafe * afternoon) %>% 
  mutate(wait      = rnorm(n = n(), mean = mu, sd = sigma))
```

We might peek at the data.

``` r
d %>%
  head()
```

    ## # A tibble: 6 x 7
    ##    cafe a_cafe b_cafe visit afternoon    mu  wait
    ##   <int>  <dbl>  <dbl> <int>     <int> <dbl> <dbl>
    ## 1     1   4.22  -1.61     1         0  4.22  3.80
    ## 2     1   4.22  -1.61     2         1  2.61  3.31
    ## 3     1   4.22  -1.61     3         0  4.22  3.60
    ## 4     1   4.22  -1.61     4         1  2.61  2.65
    ## 5     1   4.22  -1.61     5         0  4.22  5.08
    ## 6     1   4.22  -1.61     6         1  2.61  2.31

Now we've finally simulated our data, we are ready to make our version of Figure 13.1, from way back on page 388.

``` r
d %>%
  mutate(afternoon = ifelse(afternoon == 0, "M", "A"),
         day       = rep(rep(1:5, each = 2), times = n_cafes)) %>%
  filter(cafe %in% c(3, 5)) %>%
  mutate(cafe = ifelse(cafe == 3, "cafe #3", "cafe #5")) %>%
  
  ggplot(aes(x = visit, y = wait, group = day)) +
  geom_point(aes(color = afternoon), size = 2) +
  geom_line(color = "#8B9DAF") +
  scale_color_manual(values = c("#80A0C7", "#EEDA9D")) +
  scale_x_continuous(breaks = 1:10,
                     labels = rep(c("M", "A"), times = 5)) +
  coord_cartesian(ylim = 0:8) +
  labs(x = NULL, y = "wait time in minutes") +
  theme_pearl_earring +
  theme(legend.position = "none",
        axis.ticks.x    = element_blank()) +
  facet_wrap(~cafe, ncol = 1)
```

![](13_files/figure-markdown_github/unnamed-chunk-9-1.png)

### The varying slopes model.

The statistical formula for our varying-slopes model follows the form

$$
\\begin{eqnarray}
\\text{wait}\_i & \\sim & \\text{Normal} (\\mu\_i, \\sigma) \\\\
\\mu\_i & = & \\alpha\_{\\text{cafe}\_i} + \\beta\_{\\text{cafe}\_i} \\text{afternoon}\_i \\\\
\\begin{bmatrix} \\alpha\_\\text{cafe} \\\\ \\beta\_\\text{cafe} \\end{bmatrix} & \\sim & \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S}  \\bigg ) \\\\
\\mathbf S & = & \\begin{pmatrix} \\sigma\_\\alpha & 0 \\\\ 0 & \\sigma\_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma\_\\alpha & 0 \\\\ 0 & \\sigma\_\\beta \\end{pmatrix} \\\\
\\alpha & \\sim & \\text{Normal} (0, 10) \\\\
\\beta & \\sim & \\text{Normal} (0, 10) \\\\
\\sigma & \\sim & \\text{HalfCauchy} (0, 1) \\\\
\\sigma\_\\alpha & \\sim & \\text{HalfCauchy} (0, 1) \\\\
\\sigma\_\\beta & \\sim & \\text{HalfCauchy} (0, 1) \\\\
\\mathbf R & \\sim & \\text{LKJcorr} (2)
\\end{eqnarray}
$$

Of the notable new parts, $\\mathbf S$ is the covariance matrix and $\\mathbf R$ is the corresponding correlation matrix, which we might more fully express as

$$\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$$

And according to our prior, $\\mathbf R$ is distributed as LKJcorr(2). We'll use `rethinking::rlkjcorr()` to get a better sense of what that even is.

``` r
library(rethinking)

n_sim <- 1e5

set.seed(133)
r_1 <- 
  rlkjcorr(n_sim, K = 2, eta = 1) %>%
  as_tibble()

set.seed(133)
r_2 <- 
  rlkjcorr(n_sim, K = 2, eta = 2) %>%
  as_tibble()

set.seed(133)
r_4 <- 
  rlkjcorr(n_sim, K = 2, eta = 4) %>%
  as_tibble()
```

Here are the LKJcorr distributions of Figure 13.3.

``` r
ggplot(data = r_1, aes(x = V2)) +
  geom_density(color = "transparent", fill = "#DCA258", alpha = 2/3) +
  geom_density(data = r_2,
               color = "transparent", fill = "#FCF9F0", alpha = 2/3) +
  geom_density(data = r_4,
               color = "transparent", fill = "#394165", alpha = 2/3) +
  geom_text(data = tibble(x     = c(.83, .62, .46),
                          y     = c(.54, .74, 1),
                          label = c("eta = 1", "eta = 2", "eta = 4")),
            aes(x = x, y = y, label = label),
            color = "#A65141", family = "Courier") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("correlation") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-11-1.png)

Okay, let's get ready to model and switch out rethinking for brms.

``` r
detach(package:rethinking, unload = T)
library(brms)
```

As defined above, our first model has both varying intercepts and `afternoon` slopes. I should point out that the `(1 + afternoon | cafe)` syntax specifies that we'd like `brm()` to fit the random effects for `1` (i.e., the intercept) and the `afternoon` slope as correlated. Had we wanted to fit a model in which they were orthogonal, we'd have coded `(1 + afternoon || cafe)`.

``` r
 b13.1 <- 
  brm(data = d, family = gaussian,
      wait ~ 1 + afternoon + (1 + afternoon | cafe),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 2), class = sd),
                prior(cauchy(0, 2), class = sigma),
                prior(lkj(2), class = cor)),
      iter = 5000, warmup = 2000, chains = 2, cores = 2)
```

With Figure 13.4, we assess how the posterior for the correlation of the random effects compares to its prior.

``` r
post <- posterior_samples(b13.1)

post %>%
  ggplot(aes(x = cor_cafe__Intercept__afternoon)) +
  geom_density(data = r_2, aes(x = V2),
               color = "transparent", fill = "#EEDA9D", alpha = 3/4) +
  geom_density(color = "transparent", fill = "#A65141", alpha = 9/10) +
  annotate("text", label = "posterior", 
           x = -0.2, y = 2.2, 
           color = "#A65141", family = "Courier") +
  annotate("text", label = "prior", 
           x = 0, y = 0.85, 
           color = "#EEDA9D", alpha = 2/3, family = "Courier") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("correlation") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-13-1.png)

McElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the `cafe`-specific intercepts and `afternoon` slopes with `coef()`, which returns a three-dimensional list.

``` r
# coef(b13.1) %>% glimpse()

coef(b13.1)
```

    ## $cafe
    ## , , Intercept
    ## 
    ##    Estimate Est.Error     Q2.5    Q97.5
    ## 1  4.078236 0.2024872 3.686076 4.488235
    ## 2  1.928622 0.2097501 1.514000 2.339805
    ## 3  4.822860 0.2103238 4.409551 5.234264
    ## 4  3.473680 0.2069283 3.057106 3.876261
    ## 5  1.767642 0.2132421 1.349000 2.180637
    ## 6  4.389343 0.2033116 3.991142 4.784925
    ## 7  3.252541 0.2055284 2.849942 3.651973
    ## 8  4.009777 0.2100149 3.607011 4.418880
    ## 9  4.240972 0.2049488 3.833821 4.646673
    ## 10 3.712046 0.2054484 3.297671 4.113298
    ## 11 2.167484 0.2143232 1.750279 2.581908
    ## 12 4.030689 0.2038117 3.632611 4.429500
    ## 13 4.087390 0.2112857 3.676001 4.495258
    ## 14 3.598610 0.2079857 3.186678 3.996121
    ## 15 4.268300 0.2069642 3.868748 4.679022
    ## 16 3.486326 0.2093792 3.067460 3.905795
    ## 17 4.039439 0.2101853 3.623347 4.455377
    ## 18 5.841849 0.2086856 5.427196 6.250101
    ## 19 3.775981 0.2084949 3.357601 4.185389
    ## 20 3.860961 0.2018749 3.463988 4.259856
    ## 
    ## , , afternoon
    ## 
    ##      Estimate Est.Error      Q2.5       Q97.5
    ## 1  -1.4060955 0.2562645 -1.916693 -0.91303529
    ## 2  -0.9502310 0.2757002 -1.488831 -0.39873236
    ## 3  -1.8966822 0.2694034 -2.435660 -1.37973150
    ## 4  -1.1977000 0.2615148 -1.713887 -0.67886022
    ## 5  -0.5720066 0.2755359 -1.107413 -0.02803720
    ## 6  -1.4974558 0.2548006 -2.004418 -1.00442140
    ## 7  -1.0572362 0.2644412 -1.575237 -0.54195765
    ## 8  -1.7279221 0.2764010 -2.277033 -1.20567247
    ## 9  -1.5729491 0.2646116 -2.096497 -1.05555506
    ## 10 -1.0263614 0.2559372 -1.526353 -0.51432945
    ## 11 -0.4851009 0.2801514 -1.013500  0.08269592
    ## 12 -1.2638940 0.2585166 -1.771039 -0.73923533
    ## 13 -1.8172383 0.2819366 -2.372526 -1.26672584
    ## 14 -1.6333119 0.2718769 -2.179892 -1.11066481
    ## 15 -1.6839054 0.2676034 -2.221878 -1.16188602
    ## 16 -0.9646355 0.2688109 -1.480965 -0.43332147
    ## 17 -0.6902864 0.2878149 -1.236444 -0.10014676
    ## 18 -1.5518641 0.2790663 -2.102687 -0.99430412
    ## 19 -0.9188598 0.2680882 -1.433388 -0.38351117
    ## 20 -0.9522641 0.2616962 -1.447882 -0.44050993

Here's the code to extract the relevant elements from the `coef()` list, convert them to a tibble, and add the `cafe` index.

``` r
partially_pooled_params <-
  # With this line we select each of the 20 cafe's posterior mean (i.e., Estimate)
  # for both `Intercept` and `afternoon`
  coef(b13.1)$cafe[ , 1, 1:2] %>%
  as_tibble() %>%               # convert the two vectors to a tibble
  rename(Slope = afternoon) %>%
  mutate(cafe = 1:nrow(.)) %>%  # add the `cafe` index
  select(cafe, everything())    # simply moving `cafe` to the leftmost position
```

Like McElreath, we'll compute the unpooled estimates directly from the data.

``` r
# compute unpooled estimates directly from data
un_pooled_params <-
  d %>%
  # With these two lines, we compute the mean value for each cafe's wait time 
  # in the morning and then the afternoon.
  group_by(afternoon, cafe) %>%
  summarise(mean = mean(wait)) %>%
  ungroup() %>%  # Ungrouping allows us to alter afternoon, one of the grouping variables
  mutate(afternoon = ifelse(afternoon == 0, "Intercept", "Slope")) %>%
  spread(key = afternoon, value = mean) %>%  # use `spread()` just as in the previous block
  mutate(Slope = Slope - Intercept)          # Finally, here's our slope!

# Here we combine the partially-pooled and unpooled means into a single data object, 
# which will make plotting easier.
params <-
  # `bind_rows()` will stack the second tibble below the first
  bind_rows(partially_pooled_params, un_pooled_params) %>%
  # index whether the estimates are pooled
  mutate(pooled = rep(c("partially", "not"), each = nrow(.)/2)) 

# Here's a glimpse at what we've been working for
params %>%
  slice(c(1:5, 36:40))
```

    ## # A tibble: 10 x 4
    ##     cafe Intercept  Slope pooled   
    ##    <int>     <dbl>  <dbl> <chr>    
    ##  1     1      4.08 -1.41  partially
    ##  2     2      1.93 -0.950 partially
    ##  3     3      4.82 -1.90  partially
    ##  4     4      3.47 -1.20  partially
    ##  5     5      1.77 -0.572 partially
    ##  6    16      3.42 -0.836 not      
    ##  7    17      3.91 -0.348 not      
    ##  8    18      5.89 -1.50  not      
    ##  9    19      3.70 -0.733 not      
    ## 10    20      3.79 -0.774 not

Finally, here's our code for Figure 13.5.a, showing shrinkage in two dimensions.

``` r
ggplot(data = params, aes(x = Intercept, y = Slope)) +
  stat_ellipse(geom = "polygon", type = "norm", level = 1/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 2/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 3/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 4/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 5/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 6/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 7/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 8/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = 9/10, size = 0, alpha = 1/20, fill = "#E7CDC2") +
  stat_ellipse(geom = "polygon", type = "norm", level = .99,  size = 0, alpha = 1/20, fill = "#E7CDC2") +
  geom_point(aes(group = cafe, color = pooled)) +
  geom_line(aes(group = cafe), size = 1/4) +
  scale_color_manual("Pooled?",
                     values = c("#80A0C7", "#A65141")) +
  coord_cartesian(xlim = range(params$Intercept),
                  ylim = range(params$Slope)) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-17-1.png)

Learn more about `stat_ellipse()`, [here](https://ggplot2.tidyverse.org/reference/stat_ellipse.html). Let's prep for Figure 13.5.b.

``` r
# retrieve the partially-pooled estimates with `coef()`
partially_pooled_estimates <-
  coef(b13.1)$cafe[ , 1, 1:2] %>%
  as_tibble() %>%                  # convert the two vectors to a tibble
  rename(morning = Intercept) %>%  # the Intercept is the wait time for morning (i.e., `afternoon == 0`)
  mutate(afternoon = morning + afternoon,  # `afternoon` wait time is the `morning` wait time plus the afternoon slope
         cafe      = 1:n()) %>%  # Add the `cafe` index
  select(cafe, everything()) 

# Compute unpooled estimates directly from data
un_pooled_estimates <-
  d %>%
  # As above, with these two lines, we compute each cafe's mean wait value by time of day.
  group_by(afternoon, cafe) %>% 
  summarise(mean = mean(wait)) %>%
  ungroup() %>%  # ungrouping allows us to alter the grouping variable, afternoon
  mutate(afternoon = ifelse(afternoon == 0, "morning", "afternoon")) %>%
  spread(key = afternoon, value = mean)  # this seperates out the values into morning and afternoon columns

estimates <-
  bind_rows(partially_pooled_estimates, un_pooled_estimates) %>%
  mutate(pooled = rep(c("partially", "not"), each = n() / 2))
```

The code for Figure 13.5.b.

``` r
ggplot(data = estimates, aes(x = morning, y = afternoon)) +
  # Nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the 
  # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` 
  # functions in the previous plot
  mapply(function(level) {
    stat_ellipse(geom  = "polygon", type = "norm",
                 size  = 0, alpha = 1/20, fill = "#E7CDC2",
                 level = level)
    }, 
    # Enter the levels here
    level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) +
  geom_point(aes(group = cafe, color = pooled)) +
  geom_line(aes(group = cafe), size = 1/4) +
  scale_color_manual("Pooled?",
                     values = c("#80A0C7", "#A65141")) +
  coord_cartesian(xlim = range(estimates$morning),
                  ylim = range(estimates$afternoon)) +
  labs(x = "morning wait (mins)",
       y = "afternoon wait (mins)") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-19-1.png)

Example: Admission decisions and gender
---------------------------------------

Let's revisit the infamous UCB admissions data.

``` r
library(rethinking)
data(UCBadmit)
d <- UCBadmit
```

Here we detach rethinking, reload brms, and augment the data a bit.

``` r
detach(package:rethinking, unload = T)
library(brms)
rm(UCBadmit)

d <- 
  d %>%
  mutate(male    = ifelse(applicant.gender == "male", 1, 0),
         dept_id = rep(1:6, each = 2))
```

### Varying intercepts.

The statistical formula for our varying-intercepts logistic regression model follows the form

$$
\\begin{eqnarray}
\\text{admit}\_i & \\sim & \\text{Binomial} (n\_i, p\_i) \\\\
\\text{logit} (p\_i) & = & \\alpha\_{\\text{dept\_id}\_i} + \\beta \\text{male}\_i \\\\
\\alpha\_\\text{dept\_id} & \\sim & \\text{Normal} (\\alpha, \\sigma) \\\\
\\alpha & \\sim & \\text{Normal} (0, 10) \\\\
\\beta & \\sim & \\text{Normal} (0, 1) \\\\
\\sigma & \\sim & \\text{HalfCauchy} (0, 2) \\\\
\\end{eqnarray}
$$

Since there’s only one left-hand term in our `(1 | dept_id)` code, there’s only one random effect.

``` r
b13.2 <- 
  brm(data = d, family = binomial,
      admit | trials(applications) ~ 1 + male + (1 | dept_id),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(cauchy(0, 2), class = sd)),
      iter = 4500, warmup = 500, chains = 3, cores = 3,
      control = list(adapt_delta = 0.99))
```

Since we don't have a `depth=2` argument in `brms::summary()`, we'll have to get creative. One way to look at the parameters is with `b13.2$fit`:

``` r
b13.2$fit
```

    ## Inference for Stan model: 986d730d4cab1fb3046dbb2c44ac3aae.
    ## 3 chains, each with iter=4500; warmup=500; thin=1; 
    ## post-warmup draws per chain=4000, total post-warmup draws=12000.
    ## 
    ##                          mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
    ## b_Intercept             -0.60    0.02 0.67  -1.92  -0.98  -0.61  -0.24   0.78  1859    1
    ## b_male                  -0.10    0.00 0.08  -0.25  -0.15  -0.10  -0.04   0.07  4896    1
    ## sd_dept_id__Intercept    1.49    0.01 0.60   0.79   1.09   1.36   1.72   2.98  2217    1
    ## r_dept_id[1,Intercept]   1.28    0.02 0.67  -0.09   0.91   1.28   1.66   2.61  1887    1
    ## r_dept_id[2,Intercept]   1.23    0.02 0.67  -0.14   0.86   1.24   1.61   2.58  1912    1
    ## r_dept_id[3,Intercept]   0.02    0.02 0.67  -1.36  -0.34   0.02   0.40   1.33  1863    1
    ## r_dept_id[4,Intercept]  -0.02    0.02 0.67  -1.37  -0.37  -0.01   0.36   1.32  1859    1
    ## r_dept_id[5,Intercept]  -0.46    0.02 0.67  -1.83  -0.83  -0.45  -0.08   0.85  1854    1
    ## r_dept_id[6,Intercept]  -2.01    0.02 0.68  -3.42  -2.37  -2.00  -1.62  -0.68  1942    1
    ## lp__                   -61.82    0.05 2.50 -67.51 -63.24 -61.50 -60.02 -57.89  2296    1
    ## 
    ## Samples were drawn using NUTS(diag_e) at Mon Sep 24 21:10:18 2018.
    ## For each parameter, n_eff is a crude measure of effective sample size,
    ## and Rhat is the potential scale reduction factor on split chains (at 
    ## convergence, Rhat=1).

However, notice that the group-specific parameters don't match up with those in the text. Though our `r_dept_id[1,Intercept]` had a posterior mean of 1.27, the number for `a_dept[1]` in the text is 0.67. This is because the brms package presented the random effects in the **non-centered** metric. The rethinking package, in contrast, presented the random effects in the **centered** metric. On page 399, McElreath wrote:

> Remember, the values above are the *α*<sub>*D**E**P**T*</sub> estimates, and so they are deviations from the global mean *α*, which in this case has posterior mean -0.58. So department A, "\[1\]" in the table, has the highest average admission rate. Department F, "\[6\]" in the table, has the lowest.

Here's another fun fact:

``` r
# Numbers taken from the mean column on page 399 in the text
c(0.67, 0.63, -0.59, -0.62, -1.06, -2.61) %>% mean()
```

    ## [1] -0.5966667

The average of the rethinking-based **centered** random effects is within rounding error of the global mean, -0.58. If you want the random effects in the **centered** metric from brms, you can use the `coef()` function:

``` r
coef(b13.2)
```

    ## $dept_id
    ## , , Intercept
    ## 
    ##     Estimate  Est.Error       Q2.5      Q97.5
    ## 1  0.6765878 0.09859583  0.4840086  0.8703036
    ## 2  0.6303442 0.11430654  0.4070792  0.8541753
    ## 3 -0.5839830 0.07382826 -0.7286664 -0.4396857
    ## 4 -0.6157122 0.08546014 -0.7829672 -0.4491192
    ## 5 -1.0588502 0.09850475 -1.2517382 -0.8678093
    ## 6 -2.6071519 0.15873326 -2.9312366 -2.3062299
    ## 
    ## , , male
    ## 
    ##      Estimate  Est.Error       Q2.5      Q97.5
    ## 1 -0.09634538 0.08015487 -0.2549424 0.06514113
    ## 2 -0.09634538 0.08015487 -0.2549424 0.06514113
    ## 3 -0.09634538 0.08015487 -0.2549424 0.06514113
    ## 4 -0.09634538 0.08015487 -0.2549424 0.06514113
    ## 5 -0.09634538 0.08015487 -0.2549424 0.06514113
    ## 6 -0.09634538 0.08015487 -0.2549424 0.06514113

And just to confirm, the average of the posterior means of the `Intercept` random effects with `brms::coef()` is also the global mean within rounding error:

``` r
mean(coef(b13.2)$dept_id[ , "Estimate", "Intercept"])
```

    ## [1] -0.5931276

Note how `coef()` returned a three-dimensional list.

``` r
coef(b13.2) %>% str()
```

    ## List of 1
    ##  $ dept_id: num [1:6, 1:4, 1:2] 0.677 0.63 -0.584 -0.616 -1.059 ...
    ##   ..- attr(*, "dimnames")=List of 3
    ##   .. ..$ : chr [1:6] "1" "2" "3" "4" ...
    ##   .. ..$ : chr [1:4] "Estimate" "Est.Error" "Q2.5" "Q97.5"
    ##   .. ..$ : chr [1:2] "Intercept" "male"

If you just want the parameter summaries for the random intercepts, you have to use three-dimensional indexing.

``` r
coef(b13.2)$dept_id[ , , "Intercept"]  # this also works: coef(b13.2)$dept_id[ , , 1]
```

    ##     Estimate  Est.Error       Q2.5      Q97.5
    ## 1  0.6765878 0.09859583  0.4840086  0.8703036
    ## 2  0.6303442 0.11430654  0.4070792  0.8541753
    ## 3 -0.5839830 0.07382826 -0.7286664 -0.4396857
    ## 4 -0.6157122 0.08546014 -0.7829672 -0.4491192
    ## 5 -1.0588502 0.09850475 -1.2517382 -0.8678093
    ## 6 -2.6071519 0.15873326 -2.9312366 -2.3062299

So to get our brms summaries in a similar format to those in the text, we'll have to combine `coef()` with `fixef()` and `VarCorr()`.

``` r
coef(b13.2)$dept_id[, , "Intercept"] %>%
  as_tibble() %>% 
  bind_rows(fixef(b13.2) %>% 
              as_tibble()) %>% 
  bind_rows(VarCorr(b13.2)$dept_id$sd %>% 
              as_tibble())
```

    ## # A tibble: 9 x 4
    ##   Estimate Est.Error   Q2.5   Q97.5
    ##      <dbl>     <dbl>  <dbl>   <dbl>
    ## 1   0.677     0.0986  0.484  0.870 
    ## 2   0.630     0.114   0.407  0.854 
    ## 3  -0.584     0.0738 -0.729 -0.440 
    ## 4  -0.616     0.0855 -0.783 -0.449 
    ## 5  -1.06      0.0985 -1.25  -0.868 
    ## 6  -2.61      0.159  -2.93  -2.31  
    ## 7  -0.599     0.667  -1.92   0.778 
    ## 8  -0.0963    0.0802 -0.255  0.0651
    ## 9   1.49      0.602   0.789  2.98

And a little more data wrangling will make the summaries easier to read:

``` r
coef(b13.2)$dept_id[, , "Intercept"] %>%
  as_tibble() %>% 
  bind_rows(fixef(b13.2) %>% 
              as_tibble()) %>% 
  bind_rows(VarCorr(b13.2)$dept_id$sd %>% 
              as_tibble()) %>% 
  mutate(parameter = c(paste("Intercept [", 1:6, "]", sep = ""), 
                       "Intercept", "male", "sigma")) %>% 
  select(parameter, everything()) %>% 
  mutate_if(is_double, round, digits = 2)
```

    ## # A tibble: 9 x 5
    ##   parameter     Estimate Est.Error  Q2.5 Q97.5
    ##   <chr>            <dbl>     <dbl> <dbl> <dbl>
    ## 1 Intercept [1]    0.68       0.1   0.48  0.87
    ## 2 Intercept [2]    0.63       0.11  0.41  0.85
    ## 3 Intercept [3]   -0.580      0.07 -0.73 -0.44
    ## 4 Intercept [4]   -0.62       0.09 -0.78 -0.45
    ## 5 Intercept [5]   -1.06       0.1  -1.25 -0.87
    ## 6 Intercept [6]   -2.61       0.16 -2.93 -2.31
    ## 7 Intercept       -0.6        0.67 -1.92  0.78
    ## 8 male            -0.1        0.08 -0.25  0.07
    ## 9 sigma            1.49       0.6   0.79  2.98

I’m not aware of a slick and easy way to get the `n_eff` and `Rhat` summaries into the mix. But if you’re fine with working with the brms-default **non-centered** parameterization, `b13.2$fit` gets you those just fine.

One last thing. The [broom package](https://cran.r-project.org/web/packages/broom/index.html) offers a very handy way to get those brms random effects. Just throw the model `brm()` fit into the `tidy()` function.

``` r
library(broom)

tidy(b13.2) %>%
  mutate_if(is.numeric, round, digits = 2)  # This line just rounds the output
```

    ##                      term estimate std.error  lower  upper
    ## 1             b_Intercept    -0.60      0.67  -1.66   0.48
    ## 2                  b_male    -0.10      0.08  -0.23   0.04
    ## 3   sd_dept_id__Intercept     1.49      0.60   0.85   2.58
    ## 4  r_dept_id[1,Intercept]     1.28      0.67   0.21   2.34
    ## 5  r_dept_id[2,Intercept]     1.23      0.67   0.14   2.29
    ## 6  r_dept_id[3,Intercept]     0.02      0.67  -1.06   1.08
    ## 7  r_dept_id[4,Intercept]    -0.02      0.67  -1.08   1.05
    ## 8  r_dept_id[5,Intercept]    -0.46      0.67  -1.52   0.61
    ## 9  r_dept_id[6,Intercept]    -2.01      0.68  -3.10  -0.95
    ## 10                   lp__   -61.82      2.50 -66.42 -58.32

But note how, just as with `b13.2$fit`, this approach summarizes the posterior with the **non-centered** parameterization. Which is a fine parameterization. It's just a little different from what you'll get when using `precis( m13.2 , depth=2 )`, as in the text.

### Varying effects of being `male`.

Now we're ready to allow our `male` dummy to varies, too, the statistical model follows the form

$$
\\begin{eqnarray}
\\text{admit}\_i & \\sim & \\text{Binomial} (n\_i, p\_i) \\\\
\\text{logit} (p\_i) & = & \\alpha\_{\\text{dept\_id}\_i} + \\beta\_{\\text{dept\_id}\_i} \\text{male}\_i \\\\
\\begin{bmatrix} \\alpha\_\\text{dept\_id} \\\\ \\beta\_\\text{dept\_id} \\end{bmatrix} & \\sim & \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S}  \\bigg ) \\\\
\\mathbf S & = & \\begin{pmatrix} \\sigma\_\\alpha & 0 \\\\ 0 & \\sigma\_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma\_\\alpha & 0 \\\\ 0 & \\sigma\_\\beta \\end{pmatrix} \\\\
\\alpha & \\sim & \\text{Normal} (0, 10) \\\\
\\beta & \\sim & \\text{Normal} (0, 1) \\\\
(\\sigma\_\\alpha, \\sigma\_\\beta) & \\sim & \\text{HalfCauchy} (0, 2) \\\\
\\mathbf R & \\sim & \\text{LKJcorr} (2)
\\end{eqnarray}
$$

Fit the model.

``` r
b13.3 <- 
  brm(data = d, family = binomial,
      admit | trials(applications) ~ 1 + male + (1 + male | dept_id),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(cauchy(0, 2), class = sd),
                prior(lkj(2), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .99,
                     max_treedepth = 12))
```

McElreath encouraged us to make sure the chains look good. Instead of relying on convenience functions, let's do it by hand.

``` r
post <- posterior_samples(b13.3, add_chain = T)

post %>% 
  select(-lp__) %>% 
  gather(key, value, -chain, -iter) %>% 
  mutate(chain = as.character(chain)) %>% 

  ggplot(aes(x = iter, y = value, group = chain, color = chain)) +
  geom_line(size = 1/15) +
  scale_color_manual(values = c("#80A0C7", "#B1934A", "#A65141", "#EEDA9D")) +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = c(1001, 5000)) +
  theme_pearl_earring +
  theme(legend.position  = c(.825, .06),
        legend.direction = "horizontal") +
  facet_wrap(~key, ncol = 3, scales = "free_y")
```

![](13_files/figure-markdown_github/unnamed-chunk-31-1.png)

Our chains look great. While we're at it, let's examine the $\\hat{R}$ vales in a handmade plot, too.

``` r
rhat(b13.3) %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  filter(rowname != "lp__") %>% 
  
  ggplot(aes(x = `.`, y = reorder(rowname, `.`))) + 
  geom_segment(aes(xend = 1, yend = rowname),
               color = "#EEDA9D") +
  geom_point(aes(color = `.` > 1), 
             size = 2) +
  scale_color_manual(values = c("#80A0C7", "#A65141")) +
  labs(x = NULL, y = NULL) +
  theme_pearl_earring +
  theme(legend.position = "none",
        axis.ticks.y    = element_blank(),
        axis.text.y     = element_text(hjust = 0))
```

![](13_files/figure-markdown_github/unnamed-chunk-32-1.png)

Them are some respectable $\\hat{R}$ values. The plot accentuates their differences, but they’re all basically 1 (e.g., see what happens is you set `coord_cartesian(xlim = c(0.99, 1.01))`). Here are the random effects in the **centered** metric:

``` r
coef(b13.3)
```

    ## $dept_id
    ## , , Intercept
    ## 
    ##     Estimate  Est.Error       Q2.5      Q97.5
    ## 1  1.3039929 0.25721419  0.8087397  1.8273980
    ## 2  0.7411762 0.32601123  0.1099403  1.4071241
    ## 3 -0.6478157 0.08442524 -0.8153408 -0.4852648
    ## 4 -0.6185930 0.10540328 -0.8288492 -0.4142269
    ## 5 -1.1328384 0.11539156 -1.3650303 -0.9115861
    ## 6 -2.5985053 0.20083532 -3.0039884 -2.2176101
    ## 
    ## , , male
    ## 
    ##      Estimate Est.Error       Q2.5      Q97.5
    ## 1 -0.79148822 0.2700589 -1.3412015 -0.2682669
    ## 2 -0.21031359 0.3268533 -0.8743670  0.4291155
    ## 3  0.08387488 0.1379224 -0.1799176  0.3557455
    ## 4 -0.09154781 0.1407413 -0.3706974  0.1841107
    ## 5  0.12017014 0.1885609 -0.2412263  0.4969157
    ## 6 -0.12807116 0.2670044 -0.6635168  0.3915913

We may as well keep our doing-things-by-hand kick going. Instead relying on `bayesplog::mcmc_intervals()` or `tidybayes::pointintervalh()` to make our coefficient plot, we’ll combine `geom_pointrange()` and `coord_flip()`. But we will need to wrangle a bit to get those brms-based **centered** random effects into a usefully-formatted tidy tibble.

``` r
# As far as I can tell, because `coef()` yields a list, you have to take out the two 
# random effects one at a time, convert them to tibbles, and reassemble them with `bind_rows()`
coef(b13.3)$dept_id[, , 1] %>% 
  as_tibble() %>% 
  bind_rows(
    coef(b13.3)$dept_id[, , 2] %>% 
      as_tibble()
    ) %>% 
  mutate(param   = c(paste("Intercept", 1:6), paste("male", 1:6)),
         reorder = c(6:1, 12:7)) %>% 

  # plot
  ggplot(aes(x = reorder(param, reorder))) +
  geom_hline(yintercept = 0, linetype = 3, color = "#8B9DAF") +
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, y = Estimate, color = reorder < 7),
                  shape = 20, size = 3/4) +
  scale_color_manual(values = c("#394165", "#A65141")) +
  xlab(NULL) +
  coord_flip() +
  theme_pearl_earring +
  theme(legend.position = "none",
        axis.ticks.y    = element_blank(),
        axis.text.y     = element_text(hjust = 0))
```

![](13_files/figure-markdown_github/unnamed-chunk-34-1.png)

Just like in the text, our `male` slopes are much less dispersed than our intercepts.

### Shrinkage.

Figure 13.6.a depicts the correlation between the full UCB model's varying intercepts and slopes.

``` r
library(tidybayes)

post <- posterior_samples(b13.3)

post %>% 
  ggplot(aes(x = cor_dept_id__Intercept__male, y = 0)) +
  geom_halfeyeh(fill = "#394165", color = "#8B9DAF", 
                point_interval = median_qi, .width = .95) +
  scale_x_continuous(breaks = c(-1, median(post$cor_dept_id__Intercept__male), 1),
                     labels = c(-1, "-.35", 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = -1:1) +
  labs(subtitle = "The dot is at the median; the\nhorizontal bar is the 95% CI.",
       x = "correlation") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-35-1.png)

Much like for Figure 13.5.b, above, it'll take a little data processing before we're ready to reproduce Figure 13.6.b.

``` r
# Here we put the partially-pooled estimate summaries in a tibble
partially_pooled_params <-
  coef(b13.3)$dept_id[ , 1, ] %>%
  as_tibble() %>%
  rename(intercept = Intercept,
         slope     = male) %>%
  mutate(dept = 1:n()) %>%
  select(dept, everything())

# In order to calculate the unpooled estimates from the data, we'll need a function that 
# can convert probabilities into the logit metric. If you do the algebra, this is just
# a transformation of the `inv_logit_scaled()` function.
prob_to_logit <- function(x){
  -log((1 / x) -1)
  }

# compute unpooled estimates directly from data
un_pooled_params <-
  d %>%
  group_by(male, dept_id) %>%
  summarise(prob_admit = mean(admit / applications)) %>%
  ungroup() %>%
  mutate(male = ifelse(male == 0, "intercept", "slope")) %>%
  spread(key  = male, value = prob_admit) %>%
  rename(dept = dept_id) %>%
  mutate(intercept = prob_to_logit(intercept),  # Here we put our `prob_to_logit()` function to work
         slope     = prob_to_logit(slope)) %>%
  mutate(slope     = slope - intercept)

# Here we combine the partially-pooled and unpooled means into a single data object.
params <-
  bind_rows(partially_pooled_params, un_pooled_params) %>%
  mutate(pooled      = rep(c("partially", "not"), each = n() / 2)) %>%
  mutate(dept_letter = rep(LETTERS[1:6], times = 2))  # This will help with plotting

params
```

    ## # A tibble: 12 x 5
    ##     dept intercept   slope pooled    dept_letter
    ##    <int>     <dbl>   <dbl> <chr>     <chr>      
    ##  1     1     1.30  -0.791  partially A          
    ##  2     2     0.741 -0.210  partially B          
    ##  3     3    -0.648  0.0839 partially C          
    ##  4     4    -0.619 -0.0915 partially D          
    ##  5     5    -1.13   0.120  partially E          
    ##  6     6    -2.60  -0.128  partially F          
    ##  7     1     1.54  -1.05   not       A          
    ##  8     2     0.754 -0.220  not       B          
    ##  9     3    -0.660  0.125  not       C          
    ## 10     4    -0.622 -0.0820 not       D          
    ## 11     5    -1.16   0.200  not       E          
    ## 12     6    -2.58  -0.189  not       F

Here's our version of Figure 13.6.b, depicting two-dimensional shrinkage for the partially-pooled multilevel estimates (posterior means) relative to the unpooled coefficients, calculated from the data. The `ggrepel::geom_text_repel()` function will help us with the in-plot labels.

``` r
library(ggrepel)

ggplot(data = params, aes(x = intercept, y = slope)) +
  mapply(function(level){
    stat_ellipse(geom  = "polygon", type = "norm",
                 size  = 0, alpha = 1/20, fill = "#E7CDC2",
                 level = level)
    },  
    level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) +
  geom_point(aes(group = dept, color = pooled)) +
  geom_line(aes(group = dept), size = 1/4) +
  scale_color_manual("Pooled?",
                     values = c("#80A0C7", "#A65141")) +
  geom_text_repel(data = params %>% filter(pooled == "partially"),
                  aes(label = dept_letter),
                  color = "#E8DCCF", size = 4, family = "Courier", seed = 13.6) +
  coord_cartesian(xlim = range(params$intercept),
                  ylim = range(params$slope)) +
  labs(x = expression(paste("intercept (", alpha[dept_id], ")")),
       y = expression(paste("slope (", beta[dept_id], ")"))) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-37-1.png)

### Model comparison.

Fit the no-gender model.

``` r
b13.4 <- 
  brm(data = d, family = binomial,
      admit | trials(applications) ~ 1 + (1 | dept_id),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(cauchy(0, 2), class = sd)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .99,
                     max_treedepth = 12))
```

Compare the three models by the WAIC.

``` r
waic(b13.2, b13.3, b13.4)
```

    ##                 WAIC    SE
    ## b13.2         108.26 16.34
    ## b13.3          90.89  4.71
    ## b13.4         105.20 18.07
    ## b13.2 - b13.3  17.37 13.07
    ## b13.2 - b13.4   3.06  3.70
    ## b13.3 - b13.4 -14.31 15.04

> The varying slopes model, \[`b13.3`\], dominates \[the other two\]. This is despite the fact that the *average* slope in \[`b13.3`\] is nearly zero. The average isn’t what matters, however. It is the individual slopes, one for each department, that matter. If we wish to generalize to new departments, the variation in slopes suggest that it’ll be worth paying attention to gender, even if the average slope is nearly zero in the population. (pp. 402--403, *emphasis* in the original)

Example: Cross-classified `chimpanzees` with varying slopes
-----------------------------------------------------------

Retrieve the `chimpanzees` data.

``` r
library(rethinking)
data(chimpanzees)
d <- chimpanzees
```

``` r
detach(package:rethinking, unload = T)
library(brms)
rm(chimpanzees)

d <-
  d %>%
  select(-recipient) %>%
  mutate(block_id = block)
```

My math's aren't the best. But if I'm following along correctly, here's a fuller statistical expression of our cross-classified model.

$$
$$

And now each $\\mathbf R$ is a 3 × 3 correlation matrix.

Let’s fit this beast.

``` r
b13.6 <- 
  brm(data = d, family = binomial,
      pulled_left ~ 1 + prosoc_left + condition:prosoc_left +
        (1 + prosoc_left + condition:prosoc_left | actor) +
        (1 + prosoc_left + condition:prosoc_left | block_id),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(cauchy(0, 2), class = sd),
                prior(lkj(4), class = cor)),
      iter = 5000, warmup = 1000, chains = 3, cores = 3)
```

Even though it's not apparent in the syntax, our model `b13.6` was already fit using the [non-centered parameterization. Behind the scenes, Bürkner has brms do this automatically](https://github.com/paul-buerkner/brms/issues/211). It's been that way all along.

If you recall from last chapter, we can compute the number of effective samples for our parameters like so.

``` r
ratios_cp <- neff_ratio(b13.6)

neff <-
  ratios_cp %>% 
  as_tibble %>% 
  rename(neff_ratio = value) %>% 
  mutate(neff       = neff_ratio * 12000)

head(neff)
```

    ## # A tibble: 6 x 2
    ##   neff_ratio  neff
    ##        <dbl> <dbl>
    ## 1      0.329 3951.
    ## 2      0.643 7721.
    ## 3      0.690 8284.
    ## 4      0.366 4388.
    ## 5      0.556 6670.
    ## 6      0.496 5955.

Now we're ready for our variant of Figure 13.7. The handy [ggbeeswarm package](https://cran.r-project.org/web/packages/ggbeeswarm/index.html) and its `geom_quasirandom()` function will give a better sense of the distribution.

``` r
library(ggbeeswarm)

neff %>%
  ggplot(aes(x = factor(0), y = neff)) +
  geom_boxplot(fill = "#394165", color = "#8B9DAF") +
  geom_quasirandom(method = "tukeyDense",
                   size = 2/3, color = "#EEDA9D", alpha = 2/3) +
  scale_x_discrete(NULL, breaks = NULL,
                   expand = c(.75, .75)) +
  scale_y_continuous(breaks = c(0, 6000, 12000)) +
  coord_cartesian(ylim = 0:12000) +
  labs(y = "effective samples",
       subtitle = "The non-centered\nparameterization is the\nbrms default. No fancy\ncoding required.") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-41-1.png)

As in the last chapter, we'll use the `bayesplot::mcmc_neff()` function to examine the ratio of n.eff and the fill number of post-warm-up iterations, *N*. Ideally, that ratio is closer to 1 than not.

``` r
library(bayesplot)

color_scheme_set(c("#DCA258", "#EEDA9D", "#394165", "#8B9DAF", "#A65141", "#A65141"))

mcmc_neff(ratios_cp, size = 2) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-42-1.png)

Here are our standard deviation parameters.

``` r
tidy(b13.6) %>%
  filter(str_detect(term , "sd_")) %>%
  mutate_if(is.numeric, round, digits = 2)
```

    ##                                 term estimate std.error lower upper
    ## 1                sd_actor__Intercept     2.37      0.92  1.29  4.05
    ## 2              sd_actor__prosoc_left     0.45      0.36  0.04  1.14
    ## 3    sd_actor__prosoc_left:condition     0.51      0.46  0.03  1.41
    ## 4             sd_block_id__Intercept     0.23      0.20  0.02  0.62
    ## 5           sd_block_id__prosoc_left     0.57      0.40  0.07  1.31
    ## 6 sd_block_id__prosoc_left:condition     0.52      0.42  0.04  1.34

McElreath discussed `rethinking::link()` in the middle of page 407. He showed how his `link(m13.6NC)` code returned a list of four matrices, of which the `p` matrix was of primary interest. The `brms::fitted()` function doesn't work quite the same way, here.

``` r
fitted(b13.6,
       summary = F,
       nsamples = 1000) %>% 
  str()
```

    ## Using the maximum response value as the number of trials.

    ## Warning: Using 'binomial' families without specifying 'trials' on the left-hand side of the model
    ## formula is deprecated.

    ##  num [1:1000, 1:504] 0.225 0.242 0.211 0.238 0.193 ...

First off, recall that `fitted()` returns summary values, by default. If we want individual values, set `summary = FALSE`. It's also the `fitted()` default to use all posterior iterations, which is 12,000 in this case. To match the text, we need to set `nsamples = 1000`. But those are just details. The main point is that `fitted()` only returns one matrix, which is the analogue to the `p` matrix in the text.

Moving forward, before we can follow along with McElreath’s R code 13.27, we need to refit the simpler model from way back in Chapter 12.

``` r
b12.5 <- 
  brm(data = d, family = binomial,
      pulled_left ~ 1 + prosoc_left + condition:prosoc_left + 
        (1 | actor) + (1 | block_id),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1), class = sd)),
      iter = 5000, warmup = 1000, chains = 3, cores = 3)
```

Now we can compare them with `waic()`.

``` r
waic(b13.6, b12.5)
```

    ##                 WAIC    SE
    ## b13.6         534.44 19.93
    ## b12.5         532.75 19.68
    ## b13.6 - b12.5   1.69  4.10

``` r
model_weights(b13.6, b12.5, weights = "waic")
```

    ##     b13.6     b12.5 
    ## 0.3008416 0.6991584

> In this example, no matter which varying effect structure you use, you’ll find that actors vary a lot in their baseline preference for the left-hand lever. Everything else is much less important. But using the most complex model, \[`b13.6`\], tells the correct story. Because the varying slopes are adaptively regularized, the model hasn’t overfit much, relative to the simpler model that contains only the important intercept variation. (p. 408)

Continuous categories and the Gaussian process
----------------------------------------------

> There is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works.
>
> We’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 410)

### Example: Spatial autocorrelation in Oceanic tools.

``` r
# load the distance matrix
library(rethinking)
data(islandsDistMatrix)

# display short column names, so fits on screen
d_mat <- islandsDistMatrix
colnames(d_mat) <- c("Ml", "Ti", "SC", "Ya", "Fi", 
                     "Tr", "Ch", "Mn", "To", "Ha")
round(d_mat, 1)
```

    ##             Ml  Ti  SC  Ya  Fi  Tr  Ch  Mn  To  Ha
    ## Malekula   0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7
    ## Tikopia    0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3
    ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4
    ## Yap        4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2
    ## Lau Fiji   1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9
    ## Trobriand  2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7
    ## Chuuk      3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8
    ## Manus      2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7
    ## Tonga      1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0
    ## Hawaii     5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0

If you wanted to use color to more effectively visualize the values in the matirx, you might do something like this.

``` r
d_mat %>%
  as_tibble() %>%
  gather() %>%
  rename(column = key,
         distance = value) %>%
  mutate(row          = rep(rownames(d_mat), times = 10),
         row_order    = rep(9:0,            times = 10),
         column_order = rep(0:9,            each  = 10)) %>%
  
  ggplot(aes(x = reorder(column, column_order), 
             y = reorder(row,    row_order))) + 
  geom_raster(aes(fill = distance)) + 
  geom_text(aes(label = round(distance, digits = 1)),
            size = 3, family = "Courier", color = "#100F14") +
  scale_fill_gradient(low = "#FCF9F0", high = "#A65141") +
  scale_x_discrete(position = "top", expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  labs(x = NULL, y = NULL) +
  theme_pearl_earring +
  theme(axis.ticks  = element_blank(),
        axis.text.y = element_text(hjust = 0))
```

![](13_files/figure-markdown_github/unnamed-chunk-46-1.png)

Figure 13.8 shows the "shape of the function relating distance to the covariance **K**<sub>*i**j*</sub>."

``` r
tibble(
  x       = seq(from = 0, to = 4, by = .01),
  linear  = exp(-1 * x),
  squared = exp(-1 * x^2)) %>%
  
  ggplot(aes(x = x)) +
  geom_line(aes(y = linear),
            color = "#B1934A", linetype = 2) +
  geom_line(aes(y = squared),
            color = "#DCA258") +
  scale_x_continuous("distance", expand = c(0, 0)) +
  scale_y_continuous("correlation", 
                     breaks = c(0, .5, 1),
                     labels = c(0, ".5", 1)) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-47-1.png)

``` r
data(Kline2) # load the ordinary data, now with coordinates

d <- 
  Kline2 %>%
  mutate(society = 1:10)

rm(Kline2)

d %>% glimpse()
```

    ## Observations: 10
    ## Variables: 10
    ## $ culture     <fct> Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tong...
    ## $ population  <int> 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000
    ## $ contact     <fct> low, low, low, high, high, high, high, low, high, low
    ## $ total_tools <int> 13, 22, 24, 43, 33, 19, 40, 28, 55, 71
    ## $ mean_TU     <dbl> 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6
    ## $ lat         <dbl> -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9
    ## $ lon         <dbl> 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6
    ## $ lon2        <dbl> -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4
    ## $ logpop      <dbl> 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.472...
    ## $ society     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10

Switch out rethinking for brms.

``` r
detach(package:rethinking, unload = T)
library(brms)
```

Okay, it appears this is going to be a bit of a ride. It's not entirely clear to me if we can fit a Gaussian process model in brms that's a direct equivalent to what McElreath did with rethinking. But we can try. First, note our use of the `gp()` syntax in the `brm()` function, below. We're attempting to tell brms that we would like to include latitude and longitude (i.e., `lat` and `long2`, respectively) in a Gaussian process. Also note how our priors are a little different than those in the text. I'll explain, below. Let's just move ahead and fit the model.

``` r
b13.7 <- 
  brm(data = d, family = poisson,
      total_tools ~ 1 + gp(lat, lon2) + logpop,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(inv_gamma(2.874624, 0.393695), class = lscale),
                prior(cauchy(0, 1), class = sdgp)),
      iter = 1e4, warmup = 2000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.999,
                     max_treedepth = 12))
```

Here's the model summary.

``` r
posterior_summary(b13.7) %>%
  round(digits = 2)
```

    ##                   Estimate Est.Error   Q2.5  Q97.5
    ## b_Intercept           1.44      1.11  -0.76   3.75
    ## b_logpop              0.23      0.11   0.02   0.44
    ## sdgp_gplatlon2        0.52      0.36   0.16   1.41
    ## lscale_gplatlon2      0.23      0.13   0.07   0.56
    ## zgp_gplatlon2[1]     -0.59      0.78  -2.13   0.94
    ## zgp_gplatlon2[2]      0.44      0.85  -1.26   2.11
    ## zgp_gplatlon2[3]     -0.62      0.71  -1.98   0.90
    ## zgp_gplatlon2[4]      0.88      0.70  -0.44   2.31
    ## zgp_gplatlon2[5]      0.26      0.75  -1.21   1.75
    ## zgp_gplatlon2[6]     -1.00      0.79  -2.53   0.59
    ## zgp_gplatlon2[7]      0.14      0.72  -1.39   1.52
    ## zgp_gplatlon2[8]     -0.18      0.87  -1.88   1.58
    ## zgp_gplatlon2[9]      0.41      0.92  -1.50   2.14
    ## zgp_gplatlon2[10]    -0.31      0.83  -1.94   1.32
    ## lp__                -51.52      3.12 -58.57 -46.42

Our Gaussian process parameters are different than McElreath's. From the brms reference manual, here's the brms parameterization:

*k*(*x*<sub>*i*</sub>, *x*<sub>*j*</sub>)=*s**d**g**p*<sup>2</sup>exp(−||*x*<sub>*i*</sub> − *x*<sub>*j*</sub>||/2*l**s**c**a**l**e*<sup>2</sup>)

What McElreath called *η*, Bürkner called *s**d**g**p*. While McElreath estimated *η*<sup>2</sup>, brms simply estimated *s**d**g**p*. So we'll have to square our `sdgp_gplatlon2` before it's on the same scale as `etasq` in the text. Here it is.

``` r
posterior_samples(b13.7) %>% 
  transmute(sdgp_squared = sdgp_gplatlon2^2) %>% 
  mean_hdi(sdgp_squared, .width = .89) %>% 
  mutate_if(is.double, round, digits = 3)
```

    ##   sdgp_squared .lower .upper .width .point .interval
    ## 1        0.398  0.002  0.757   0.89   mean       hdi

Now we're in the ballpark. In our model `brm()` code, above, we just went with the flow and kept the `cauchy(0, 1)` prior on `sdgp`.

Now look at the denominator of the inner part of Bürkner equation, 2*l**s**c**a**l**e*<sup>2</sup>. This appears to be the brms equivalent to McElreath's *ρ*<sup>2</sup>. Or at least it's what we've got. Anyway, also note that McElreath estimated *ρ*<sup>2</sup> directly as `rhosq`. If I'm doing the algebra correctly--and that may well be a big if--, we might expect:

*ρ*<sup>2</sup> = 1/(2 × (*l**s**c**a**l**e*<sup>2</sup>))

But that doesn't appear to be the case. *Sigh*.

``` r
posterior_samples(b13.7) %>% 
  transmute(rho_squared = 1/(2*(lscale_gplatlon2^2))) %>% 
  mean_hdi(rho_squared, .width = .89) %>% 
  mutate_if(is.double, round, digits = 3)
```

    ##   rho_squared .lower .upper .width .point .interval
    ## 1      21.812  0.395 46.485   0.89   mean       hdi

Oh man, that isn't even close to the 2.67 McElreath reported in the text. The plot deepens. If you look back, you'll see we used a very different prior for *l**s**c**a**l**e*. Here is it: `inv_gamma(2.874624, 0.393695)`. Use `get_prior()` to discover where that came from.

``` r
get_prior(data = d, family = poisson,
      total_tools ~ 1 + gp(lat, lon2) + logpop)
```

    ##                           prior     class         coef group resp dpar nlpar bound
    ## 1                                       b                                         
    ## 2                                       b       logpop                            
    ## 3           student_t(3, 3, 10) Intercept                                         
    ## 4                normal(0, 0.5)    lscale                                         
    ## 5 inv_gamma(2.874624, 0.393695)    lscale gp(lat,lon2)                            
    ## 6           student_t(3, 0, 10)      sdgp                                         
    ## 7                                    sdgp gp(lat,lon2)

That is, we used the brms default prior for *l**s**c**a**l**e*. In a [GitHub exchange](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/issues/8), Bürkner pointed out that brms uses special priors for *l**s**c**a**l**e* parameters based on Michael Betancourt \[of the Stan team\]'s [vignette on the topic](https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html). Though it isn't included in this document, I also ran the model with the `cauchy(0, 1)` prior and the results were quite similar. So the big discrepancy between our model and the one in the text isn't based on that prior.

Now that we’ve started, we may as well keep going down the comparison train. Let’s reproduce McElreath’s model with rethinking.

Switch out brms for rethinking.

``` r
detach(package:brms, unload = T)
library(rethinking)
```

``` r
m13.7 <- map2stan(
    alist(
        total_tools ~ dpois(lambda),
        log(lambda) <- a + g[society] + bp*logpop,
        g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ),
        a ~ dnorm(0,10),
        bp ~ dnorm(0,1),
        etasq ~ dcauchy(0,1),
        rhosq ~ dcauchy(0,1)
    ),
    data=list(
        total_tools=d$total_tools,
        logpop=d$logpop,
        society=d$society,
        Dmat=islandsDistMatrix),
    warmup=2000 , iter=1e4 , chains=4)
```

Alright, now we'll work directly with the posteriors to make some visual comparisons.

``` r
# rethinking-based posterior
post_m13.7 <- rethinking::extract.samples(m13.7)[2:5] %>% as_tibble()

detach(package:rethinking, unload = T)
library(brms)

# brms-based posterior
post_b13.7 <- posterior_samples(b13.7)
```

Here's the model intercept posterior, by package:

``` r
post_m13.7[, "a"] %>% 
  bind_rows(post_b13.7[, "b_Intercept"] %>% 
              as_tibble() %>% 
              rename(a = value)) %>% 
  mutate(package = rep(c("rethinking", "brms"), each = nrow(post_m13.7))) %>% 
  
  ggplot(aes(x = a, fill = package)) +
  geom_density(size = 0, alpha = 1/2) +
  scale_fill_manual(NULL, values = c("#80A0C7", "#A65141")) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Not identical, but pretty close",
       x     = "intercept") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-56-1.png)

The slope:

``` r
post_m13.7[, "bp"] %>% 
  bind_rows(post_b13.7[, "b_logpop"] %>% 
              as_tibble() %>% 
              rename(bp = value)) %>% 
  mutate(package = rep(c("rethinking", "brms"), each = nrow(post_m13.7))) %>% 
  
  ggplot(aes(x = bp, fill = package)) +
  geom_density(size = 0, alpha = 1/2) +
  scale_fill_manual(NULL, values = c("#80A0C7", "#A65141")) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Again, pretty close",
       x     = "slope") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-57-1.png)

This one, *η*<sup>2</sup>, required a little transformation:

``` r
post_m13.7[, "etasq"] %>% 
  bind_rows(post_b13.7[, "sdgp_gplatlon2"] %>% 
              as_tibble() %>%
              mutate(value = value^2) %>% 
              rename(etasq = value)) %>% 
  mutate(package = rep(c("rethinking", "brms"), each = nrow(post_m13.7))) %>% 
  
  ggplot(aes(x = etasq, fill = package)) +
  geom_density(size = 0, alpha = 1/2) +
  scale_fill_manual(NULL, values = c("#80A0C7", "#A65141")) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Still in the same ballpark",
       x     = expression(eta^2)) +
  coord_cartesian(xlim = 0:3) +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-58-1.png)

*ρ*<sup>2</sup> required more extensive transformation of the brms posterior:

``` r
post_m13.7[, "rhosq"] %>%
  bind_rows(post_b13.7[, "lscale_gplatlon2"] %>% 
              as_tibble() %>%
              transmute(value = 1/(2*(value^2))) %>%
              # transmute(value = value^2) %>% 
              rename(rhosq = value)) %>% 
  mutate(package = rep(c("rethinking", "brms"), each = nrow(post_m13.7))) %>% 
  
  ggplot(aes(x = rhosq, fill = package)) +
  geom_density(size = 0) +
  scale_fill_manual(NULL, values = c("#80A0C7", "#A65141")) +
  labs(title    = "Holy smokes are those not the same!",
       subtitle = "Notice how differently the y axes got scaled. Also, that brms density is\nright skewed for days.",
       x        = expression(rho^2)) +
  coord_cartesian(xlim = 0:50) +
  theme_pearl_earring +
  theme(legend.position = "none") +
  facet_wrap(~package, scales = "free_y")
```

![](13_files/figure-markdown_github/unnamed-chunk-59-1.png)

I'm in clinical psychology. Folks in my field don't tend to use Gaussian processes, so getting to the bottom of this is low on my to-do list. Perhaps one of y'all are more experienced with Gaussian processes and see a flaw somewhere in my code. Please [hit me up](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/issues) if you do.

Anyways, here's our brms + ggplot2 version of Figure 13.9.

``` r
ggplot(data = tibble(x = c(0, 50.2)), aes(x = x)) +
  mapply(function(etasq, rhosq) {
    stat_function(fun = function(x, etasq, rhosq) etasq*exp(-rhosq*x^2), 
                  args = list(etasq = etasq, rhosq = rhosq), 
                  size = 1/4,
                  alpha = 1/4,
                  color = "#EEDA9D")
  }, 
  etasq = post_b13.7[1:100, "sdgp_gplatlon2"]^2,
  rhosq = post_b13.7[1:100, "lscale_gplatlon2"]^2*.5
  ) +
  stat_function(fun = function(x) median(post_b13.7$sdgp_gplatlon2)^2 *exp(-median(post_b13.7[1:100, "lscale_gplatlon2"] )^2*.5*x^2),
                color = "#EEDA9D", size = 1.1) +
  coord_cartesian(ylim = 0:1) +
  scale_x_continuous(breaks = seq(from = 0, to = 50, by = 10),
                     expand = c(0, 0)) +
  labs(x = "distance (thousand km)", 
       y = "covariance") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-60-1.png)

Do note the scale on which we placed our x axis. Our brms parameterization resulted in a gentler decline in spatial covariance.

Let's finish this up and "push the parameters back through the function for **K**, the covariance matrix" (p. 415).

``` r
# compute posterior median covariance among societies
k <- matrix(0, nrow = 10, ncol = 10)
for (i in 1:10)
    for (j in 1:10)
        k[i, j] <- median(post_b13.7$sdgp_gplatlon2^2) * exp(-median(post_b13.7$lscale_gplatlon2^2) * islandsDistMatrix[i, j]^2)

diag(k) <- median(post_b13.7$sdgp_gplatlon2^2) + 0.01

k %>% round(2)
```

    ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
    ##  [1,] 0.19 0.18 0.18 0.09 0.17 0.16 0.12 0.13 0.16  0.05
    ##  [2,] 0.18 0.19 0.18 0.09 0.17 0.16 0.13 0.14 0.16  0.06
    ##  [3,] 0.18 0.18 0.19 0.10 0.17 0.16 0.14 0.15 0.15  0.06
    ##  [4,] 0.09 0.09 0.10 0.19 0.06 0.14 0.17 0.17 0.04  0.02
    ##  [5,] 0.17 0.17 0.17 0.06 0.19 0.12 0.10 0.10 0.18  0.07
    ##  [6,] 0.16 0.16 0.16 0.14 0.12 0.19 0.16 0.18 0.10  0.03
    ##  [7,] 0.12 0.13 0.14 0.17 0.10 0.16 0.19 0.17 0.07  0.05
    ##  [8,] 0.13 0.14 0.15 0.17 0.10 0.18 0.17 0.19 0.08  0.03
    ##  [9,] 0.16 0.16 0.15 0.04 0.18 0.10 0.07 0.08 0.19  0.07
    ## [10,] 0.05 0.06 0.06 0.02 0.07 0.03 0.05 0.03 0.07  0.19

And we'll continue to follow suit and change these to a correlation matrix.

``` r
# convert to correlation matrix
rho <- round(cov2cor(k), 2)
# add row/col names for convenience
colnames(rho) <- c("Ml","Ti","SC","Ya","Fi","Tr","Ch","Mn","To","Ha")
rownames(rho) <- colnames(rho)

rho %>% round(2)
```

    ##      Ml   Ti   SC   Ya   Fi   Tr   Ch   Mn   To   Ha
    ## Ml 1.00 0.94 0.93 0.44 0.89 0.80 0.63 0.69 0.82 0.26
    ## Ti 0.94 1.00 0.94 0.47 0.89 0.81 0.68 0.71 0.81 0.31
    ## SC 0.93 0.94 1.00 0.52 0.86 0.84 0.72 0.76 0.77 0.29
    ## Ya 0.44 0.47 0.52 1.00 0.29 0.74 0.86 0.85 0.21 0.12
    ## Fi 0.89 0.89 0.86 0.29 1.00 0.62 0.49 0.51 0.93 0.36
    ## Tr 0.80 0.81 0.84 0.74 0.62 1.00 0.83 0.92 0.51 0.16
    ## Ch 0.63 0.68 0.72 0.86 0.49 0.83 1.00 0.89 0.37 0.24
    ## Mn 0.69 0.71 0.76 0.85 0.51 0.92 0.89 1.00 0.40 0.15
    ## To 0.82 0.81 0.77 0.21 0.93 0.51 0.37 0.40 1.00 0.34
    ## Ha 0.26 0.31 0.29 0.12 0.36 0.16 0.24 0.15 0.34 1.00

The correlations in our `rho` matrix look a little higher than those in the text. Before we get see them in a plot, let's consider `psize`. If you really want to scale the points in Figure 13.10.a like McElreath did, you can make the `psize` variable in a tidyverse sort of way as follows. However, if you compare the `psize` method and the default ggplot2 method using just `logpop`, you'll see the difference is negligible. In that light, I'm going to be lazy and just use `logpop` in my plots.

``` r
d %>% 
  transmute(psize = logpop / max(logpop)) %>% 
  transmute(psize = exp(psize * 1.5) - 2)
```

    ##        psize
    ## 1  0.3134090
    ## 2  0.4009582
    ## 3  0.6663711
    ## 4  0.7592196
    ## 5  0.9066890
    ## 6  0.9339560
    ## 7  0.9834797
    ## 8  1.1096138
    ## 9  1.2223112
    ## 10 2.4816891

As far as I can figure, you still have to get `rho` into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so.

``` r
tidy_rho <-
  rho %>%
  as.data.frame() %>% 
  rownames_to_column() %>% 
  bind_cols(d %>% select(culture, logpop, total_tools, lon2, lat)) %>% 
  gather(colname, correlation, -rowname, -culture, -logpop, -total_tools, -lon2, -lat) %>% 
  mutate(group = paste(pmin(rowname, colname), pmax(rowname, colname))) %>% 
  select(rowname, colname, group, culture, everything())

head(tidy_rho)
```

    ##   rowname colname group    culture   logpop total_tools  lon2   lat correlation
    ## 1      Ml      Ml Ml Ml   Malekula 7.003065          13 -12.5 -16.3        1.00
    ## 2      Ti      Ml Ml Ti    Tikopia 7.313220          22 -11.2 -12.3        0.94
    ## 3      SC      Ml Ml SC Santa Cruz 8.188689          24 -14.0 -10.7        0.93
    ## 4      Ya      Ml Ml Ya        Yap 8.474494          43 -41.9   9.5        0.44
    ## 5      Fi      Ml Fi Ml   Lau Fiji 8.909235          33  -1.9 -17.7        0.89
    ## 6      Tr      Ml Ml Tr  Trobriand 8.987197          19 -29.1  -8.7        0.80

Okay, here's our version of Figure 13.10.a.

``` r
tidy_rho %>%       
  ggplot(aes(x = lon2, y = lat)) +
  geom_line(aes(group = group, alpha = correlation^2),
            color = "#80A0C7") +
  geom_point(data = d, aes(size = logpop), color = "#DCA258") +
  geom_text_repel(data = d, aes(label = culture), 
                  seed = 0, point.padding = .3, size = 3, color = "#FCF9F0") +
  scale_alpha_continuous(range = c(0, 1)) +
  labs(x = "longitude",
       y = "latitude") +
  coord_cartesian(xlim = range(d$lon2),
                  ylim = range(d$lat)) +
  theme(legend.position = "none") +
  theme_pearl_earring 
```

![](13_files/figure-markdown_github/unnamed-chunk-65-1.png)

Yep, as expressed by the intensity of the colors of the connecting lines, those correlations are more pronounced than those in the text. Here's our version of Figure 13.10.b.

``` r
# new data for fitted()
nd <- 
  tibble(logpop = seq(from = 6, to = 14, length.out = 30),
         lat    = median(d$lat),
         lon2   = median(d$lon2))

# fitted()
ftd <-
  fitted(b13.7, newdata = nd) %>% 
  as_tibble() %>% 
  bind_cols(nd)
  
# plot
tidy_rho %>% 
  ggplot(aes(x = logpop)) +
  geom_ribbon(data = ftd,
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "#394165", alpha = .5) +
  geom_line(data = ftd,
            aes(y = Estimate), color = "#100F14", linetype = 1, size = 1.1) + #  80A0C7 100F14
  geom_line(aes(y = total_tools, group = group, alpha = correlation^2),
            color = "#80A0C7") +
  geom_point(data = d, 
             aes(y = total_tools, size = logpop), color = "#DCA258") +
  geom_text_repel(data = d, 
                  aes(y = total_tools, label = culture), 
                  seed = 0, point.padding = .3, size = 3, color = "#FCF9F0") +
  scale_alpha_continuous(range = c(0, 1)) +
  labs(x = "log population",
       y = "total tools") +
  coord_cartesian(xlim = range(d$logpop),
                  ylim = range(d$total_tools)) +
  theme(legend.position = "none") +
  theme_pearl_earring
```

![](13_files/figure-markdown_github/unnamed-chunk-66-1.png)

Same deal. Our higher correlations make for a more intensely-webbed plot. To learn more on Bürkner's thoughts on this model in brms, check out the [thread on this issue](https://github.com/paul-buerkner/brms/issues/300).

~~Summary~~ Bonus: Another Berkley-admissions-data-like example.
----------------------------------------------------------------

[McElreath uploaded recordings](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA/playlists) of him teaching out of his text for a graduate course during the 2017/2018 fall semester. In the beginning of [lecture 13 from week 7](https://www.youtube.com/watch?v=rSQ1XMwO_9A&t), he discussed a paper from [van der Lee and Ellemers (2015) published an article in PNAS](http://www.pnas.org/content/112/40/12349.abstract). Their paper suggested male researchers were more likely than female researchers to get research funding in the Netherlands. In their initial analysis (p. 12350) they provided a simple *χ*<sup>2</sup> test to test the null hypothesis there was no difference in success for male versus female researchers, for which they reported *χ*<sub>*d**f* = 1</sub><sup>2</sup> = 4.01, *p* = .045. Happily, van der Lee and Ellemers provided their data values in their supplemental material (i.e., [Table S1.](http://www.pnas.org/content/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf)), which McElreath also displayed in his video.

Their data follows the same structure as the Berkley admissions data. In his lecture, McElreath suggested their *χ*<sup>2</sup> test is an example of Simpson’s paradox, just as with the Berkley data. He isn't the first person to raise this criticism (see [Volker and SteenBeek’s critique](http://www.pnas.org/content/112/51/E7036.full), which McElreath also pointed to in the lecture).

Here are the data:

``` r
funding <- 
  tibble(
    discipline   = rep(c("Chemical sciences", "Physical sciences", "Physics", "Humanities", "Technical sciences",  "Interdisciplinary", "Earth/life sciences", "Social sciences", "Medical sciences"),
                     each = 2),
    gender       = rep(c("m", "f"), times = 9),
    applications = c(83, 39, 135, 39, 67, 9, 230, 166, 189, 62, 105, 78, 156, 126, 425, 409, 245, 260) %>% as.integer(),
    awards       = c(22, 10, 26, 9, 18, 2, 33, 32, 30, 13, 12, 17, 38, 18, 65, 47, 46, 29) %>% as.integer(),
    rejects      = c(61, 29, 109, 30, 49, 7, 197, 134, 159, 49, 93, 61, 118, 108, 360, 362, 199, 231) %>% as.integer(),
    male         = ifelse(gender == "f", 0, 1) %>% as.integer()
  )

funding
```

    ## # A tibble: 18 x 6
    ##    discipline          gender applications awards rejects  male
    ##    <chr>               <chr>         <int>  <int>   <int> <int>
    ##  1 Chemical sciences   m                83     22      61     1
    ##  2 Chemical sciences   f                39     10      29     0
    ##  3 Physical sciences   m               135     26     109     1
    ##  4 Physical sciences   f                39      9      30     0
    ##  5 Physics             m                67     18      49     1
    ##  6 Physics             f                 9      2       7     0
    ##  7 Humanities          m               230     33     197     1
    ##  8 Humanities          f               166     32     134     0
    ##  9 Technical sciences  m               189     30     159     1
    ## 10 Technical sciences  f                62     13      49     0
    ## 11 Interdisciplinary   m               105     12      93     1
    ## 12 Interdisciplinary   f                78     17      61     0
    ## 13 Earth/life sciences m               156     38     118     1
    ## 14 Earth/life sciences f               126     18     108     0
    ## 15 Social sciences     m               425     65     360     1
    ## 16 Social sciences     f               409     47     362     0
    ## 17 Medical sciences    m               245     46     199     1
    ## 18 Medical sciences    f               260     29     231     0

Let’s fit a few models.

First, we’ll fit an analogue to the initial van der Lee and Ellemers *χ*<sup>2</sup> test. Since we’re Bayesian modelers, we’ll use a simple logistic regression, using `male` (dummy coded 0 = female, 1 = male) to predict admission (i.e., `awards`).

``` r
b13.bonus_0 <- 
  brm(data = funding, family = binomial,
      awards | trials(applications) ~ 1 + male,
      # Note our continued use of weakly-regularizing priors
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4)
```

If you inspect them, the chains look great. Here are the posterior summaries:

``` r
tidy(b13.bonus_0) %>%
  filter(term != "lp__") %>%
  mutate_if(is.numeric, round, digits = 2)
```

    ##          term estimate std.error lower upper
    ## 1 b_Intercept    -1.74      0.08 -1.88 -1.61
    ## 2      b_male     0.21      0.10  0.04  0.38

Yep, the 95% intervals for `male` dummy exclude zero. If you wanted a one-sided Bayesian *p*-value, you might do something like:

``` r
posterior_samples(b13.bonus_0) %>%
  summarise(One_sided_Bayesian_p_value = filter(., b_male <= 0) %>% nrow()/nrow(.))
```

    ##   One_sided_Bayesian_p_value
    ## 1                  0.0209375

Pretty small. But recall how Simpson's paradox helped us understand the Berkley data. Different departments in Berkley had different acceptance rates AND different ratios of male and female applicants. Similarly, different academic disciplines in the Netherlands might have different `award` rates for funding AND different ratios of male and female applications.

Just like in section 13.2, let's fit two more models. The first model will allow intercepts to vary by discipline. The second model will allow intercepts and the `male` dummy slopes to vary by discipline.

``` r
b13.bonus_1 <- 
  brm(data = funding, family = binomial,
      awards | trials(applications) ~ 1 + male + (1 | discipline),
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b),
                prior(cauchy(0, 1), class = sd)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .99))

b13.bonus_2 <- 
  brm(data = funding, family = binomial,
      awards | trials(applications) ~ 1 + male + (1 + male | discipline),
      prior = c(prior(normal(0, 4), class = Intercept),
                prior(normal(0, 4), class = b),
                prior(cauchy(0, 1), class = sd),
                prior(lkj(4), class = cor)),
      iter = 5000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = .99))
```

We'll compare the models with information criteria.

``` r
waic(b13.bonus_0, b13.bonus_1, b13.bonus_2)
```

    ##                             WAIC   SE
    ## b13.bonus_0               129.64 8.89
    ## b13.bonus_1               125.66 7.30
    ## b13.bonus_2               116.32 5.58
    ## b13.bonus_0 - b13.bonus_1   3.98 6.21
    ## b13.bonus_0 - b13.bonus_2  13.32 5.55
    ## b13.bonus_1 - b13.bonus_2   9.34 2.83

The WAIC suggests the varying intercepts/varying slopes model made the best sense of the data. Here's what the random intercepts look like in a coefficient plot.

``` r
coef(b13.bonus_2)$discipline[, , 2] %>% 
  as_tibble() %>% 
  mutate(discipline = c("Chemical sciences", "Physical sciences",
                        "Physics", "Humanities", "Technical sciences",
                        "Interdisciplinary", "Earth/life sciences",
                        "Social sciences", "Medical sciences")) %>%
  
  ggplot(aes(x = reorder(discipline, Estimate), y = Estimate,
             ymin = Q2.5,
             ymax = Q97.5)) +
  geom_hline(yintercept = 0, color = "#E8DCCF", alpha = 1/4) +
  geom_hline(yintercept = fixef(b13.bonus_2)[2], linetype = 3, color = "#80A0C7") +
  geom_pointrange(shape = 20, size = 3/4, color = "#8B9DAF") +
  labs(title    = "Random slopes for the male dummy",
       subtitle = "The vertical dotted line is the posterior mean of the fixed effect for the\nmale dummy. The dots and horizontal lines are the posterior means and\npercentile-based 95% intervals, respectively. The values are on the log scale.",
       x        = NULL, y = NULL) +
  coord_flip(ylim = -1:1) +
  theme_pearl_earring +
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

![](13_files/figure-markdown_github/unnamed-chunk-71-1.png)

Note how the 95% intervals for all the random `male` slopes contain zero within their bounds. Here are the fixed effects:

``` r
tidy(b13.bonus_2) %>%
  filter(str_detect(term , "b_")) %>%
  mutate_if(is.numeric, round, digits = 2)
```

    ##          term estimate std.error lower upper
    ## 1 b_Intercept    -1.63      0.15 -1.85 -1.37
    ## 2      b_male     0.15      0.17 -0.14  0.42

And if you wanted a one-sided Bayesian *p*-value for the `male` dummy for the full model:

``` r
posterior_samples(b13.bonus_2) %>%
  summarise(One_sided_Bayesian_p_value = filter(., b_male <= 0) %>% nrow()/nrow(.))
```

    ##   One_sided_Bayesian_p_value
    ## 1                  0.1856875

So, the estimate of the gender bias is small and consistent with the null hypothesis. Which is good! We want gender equality for things like funding success.

Reference
---------

[McElreath, R. (2016). *Statistical rethinking: A Bayesian course with examples in R and Stan.* Chapman & Hall/CRC Press.](https://xcelab.net/rm/statistical-rethinking/)

Session info
------------

``` r
sessionInfo()
```

    ## R version 3.5.1 (2018-07-02)
    ## Platform: x86_64-apple-darwin15.6.0 (64-bit)
    ## Running under: macOS High Sierra 10.13.6
    ## 
    ## Matrix products: default
    ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
    ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
    ## 
    ## locale:
    ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
    ## 
    ## attached base packages:
    ## [1] parallel  stats     graphics  grDevices utils     datasets  methods   base     
    ## 
    ## other attached packages:
    ##  [1] rethinking_1.59    brms_2.5.0         bayesplot_1.6.0    ggbeeswarm_0.6.0   ggrepel_0.8.0     
    ##  [6] tidybayes_1.0.1    broom_0.4.5        Rcpp_0.12.18       rstan_2.17.3       StanHeaders_2.17.2
    ## [11] bindrcpp_0.2.2     dutchmasters_0.1.0 forcats_0.3.0      stringr_1.3.1      dplyr_0.7.6       
    ## [16] purrr_0.2.5        readr_1.1.1        tidyr_0.8.1        tibble_1.4.2       ggplot2_3.0.0     
    ## [21] tidyverse_1.2.1   
    ## 
    ## loaded via a namespace (and not attached):
    ##  [1] colorspace_1.3-2          ggridges_0.5.0            rsconnect_0.8.8          
    ##  [4] rprojroot_1.3-2           ggstance_0.3              markdown_0.8             
    ##  [7] base64enc_0.1-3           rstudioapi_0.7            svUnit_0.7-12            
    ## [10] DT_0.4                    mvtnorm_1.0-8             lubridate_1.7.4          
    ## [13] xml2_1.2.0                bridgesampling_0.4-0      codetools_0.2-15         
    ## [16] mnormt_1.5-5              knitr_1.20                shinythemes_1.1.1        
    ## [19] jsonlite_1.5              LaplacesDemon_16.1.1      shiny_1.1.0              
    ## [22] compiler_3.5.1            httr_1.3.1                backports_1.1.2          
    ## [25] assertthat_0.2.0          Matrix_1.2-14             lazyeval_0.2.1           
    ## [28] cli_1.0.0                 later_0.7.3               htmltools_0.3.6          
    ## [31] tools_3.5.1               igraph_1.2.1              coda_0.19-1              
    ## [34] gtable_0.2.0              glue_1.2.0                reshape2_1.4.3           
    ## [37] cellranger_1.1.0          nlme_3.1-137              crosstalk_1.0.0          
    ## [40] psych_1.8.4               rvest_0.3.2               mime_0.5                 
    ## [43] miniUI_0.1.1.1            gtools_3.8.1              nleqslv_3.3.2            
    ## [46] MASS_7.3-50               zoo_1.8-2                 scales_0.5.0             
    ## [49] colourpicker_1.0          hms_0.4.2                 promises_1.0.1           
    ## [52] Brobdingnag_1.2-5         inline_0.3.15             shinystan_2.5.0          
    ## [55] yaml_2.1.19               gridExtra_2.3             loo_2.0.0                
    ## [58] stringi_1.2.3             dygraphs_1.1.1.5          rlang_0.2.1              
    ## [61] pkgconfig_2.0.1           matrixStats_0.54.0        HDInterval_0.2.0         
    ## [64] evaluate_0.10.1           lattice_0.20-35           bindr_0.1.1              
    ## [67] rstantools_1.5.0          htmlwidgets_1.2           labeling_0.3             
    ## [70] tidyselect_0.2.4          plyr_1.8.4                magrittr_1.5             
    ## [73] R6_2.2.2                  pillar_1.2.3              haven_1.1.2              
    ## [76] foreign_0.8-70            withr_2.1.2               xts_0.10-2               
    ## [79] abind_1.4-5               modelr_0.1.2              crayon_1.3.4             
    ## [82] arrayhelpers_1.0-20160527 utf8_1.1.4                rmarkdown_1.10           
    ## [85] grid_3.5.1                readxl_1.1.0              threejs_0.3.1            
    ## [88] digest_0.6.15             xtable_1.8-2              httpuv_1.4.4.2           
    ## [91] stats4_3.5.1              munsell_0.5.0             beeswarm_0.2.3           
    ## [94] vipor_0.4.5               shinyjs_1.0
