[
["index.html", "Statistical Rethinking with brms, ggplot2, and the tidyverse This is a love letter Why this? My assumptions about you How to use and understand this project You can do this, too", " Statistical Rethinking with brms, ggplot2, and the tidyverse A Solomon Kurz 2018-09-26 This is a love letter I love McElreath’s Statistical Rethinking text. It’s the entry-level textbook for applied researchers I spent a couple years looking for. McElreath’s freely-available lectures on the book are really great, too. However, I’ve come to prefer using Bürkner’s brms package when doing Bayeisn regression in R. It’s just spectacular. I also prefer plotting with Wickham’s ggplot2, and recently converted to using tidyverse-style syntax (which you might learn about here or here). So, this project is an attempt to reexpress the code in McElreath’s textbook. His models are re-fit in brms, plots are reproduced or reimagined with ggplot2, and the general data wrangling code now predominantly follows the tidyverse style. Why this? I’m not a statistician and I have no formal background in computer science. Though I benefited from a sweet of statistics courses in grad school, a large portion of my training has been outside of the classroom, working with messy real-world data, and searching online for help. One of the great resources I happened on was idre, the UCLA Institute for Digital Education, which offers an online portfolio of richly annotated textbook examples. Their online tutorials are among the earliest inspirations for this project. We need more resources like them. With that in mind, one of the strengths of McElreath’s text is its thorough integration with the rethinking package. The rethinking package is a part of the R ecosystem, which is great because R is free and open source. And McElreath has made the source code for rethinking publically available, too. Since he completed his text, many other packages have been developed to help users of the R ecosystem interface with Stan. Of those alternative packages, I think Bürkner’s brms package is the best for general-purpose Bayesian data analysis. It’s flexible, uses reasonably-approachable syntax, has sensible defaults, and offers a vast array of post-processing convenience functions. And brms has only gotten better over time. To my knowledge, there are no textbooks on the market that highlight the brms package, which seems like an evil worth correcting. In addition, McElreath’s data wrangling code is based in the base R style and he made most of his figures with base R plots. Though there are benefits to sticking close to base R functions (e.g., less dependencies leading to a lower likelihood that your code will break in the future), there are downsides. For beginners, base R functions can be difficult both to learn and to read. Happily, in recent years Hadley Wickham and others have been developing a group of packages collectively called the tidyverse. The tidyverse packages (e.g., dplyr, tidyr, purrr) were developed according to an underlying philosophy and they are designed to work together coherently and seamlessly. Though not all within the R community share this opinion, I am among those who think the tydyverse style of coding is generally easier to learn and sufficiently powerful that these packages can accommodate the bulk of your data needs. I also find tydyverse-style syntax easier to read. And of course, the widely-used ggplot2 package is part of the tidyverse, too. To be clear, students can get a great education in both Bayesian statistics and programming in R with McElreath’s text just the way it is. Just go slow, work through all the examples, and read the text closely. It’s a pedagogical boon. I could not have done better or even closely so. But what I can offer is a parallel introduction on how to fit the statistical models with the ever-improving and already-quite-impressive brms package. I can throw in examples of how to perform other operations according to the ethic of the tidyverse. And I can also offer glimpses of some of the other great packages in the R + Stan ecosystem (e.g., loo, bayesplot, and tidybayes). My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic, or a researcher of some sort. So I’m presuming you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons before diving into Statistical Rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s R Programming for Data Science. And the best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science, which I extensively appeal to throughout this project. That said, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to McElreath’s Statistical Rethinking text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, some of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS or Healy’s Data Visualization: A practical introduction. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide, and R Markdown: The Definitive Guide. R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit what packages a given function came from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::mode_hdi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. You can do this, too This project is powered by Yihui Xie’s bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s Happy Git and GitHub for the useR. The source code of the project is available here. "],
["the-golem-of-prague.html", "1 The Golem of Prague Reference Session info", " 1 The Golem of Prague Rabbi Loew and Golem by Mikoláš Aleš, 1899 I retrieved the picture from here. Ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. (p. 1) McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time consuming this chapter’s material by watching McElreath present it? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets ## [8] methods base ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ## [3] ggstance_0.3 tidyselect_0.2.4 ## [5] htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 ## [9] DT_0.4 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 ## [15] knitr_1.20 rstudioapi_0.7 ## [17] stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 ## [21] rstan_2.17.3 mnormt_1.5-5 ## [23] bridgesampling_0.4-0 rprojroot_1.3-2 ## [25] coda_0.19-1 xfun_0.3 ## [27] R6_2.2.2 markdown_0.8 ## [29] HDInterval_0.2.0 reshape_0.8.7 ## [31] assertthat_0.2.0 promises_1.0.1 ## [33] scales_0.5.0 beeswarm_0.2.3 ## [35] gtable_0.2.0 rlang_0.2.1 ## [37] extrafontdb_1.0 lazyeval_0.2.1 ## [39] broom_0.4.5 inline_0.3.15 ## [41] yaml_2.1.19 reshape2_1.4.3 ## [43] abind_1.4-5 modelr_0.1.2 ## [45] threejs_0.3.1 crosstalk_1.0.0 ## [47] backports_1.1.2 httpuv_1.4.4.2 ## [49] rsconnect_0.8.8 extrafont_0.17 ## [51] tools_3.5.1 bookdown_0.7 ## [53] psych_1.8.4 ggplot2_3.0.0 ## [55] RColorBrewer_1.1-2 ggridges_0.5.0 ## [57] Rcpp_0.12.18 plyr_1.8.4 ## [59] base64enc_0.1-3 progress_1.2.0 ## [61] purrr_0.2.5 prettyunits_1.0.2 ## [63] zoo_1.8-2 LaplacesDemon_16.1.1 ## [65] haven_1.1.2 magrittr_1.5 ## [67] colourpicker_1.0 mvtnorm_1.0-8 ## [69] tidybayes_1.0.1 matrixStats_0.54.0 ## [71] hms_0.4.2 shinyjs_1.0 ## [73] mime_0.5 evaluate_0.10.1 ## [75] arrayhelpers_1.0-20160527 xtable_1.8-2 ## [77] shinystan_2.5.0 readxl_1.1.0 ## [79] gridExtra_2.3 rstantools_1.5.0 ## [81] compiler_3.5.1 tibble_1.4.2 ## [83] maps_3.3.0 crayon_1.3.4 ## [85] StanHeaders_2.17.2 htmltools_0.3.6 ## [87] later_0.7.3 tidyr_0.8.1 ## [89] lubridate_1.7.4 MASS_7.3-50 ## [91] Matrix_1.2-14 readr_1.1.1 ## [93] cli_1.0.0 bindr_0.1.1 ## [95] igraph_1.2.1 forcats_0.3.0 ## [97] pkgconfig_2.0.1 foreign_0.8-70 ## [99] xml2_1.2.0 svUnit_0.7-12 ## [101] dygraphs_1.1.1.5 vipor_0.4.5 ## [103] rvest_0.3.2 stringr_1.3.1 ## [105] digest_0.6.15 rmarkdown_1.10 ## [107] cellranger_1.1.0 shiny_1.1.0 ## [109] gtools_3.8.1 nlme_3.1-137 ## [111] jsonlite_1.5 bindrcpp_0.2.2 ## [113] mapproj_1.2.6 pillar_1.2.3 ## [115] lattice_0.20-35 loo_2.0.0 ## [117] httr_1.3.1 glue_1.2.0 ## [119] xts_0.10-2 shinythemes_1.1.1 ## [121] pander_0.6.2 stringi_1.2.3 ## [123] dplyr_0.7.6 "],
["small-worlds-and-large-worlds.html", "2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Reference Session info", " 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken wrote a great paper by this name. 2.1.1 Counting possibilities. Throughout this project, we’ll use the tidyverse for data wrangling. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. Other than the pipe, the other big thing to be aware of is tibbles. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of data frames. So whenever we talk about data frames, we’re also talking about tibbles. For more on the topic, check out R4SD, Chapter 10. So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) head(d) ## # A tibble: 4 x 5 ## p_1 p_2 p_3 p_4 p_5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% gather() %&gt;% mutate(x = rep(1:4, times = 5), possibility = rep(1:5, each = 4)) %&gt;% ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data Wrangling Part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and dplyr::gather(). Here’s the basic structure of the possibilities per marble draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) ## # A tibble: 3 x 3 ## draw marbles possibilities ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 4 ## 2 2 4 16 ## 3 3 4 64 If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # ... with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3)/(4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # ... with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # ... with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # ... with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(limits = c(0, 4), breaks = NULL) + scale_y_continuous(limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() + labs(x = NULL, y = NULL) To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(limits = c(0, 4), breaks = NULL) + scale_y_continuous(limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() + labs(x = NULL, y = NULL) Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # If we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x){ rowSums(x == &quot;b&quot;) } n_white &lt;- function(x){ rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # ... with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index){ ifelse(index == &quot;a&quot;, position, ifelse(index == &quot;b&quot;, position + 4, position + 4 * 2) ) } If you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of * R Programming for Data Science* Anyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # ... with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # ... with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(#pie_index == &quot;a&quot; rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # pie_index == &quot;b&quot; rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # pie_index == &quot;c&quot;, rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(limits = c(0, 12), breaks = NULL) + scale_y_continuous(limits = c(0.75, 3.5), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() + labs(x = NULL, y = NULL) 2.1.2 Using prior information. We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) Here’s the table in the middle of page 25. ( t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) ) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 `ways to produce` `previous counts` `new count` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 1 3 3 ## 3 b b w w 2 8 16 ## 4 b b b w 3 9 27 ## 5 b b b b 4 0 0 We might update to reproduce the table a the top of page 26, like this. ( t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) ) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 `prior count` `factory count` `new count` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 3 3 9 ## 3 b b w w 16 2 32 ## 4 b b b w 27 1 27 ## 5 b b b b 0 0 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post Data Wrangling Part 1: Basic to Advanced Ways to Select Columns. 2.1.3 From counts to probability. The opening sentences are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as: plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of In other words: plausibility of \\(p\\) after \\(D_{\\text{new}}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_{\\text{new}} \\times\\) prior plausibility of \\(p\\) But since we have to standardize the results to get them into a probability metric, the full equation is: \\[\\text{plausibility of } p \\text{ after } D_{\\text{new}} = \\frac{\\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}}\\] You might make the table in the middle of page 27 like this. t %&gt;% select(p_1:p_4) %&gt;% mutate(p = seq(from = 0, to = 1, by = .25), `ways to produce data` = c(0, 3, 8, 9, 0)) %&gt;% mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 p `ways to produce data` plausibility ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 0.25 3 0.15 ## 3 b b w w 0.5 8 0.4 ## 4 b b b w 0.75 9 0.45 ## 5 b b b b 1 0 0 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == &quot;w&quot;), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(n_trials, p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% left_join(d, by = &quot;n_trials&quot;) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_success = lag(n_success, k = 1), lagged_n_trials = lag(n_trials, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), strip = str_c(&quot;n = &quot;, n_trials), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), ) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. For more on left_join(), check out R4DS Chapters 12.3 and 13.4. And to learn about tidyr::expand(), go here. 2.2.3 Evaluate. It’s worth repeating the Rethinking: Deflationary statistics box, here. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) 2.3 Components of the model a likelihood function: “the number of ways each conjecture could produce an observation” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data” a prior: “the initial plausibility of each conjectured cause of the data” 2.3.1 Likelihood. If you let the count of water be \\(w\\) and the number of tosses be \\(n\\), then the binomial likelihood may be expressed as: \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Given a probability of .5, the binomial likelihood of 6 out of 9 tosses coming out water is: dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Let’s do so over the parameter space. tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2 Parameters. McElreath started off his Rethinking: Datum or parameter? box with: It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34) For more in this topic, check out his lecture Understanding Bayesian Statistics without Frequentist Language. 2.3.3 Prior. So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35) To learn more about “regularizing or weakly informative priors,” check out the Prior Choice Recommendations wiki from the Stan team. 2.3.4 Posterior. If we continue to focus on the globe tossing example, the posterior probability a toss will be water may be expressed as: \\[\\text{Pr} (p|w) = \\frac{\\text{Pr} (w|p) \\text{Pr} (p)}{\\text{Pr} (w)}\\] More generically and in words, this is: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\] 2.4 Making the model go Here’s the data wrangling for Figure 2.6. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length/2), exp(-abs(probability - .5) / .25) / ( 2 * .25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% gather(key, value, -probability, -row) %&gt;% ungroup() %&gt;% mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them with gridExtra::grid.arrange(). p1 &lt;- d %&gt;% filter(key == &quot;prior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;prior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p2 &lt;- d %&gt;% filter(key == &quot;likelihood&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;likelihood&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p3 &lt;- d %&gt;% filter(key == &quot;posterior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) I’m not sure if it’s the same McElreath used in the text, but the formula I used for the tirangle-shaped prior is the Laplace distribution with a location of .5 and a dispersion of .25. Also, to learn all about dplyr::filter(), check out Baert’s Data Wrangling Part 3: Basic and more advanced ways to filter rows. 2.4.1 Grid approximation. We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. (d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the right panel of Figure 2.7. d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here it is with just 5 points, the left hand panel of Figure 2.7. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) 2.4.2 Quadratic approximation. Apply the quadratic approximation to the globe tossing data with rethinking::map(). library(rethinking) globe_qa &lt;- rethinking::map( alist( w ~ dbinom(9, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(w = 6)) # display summary of quadratic approximation precis(globe_qa) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.16 0.42 0.92 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- rethinking::map( alist( w ~ dbinom(9*2, p), p ~ dunif(0, 1) ), data = list(w = 6*2)) globe_qa_36 &lt;- rethinking::map( alist( w ~ dbinom(9*4, p), p ~ dunif(0, 1) ), data = list(w = 6*4)) precis(globe_qa_18) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.11 0.49 0.84 precis(globe_qa_36) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.08 0.54 0.79 Here’s the legwork for Figure 2.8. n_grid &lt;- 100 tibble(p_grid = seq(from = 0, to = 1, length.out = n_grid) %&gt;% rep(., times = 3), prior = 1, w = rep(c(6, 12, 24), each = n_grid), n = rep(c(9, 18, 36), each = n_grid), m = .67, s = rep(c(.16, .11, .08), each = n_grid)) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.3 Markov chain Monte Carlo. Since the main goal of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) Here we’ll re-fit the last model from above wherein \\(w = 24\\) and \\(n = 36\\). globe_qa_brms &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior = prior(normal(.5, 1), class = Intercept), control = list(adapt_delta = 0.9)) With brms, the posterior_summary() function is an analogue to rethinking::precis(). We will, however, need to use round() to reduce the output to a reasonable number of decimal places. posterior_summary(globe_qa_brms) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.66 0.08 0.50 0.79 ## lp__ -3.39 0.66 -5.24 -2.90 The b_Intercept row is the probability. Don’t worry about the second line, for now. We’ll explain the details of brms modeling in later chapters. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.5.0 Rcpp_0.12.18 rethinking_1.59 rstan_2.17.3 StanHeaders_2.17.2 ## [6] gridExtra_2.3 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 ## [11] readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 mnormt_1.5-5 ## [22] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 ## [25] xfun_0.3 R6_2.2.2 markdown_0.8 ## [28] HDInterval_0.2.0 reshape_0.8.7 assertthat_0.2.0 ## [31] promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [34] gtable_0.2.0 rlang_0.2.1 extrafontdb_1.0 ## [37] lazyeval_0.2.1 broom_0.4.5 inline_0.3.15 ## [40] yaml_2.1.19 reshape2_1.4.3 abind_1.4-5 ## [43] modelr_0.1.2 threejs_0.3.1 crosstalk_1.0.0 ## [46] backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 ## [52] psych_1.8.4 RColorBrewer_1.1-2 ggridges_0.5.0 ## [55] plyr_1.8.4 base64enc_0.1-3 progress_1.2.0 ## [58] prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 ## [64] mvtnorm_1.0-8 tidybayes_1.0.1 matrixStats_0.54.0 ## [67] hms_0.4.2 shinyjs_1.0 mime_0.5 ## [70] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [73] shinystan_2.5.0 readxl_1.1.0 rstantools_1.5.0 ## [76] compiler_3.5.1 maps_3.3.0 crayon_1.3.4 ## [79] htmltools_0.3.6 later_0.7.3 lubridate_1.7.4 ## [82] MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 ## [85] bindr_0.1.1 igraph_1.2.1 pkgconfig_2.0.1 ## [88] foreign_0.8-70 xml2_1.2.0 svUnit_0.7-12 ## [91] dygraphs_1.1.1.5 vipor_0.4.5 rvest_0.3.2 ## [94] digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 ## [97] shiny_1.1.0 gtools_3.8.1 nlme_3.1-137 ## [100] jsonlite_1.5 bindrcpp_0.2.2 mapproj_1.2.6 ## [103] pillar_1.2.3 lattice_0.20-35 loo_2.0.0 ## [106] httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["sampling-the-imaginary.html", "3 Sampling the Imaginary 3.1 Sampling from a grid-like approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice in brms Reference Session info", " 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time useing the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 3.1 Sampling from a grid-like approximate posterior Here we use grid approximation, again, to generate samples. # how many grid points would you like? n &lt;- 1000 n_success &lt;- 6 n_tirals &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_tirals, prob = p_grid)) %&gt;% mutate(posterior = likelihood * prior) %&gt;% mutate(posterior = posterior / sum(posterior)) ) ## # A tibble: 1,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.00100 1 8.43e-17 8.43e-19 ## 3 0.00200 1 5.38e-15 5.38e-17 ## 4 0.00300 1 6.11e-14 6.11e-16 ## 5 0.00400 1 3.42e-13 3.42e-15 ## 6 0.00501 1 1.30e-12 1.30e-14 ## 7 0.00601 1 3.87e-12 3.88e-14 ## 8 0.00701 1 9.73e-12 9.74e-14 ## 9 0.00801 1 2.16e-11 2.16e-13 ## 10 0.00901 1 4.37e-11 4.38e-13 ## # ... with 990 more rows Now we’re ready to sample. samples &lt;- sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T) glimpse(samples) ## num [1:10000] 0.454 0.84 0.424 0.465 0.737 ... We’ll plot the zigzagging left panel of Figure 3.1 with geom_line(). But before we do, we’ll need to add a variable numbering the samples. And even before that, perhaps you noticed that glimpse(samples) told us samples is a numeric vector. Since ggplot2 requires we use data frames, of which tibbles are special case, we’ll use as_tibble() to convert samples to a tibble. samples &lt;- samples %&gt;% as_tibble() %&gt;% mutate(sample_number = 1:n()) head(samples) ## # A tibble: 6 x 2 ## value sample_number ## &lt;dbl&gt; &lt;int&gt; ## 1 0.454 1 ## 2 0.840 2 ## 3 0.424 3 ## 4 0.465 4 ## 5 0.737 5 ## 6 0.644 6 But notice what happened. When we simply converted the samples vector with as_tibble(), that vector was renamed quite generically as value. One way around this is with rename() (i.e., rename(samples = value)). But this is already becoming cumbersome. Here’s a more compact way. samples &lt;- tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T)) %&gt;% mutate(sample_number = 1:n()) glimpse(samples) ## Observations: 10,000 ## Variables: 2 ## $ samples &lt;dbl&gt; 0.3803804, 0.6006006, 0.5805806, 0.6116116, 0.3663664, 0.4574575, 0.58858... ## $ sample_number &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21... This time we just made samples a vector within a tibble from the start. Anyway, here’s the right panel for Figure 3.1. samples %&gt;% ggplot(aes(x = sample_number, y = samples)) + geom_line(size = 1/10) + labs(x = &quot;sample number&quot;, y = &quot;proportion of water (p)&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;) + coord_cartesian(xlim = 0:1) + xlab(&quot;proportion of water (p)&quot;) 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly now it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you’d first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.172 To learn more about dplyr::summarise() and related functions, check out Baert’s Data Wrangling Part 4: Summarizing and slicing your data and Chapter 5.6 of R4DS. If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(samples &lt; .5) %&gt;% summarise(sum = n() / 1e4) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.171 You can use &amp; within filter(), too. samples %&gt;% filter(samples &gt; .5 &amp; samples &lt; .75) %&gt;% summarise(sum = n() / 1e4) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.607 3.2.2 Intervals of defined mass. We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), and a some careful filtering. # upper left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + # note this next line is the only difference in code from the last plot geom_ribbon(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ll come back for the lower two panels in a bit. Since we’ve saved our samples vector within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$samples, prob = .8)) ## 80% ## 0.7597598 That value will come in handy for the lower left panel of Figure 3.2, so we saved it. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(): samples %&gt;% select(samples) %&gt;% pull() %&gt;% quantile(prob = .8) ## 80% ## 0.7597598 And we might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(samples, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.760 Here’s the summarise() approach with two probabilities: samples %&gt;% summarise(`10th percentile` = quantile(samples, p = .1), `90th percentile` = quantile(samples, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.447 0.814 The tydiverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$samples, prob = c(.1, .9))) ## 10% 90% ## 0.4473473 0.8138138 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_tirals &lt;- 3 # update d d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_tirals, prob = p_grid)) %&gt;% mutate(posterior = likelihood * prior) %&gt;% mutate(posterior = posterior / sum(posterior)) # here&#39;s our new samples tibble ( samples &lt;- tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T)) ) ## # A tibble: 10,000 x 1 ## samples ## &lt;dbl&gt; ## 1 0.522 ## 2 0.815 ## 3 0.833 ## 4 0.870 ## 5 0.981 ## 6 0.982 ## 7 0.990 ## 8 0.970 ## 9 0.633 ## 10 0.985 ## # ... with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$samples, prob = c(.25, .75)) ## 25% 75% ## 0.7097097 0.9309309 rethinking::PI(samples$samples, prob = .5) ## 25% 75% ## 0.7097097 0.9309309 Now’s a good time to introduce Matthew Kay’s tidybayes package, which offers an array of convenience functions for Bayesian models of the type we’ll be working with in this project. library(tidybayes) median_qi(samples$samples, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.8428428 0.7097097 0.9309309 0.5 median qi The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$samples, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.8428428 0.7097097 0.9309309 0.50 median qi ## 2 0.8428428 0.5655656 0.9749750 0.80 median qi ## 3 0.8428428 0.2692593 0.9989990 0.99 median qi The .width column in the output indexed which line presented which interval. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$samples, prob = .5) ## |0.5 0.5| ## 0.8428428 1.0000000 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$samples, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9549621 0.8428428 1 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If you just want to extract a quick value out of, say, mode_hdi(), you could do so with brackets. E.g., here we pull the lower bound of the 50% HPDI. median_qi(samples$samples, .width = .5)[, &quot;ymin&quot;] ## [1] 0.7097097 Now we have that skill, we can use it to make Figure 3.3. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; median_qi(samples$samples, .width = .5)[, &quot;ymin&quot;] &amp; p_grid &lt; median_qi(samples$samples, .width = .5)[, &quot;ymax&quot;]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; median_hdi(samples$samples, .width = .5)[, &quot;ymin&quot;] &amp; p_grid &lt; median_hdi(samples$samples, .width = .5)[, &quot;ymax&quot;]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0.00400 ## 2 0.999 1 0.997 0.00398 ## 3 0.998 1 0.994 0.00397 ## 4 0.997 1 0.991 0.00396 ## 5 0.996 1 0.988 0.00395 ## 6 0.995 1 0.985 0.00394 ## 7 0.994 1 0.982 0.00392 ## 8 0.993 1 0.979 0.00391 ## 9 0.992 1 0.976 0.00390 ## 10 0.991 1 0.973 0.00389 ## # ... with 990 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0.00400 Here’s the rethinking::chainmode() method. rethinking::chainmode(samples$samples, adj = 0.01) ## [1] 0.9880178 And you can also do whis with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(samples) ## # A tibble: 1 x 6 ## samples .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.955 0.472 1 0.95 mode hdi samples %&gt;% mode_qi(samples) ## # A tibble: 1 x 6 ## samples .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.955 0.403 0.994 0.95 mode qi But medians and means are typical, too. samples %&gt;% summarise(mean = mean(samples), median = median(samples)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.802 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- samples %&gt;% mean_qi(samples) %&gt;% bind_rows( samples %&gt;% median_qi(samples), samples %&gt;% mode_qi(samples) ) %&gt;% select(samples, .point) %&gt;% # these last two columns will help us annotate mutate(x = samples + c(-.03, .03, -.03), y = c(.0005, .00125, .002)) ) ## # A tibble: 3 x 4 ## samples .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.802 mean 0.772 0.0005 ## 2 0.843 median 0.873 0.00125 ## 3 0.955 mode 0.925 0.002 The plot: d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$samples) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be: d %&gt;% mutate(loss = posterior * abs(0.5 - p_grid)) %&gt;% summarise(`expected loss` = sum(loss)) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 0.313 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package, which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() ) ## # A tibble: 1,000 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.800 ## 2 0.00100 0.799 ## 3 0.00200 0.798 ## 4 0.00300 0.797 ## 5 0.00400 0.796 ## 6 0.00501 0.795 ## 7 0.00601 0.794 ## 8 0.00701 0.793 ## 9 0.00801 0.792 ## 10 0.00901 0.791 ## # ... with 990 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.8408408. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(samples)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # ammend our loss function make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8008008. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(samples)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.802 3.3 Sampling to simulate prediction McElreath’s four good reasons for posterior simulation were: Model checking Software validation Research design Forecasting 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_{observed} = 0 \\text{ through }2\\), the denisties are: tibble(n = 2, probability = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = probability)) ## # A tibble: 3 x 4 ## n probability w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(331) rbinom(1, size = 2, prob = .7) ## [1] 0 Here are ten reproducible draws. set.seed(331) rbinom(10, size = 2, prob = .7) ## [1] 0 0 1 1 2 2 1 2 1 2 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(331) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% group_by(draws) %&gt;% count() %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## # Groups: draws [3] ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9059 0.0906 ## 2 1 41949 0.419 ## 3 2 48992 0.490 Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(331) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability){ set.seed(331) rbinom(n_draws, size = n, prob = probability) } d &lt;- tibble(n = c(3, 6, 9)) %&gt;% expand(n, probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest() head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 3 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 1 ## 5 n = 3 p = 0.3 0 ## 6 n = 3 p = 0.3 0 The results look as follows: d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). You’re welcome. 3.3.2.2 Is the model adequate? Let’s update our simulate_binom() golem to keep the number of trials constant at 9. n_draws &lt;- 1e4 n_trials &lt;- 9 probability &lt;- .6 set.seed(331) tibble(draws = rbinom(n_draws, size = n_trials, prob = probability)) %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;simulated water count&quot;, breaks = seq(from = 0, to = 9, by = 3)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) But notice that in this simulation we held \\(p\\) at a constant .6. There was no posterior uncertainty packed into the model. Let’s refresh ourselves on what the model was: # how many grid points would you like? n &lt;- 1000 n_success &lt;- 6 n_tirals &lt;- 9 d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_tirals, prob = p_grid)) %&gt;% mutate(posterior = likelihood * prior) %&gt;% mutate(posterior = posterior / sum(posterior)) # samples! set.seed(33.22) samples &lt;- tibble(samples = sample(d$p_grid, prob = d$posterior, size = 1e4, replace = T)) head(samples) ## # A tibble: 6 x 1 ## samples ## &lt;dbl&gt; ## 1 0.445 ## 2 0.394 ## 3 0.458 ## 4 0.630 ## 5 0.585 ## 6 0.518 Let’s use it to simulate and make the middle panels of Figure 3.6. # the simulation set.seed(3322) samples &lt;- samples %&gt;% mutate(w = rbinom(n_draws, size = n_trials, prob = samples), key = str_c(&quot;p = &quot;, round(samples, digits = 1))) # the plot samples %&gt;% filter(key != &quot;p = 1&quot;) %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ key, ncol = 9, scales = &quot;free_y&quot;) The top panel of Figure 3.6 is just the density of samples. samples %&gt;% ggplot(aes(x = samples)) + geom_density(fill = &quot;grey50&quot;, color = &quot;transparent&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Posterior probability&quot;) + theme(panel.grid = element_blank()) And the bottom panel is the histogram of w without faceting by levels of samples. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longst sequence of the sampe values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 4 3 6 4 3 3 4 5 4 6 Notice this collapses (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 1 0 1 1 0 1 1 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with unaggregated samples. simulate_disaggregated_binom &lt;- function(samples){ set.seed(37) rbinom(9, size = 1, prob = samples) } ( disaggregated_samples &lt;- samples %&gt;% select(samples) %&gt;% mutate(iteration = 1:n(), draws = map(samples, simulate_disaggregated_binom)) %&gt;% unnest() ) ## # A tibble: 90,000 x 3 ## samples iteration draws ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.445 1 0 ## 2 0.445 1 0 ## 3 0.445 1 1 ## 4 0.445 1 0 ## 5 0.445 1 1 ## 6 0.445 1 1 ## 7 0.445 1 0 ## 8 0.445 1 0 ## 9 0.445 1 0 ## 10 0.394 2 0 ## # ... with 89,990 more rows Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. disaggregated_samples %&gt;% group_by(iteration) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”. rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. disaggregated_samples %&gt;% group_by(iteration) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 6), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 3.4 Summary Let’s practice in brms Open brms. library(brms) In brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of the project for Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 1, prior = prior(beta(1, 1), class = Intercept), control = list(adapt_delta = .99)) We’ll learn more about the beta distribution in Chapter 11. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”. posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.63 0.14 0.35 0.87 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with fitted() within the brms framework. But we will have to specify scale = &quot;linear&quot; in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. fitted_samples &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() glimpse(fitted_samples) ## Observations: 4,000 ## Variables: 1 ## $ V1 &lt;dbl&gt; 0.5899375, 0.5898957, 0.4581620, 0.4788888, 0.3914035, 0.5739587, 0.5184822, 0.66335... By default, we have a generically-named vector V1 of 4000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. fitted_samples %&gt;% ggplot(aes(x = V1)) + geom_density(fill = &quot;grey50&quot;, color = &quot;transparent&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Posterior probability&quot;) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of “w” counts. # the simulation set.seed(33.22) fitted_samples &lt;- fitted_samples %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = V1)) %&gt;% mutate(key = str_c(&quot;p = &quot;, round(V1, digits = 1))) # the plot fitted_samples %&gt;% filter(key != &quot;p = 1&quot;) %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ key, ncol = 9, scales = &quot;free_y&quot;) And also like with samples, we can omit the facet_wrap() function to make histogram in the bottom panel of Figure 3.6. fitted_samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0s and 1s, too. And we could even use those disaggregated 0s and 1s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.5.0 Rcpp_0.12.18 tidybayes_1.0.1 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 ## [7] purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 rstan_2.17.3 ## [22] mnormt_1.5-5 bridgesampling_0.4-0 rprojroot_1.3-2 ## [25] coda_0.19-1 xfun_0.3 R6_2.2.2 ## [28] markdown_0.8 HDInterval_0.2.0 reshape_0.8.7 ## [31] assertthat_0.2.0 promises_1.0.1 scales_0.5.0 ## [34] beeswarm_0.2.3 gtable_0.2.0 rethinking_1.59 ## [37] rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [40] broom_0.4.5 inline_0.3.15 yaml_2.1.19 ## [43] reshape2_1.4.3 abind_1.4-5 modelr_0.1.2 ## [46] threejs_0.3.1 crosstalk_1.0.0 backports_1.1.2 ## [49] httpuv_1.4.4.2 rsconnect_0.8.8 extrafont_0.17 ## [52] tools_3.5.1 bookdown_0.7 psych_1.8.4 ## [55] RColorBrewer_1.1-2 ggridges_0.5.0 plyr_1.8.4 ## [58] base64enc_0.1-3 progress_1.2.0 prettyunits_1.0.2 ## [61] zoo_1.8-2 LaplacesDemon_16.1.1 haven_1.1.2 ## [64] magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 ## [67] matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 ## [70] mime_0.5 evaluate_0.10.1 arrayhelpers_1.0-20160527 ## [73] xtable_1.8-2 shinystan_2.5.0 readxl_1.1.0 ## [76] gridExtra_2.3 rstantools_1.5.0 compiler_3.5.1 ## [79] maps_3.3.0 crayon_1.3.4 StanHeaders_2.17.2 ## [82] htmltools_0.3.6 later_0.7.3 lubridate_1.7.4 ## [85] MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 ## [88] bindr_0.1.1 igraph_1.2.1 pkgconfig_2.0.1 ## [91] foreign_0.8-70 xml2_1.2.0 svUnit_0.7-12 ## [94] dygraphs_1.1.1.5 vipor_0.4.5 rvest_0.3.2 ## [97] digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 ## [100] shiny_1.1.0 gtools_3.8.1 nlme_3.1-137 ## [103] jsonlite_1.5 bindrcpp_0.2.2 mapproj_1.2.6 ## [106] viridisLite_0.3.0 pillar_1.2.3 lattice_0.20-35 ## [109] loo_2.0.0 httr_1.3.1 glue_1.2.0 ## [112] xts_0.10-2 shinythemes_1.1.1 pander_0.6.2 ## [115] stringi_1.2.3 "],
["linear-models.html", "4 Linear Models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 A Gaussian model of height 4.4 Adding a predictor 4.5 Polynomial regression Reference Session info", " 4 Linear Models Linear regression is the geocentric model of applied statistics. By “linear regression”, we will mean a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Like geocentrism, linear regression can usefully describe a very large variety of natural phenomena. Like geocentrism, linear is a descriptive model that corresponds to many different process models. If we read its structure too literally, we’re likely to make mistakes. But used wisely, these little linear golems continue to be useful. (p. 71) 4.1 Why normal distributions are normal After laying out his soccer field coin toss shuffle premise, McElreath wrote: It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72) 4.1.1 Normal by addition. Here’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2. library(tidyverse) # We set the seed to make the results of `runif()` reproducible. set.seed(1000) pos &lt;- replicate(100, runif(16, -1, 1)) %&gt;% # Here&#39;s the simulation as_tibble() %&gt;% # For data manipulation, we&#39;ll make this a tibble rbind(0, .) %&gt;% # Here we add a row of zeros above the simulation results mutate(step = 0:16) %&gt;% # This adds a step index gather(key, value, -step) %&gt;% # Here we convert the data to the long format mutate(person = rep(1:100, each = 17)) %&gt;% # This adds a person id index # The next two lines allow us to make culmulative sums within each person group_by(person) %&gt;% mutate(position = cumsum(value)) %&gt;% ungroup() # Ungrouping allows for further data manipulation We might glimpse() at the data. glimpse(pos) ## Observations: 1,700 ## Variables: 5 ## $ step &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6,... ## $ key &lt;chr&gt; &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, ... ## $ value &lt;dbl&gt; 0.00000000, -0.34424258, 0.51769297, -0.77212721, 0.38151030, 0.03280481, -0.8... ## $ person &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... ## $ position &lt;dbl&gt; 0.00000000, -0.34424258, 0.17345039, -0.59867682, -0.21716652, -0.18436171, -1... And here’s the actual plot code. ggplot(data = pos, aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&quot;step number&quot;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &quot;none&quot;) Here’s the code for the bottom three plots of Figure 4.2. # Figure 4.2.a. pos %&gt;% filter(step == 4) %&gt;% ggplot(aes(x = position)) + geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;4 steps&quot;) # Figure 4.2.b. pos %&gt;% filter(step == 8) %&gt;% ggplot(aes(x = position)) + geom_density(color = &quot;dodgerblue2&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;8 steps&quot;) # An intermediary step to get an SD value pos %&gt;% filter(step == 16) %&gt;% summarise(sd = sd(position)) ## # A tibble: 1 x 1 ## sd ## &lt;dbl&gt; ## 1 2.38 # Figure 4.2.c. pos %&gt;% filter(step == 16) %&gt;% ggplot(aes(x = position)) + geom_line(data = tibble(position = seq(from = -7, to = 7, by = .1)), aes(x = position, y = dnorm(position, 0, 2.381768)), linetype = 2) + # 2.381768 came from the previous code block geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + coord_cartesian(xlim = -6:6) + labs(title = &quot;16 steps&quot;, y = &quot;density&quot;) While we were at it, we explored a few ways to express densities. The main action was with the geom_line() and geom_density() functions. 4.1.2 Normal by multiplication. Here’s McElreath’s simple random growth rate. set.seed(1) prod(1 + runif(12, 0, 0.1)) ## [1] 1.769489 In the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values and then computed their product. Consider a more explicit variant of the code. set.seed(1) tibble(a = 1, b = runif(12, 0, 0.1)) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c)) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 1.77 Same result. Let’s do this many times with replicate() and plot the results. set.seed(.1) growth &lt;- replicate(10000, prod(1 + runif(12, 0, 0.1))) %&gt;% as_tibble() ggplot(data = growth, aes(x = value)) + geom_density() “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. # sample big set.seed(412) big &lt;- replicate(10000, prod(1 + runif(12, 0, 0.5))) # sample small set.seed(412) small &lt;- replicate(10000, prod(1 + runif(12, 0, 0.01))) # combine them tibble(samples = c(big, small), distribution = rep(c(&quot;big&quot;, &quot;small&quot;), each = 10000)) %&gt;% # plot ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;, color = &quot;transparent&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) Yep, the small samples were more Gaussian. 4.1.3 Normal by log-multiplication. Instead of saving our tibble, we’ll just feed it directly into our plot. set.seed(12) replicate(10000, log(prod(1 + runif(12, 0, 0.5)))) %&gt;% as_tibble() %&gt;% ggplot(aes(x = value)) + geom_density(color = &quot;transparent&quot;, fill = &quot;gray33&quot;) What we did was really compact. Walking it out a bit, here’s what we all did within the second argument within replicate() (i.e., everything within log()). tibble(a = runif(12, 0, 0.5), b = 1) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c) %&gt;% log()) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 2.37 And based on the first argument within replicate(), we did that 10,000 times, after which we converted the results to a tibble and then fed those data into ggplot2 4.1.4 Using Gaussian distributions. I really like these justifications. 4.1.4.1 Ontological justification. The Gaussian is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process… (p. 75) But they can still be useful. 4.1.4.2 Epistemological justification. Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75–76) In the Overthinking: Gaussian distribution box that follows, McElreath gave the formula. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is \\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp} \\Bigg (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\Bigg)\\] 4.2 A language for describing models Our mathy ways of summarizing models will be something like \\[ \\begin{eqnarray} \\text{outcome}_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\beta \\times \\text{predictor}_i \\\\ \\beta &amp; \\sim &amp; \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy}(0, 1) \\end{eqnarray} \\] And as McElreath then followed up with, “If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome! 4.2.1 Re-describing the globe tossing model. For the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was \\[ \\begin{eqnarray} w &amp; \\sim &amp; \\text{Binomial}(n, p) \\\\ p &amp; \\sim &amp; \\text{Uniform}(0, 1) \\end{eqnarray} \\] We can break McElreath’s R code 4.6 down a little bit with a tibble like so. # how many `p_grid` points would you like? n_points &lt;- 100 d &lt;- tibble(w = 6, n = 9, p_grid = seq(from = 0, to = 1, length.out = n_points)) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(w, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior) %&gt;% # this last bit converts the posterior to the probability scale mutate(posterior = posterior / sum(posterior)) head(d) ## # A tibble: 6 x 6 ## w n p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 9 0 1 0. 0. ## 2 6 9 0.0101 1 8.65e-11 8.74e-12 ## 3 6 9 0.0202 1 5.37e- 9 5.43e-10 ## 4 6 9 0.0303 1 5.93e- 8 5.99e- 9 ## 5 6 9 0.0404 1 3.23e- 7 3.26e- 8 ## 6 6 9 0.0505 1 1.19e- 6 1.21e- 7 In case you were curious, here’s what they look like: d %&gt;% select(-w, -n) %&gt;% gather(key, value, -p_grid) %&gt;% # this line allows us to dictate the order the panels will appear in mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) + geom_ribbon() + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, scales = &quot;free&quot;) The posterior is a combination of the prior and the likelihood. And when the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. 4.3 A Gaussian model of height There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. (p. 79) 4.3.1 The data. Let’s get the data from McElreath’s rethinking package. library(rethinking) data(Howell1) d &lt;- Howell1 Here we open our main statistical package, Bürkner’s brms. But before we do, we’ll need to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, we will always make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Go ahead and investigate the data with str(), the tidyverse analogue for which is glimpse(). d %&gt;% str() ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... Here are the height values. d %&gt;% select(height) %&gt;% head() ## height ## 1 151.765 ## 2 139.700 ## 3 136.525 ## 4 156.845 ## 5 145.415 ## 6 163.830 We can use filter() to make an adults-only data frame. d2 &lt;- d %&gt;% filter(age &gt;= 18) 4.3.1.1 Overthinking: Data frames. This probably reflects my training history, but the structure of a data frame seems natural and inherently appealing, to me. So I can’t relate to the “annoying” comment. But if you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan. 4.3.1.2 Overthinking: Index magic. For more on indexing, check out chapter 9 of Roger Peng’s R Programming for Data Science or even the Subsetting subsection from R4DS. 4.3.2 The model. The likelihood for our model is \\[h_i \\sim \\text{Normal}(\\mu, \\sigma)\\] Our \\(\\mu\\) prior will be \\[\\mu \\sim \\text{Normal}(178, 20)\\] And our prior for \\(\\sigma\\) will be \\[\\sigma \\sim \\text{Uniform}(0, 50)\\] Here’s the shape of the prior for \\(\\mu\\) in \\(N(178, 20)\\). ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + ylab(&quot;density&quot;) And here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the y axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous(). tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) We can simulate from both priors at once to get a prior probability distribution of heights. n &lt;- 1e4 set.seed(432) tibble(sample_mu = rnorm(n, mean = 178, sd = 20), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% ggplot(aes(x = x)) + geom_density(fill = &quot;black&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Prior predictive distribution for &quot;, italic(h[i]))), x = NULL) + theme(panel.grid = element_blank()) As McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p. 83). 4.3.3 Grid approximation of the posterior distribution. As McElreath explained, you’ll never use this for practical data analysis. But I found this useful for understanding what exactly we’re doing with Bayesian estimation. So let’s play along. n &lt;- 200 d_grid &lt;- tibble(mu = seq(from = 140, to = 160, length.out = n), sigma = seq(from = 4, to = 9, length.out = n)) %&gt;% # we&#39;ll accomplish with `tidyr::expand()` what McElreath did with base R `expand.grid()` expand(mu, sigma) head(d_grid) ## # A tibble: 6 x 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 ## 2 140 4.03 ## 3 140 4.05 ## 4 140 4.08 ## 5 140 4.10 ## 6 140 4.13 d_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computateions by making a custom function which we’ll plug into purrr::map2(). grid_function &lt;- function(mu, sigma){ dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’re ready to complete the tibble. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest() %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) head(d_grid) ## # A tibble: 6 x 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 -3813. -5.72 -3.91 -3822. 0 ## 2 140 4.03 -3778. -5.72 -3.91 -3787. 0 ## 3 140 4.05 -3743. -5.72 -3.91 -3753. 0 ## 4 140 4.08 -3709. -5.72 -3.91 -3719. 0 ## 5 140 4.10 -3676. -5.72 -3.91 -3686. 0 ## 6 140 4.13 -3644. -5.72 -3.91 -3653. 0 In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + coord_cartesian(xlim = range(d_grid$mu), ylim = range(d_grid$sigma)) + theme(panel.grid = element_blank()) We’ll make our heat map with geom_raster(aes(fill = probability)). d_grid %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_raster(aes(fill = probability)) + scale_fill_viridis_c() + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior. We can use dplyr::sample_n() to sample rows, with replacement, from d_grid. set.seed(434) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) We can use gather() and then facet_warp() to plot the densities for both mu and sigma at once. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) We’ll use the tidybayes package to compute their posterior modes and 95% HDIs. library(tidybayes) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.95 mode hdi ## 2 sigma 7.72 7.24 8.37 0.95 mode hdi Let’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5). 4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior. Since we’ll be fitting models with brms almost exclusively from here on out, this section is largely mute. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3. set.seed(4341) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 161.925 147.955 156.845 161.290 163.830 161.290 153.035 159.385 139.700 154.305 146.050 154.940 ## [13] 149.860 152.400 153.035 160.655 161.290 148.590 144.780 154.305 For our first step using d3, we’ll redefine d_grid. n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- tibble(mu = seq(from = 150, to = 170, length.out = n), sigma = seq(from = 4, to = 20, length.out = n)) %&gt;% expand(mu, sigma) Second, we’ll redefine our custom grid_function() function to operate over the height values of d3. grid_function &lt;- function(mu, sigma){ dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’ll use the amended grid_function() to make the posterior. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest() %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) Next we’ll sample_n() and plot. set.seed(4341) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) Behold the updated densities. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales= &quot;free&quot;) Not so Gaussian with that small \\(n\\). 4.3.5 Fitting the model with map() brm(). We won’t actually use rethinking::map()–which you should not conflate with purrr::map()–, but will jumpt straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Here’s the first model. b4.1 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4) ## Warning: There were 298 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See ## http://mc-stan.org/misc/warnings.html#bfmi-low ## Warning: Examine the pairs() plot to diagnose sampling problems McElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in Chapter 8, HMC tends to work better when you default to a half Cauchy for \\(\\sigma\\). Here’s how to do so. b4.1_half_cauchy &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) This leads to an important point. After running an HMC model, it’s a good idea to inspect the chains. As we’ll see, McElreath coverd this in Chapter 8. Here’s a typical way to do so in brms. plot(b4.1_half_cauchy) If you want detailed diagnostics for the HMC chains, call launch_shinystan(b4.1). That’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object. print(b4.1_half_cauchy) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 154.61 0.41 153.81 155.39 3655 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 7.74 0.29 7.20 8.34 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summary() function works in a similar way. You can also get a Stan-like summary with this: b4.1_half_cauchy$fit ## Inference for Stan model: e761f77e7a9fffcea18f77c43680efd1. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 154.61 0.01 0.41 153.81 154.33 154.62 154.88 155.39 3655 1 ## sigma 7.74 0.00 0.29 7.20 7.54 7.73 7.95 8.34 4000 1 ## lp__ -1227.50 0.02 0.97 -1230.11 -1227.87 -1227.23 -1226.82 -1226.54 1957 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Sep 20 16:18:06 2018. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Whereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print(). summary(b4.1_half_cauchy, prob = .89) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## Intercept 154.61 0.41 153.95 155.26 3655 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## sigma 7.74 0.29 7.29 8.22 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Anyways, here’s the shockingly-narrow-\\(\\mu\\)-prior model. b4.2 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, .1), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 3000, warmup = 2000, chains = 4, cores = 4) plot(b4.2) I had to increase the warmup due to convergence issues. After doing so, everything looks to be on the up and up. The chains look great. Here’s the model summary(). summary(b4.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 3000; warmup = 2000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 177.86 0.10 177.67 178.06 3431 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 24.59 0.93 22.79 26.46 2380 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3.6 Sampling from a map() brm() fit. brms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. For example: vcov(b4.1_half_cauchy) ## Intercept ## Intercept 0.1640903 This only returns the first element in the matrix it did for rethinking. That is, it appears brms::vcov() only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters (i.e., those used to model \\(\\mu\\)). However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. post &lt;- posterior_samples(b4.1_half_cauchy) cov(post[, 1:2]) ## b_Intercept sigma ## b_Intercept 0.164090286 -0.003473295 ## sigma -0.003473295 0.086248503 That was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix. post[, 1:2] %&gt;% cov() %&gt;% diag() ## b_Intercept sigma ## 0.1640903 0.0862485 post %&gt;% select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.00000000 -0.02919607 ## sigma -0.02919607 1.00000000 With our post &lt;- posterior_samples(b4.1_half_cauchy) code from a few lines above, we’ve already done the brms version of what McElreath did with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package, in brms we just extracted the iterations of the HMC chains and put them in a data frame. head(post) ## b_Intercept sigma lp__ ## 1 154.7596 7.782081 -1226.606 ## 2 154.5020 7.294843 -1227.733 ## 3 154.1256 7.909758 -1227.374 ## 4 154.4817 7.633832 -1226.610 ## 5 154.8147 7.616862 -1226.713 ## 6 154.2461 7.837140 -1226.969 Notice how our data frame, post, includes a third vector, lp__. That’s the log posterior. See the brms manual for details. The log posterior will largely be outside of our focus in this project. The summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. E.g., summary(post[, 1:2]) ## b_Intercept sigma ## Min. :153.1 Min. :6.897 ## 1st Qu.:154.3 1st Qu.:7.540 ## Median :154.6 Median :7.728 ## Mean :154.6 Mean :7.743 ## 3rd Qu.:154.9 3rd Qu.:7.947 ## Max. :156.1 Max. :8.984 Here’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here. t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75))) ## 50% 2.5% 75% ## b_Intercept 154.617067 153.807141 154.882931 ## sigma 7.727842 7.204762 7.946592 The base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% summarise(mean = mean(value), SD = sd(value), `2.5_percentile` = quantile(value, probs = .025), `97.5_percentile` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## # A tibble: 2 x 5 ## parameter mean SD `2.5_percentile` `97.5_percentile` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.41 154. 155. ## 2 sigma 7.74 0.290 7.2 8.34 You can always get pretty similar information by just putting the brm() fit object into posterior_summary(). posterior_summary(b4.1_half_cauchy) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.608587 0.4050806 153.807141 155.386803 ## sigma 7.743288 0.2936810 7.204762 8.339784 ## lp__ -1227.500987 0.9669972 -1230.106052 -1226.542634 And if you’re willing to drop the posterior \\(SD\\)s, you can use tidybayes::mean_qi(), too. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% mean_qi(value) ## # A tibble: 2 x 7 ## parameter value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 155. 154. 155. 0.95 mean qi ## 2 sigma 7.74 7.20 8.34 0.95 mean qi 4.3.6.1 Overthinking: Under the hood with multivariate sampling. Again, brms::posterior_samples() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the iterations from the HMC chains. McElreath coverd all of this in Chapter 8. You might also look at the brms reference manual or GitHub page for details. 4.3.6.2 Overthinking: Getting \\(\\sigma\\) right. There’s no need to fret about this in brms. With HMC, we are not constraining the posteriors to the multivariate normal distribution. Here’s our posterior density for \\(\\sigma\\). ggplot(data = post, aes(x = sigma)) + geom_density(size = 1/10, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma)) + theme(panel.grid = element_blank()) See? HMC handled the mild skew just fine. But sometimes you want to actually model \\(\\sigma\\), such as in the case where your variances are systematically heterogeneous. Bürkner calls these kinds of models distributional models, which you can learn more about in his vignette Estimating Distributional Models with brms. As he explained in the vignette, you actually model \\(\\text{log}(\\sigma)\\) in those instances. If you’re curious, we’ll practice with a model like this in Chapter 9. 4.4 Adding a predictor Here’s our scatter plot of weight and height. ggplot(data = d2, aes(x = weight, y = height)) + geom_point(shape = 1, size = 2) + theme_bw() + theme(panel.grid = element_blank()) 4.4.1 The linear model strategy In our new univariable model \\[ \\begin{eqnarray} h_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i \\\\ \\alpha &amp; \\sim &amp; \\text{Normal}(178, 100) \\\\ \\beta &amp; \\sim &amp; \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim &amp; \\text{Uniform}(0, 50) \\end{eqnarray} \\] 4.4.2 Fitting the model. The brms::brm() syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 95. \\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\): family = gaussian \\(\\mu_i = \\alpha + \\beta x_i\\): height ~ 1 + weight \\(\\alpha \\sim \\text{Normal}(156, 100)\\): prior(normal(156, 100), class = Intercept \\(\\beta \\sim \\text{Normal}(0, 10)\\): prior(normal(0, 10), class = b) \\(\\sigma \\sim \\text{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma) Thus, to add a predictor you just the + operator in the model formula. b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight, prior = c(prior(normal(156, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 41000, warmup = 40000, chains = 4, cores = 4) This was another example of how using a uniform prior for \\(\\sigma\\) required we use an unusually large number of warmup iterations before the HMC chains converged on the posterior. Change the prior to cauchy(0, 1) and the chains converge with no problem, resulting in much better effective samples, too. Here are the trace plots. plot(b4.3) 4.4.3 Interpreting the model fit. “One trouble with statistical models is that they are hard to understand” (p. 97). Welcome to the world of applied statistics. 4.4.3.1 Tables of estimates. With a little [] subsetting we can exclude the log posterior from the summary. posterior_summary(b4.3)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 113.8888392 1.87220496 110.1391459 117.7382878 ## b_weight 0.9048055 0.04113592 0.8198569 0.9885991 ## sigma 5.0998088 0.19065155 4.7427060 5.4946895 Again, brms doesn’t have a convenient corr = TRUE argument for plot() or summary(). But you can get that information after putting the chains in a data frame. posterior_samples(b4.3) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight sigma ## b_Intercept 1.00 -0.99 -0.01 ## b_weight -0.99 1.00 0.01 ## sigma -0.01 0.01 1.00 With centering, we can reduce the correlations among the parameters. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) Fit the weight_c model, b4.4. b4.4 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 46000, warmup = 45000, chains = 4, cores = 4, control = list(adapt_delta = 0.8, max_treedepth = 10)) plot(b4.4) posterior_summary(b4.4)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.5910328 0.26749344 154.0735689 155.1104156 ## b_weight_c 0.9047517 0.04418571 0.8177148 0.9928514 ## sigma 5.1018451 0.19857662 4.7365758 5.5195586 Like before, the uniform prior required extensive warmup iterations to produce a good posterior. This is easily fixed using a half Cauchy prior, instead. Anyways, the effective samples improved. Here’s the parameter correlation info. posterior_samples(b4.4) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight_c sigma ## b_Intercept 1.00 0.04 0.01 ## b_weight_c 0.04 1.00 0.00 ## sigma 0.01 0.00 1.00 See? Now all the correlations are quite low. Also, if you prefer a visual approach, you might do pairs(b4.4). 4.4.3.2 Plotting posterior inference against the data. Here is the code for Figure 4.4. Note our use of the fixef() function. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + theme_bw() + theme(panel.grid = element_blank()) 4.4.3.3 Adding uncertainty around the mean. Be default, we extract all the posterior iterations with posterior_samples(). post &lt;- posterior_samples(b4.3) post %&gt;% slice(1:5) # This serves a similar function as `head()` ## b_Intercept b_weight sigma lp__ ## 1 108.6992 1.0221552 5.059047 -1086.033 ## 2 108.2508 1.0201600 5.056258 -1087.238 ## 3 108.3927 1.0165873 5.161515 -1087.003 ## 4 108.9053 1.0126921 5.172930 -1085.447 ## 5 111.6827 0.9582711 4.958882 -1083.380 Here are the four models leading up to McElreath’s Figure 4.5. To reduce my computation time, I used a half Cauchy(0, 1) prior on \\(\\sigma\\). If you are willing to wait for the warmups, switching that out for McElreath’s uniform prior should work fine as well. N &lt;- 10 b10 &lt;- brm(data = d2 %&gt;% slice(1:N), # note our tricky use of `N` and `slice()` family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) N &lt;- 50 b50 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) N &lt;- 150 b150 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) N &lt;- 352 b352 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) I’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information. plot(b10) print(b10) plot(b50) print(b50) plot(b150) print(b150) plot(b352) print(b352) We’ll need to put the chains of each model into data frames. post10 &lt;- posterior_samples(b10) post50 &lt;- posterior_samples(b50) post150 &lt;- posterior_samples(b150) post352 &lt;- posterior_samples(b352) Here is the code for the four individual plots. p10 &lt;- ggplot(data = d2[1:10 , ], aes(x = weight, y = height)) + geom_abline(intercept = post10[1:20, 1], slope = post10[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = quantile(d2$weight, c(0, 1)), ylim = quantile(d2$height, c(0, 1))) + labs(subtitle = &quot;N = 10&quot;) + theme_bw() + theme(panel.grid = element_blank()) p50 &lt;- ggplot(data = d2[1:50 , ], aes(x = weight, y = height)) + geom_abline(intercept = post50[1:20, 1], slope = post50[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = quantile(d2$weight, c(0, 1)), ylim = quantile(d2$height, c(0, 1))) + labs(subtitle = &quot;N = 50&quot;) + theme_bw() + theme(panel.grid = element_blank()) p150 &lt;- ggplot(data = d2[1:150 , ], aes(x = weight, y = height)) + geom_abline(intercept = post150[1:20, 1], slope = post150[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = quantile(d2$weight, c(0, 1)), ylim = quantile(d2$height, c(0, 1))) + labs(subtitle = &quot;N = 150&quot;) + theme_bw() + theme(panel.grid = element_blank()) p352 &lt;- ggplot(data = d2[1:352 , ], aes(x = weight, y = height)) + geom_abline(intercept = post352[1:20, 1], slope = post352[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = quantile(d2$weight, c(0, 1)), ylim = quantile(d2$height, c(0, 1))) + labs(subtitle = &quot;N = 352&quot;) + theme_bw() + theme(panel.grid = element_blank()) Note how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %&gt;% slice(1:10) instead. Anyway, we saved each of these plots as objects. With a little help of the multiplot() function we are going to arrange those plot objects into a grid in order to reproduce Figure 4.5. Behold the code for the multiplot() function: multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } We’re finally ready to use multiplot() to make Figure 4.5. multiplot(p10, p150, p50, p352, cols = 2) 4.4.3.4 Plotting regression intervals and contours. Remember, if you want to plot McElreath’s mu_at_50 with ggplot2, you’ll need to save it as a data frame or a tibble. mu_at_50 &lt;- post %&gt;% transmute(mu_at_50 = b_Intercept + b_weight * 50) head(mu_at_50) ## mu_at_50 ## 1 159.8070 ## 2 159.2588 ## 3 159.2221 ## 4 159.5399 ## 5 159.5962 ## 6 159.1564 And here is a version McElreath’s Figure 4.6 density plot. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() We’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean. mean_hdi(mu_at_50[,1], .width = c(.89, .95)) ## y ymin ymax .width .point .interval ## 1 159.1291 158.5564 159.6345 0.89 mean hdi ## 2 159.1291 158.4681 159.7840 0.95 mean hdi If you wanted to express those sweet 95% HPDIs on your density plot, you might use tidybayes::stat_pointintervalh(). Since stat_pointintervalh() also returns a point estimate, we’ll throw in the mode. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() In brms, you would use fitted() to do what McElreath accomplished with link(). mu &lt;- fitted(b4.3, summary = F) str(mu) ## num [1:4000, 1:352] 158 157 157 157 158 ... When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your analysis. Because we had 4000 post-warmup iterations and \\(n\\) = 352, fitted() returned a matrix of 4000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead. Much like rethinking’s link(), fitted() can accommodate custom predictor values with its newdata argument. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) mu &lt;- fitted(b4.3, summary = F, newdata = weight_seq) %&gt;% as_tibble() %&gt;% mutate(Iter = 1:4000) %&gt;% select(Iter, everything()) str(mu) Anticipating ggplot2, we went ahead and converted the output to a tibble. But we might do a little more data processing with the aid of tidyr::gather(). With the gather() function, we’ll convert the data from the wide format to the long format. If you’re new to the distinction between wide and long data, you can learn more here or here. mu &lt;- mu %&gt;% gather(key, value, V1:V46) %&gt;% # We might reformat `key` (i.e., the default name for our `weight` index), to numerals mutate(key = str_extract(key, &quot;\\\\d+&quot;) %&gt;% as.integer()) %&gt;% # Learn more about `str_extract()` here: http://www.fabianheld.com/stringr/ # We might `rename()` the last two vectors more descriptively rename(weight = key, height = value) %&gt;% # Finally, we can use algebra to convert the `weight` values to the original ones we wanted mutate(weight = weight + 24) Enough data processing. Here we reproduce McElreath’s Figure 4.7.a. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(Iter &lt; 101), alpha = .1) # or prettied up a bit d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(Iter &lt; 101), color = &quot;navyblue&quot;, alpha = .05) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) With fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = T argument. mu_summary &lt;- fitted(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) head(mu_summary) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 137. 0.868 135. 138. 25 ## 2 137. 0.829 136. 139. 26 ## 3 138. 0.790 137. 140. 27 ## 4 139. 0.751 138. 141. 28 ## 5 140. 0.713 139. 142. 29 ## 6 141. 0.675 140. 142. 30 Here it is, our analogue to Figure 4.7.b. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_ribbon(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = mu_summary, aes(y = Estimate)) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And if you wanted to use intervals other than the default 95% ones, you’d enter a probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(.25, .75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile. 4.4.3.4.1 Overthinking: How link fitted() works. Similar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. 4.4.3.5 Prediction intervals. Even though our full statistical model (omitting priors for the sake of simplicity) is \\[h_i \\sim \\text{Normal}(\\mu_i = \\alpha + \\beta x_, \\sigma)\\] we’ve only been plotting the \\(\\mu\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to predict(). Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim(). We can reuse our weight_seq data from before. But in case you forgot, here’s that code again. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) The predict() code looks a lot like what we used for fitted(). pred_height &lt;- predict(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_height %&gt;% slice(1:6) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 136. 5.18 126. 147. 25 ## 2 137. 5.14 127. 148. 26 ## 3 138. 5.18 128. 148. 27 ## 4 139. 5.09 129. 149. 28 ## 5 140. 5.10 130. 150. 29 ## 6 141. 5.18 131. 151. 30 This time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of both \\(\\mu\\) and \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Our plot for Figure 4.8: d2 %&gt;% ggplot(aes(x = weight)) + geom_ribbon(data = pred_height, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_ribbon(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = mu_summary, aes(y = Estimate)) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5 Polynomial regression Remember d? d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, ... ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, ... ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20... ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,... The quadratic is probably the most commonly used polynomial regression model. \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\] McElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so. d &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight))/sd(weight)) Here’s the quadratic model in brms. b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) plot(b4.5) print(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + I(weight_s^2) ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 146.68 0.37 145.97 147.41 4000 1.00 ## weight_s 21.39 0.29 20.82 21.95 3481 1.00 ## Iweight_sE2 -8.42 0.28 -8.98 -7.89 3632 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 5.77 0.18 5.44 6.13 3649 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our quadratic plot requires new fitted()- and predict()-oriented wrangling. weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) fitd_quad &lt;- fitted(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_quad &lt;- predict(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) Behold the code for our version of Figure 4.9.a. You’ll notice how little the code changed from that for Figure 4.8, above. ggplot(data = d, aes(x = weight_s, y = height)) + geom_ribbon(data = pred_quad, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_ribbon(data = fitd_quad, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = fitd_quad, aes(y = Estimate), size = 1/4) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) From a formula perspective, the cubic model is a simple extenstion of the quadratic: \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\] Fit it like so. b4.6 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) And now we’ll fit the good old linear model. b4.7 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) Here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.c, the cubic model. fitd_cub &lt;- fitted(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_cub &lt;- predict(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s, y = height)) + geom_ribbon(data = pred_cub, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_ribbon(data = fitd_cub, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = fitd_cub, aes(y = Estimate), size = 1/4) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.a, the linear model. fitd_line &lt;- fitted(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_line &lt;- predict(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s, y = height)) + geom_ribbon(data = pred_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_ribbon(data = fitd_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = fitd_line, aes(y = Estimate), size = 1/4) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5.0.0.1 Overthinking: Converting back to natural scale. You can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the cubic model. at &lt;- c(-2, -1, 0, 1, 2) ggplot(data = d, aes(x = weight_s, y = height)) + geom_ribbon(data = pred_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_ribbon(data = fitd_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), fill = &quot;grey70&quot;) + geom_line(data = fitd_line, aes(y = Estimate), size = 1/4) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + # Here it is! scale_x_continuous(&quot;standardized weight converted back&quot;, breaks = at, labels = round(at*sd(d$weight) + mean(d$weight), 1)) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.0.1 brms_2.5.0 Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 ## [6] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 ## [11] tidyr_0.8.1 tibble_1.4.2 ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 mnormt_1.5-5 ## [22] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 ## [25] xfun_0.3 R6_2.2.2 markdown_0.8 ## [28] HDInterval_0.2.0 reshape_0.8.7 assertthat_0.2.0 ## [31] promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [34] gtable_0.2.0 rethinking_1.59 rlang_0.2.1 ## [37] extrafontdb_1.0 lazyeval_0.2.1 broom_0.4.5 ## [40] inline_0.3.15 yaml_2.1.19 reshape2_1.4.3 ## [43] abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [46] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 ## [49] rsconnect_0.8.8 extrafont_0.17 tools_3.5.1 ## [52] bookdown_0.7 psych_1.8.4 RColorBrewer_1.1-2 ## [55] ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [58] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 ## [61] LaplacesDemon_16.1.1 haven_1.1.2 magrittr_1.5 ## [64] colourpicker_1.0 mvtnorm_1.0-8 matrixStats_0.54.0 ## [67] hms_0.4.2 shinyjs_1.0 mime_0.5 ## [70] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [73] shinystan_2.5.0 readxl_1.1.0 gridExtra_2.3 ## [76] rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [79] crayon_1.3.4 htmltools_0.3.6 later_0.7.3 ## [82] lubridate_1.7.4 MASS_7.3-50 Matrix_1.2-14 ## [85] cli_1.0.0 bindr_0.1.1 igraph_1.2.1 ## [88] pkgconfig_2.0.1 foreign_0.8-70 xml2_1.2.0 ## [91] svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 ## [94] rvest_0.3.2 digest_0.6.15 rmarkdown_1.10 ## [97] cellranger_1.1.0 shiny_1.1.0 gtools_3.8.1 ## [100] nlme_3.1-137 jsonlite_1.5 bindrcpp_0.2.2 ## [103] mapproj_1.2.6 viridisLite_0.3.0 pillar_1.2.3 ## [106] lattice_0.20-35 loo_2.0.0 httr_1.3.1 ## [109] glue_1.2.0 xts_0.10-2 shinythemes_1.1.1 ## [112] pander_0.6.2 stringi_1.2.3 "],
["multivariate-linear-models.html", "5 Multivariate Linear Models 5.1 Spurious associations 5.2 Masked relationship 5.3 When adding variables hurts 5.4 Categorical varaibles 5.5 Ordinary least squares and lm() Reference Session info", " 5 Multivariate Linear Models McElreath’s listed reasons for multivaraiable regression include: statistical control for confounds multiple causation interactions We’ll approach the first two in this chapter. Interactions are reserved for Chapter 6. 5.1 Spurious associations Load the Waffle House data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce Unload rethinking and load brms and, while we’re at it, the tidyverse. rm(WaffleDivorce) detach(package:rethinking, unload = T) library(brms) library(tidyverse) I’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g., head(d) glimpse(d) Now we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package. But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of &quot;firebrick&quot;. # install.packages(&quot;ggrepel&quot;, depencencies = T) library(ggrepel) d %&gt;% ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, size = 1/2, color = &quot;firebrick4&quot;, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + geom_text_repel(data = d %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;OK&quot;, &quot;AR&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;NJ&quot;)), aes(label = Loc), size = 3, seed = 1042) + # this makes it reproducible scale_x_continuous(limits = c(0, 55)) + coord_cartesian(xlim = 0:50, ylim = 5:15) + labs(x = &quot;Waffle Houses per million&quot;, y = &quot;Divorce rate&quot;) + theme_bw() + theme(panel.grid = element_blank()) With coord_map() and help from the fiftystater package (which gives us access to lat/long data for all fifty states via fifty_states), we can plot our three major variables in a map format. library(fiftystater) d %&gt;% # first we&#39;ll standardize the three variables to put them all on the same scale mutate(Divorce_z = (Divorce - mean(Divorce)) / sd(Divorce), MedianAgeMarriage_z = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage), Marriage_z = (Marriage - mean(Marriage)) / sd(Marriage), # need to make the state names lowercase to match with the map data Location = str_to_lower(Location)) %&gt;% # here we select the relevant variables and put them in the long format to facet with `facet_wrap()` select(Divorce_z:Marriage_z, Location) %&gt;% gather(key, value, -Location) %&gt;% ggplot(aes(map_id = Location)) + geom_map(aes(fill = value), map = fifty_states, color = &quot;firebrick&quot;, size = 1/15) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_gradient(low = &quot;#f8eaea&quot;, high = &quot;firebrick4&quot;) + coord_map() + theme_bw() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~key) One of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d %&gt;% distinct(Location) to see for yourself. Those missing data should motivate the skills we’ll cover in Chapter 14. But let’s get back on track. Here we’ll officially standardize the predictor, MedianAgeMarriage. d &lt;- d %&gt;% mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage)) Now we’re ready to fit the first univariable model. b5.1 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) The summary: print(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.22 9.26 10.12 5624 1.00 ## MedianAgeMarriage_s -1.04 0.21 -1.45 -0.62 5608 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.52 0.16 1.24 1.87 6000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll employ fitted() to make Figure 5.2.b. In preparation for fitted() we’ll make a new tibble, nd, composed of a handful of densely-packed values for our predictor, MedianAgeMarriage_s. With the newdata argument, we’ll use those values to return model-implied expected values for Divorce. # Determine the range of `MedianAgeMarriage_s` values we&#39;d like to feed into `fitted()` nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30)) # Now use `fitted()` to get the model-implied trajectories fitd_b5.1 &lt;- fitted(b5.1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # Plot ggplot(data = fitd_b5.1, aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;) + geom_point(data = d, aes(x = MedianAgeMarriage_s, y = Divorce), size = 2, color = &quot;firebrick4&quot;) + labs(y = &quot;Divorce&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = range(d$Divorce)) + theme_bw() + theme(panel.grid = element_blank()) Before fitting the next model, we’ll standardize Marriage. d &lt;- d %&gt;% mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage)) We’re ready to fit our second univariable model. b5.2 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.25 9.20 10.17 5636 1.00 ## Marriage_s 0.64 0.24 0.16 1.11 5626 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.75 0.18 1.43 2.15 4933 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ll wangle and plot our version of Figure 5.2.a. nd &lt;- tibble(Marriage_s = seq(from = -2.5, to = 3.5, length.out = 30)) fitd_b5.2 &lt;- fitted(b5.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ggplot(data = fitd_b5.2, aes(x = Marriage_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;) + geom_point(data = d, aes(x = Marriage_s, y = Divorce), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = range(d$Divorce)) + labs(y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other. So we’ll build a multivariate model with the goal of measuring the partial value of each predictor. The question we want answered is: What is the predictive value of a variable, once I already know all of the other predictor variables? (p. 123, emphasis in the original) 5.1.1 Multivariate notation. Now we’ll get both predictors in there with our very first multivariable model. We can write the statistical model as \\[ \\begin{eqnarray} \\text{Divorce}_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{Marriage_s}_i + \\beta_2 \\text{MedianAgeMarriage_s}_i \\\\ \\alpha &amp; \\sim &amp; \\text{Normal}(10, 10) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim &amp; \\text{Uniform}(0, 10) \\end{eqnarray} \\] It might help to read the \\(+\\) symbols as “or” and then say: A State’s divorce rate can be a function of its marriage rate or its median age at marriage. The “or” indicates independent associations, which may be purely statistical or rather causal. (p. 124, emphasis in the original) 5.1.2 Fitting the model. Much like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on. b5.3 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.22 9.27 10.11 5164 1.00 ## Marriage_s -0.12 0.30 -0.70 0.47 3705 1.00 ## MedianAgeMarriage_s -1.12 0.30 -1.70 -0.52 3839 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.52 0.16 1.25 1.88 5157 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The stanplot() function is an easy way to get a default coefficient plot. You just put the brmsfit object into the function. stanplot(b5.3) There are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package. A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the posterior_samples() instead of the brmsfit object. # install.packages(&quot;bayesplot&quot;, dependencies = T) library(bayesplot) post &lt;- posterior_samples(b5.3) color_scheme_set(&quot;red&quot;) mcmc_intervals(post[, 1:4], prob = .5, point_est = &quot;median&quot;) + labs(title = &quot;My fancy bayesplot-based coefficient plot&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.line.x = element_line(size = 1/4), axis.line.y = element_blank(), axis.ticks.y = element_blank()) Because bayesplot produces a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out this vignette. The tidybaes::stat_pointintervalh() function offers a third way, this time with a more ground-up ggplot2 workflow. library(tidybayes) post %&gt;% select(-lp__) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, size = 3/4, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = NULL, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), panel.grid.major.y = element_line(color = alpha(&quot;firebrick4&quot;, 1/4), linetype = 3), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) The substantive interpretation of all those coefficient plots is: “Once we know median age at marriage for a State, there is little or no additive predictive power in also knowing the rate of marriage in that State” (p. 126, emphasis in the original). 5.1.3 Plotting multivariate posteriors. McElreath’s prose is delightfully deflationary. “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (p. 126). Now you’re inspired, let’s learn three: Predictor residual plots Counterfactual plots Posterior prediction plots 5.1.3.1 Predictor residual plots. To get ready to make our residual plots, we’ll predict Marriage_s with MedianAgeMarriage_s. b5.4 &lt;- brm(data = d, family = gaussian, Marriage_s ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Marriage_s ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.00 0.10 -0.21 0.20 4252 1.00 ## MedianAgeMarriage_s -0.71 0.10 -0.91 -0.51 5222 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.72 0.08 0.59 0.89 4506 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With fitted(), we compute the expected values for each state (with the exception of Nevada). Since the MedianAgeMarriage_s values for each state are in the date we used to fit the model, we’ll omit the newdata argument. fitd_b5.4 &lt;- fitted(b5.4) %&gt;% as_tibble() %&gt;% bind_cols(d) head(fitd_b5.4) ## # A tibble: 6 x 19 ## Estimate Est.Error Q2.5 Q97.5 Location Loc Population MedianAgeMarria… Marriage Marriage.SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.433 0.122 0.195 0.677 Alabama AL 4.78 25.3 20.2 1.27 ## 2 0.490 0.126 0.244 0.743 Alaska AK 0.71 25.2 26 2.93 ## 3 0.146 0.106 -0.0627 0.358 Arizona AZ 6.33 25.8 20.3 0.98 ## 4 1.01 0.180 0.657 1.36 Arkansas AR 2.92 24.3 26.4 1.7 ## 5 -0.428 0.119 -0.667 -0.193 Califor… CA 37.2 26.8 19.1 0.39 ## 6 0.203 0.108 -0.0110 0.419 Colorado CO 5.03 25.7 23.5 1.24 ## # ... with 9 more variables: Divorce &lt;dbl&gt;, Divorce.SE &lt;dbl&gt;, WaffleHouses &lt;int&gt;, South &lt;int&gt;, ## # Slaves1860 &lt;int&gt;, Population1860 &lt;int&gt;, PropSlaves1860 &lt;dbl&gt;, MedianAgeMarriage_s &lt;dbl&gt;, ## # Marriage_s &lt;dbl&gt; After a little data processing, we can make Figure 5.3. fitd_b5.4 %&gt;% ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + coord_cartesian(ylim = range(d$Marriage_s)) + theme_bw() + theme(panel.grid = element_blank()) We get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual for details. With our residuals in hand, we just need a little more data processing to make Figure 5.4.a. res_b5.4 &lt;- residuals(b5.4) %&gt;% # To use this in ggplot2, we need to make it a tibble or data frame as_tibble() %&gt;% bind_cols(d) # for the annotation at the top text &lt;- tibble(Estimate = c(- 0.5, 0.5), Divorce = 14.1, label = c(&quot;slower&quot;, &quot;faster&quot;)) res_b5.4 %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(limits = c(-2, 2)) + coord_cartesian(xlim = range(res_b5.4$Estimate), ylim = c(6, 14.1)) + labs(x = &quot;Marriage rate residuals&quot;) + theme_bw() + theme(panel.grid = element_blank()) To get the MedianAgeMarriage_s residuals, we have to fit the corresponding model first. b5.4b &lt;- brm(data = d, family = gaussian, MedianAgeMarriage_s ~ 1 + Marriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) And now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to Figure 5.4.b. text &lt;- tibble(Estimate = c(- 0.7, 0.5), Divorce = 14.1, label = c(&quot;younger&quot;, &quot;older&quot;)) residuals(b5.4b) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(limits = c(-2, 3)) + coord_cartesian(xlim = range(res_b5.4$Estimate), ylim = c(6, 14.1)) + labs(x = &quot;Age of marriage residuals&quot;) + theme_bw() + theme(panel.grid = element_blank()) 5.1.3.2 Counterfactual plots. A second sort of inferential plot displays the implied predictions of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variable you like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 129, emphasis in the original) Making Figure 5.5.a requires a little more data wrangling than before. # We need new `nd` data nd &lt;- tibble(Marriage_s = seq(from = -3, to = 3, length.out = 30), MedianAgeMarriage_s = rep(mean(d$MedianAgeMarriage_s), times = 30)) fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # Since `fitted()` and `predict()` name their intervals the same way, we&#39;ll need to # `rename()` then to keep them straight. rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% # Note how we&#39;re just nesting the `predict()` code right inside `bind_cols()` bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # Since we only need the intervals, we&#39;ll use `transmute()` rather than `mutate()` transmute(p_ll = Q2.5, p_ul = Q97.5) ) %&gt;% bind_cols(nd) %&gt;% # We&#39;re finally ready to plot ggplot(aes(x = Marriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = f_ll, ymax = f_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMedianAgeMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) We follow the same process for Figure 5.5.b. # new data nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30), Marriage_s = rep(mean(d$Marriage_s), times = 30)) # `fitted()` + `predict()` fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5) ) %&gt;% bind_cols(nd) %&gt;% # plot ggplot(aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = f_ll, ymax = f_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) A tension with such plots, however, lies in their counterfactual nature. In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not… …If our goal is to intervene in the world, there may not be any realistic way to manipulate each predictor without also manipulating the others. This is a serious obstacle to applied science, whether you are an ecologist, an economist, or an epidemiologist [or a psychologist] (p. 131) 5.1.3.3 Posterior prediction plots. “In addition to understanding the estimates, it’s important to check the model fit against the observed data” (p. 131). For more on the topic, check out Gabry and colleagues’ Visualization in Bayesian workflow or Simpson’s related blog post Touch me, I want to feel your data. In this version of Figure 5.6.a, the thin lines are the 95% intervals and the thicker lines are +/- the posterior \\(SD\\), both of which are returned when you use fitted(). fitted(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Divorce, y = Estimate)) + geom_abline(linetype = 2, color = &quot;grey50&quot;, size = .5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 3/4) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), size = 1/4, color = &quot;firebrick4&quot;) + geom_linerange(aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), size = 1/2, color = &quot;firebrick4&quot;) + # Note our use of the dot placeholder, here: https://magrittr.tidyverse.org/reference/pipe.html geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)), aes(label = Loc), hjust = 0, nudge_x = - 0.65) + labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) In order to make Figure 5.6.b, we need to clarify the relationships among fitted(), predict(), and residuals(). Here’s my attempt in a table. tibble(`brms function` = c(&quot;fitted&quot;, &quot;predict&quot;, &quot;residual&quot;), mean = c(&quot;same as the data&quot;, &quot;same as the data&quot;, &quot;in a deviance-score metric&quot;), scale = c(&quot;excludes sigma&quot;, &quot;includes sigma&quot;, &quot;excludes sigma&quot;)) %&gt;% knitr::kable() brms function mean scale fitted same as the data excludes sigma predict same as the data includes sigma residual in a deviance-score metric excludes sigma Hopefully this clarifies that if we want to incorporate the prediction interval in a deviance metric, we’ll need to first use predict() and then subtract the intervals from their corresponding Divorce values in the data. residuals(b5.3) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5) ) %&gt;% bind_cols(d) %&gt;% # here we put our `predict()` intervals into a deviance metric mutate(p_ll = Divorce - p_ll, p_ul = Divorce - p_ul) %&gt;% # The plot ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) + geom_hline(yintercept = 0, size = 1/2, color = &quot;firebrick4&quot;, alpha = 1/10) + geom_pointrange(aes(ymin = f_ll, ymax = f_ul), size = 2/5, shape = 20, color = &quot;firebrick4&quot;) + geom_segment(aes(y = Estimate - Est.Error, yend = Estimate + Est.Error, x = Loc, xend = Loc), size = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(y = p_ll, yend = p_ul, x = Loc, xend = Loc), size = 3, color = &quot;firebrick4&quot;, alpha = 1/10) + labs(x = NULL, y = NULL) + coord_flip(ylim = c(-6, 5)) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Compared to the last couple plots, Figure 5.6.c is pretty simple. residuals(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(wpc = WaffleHouses / Population) %&gt;% ggplot(aes(x = wpc, y = Estimate)) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, size = 1/2, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;AR&quot;, &quot;MS&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;ID&quot;)), aes(label = Loc), seed = 5.6) + scale_x_continuous(limits = c(0, 45)) + coord_cartesian(xlim = range(0, 40)) + labs(x = &quot;Waffles per capita&quot;, y = &quot;Divorce error&quot;) + theme_bw() + theme(panel.grid = element_blank()) More McElreath inspiration: “No matter how many predictors you’ve already included in a regression, it’s still possible to find spurious correlations with the remaining variation” (p. 134). To keep our deflation train going, it’s worthwhile to repeat the message in McElreath’s Rethinking: Stats, huh, yeah what is it good for? box. Often people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is real or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of another predictor, we might never notice this. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 134) 5.1.3.4 Overthinking: Simulating spurious association. N &lt;- 100 # number of cases set.seed(135) # setting the seed makes the results reproducible d &lt;- tibble(x_real = rnorm(N), # x_real as Gaussian with mean 0 and SD 1 (i.e., the defaults) x_spur = rnorm(N, x_real), # x_spur as Gaussian with mean = x_real y = rnorm(N, x_real)) # y as Gaussian with mean = x_real Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) We may as well fit a model. brm(data = d, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) %&gt;% fixef() %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.05 0.10 -0.24 0.14 ## x_real 1.17 0.14 0.90 1.44 ## x_spur -0.05 0.09 -0.23 0.11 5.2 Masked relationship Let’s load those tasty milk data. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) You might inspect the data like this. d %&gt;% select(kcal.per.g, mass, neocortex.perc) %&gt;% pairs(col = &quot;firebrick4&quot;) By just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math. McElreath has us start of with a simple univaraible milk model. b5.5 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex.perc, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) The uniform prior was difficult on Stan. After playing around a bit, I just switched to a unit-scale half Cauchy. Similar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 14 where we’ll learn how to do better. Here’s how to explicitly drop the cases with missing values on the predictor. dcc &lt;- d %&gt;% filter(complete.cases(.)) But anyway, let’s inspect the parameter summary. print(b5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc ## Data: d (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.348 0.538 -0.708 1.435 5444 1.000 ## neocortex.perc 0.005 0.008 -0.011 0.020 5476 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.192 0.039 0.133 0.282 3669 1.002 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice now we set digits = 3 within print() much the way McElreath set digits=3 within precis()? To get the brms answer to what McElreath did with coef(), we’ll use the fixef() function. fixef(b5.5)[2] * (76 - 55) ## [1] 0.09660848 Yes, indeed, “that’s less than 0.1 kilocalories” (p. 137). Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.7, top left. nd &lt;- tibble(neocortex.perc = 54:80) fitted(b5.5, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = Q25, ymax = Q75), fill = &quot;firebrick4&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;, size = 1/2) + geom_point(data = dcc, aes(x = neocortex.perc, y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + labs(y = &quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Do note the probs argument in the fitted() code, above. Let’s make the log_mass variable. dcc &lt;- dcc %&gt;% mutate(log_mass = log(mass)) Now we use log_mass as the new sole predictor. b5.6 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, control = list(adapt_delta = 0.9)) print(b5.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.706 0.057 0.594 0.820 5265 1.001 ## log_mass -0.032 0.024 -0.079 0.016 4634 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.184 0.039 0.126 0.278 3630 1.000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, top right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30)) fitted(b5.6, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = Q25, ymax = Q75), fill = &quot;firebrick4&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;, size = 1/2) + geom_point(data = dcc, aes(x = log_mass, y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + labs(y = &quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Finally, we’re ready to fit with both predictors included in the “joint model.” Here’s the statistical formula \\[ \\begin{eqnarray} \\text{kcal.per.g}_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{neocortex.perc}_i + \\beta_2 \\text{log}(\\text{mass}_i) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal}(0, 100) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim &amp; \\text{Uniform}(0, 1) \\end{eqnarray} \\] Note, the HMC chains required a longer warmup period and a higher adapt_delta setting for the model to converge properly. Life will be much better once we ditch the uniform prior for good. b5.7 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + neocortex.perc + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, control = list(adapt_delta = 0.999)) print(b5.7, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -1.082 0.579 -2.206 0.078 3304 1.000 ## neocortex.perc 0.028 0.009 0.010 0.045 3222 1.000 ## log_mass -0.096 0.028 -0.150 -0.041 3309 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.140 0.030 0.095 0.211 3366 1.000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, bottom left. nd &lt;- tibble(neocortex.perc = 54:80 %&gt;% as.double(), log_mass = mean(dcc$log_mass)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = Q25, ymax = Q75), fill = &quot;firebrick4&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;, size = 1/2) + geom_point(data = dcc, aes(x = neocortex.perc, y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + labs(y = &quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) And make Figure 5.7, bottom right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30), neocortex.perc = mean(dcc$neocortex.perc)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_ribbon(aes(ymin = Q25, ymax = Q75), fill = &quot;firebrick4&quot;, alpha = 1/5) + geom_line(color = &quot;firebrick4&quot;, size = 1/2) + geom_point(data = dcc, aes(x = log_mass, y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + labs(y = &quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) What [this regression model did was] ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model [asked] if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we statistically account for both. (pp. 140–141, emphasis in the original) 5.2.0.1 Overthinking: Simulating a masking relationship. N &lt;- 100 # number of cases rho &lt;- .7 # correlation between x_pos and x_neg set.seed(141) # setting the seed makes the results reproducible d &lt;- tibble(x_pos = rnorm(N), # x_pos as a standard Gaussian x_neg = rnorm(N, rho*x_pos, sqrt(1 - rho^2)), # x_neg correlated with x_pos y = rnorm(N, x_pos - x_neg)) # y equally associated with x_pos and x_neg Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) Here we fit the models with a little help from the update() function. b5.O.both &lt;- brm(data = d, family = gaussian, y ~ 1 + x_pos + x_neg, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma))) b5.O.pos &lt;- update(b5.O.both, formula = y ~ 1 + x_pos) b5.O.neg &lt;- update(b5.O.both, formula = y ~ 1 + x_neg) Compare the coefficients. fixef(b5.O.pos) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.01 0.13 -0.25 0.27 ## x_pos 0.32 0.14 0.04 0.59 fixef(b5.O.neg) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.07 0.12 -0.17 0.32 ## x_neg -0.51 0.14 -0.79 -0.23 fixef(b5.O.both) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.08 0.10 -0.13 0.28 ## x_pos 1.05 0.13 0.78 1.30 ## x_neg -1.18 0.14 -1.45 -0.91 5.3 When adding variables hurts Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will say that a very large range of parameter values are plausible, from tiny associations to massive ones, even if all of the variables are in reality strongly associated with the outcome variable. (pp. 141–142) 5.3.1 Multicollinear legs. Let’s simulate some leg data. N &lt;- 100 set.seed(531) d &lt;- tibble(height = rnorm(N, mean = 10, sd = 2), leg_prop = runif(N, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop*height + rnorm(N, mean = 0, sd = 0.02), leg_right = leg_prop*height + rnorm(N, mean = 0, sd = 0.02)) leg_left and leg_right are highly correlated. d %&gt;% select(leg_left:leg_right) %&gt;% cor() %&gt;% round(digits = 4) ## leg_left leg_right ## leg_left 1.0000 0.9995 ## leg_right 0.9995 1.0000 Have you ever even seen a \\(\\rho = .9995\\) correlation, before? Here it is in a plot. d %&gt;% ggplot(aes(x = leg_left, y = leg_right)) + geom_point(alpha = 1/2, color = &quot;firebrick4&quot;) + theme_bw() + theme(panel.grid = element_blank()) Here’s our attempt to predict height with both legs. b5.8 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) Let’s inspect the damage. print(b5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left + leg_right ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.91 0.30 0.31 1.51 5353 1.00 ## leg_left 1.08 2.14 -3.12 5.27 2270 1.00 ## leg_right 0.92 2.15 -3.26 5.16 2271 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.61 0.04 0.53 0.71 2798 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior [estimates] and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 143). Here’s our coefficient plot using brms::stanplot() with a little help from bayesplot::color_scheme_set(). color_scheme_set(&quot;red&quot;) stanplot(b5.8, type = &quot;intervals&quot;, prob = .5, prob_outer = .95, point_est = &quot;median&quot;) + labs(title = &quot;The coefficient plot for the two-leg model&quot;, subtitle = &quot;Holy smokes; look at the widths of those betas!&quot;) + theme_bw() + theme(text = element_text(size = 14), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Now you can use the brms::stanplot() function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with color_scheme_set(). This is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients, Figure 5.8.a. pairs(b5.8, pars = parnames(b5.8)[2:3]) If you’d like a nicer and more focused attempt, you might have to revert to the posterior_samples() function and a little ggplot2 code. post &lt;- posterior_samples(b5.8) post %&gt;% ggplot(aes(x = b_leg_left, y = b_leg_right)) + geom_point(color = &quot;firebrick&quot;, alpha = 1/10, size = 1/3) + theme_bw() + theme(panel.grid = element_blank()) While we’re at it, you can make a similar plot with the mcmc_scatter() function. post %&gt;% mcmc_scatter(pars = c(&quot;b_leg_left&quot;, &quot;b_leg_right&quot;), size = 1/3, alpha = 1/10) + theme_bw() + theme(panel.grid = element_blank()) But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign. post %&gt;% select(b_leg_left:b_leg_right) %&gt;% cor() ## b_leg_left b_leg_right ## b_leg_left 1.0000000 -0.9994878 ## b_leg_right -0.9994878 1.0000000 On pages 143–144, McElreath clarified that “from the computer’s perspective, this likelihood is really:” \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + (\\beta_1 + \\beta_2) x_i \\end{eqnarray} \\] Accordingly, here’s the posterior of the sum of the two regression coefficients, Figure 5.8.b. We’ll use tidybayes::geom_halfeyeh() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base. post %&gt;% ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Sum the multicollinear coefficients&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;) + theme_bw() + theme(panel.grid = element_blank()) Now we fit the model after ditching one of the leg lengths. b5.9 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.92 0.30 0.33 1.50 6000 1.00 ## leg_left 2.00 0.07 1.87 2.14 6000 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.61 0.04 0.53 0.70 5468 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That posterior \\(SD\\) looks much better. Compare this density to the one in Figure 5.8.b. posterior_samples(b5.9) %&gt;% ggplot(aes(x = b_leg_left, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Just one coefficient needed&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;, x = &quot;only b_leg_left, this time&quot;) + theme_bw() + theme(panel.grid = element_blank()) When two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such a case. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. (p. 145, emphasis in the original) 5.3.2 Multicollinear milk. Multicollinearity arises in real data, too. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = TRUE) library(brms) We’ll follow the text and fit the two univariable models, first. Note our use of update(). # kcal.per.g regressed on perc.fat b5.10 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + perc.fat, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) # kcal.per.g regressed on perc.lactose b5.11 &lt;- update(b5.10, newdata = d, formula = kcal.per.g ~ 1 + perc.lactose) posterior_summary(b5.10) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.301 0.039 0.222 0.379 ## b_perc.fat 0.010 0.001 0.008 0.012 ## sigma 0.080 0.012 0.061 0.106 ## lp__ 24.009 1.275 20.807 25.490 posterior_summary(b5.11) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.166 0.046 1.074 1.255 ## b_perc.lactose -0.011 0.001 -0.012 -0.009 ## sigma 0.067 0.010 0.051 0.090 ## lp__ 28.788 1.276 25.532 30.279 If you’d like to get just the 95% intervals similar to the way McElreath reported them in the prose on page 146, you might use the handy posterior_interval() function. posterior_interval(b5.10)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## 0.008 0.012 posterior_interval(b5.11)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## -0.012 -0.009 Now “watch what happens when we place both predictor varaibles in the same regression model” (p. 146) b5.12 &lt;- update(b5.11, newdata = d, formula = kcal.per.g ~ 1 + perc.fat + perc.lactose) posterior_summary(b5.12) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.014 0.223 0.590 1.466 ## b_perc.fat 0.002 0.003 -0.004 0.007 ## b_perc.lactose -0.009 0.003 -0.014 -0.004 ## sigma 0.068 0.010 0.051 0.091 ## lp__ 27.662 1.494 23.898 29.547 You can make custom pairs plots with GGalley, which will also compute the point estimates for the bivariate correlations. Here’s a default plot. #install.packages(&quot;GGally&quot;, dependencies = T) library(GGally) ggpairs(data = d, columns = c(3:4, 6)) + theme_bw() But you can customize these, too. E.g., my_diag &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;firebrick4&quot;, size = 0) } my_lower &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick4&quot;, size = 1/3, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(color = &quot;firebrick&quot;, alpha = .8, size = 1/4) } # Then plug those custom functions into `ggpairs()` ggpairs(data = d, columns = c(3:4, 6), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(strip.background = element_rect(fill = &quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) McElreath wrote “these two variables form essentially a single axis of variation” (p. 148). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also showed the Pearson’s correlation coefficients. 5.3.2.1 Overthinking: Simulating collinearity. First we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block. sim_coll &lt;- function(seed, rho){ set.seed(seed) d &lt;- d %&gt;% mutate(x = rnorm(n(), mean = perc.fat * rho, sd = sqrt((1 - rho^2) * var(perc.fat)))) m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d) sqrt(diag(vcov(m)))[2] # parameter SD } # how many simulations per `rho`-value would you like? n_seed &lt;- 100 # how many `rho`-values from 0 to .99 would you like to evaluate the process over? n_rho &lt;- 30 d &lt;- tibble(seed = 1:n_seed) %&gt;% expand(seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2(seed, rho, sim_coll)) %&gt;% unnest() %&gt;% group_by(rho) %&gt;% # we&#39;ll `summarise()` our output by the mean and 95% intervals summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) We’ve added 95% interval bands to our version of Figure 5.10. d %&gt;% ggplot(aes(x = rho, y = mean)) + geom_line(color = &quot;firebrick4&quot;) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;firebrick&quot;, alpha = 1/4) + labs(x = expression(rho), y = &quot;parameter SD&quot;) + coord_cartesian(ylim = c(0, .0072)) + theme_bw() + theme(panel.grid = element_blank()) Did you notice we used the base R lm() function to fit the models? As McElreath rightly pointed out, lm() presumes flat priors. Proper Bayesian modeling could improve on that. But then we’d have to wait for a whole lot of HMC chains to run and until our personal computers or the algorithms we use to fit our Bayesian models become orders of magnitude faster, we just don’t have time for that. 5.3.3 Post-treatment bias. It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph: seed and sprout plants measure heights apply different antifungal soil treatments (i.e., the experimental manipulation) measure (a) the heights and (b) the presence of fungus Based on the design, let’s simulate our data. N &lt;- 100 set.seed(17) d &lt;- tibble(h0 = rnorm(N, mean = 10, sd = 2), treatment = rep(0:1, each = N / 2), fungus = rbinom(N, size = 1, prob = .5 - treatment * 0.4), h1 = h0 + rnorm(N, mean = 5 - 3 * fungus, sd = 1)) We’ll use head() to peek at the data. d %&gt;% head() ## # A tibble: 6 x 4 ## h0 treatment fungus h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.97 0 1 12.9 ## 2 9.84 0 1 11.9 ## 3 9.53 0 0 15.8 ## 4 8.37 0 1 11.1 ## 5 11.5 0 1 13.1 ## 6 9.67 0 0 15.7 These data + the model were rough on Stan, at first, which spat out warnings about divergent transitions. The model ran fine after setting warmup = 1000 and adapt_delta = 0.99. b5.13 &lt;- brm(data = d, family = gaussian, h1 ~ 1 + h0 + treatment + fungus, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.99)) print(b5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ 1 + h0 + treatment + fungus ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.25 0.54 4.24 6.31 3162 1.00 ## h0 0.96 0.05 0.86 1.06 2910 1.00 ## treatment 0.22 0.23 -0.23 0.67 2515 1.00 ## fungus -3.01 0.27 -3.55 -2.50 1945 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.08 0.08 0.93 1.25 900 1.01 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now fit the model after excluding fungus, our post-treatment variable. b5.14 &lt;- update(b5.13, formula = h1 ~ 1 + h0 + treatment) print(b5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 + treatment ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 4.43 0.82 2.81 6.02 1928 1.00 ## h0 0.90 0.08 0.75 1.06 1919 1.00 ## treatment 1.26 0.32 0.60 1.86 909 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.67 0.12 1.44 1.93 3179 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “Now the impact of treatment is strong and positive, as it should be” (p. 152). In this case, there were really two outcomes. The first was the one we modeled, the height at the end of the experiment (i.e., h1). The second outcome, which was clearly related to h1, was the presence of fungus, captured by our binomial variable fungus. If you wanted to model that, you’d fit a logistic regression model, which we’ll learn about in Chapter 10. 5.4 Categorical varaibles Many readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are included in a model… Knowing how the machine works removes a lot of this difficulty. (p. 153, emphasis in the original) 5.4.1 Binary categories. Reload the Howell1 data. library(rethinking) data(Howell1) d &lt;- Howell1 Unload rethinking and load brms. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Just in case you forgot what these data were like: d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, ... ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, ... ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20... ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,... Let’s fit the first height model with the male dummy. Note. The uniform prior McElreath used in the text in conjunction with the brms::brm() function seemed to cause problems for the HMC chains, here. After experimenting with start values, increasing warmup, and increasing adapt_delta, switching out the uniform prior did the trick. Anticipating Chapter 8, I recommend you use a weakly-regularizing half Cauchy for \\(\\sigma\\). b5.15 &lt;- brm(data = d, family = gaussian, height ~ 1 + male, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + male ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 134.82 1.58 131.73 137.98 5326 1.00 ## male 7.33 2.28 2.91 11.84 6000 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.38 0.85 25.77 29.12 5562 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our samples from the posterior are already in the HMC iterations. All we need to do is put them in a data frame and then put them to work. post &lt;- posterior_samples(b5.15) post %&gt;% transmute(male_height = b_Intercept + b_male) %&gt;% mean_qi(.width = .89) ## male_height .lower .upper .width .point .interval ## 1 142.1501 139.5012 144.8688 0.89 mean qi You can also do this with fitted(). nd &lt;- tibble(male = 1) fitted(b5.15, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 142.1501 1.683553 138.8598 145.5328 And you could even plot. fitted(b5.15, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied male heights&quot;, x = expression(alpha + beta[&quot;male&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.1.1 Overthinking: Re-parameterizing the model. The reparameterized model follows the form \\[ \\begin{eqnarray} \\text{height}_i &amp; \\sim &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha_\\text{female} (1 - \\text{male}_i) + \\alpha_\\text{male} \\text{male}_i \\end{eqnarray} \\] So then a female dummy would satisfy the condition \\(\\text{female}_i = (1 - \\text{male}_i)\\). Let’s make that dummy. d &lt;- d %&gt;% mutate(female = 1 - male) Everyone has their own idiosyncratic way of coding. One of my quirks is I always explicitly specify a model’s intercept following the form y ~ 1 + x, where y is the criterion, x stands for the predictors, and 1 is the intercept. You don’t have to do this, of course. You could just code y ~ x to get the same results. The brm() function assumes you want that intercept. One of the reasons I like the verbose version is it reminds me to think about the intercept and to include it in my priors. Another nice feature is that is helps me make sense of the code for this model: height ~ 0 + male + female. When we replace … ~ 1 + … with … ~ 0 + …, we tell brm() to remove the intercept. Removing the intercept allows us to include ALL levels of a given categorical variable in our model. In this case, we’ve expressed sex as two dummies, female and male. Taking out the intercept lets us put both dummies into the formula. b5.15b &lt;- brm(data = d, family = gaussian, height ~ 0 + male + female, prior = c(prior(normal(178, 100), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.15b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + male + female ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## male 142.34 1.72 138.88 145.71 6000 1.00 ## female 134.64 1.63 131.42 137.83 5523 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.40 0.83 25.82 29.11 6000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If we wanted the formal difference score from such a model, we’d subtract. posterior_samples(b5.15b) %&gt;% transmute(dif = b_male - b_female) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied difference score&quot;, x = expression(alpha[&quot;male&quot;] - alpha[&quot;female&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.2 Many categories. When there are more than two categories, you’ll need more than one dummy variable. Here’s the general rule: To include \\(k\\) categories in a linear model, you require \\(k - 1\\) dummy variables. Each dummy variable indicates, with the value 1, a unique category. The category with no dummy variable assigned to it ends up again as the “intercept” category. (p. 155) We’ll practice with milk. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) With the tidyverse, we can peek at clade with distinct() in the place of base R unique(). d %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape As clade has 4 categories, let’s convert these to 4 dummy variables. d &lt;- d %&gt;% mutate(clade_nwm = ifelse(clade == &quot;New World Monkey&quot;, 1, 0), clade_owm = ifelse(clade == &quot;Old World Monkey&quot;, 1, 0), clade_s = ifelse(clade == &quot;Strepsirrhine&quot;, 1, 0), clade_ape = ifelse(clade == &quot;Ape&quot;, 1, 0)) Now we’ll fit the model with three of the four dummies. In this model, clade_ape is the reference category captured by the intercept. b5.16 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, control = list(adapt_delta = 0.8)) print(b5.16) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.55 0.04 0.46 0.63 5004 1.00 ## clade_nwm 0.17 0.06 0.05 0.29 5170 1.00 ## clade_owm 0.24 0.07 0.11 0.38 5466 1.00 ## clade_s -0.04 0.07 -0.18 0.10 5639 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.17 6000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we grab the chains, our draws from the posterior. post &lt;- b5.16 %&gt;% posterior_samples() head(post) ## b_Intercept b_clade_nwm b_clade_owm b_clade_s sigma lp__ ## 1 0.5717516 0.12407657 0.1861115 0.05084326 0.1260440 8.534763 ## 2 0.6085899 0.05062493 0.1826776 -0.23742201 0.1453868 6.312997 ## 3 0.6097739 0.05830101 0.1905792 -0.21664781 0.1441479 7.143810 ## 4 0.4802237 0.26795837 0.3555692 0.07913364 0.1168783 8.398402 ## 5 0.5683417 0.13334897 0.2495454 -0.06637570 0.1432624 9.889714 ## 6 0.5730083 0.10396287 0.2007451 -0.06423019 0.1051636 9.979689 You might compute averages for each category and summarizing the results with the transpose of base R’s apply() function, rounding to two digits of precision. post$mu_ape &lt;- post$b_Intercept post$mu_nwm &lt;- post$b_Intercept + post$b_clade_nwm post$mu_owm &lt;- post$b_Intercept + post$b_clade_owm post$mu_s &lt;- post$b_Intercept + post$b_clade_s round(t(apply(post[ ,7:10], 2, quantile, c(.5, .025, .975))), digits = 2) ## 50% 2.5% 97.5% ## mu_ape 0.55 0.46 0.63 ## mu_nwm 0.72 0.63 0.80 ## mu_owm 0.79 0.69 0.89 ## mu_s 0.51 0.39 0.62 Here’s a more tidyverse sort of way to get the same thing, but this time with means and HPDIs via the tidybayes::mean_hdi() function. post %&gt;% transmute(mu_ape = b_Intercept, mu_nwm = b_Intercept + b_clade_nwm, mu_owm = b_Intercept + b_clade_owm, mu_s = b_Intercept + b_clade_s) %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu_ape 0.55 0.46 0.63 0.95 mean hdi ## 2 mu_nwm 0.71 0.63 0.8 0.95 mean hdi ## 3 mu_owm 0.79 0.68 0.89 0.95 mean hdi ## 4 mu_s 0.51 0.39 0.62 0.95 mean hdi You could also summarize with fitted(). nd &lt;- tibble(clade_nwm = c(1, 0, 0, 0), clade_owm = c(0, 1, 0, 0), clade_s = c(0, 0, 1, 0), primate = c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;)) fitted(b5.16, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(primate = rep(c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;), each = n() / 4)) %&gt;% ggplot(aes(x = value, y = reorder(primate, value))) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + labs(x = &quot;kcal.per.g&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) And there are multiple ways to compute summary statistics for the difference between NWM and OWM, too. # base R quantile(post$mu_nwm - post$mu_owm, probs = c(.5, .025, .975)) ## 50% 2.5% 97.5% ## -0.07164873 -0.20835597 0.06272147 # tidyverse + tidybayes post %&gt;% transmute(dif = mu_nwm - mu_owm) %&gt;% median_qi(dif) ## dif .lower .upper .width .point .interval ## 1 -0.07164873 -0.208356 0.06272147 0.95 median qi 5.4.3 Adding regular predictor variables. If we wanted to fit the model including perc.fat as an additional predictor, the basic statistical formula would be \\[\\mu_i = \\alpha + \\beta_\\text{clade_nwm} \\text{clade_nwm}_i + \\beta_\\text{clade_owm} \\text{clade_owm}_i + \\beta_\\text{clade_s} \\text{clade_s}_i + \\beta_\\text{perc.fat} \\text{perc.fat}_i\\] The corresponding formula argument within brm() would be kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s + perc.fat. 5.4.4 Another approach: Unique intercepts. Using the code below, there’s no need to transform d$clade into d$clade_id. The advantage of this approach is the indices in the model summary are more descriptive than a[1] through a[4]. b5.16_alt &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 0 + clade, prior = c(prior(normal(.6, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4) print(b5.16_alt) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 0 + clade ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## cladeApe 0.54 0.04 0.46 0.63 6000 1.00 ## cladeNewWorldMonkey 0.71 0.04 0.63 0.80 6000 1.00 ## cladeOldWorldMonkey 0.79 0.05 0.68 0.89 6000 1.00 ## cladeStrepsirrhine 0.51 0.06 0.39 0.62 6000 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.18 6000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See? This is much easier than trying to remember which one was which in an arbitrary numeric index. 5.5 Ordinary least squares and lm() Since this section centers on the frequentist lm() function, I’m going to largely ignore it. A couple things, though. You’ll note how the brms package uses the lm()-like design formula syntax. Although not as pedagogical as the more formal rethinking syntax, it has the advantage of cohering with the popular lme4 syntax for multilevel models. Also, on page 161 McElreath clarified that one cannot use the I() syntax with his rethinking package. Not so with brms. The I() syntax works just fine with brms::brm(). We’ve already made use of it in the “Polynomial regression” section of Chapter 4. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GGally_1.4.0 tidybayes_1.0.1 bayesplot_1.6.0 fiftystater_1.0.1 ggrepel_0.8.0 ## [6] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 ## [11] tidyr_0.8.1 tibble_1.4.2 tidyverse_1.2.1 brms_2.5.0 Rcpp_0.12.18 ## [16] rstan_2.17.3 StanHeaders_2.17.2 ggplot2_3.0.0 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] labeling_0.3 mnormt_1.5-5 bridgesampling_0.4-0 ## [22] rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 ## [28] reshape_0.8.7 assertthat_0.2.0 promises_1.0.1 ## [31] scales_0.5.0 beeswarm_0.2.3 gtable_0.2.0 ## [34] rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [37] broom_0.4.5 inline_0.3.15 yaml_2.1.19 ## [40] reshape2_1.4.3 abind_1.4-5 modelr_0.1.2 ## [43] threejs_0.3.1 crosstalk_1.0.0 backports_1.1.2 ## [46] httpuv_1.4.4.2 rsconnect_0.8.8 extrafont_0.17 ## [49] tools_3.5.1 bookdown_0.7 psych_1.8.4 ## [52] RColorBrewer_1.1-2 ggridges_0.5.0 plyr_1.8.4 ## [55] base64enc_0.1-3 progress_1.2.0 prettyunits_1.0.2 ## [58] zoo_1.8-2 LaplacesDemon_16.1.1 haven_1.1.2 ## [61] magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 ## [64] matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 ## [67] mime_0.5 evaluate_0.10.1 arrayhelpers_1.0-20160527 ## [70] xtable_1.8-2 shinystan_2.5.0 readxl_1.1.0 ## [73] gridExtra_2.3 rstantools_1.5.0 compiler_3.5.1 ## [76] maps_3.3.0 crayon_1.3.4 htmltools_0.3.6 ## [79] later_0.7.3 lubridate_1.7.4 MASS_7.3-50 ## [82] Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [85] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [88] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 ## [91] vipor_0.4.5 rvest_0.3.2 digest_0.6.15 ## [94] rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [97] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [100] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 ## [103] pillar_1.2.3 lattice_0.20-35 loo_2.0.0 ## [106] httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["overfitting-regularization-and-information-criteria.html", "6 Overfitting, Regularization, and Information Criteria 6.1 The problem with parameters 6.2 Information theory and model performance 6.3 Regularization 6.4 Information criteria 6.5 Using information criteria 6.6 Summary Bonus: \\(R^2\\) talk Reference Session info", " 6 Overfitting, Regularization, and Information Criteria In this chapter we contend with two contrasting kinds of statistical error: overfitting, “which leads to poor prediction by learning too much from the data” underfitting, “which leads to poor prediction by learning too little from the data” (p. 166, emphasis added) 6.1 The problem with parameters The \\(R^2\\) is a popular way to measure how well you can retrodict the data. It traditionally follows the form \\[R^2 = \\frac{\\text{var(outcome)} - \\text{var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\] By \\(\\text{var()}\\), of course, we meant variance (i.e., the var() function in R). McElreath’s not a fan of the \\(R^2\\). But it’s important in my field, so instead of a summary at the end of the chapter, we will cover the Bayesian version of \\(R^2\\) and how to use it in brms. 6.1.1 More parameters always improve fit. We’ll start off by making the data with brain size and body size for seven species. library(tidyverse) ( d &lt;- tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) ) ## # A tibble: 7 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 africanus 452 35.5 ## 3 habilis 612 34.5 ## 4 boisei 521 41.5 ## 5 rudolfensis 752 55.5 ## 6 ergaster 871 61 ## 7 sapiens 1350 53.5 Let’s get ready for Figure 6.2. The plots in this chapter will be characterized by theme_classic() + theme(text = element_text(family = &quot;Courier&quot;)). Our color palette will come from the rcartocolor package, which provides color schemes designed by ‘CARTO’. # install.packages(&quot;rcartocolor&quot;, dependencies = T) library(rcartocolor) The specific palette we’ll be using is “BurgYl.” In addition to palettes, the rcartocolor package offers a few convenience functions which make it easier to use their palettes. The carto_pal() function will return the HEX numbers associated with a given palette’s colors and the display_carto_pal() function will display the actual colors. carto_pal(7, &quot;BurgYl&quot;) ## [1] &quot;#fbe6c5&quot; &quot;#f5ba98&quot; &quot;#ee8a82&quot; &quot;#dc7176&quot; &quot;#c8586c&quot; &quot;#9c3f5d&quot; &quot;#70284a&quot; display_carto_pal(7, &quot;BurgYl&quot;) We’ll be using a diluted version of the third color for the panel background (i.e., theme(panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)))) and the darker purples for other plot elements. Here’s the plot. library(ggrepel) d %&gt;% ggplot(aes(x = mass, y = brain, label = species)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_text_repel(size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[7], family = &quot;Courier&quot;, seed = 438) + coord_cartesian(xlim = 30:65) + labs(x = &quot;body mass (kg)&quot;, y = &quot;brain volume (cc)&quot;, subtitle = &quot;Average brain volume by body\\nmass for six hominin species&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Let’s fit the first six models in bulk. First we’ll make a custom function, fit_lm(), into which we’ll feed the desired names and formulas of our models. We’ll make a tibble initially composed of those names (i.e., model) and formulas (i.e., formula). Via purrr::map2() within mutate(), we’ll then fit the models and save the model objects within the tibble. The broom package provides an array of convenience functions to convert statistical analysis summaries into tidy data objects. We’ll employ broom::tidy() and broom::glance() to extract information from the model fits. library(broom) fit_lm &lt;- function(model, formula){ model &lt;- lm(data = d, formula = formula) } fits &lt;- tibble(model = str_c(&quot;b6.&quot;, 1:6), formula = c(&quot;brain ~ mass&quot;, &quot;brain ~ mass + I(mass^2)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)&quot;)) %&gt;% mutate(fit = map2(model, formula, fit_lm)) %&gt;% mutate(tidy = map(fit, tidy), glance = map(fit, glance)) # what did we just do? print(fits) ## # A tibble: 6 x 5 ## model formula fit tidy glance ## &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; &lt;list&gt; ## 1 b6.1 brain ~ mass &lt;S3:… &lt;dat… &lt;data… ## 2 b6.2 brain ~ mass + I(mass^2) &lt;S3:… &lt;dat… &lt;data… ## 3 b6.3 brain ~ mass + I(mass^2) + I(mass^3) &lt;S3:… &lt;dat… &lt;data… ## 4 b6.4 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) &lt;S3:… &lt;dat… &lt;data… ## 5 b6.5 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) &lt;S3:… &lt;dat… &lt;data… ## 6 b6.6 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6) &lt;S3:… &lt;dat… &lt;data… Our fits object is a nested tibble. To learn more about this bulk approach to fitting models, check out Hadley Wickham’s talk Managing many models with R. As you might learn in the talk, we can extract the \\(R^2\\) from each model with map_dbl(&quot;r.squared&quot;), which we’ll then display in a plot. fits &lt;- fits %&gt;% mutate(r2 = glance %&gt;% map_dbl(&quot;r.squared&quot;)) %&gt;% mutate(r2_text = round(r2, digits = 2) %&gt;% as.character() %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) fits %&gt;% ggplot(aes(x = r2, y = formula, label = r2_text)) + geom_text(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 3.5) + labs(x = expression(italic(R)^2), y = NULL) + scale_x_continuous(limits = 0:1, breaks = 0:1) + theme_classic() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If we wanted to look at the model coefficients, we could unnest(tidy) and wrangle a bit. fits %&gt;% unnest(tidy) %&gt;% select(model, term:estimate) %&gt;% mutate_if(is.double, round, digits = 1) %&gt;% complete(term = distinct(., term), model) %&gt;% spread(key = term, value = estimate) %&gt;% select(model, `(Intercept)`, mass, everything()) ## # A tibble: 6 x 8 ## model `(Intercept)` mass `I(mass^2)` `I(mass^3)` `I(mass^4)` `I(mass^5)` `I(mass^6)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b6.1 -228. 20.7 NA NA NA NA NA ## 2 b6.2 -2618. 127. -1.1 NA NA NA NA ## 3 b6.3 21990. -1474. 32.8 -0.2 NA NA NA ## 4 b6.4 322887. -27946. 892. -12.4 0.1 NA NA ## 5 b6.5 -1535342. 180049 -8325. 190. -2.1 0 NA ## 6 b6.6 10849891. -1473228. 82777 -2463. 40.9 -0.4 0 For Figure 6.3, we’ll make each plot individually and them glue them together with gridExtra::grid.arrange(). Since they all share a common stucture, we’ll start by specifying a base plot which we’ll save as p. p &lt;- d %&gt;% ggplot(aes(x = mass, y = brain)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) + coord_cartesian(ylim = c(300, 1500)) + labs(x = &quot;body mass (kg)&quot;, y = &quot;brain volume (cc)&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Now for each subplot, we’ll tack the subplot-specific components onto p. The main action is in stat_smooth(). For each subplot, the first three lines in stat_smooth() are identical, with only the bottom formula line differing. Like McElreath did in the text, we also adjust the y-axis range for the last two plots. # linear p1 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, # Note our rare use of 89% confidence intervals color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ x) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .49&quot;))) # quadratic p2 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 2)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .54&quot;))) # cubic p3 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 3)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .68&quot;))) # fourth-order polynomial p4 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 4)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .81&quot;))) # fifth-order polynomial p5 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 5)) + coord_cartesian(ylim = c(150, 1900)) + # We&#39;re adjusting the y-axis range for this plot (and the next) ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = .99&quot;))) # sixth-order polynomial p6 &lt;- p + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) + # to mark off 0 on the y-axis stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 6)) + coord_cartesian(ylim = c(-300, 1500)) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = 1&quot;))) Okay, now we’re ready to combine six subplots and produce our version of Figure 6.3. library(gridExtra) grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2) 6.1.2 Too few parameters hurts, too. Fit the intercept only model, b6.7. b6.7 &lt;- lm(data = d, brain ~ 1) summary(b6.7) ## ## Call: ## lm(formula = brain ~ 1, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -275.71 -227.21 -101.71 97.79 636.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 713.7 121.8 5.86 0.00109 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 322.2 on 6 degrees of freedom With the intercept-only model, we didn’t even get an \\(R^2\\) value in the summary.broom::glance() offers a quick way to get one. glance(b6.7) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance ## 1 0 0 322.2477 NA NA 1 -49.82029 103.6406 103.5324 623061.4 ## df.residual ## 1 6 Zero. Our intercept-only b6.7 explained exactly zero variance in brain. All it did was tell us what the unconditional mean and variance (i.e., ‘Residual standard error’) were. I hope that makes sense. They were the only things in the model: \\(\\text{brain}_i \\sim \\text{Normal}(\\mu = \\alpha, \\sigma)\\). To get the intercept-only model for Figure 6.4, we plug formula = y ~ 1 into the stat_smooth() function. p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ 1) + ggtitle(NULL, subtitle = expression(paste(italic(R)^2, &quot; = 0&quot;))) 6.1.2.1 Overthinking: Dropping rows. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of d like this. d %&gt;% filter(row_number() != 2) ## # A tibble: 6 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 habilis 612 34.5 ## 3 boisei 521 41.5 ## 4 rudolfensis 752 55.5 ## 5 ergaster 871 61 ## 6 sapiens 1350 53.5 We can then extend that logic into a custom function, make_lines(), that will drop a row from d, fit the simple model brain ~ mass, and then use base R predict() to return the model-implied trajectory over new data values. # because these lines are straight, we only need new data over two points of `mass` nd &lt;- tibble(mass = c(30, 70)) make_lines &lt;- function(row){ my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass) predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } Here we’ll make a tibble, lines, which will specify rows 1 through 7 in the row column. We’ll then feed those row numbers into our custom make_lines() function, which will return the predicted values and their corresponding mass values, per model. ( lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) ) ## # A tibble: 14 x 3 ## row brain mass ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 436. 30 ## 2 1 1201. 70 ## 3 2 421. 30 ## 4 2 1205. 70 ## 5 3 323. 30 ## 6 3 1264. 70 ## 7 4 423. 30 ## 8 4 1221. 70 ## 9 5 376. 30 ## 10 5 1335. 70 ## 11 6 332. 30 ## 12 6 1433. 70 ## 13 7 412. 30 ## 14 7 964. 70 Now we’re ready to plot the left panel of Figure 6.5. p + scale_x_continuous(expand = c(0, 0)) + geom_line(data = lines, aes(x = mass, y = brain, group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) To make the right panel for Figure 6.5, we’ll need to increase the number of mass points in our nd data and redefine the make_lines() function to fit the sixth-order-polynomial model. # because these lines will be very curvy, we&#39;ll need new data over many points of `mass` nd &lt;- tibble(mass = seq(from = 30, to = 65, length.out = 200)) # redifine the function make_lines &lt;- function(row){ my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)) predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } # make our new tibble lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) # plot! p + geom_line(data = lines, aes(group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) + coord_cartesian(ylim = -300:2000) 6.2 Information theory and model performance Whether you end up using regularization or information criteria or both, the first thing you must do is pick a criterion of model performance. What do you want the model to do well at? We’ll call this criterion the target, and in this section you’ll see how information theory provides a common and useful target, the out-of-sample deviance. (p. 174, emphasis in the original) 6.2.1 Firing the weatherperson. If you let rain = 1 and sun = 0, here’s a way to make a plot of the first table of page 175, the weatherperson’s predictions. weatherperson &lt;- tibble(day = 1:10, prediction = rep(c(1, 0.6), times = c(3, 7)), observed = rep(c(1, 0), times = c(3, 7))) weatherperson %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;)) Here’s how the newcomer faired: newcomer &lt;- tibble(day = 1:10, prediction = 0, observed = rep(c(1, 0), times = c(3, 7))) newcomer %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), text = element_text(family = &quot;Courier&quot;)) If we do the math entailed in the tibbles, we’ll see why the newcomer could boast “I’m the best person for the job” (p. 175). weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person) %&gt;% summarise(hit_rate = mean(hit)) ## # A tibble: 2 x 2 ## person hit_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer 0.7 ## 2 weatherperson 0.58 6.2.1.1 Costs and benefits. Our new points variable doesn’t fit into the nice color-based geom_tile() plots from above. But we can still do the math. weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), points = ifelse(observed == 1 &amp; prediction != 1, -5, ifelse(observed == 1 &amp; prediction == 1, -1, -1 * prediction))) %&gt;% group_by(person) %&gt;% summarise(happiness = sum(points)) ## # A tibble: 2 x 2 ## person happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer -15 ## 2 weatherperson -7.2 6.2.1.2 Measuring accuracy. weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person, hit) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(power = hit ^ n, term = rep(letters[1:2], times = 2)) %&gt;% select(person, term, power) %&gt;% spread(key = term, value = power) %&gt;% mutate(probability_correct_sequence = a * b) ## # A tibble: 2 x 4 ## person a b probability_correct_sequence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 newcomer 0 1 0 ## 2 weatherperson 0.00164 1 0.00164 6.2.2 Information and uncertainty. The formula for information entropy is: \\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i)\\] McElreath put it in words as “the uncertainty contained in a probability distribution is the average log-probability of the event” (p. 178). We’ll compute the information entropy for weather at the first unnamed location, which we’ll call McElreath's house, and Abu Dhabi at once. tibble(place = c(&quot;McElreath&#39;s house&quot;, &quot;Abu Dhabi&quot;), p_rain = c(.3, .01)) %&gt;% mutate(p_shine = 1 - p_rain) %&gt;% group_by(place) %&gt;% mutate(H_p = (p_rain * log(p_rain) + p_shine * log(p_shine)) %&gt;% mean() * -1) ## # A tibble: 2 x 4 ## # Groups: place [2] ## place p_rain p_shine H_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 McElreath&#39;s house 0.3 0.7 0.611 ## 2 Abu Dhabi 0.01 0.99 0.0560 The uncertainty is less in Abu Dhabi because it rarely rains, there. If you have sun, rain and snow, the entropy for weather is: p &lt;- c(.7, .15, .15) -sum(p * log(p)) ## [1] 0.8188085 6.2.3 From entropy to accuracy. The formula for the Kullback-Leibler divergence (i.e., K-L divergence) is \\[D_{\\text{KL}} (p, q) = \\sum_i p_i \\big ( \\text{log} (p_i) - \\text{log} (q_i) \\big ) = \\sum_i p_i \\text{log} \\Bigg ( \\frac{p_i}{q_i} \\Bigg )\\] which, in plainer language, is what McElreath described as “the average difference in log probability between the target (p) and model (q)” (p. 179). In McElreath’s example \\(p_1 = .3\\) \\(p_2 = .7\\) \\(q_1 = .25\\) \\(q_2 = .75\\) With those values, we can compute \\(D_{\\text{KL}} (p, q)\\) within a tibble like so: tibble(p_1 = .3, p_2 = .7, q_1 = .25, q_2 = .75) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.25 0.75 0.00640 Our systems in this section are binary (e.g., \\(q = \\lbrace q_i, q_2 \\rbrace\\)). Thus if you know \\(q_1 = .3\\) you know of a necessity \\(q_2 = 1 - q_1\\). Therefore we can code the tibble for the next example of when \\(p = q\\) like this: tibble(p_1 = .3) %&gt;% mutate(p_2 = 1 - p_1, q_1 = p_1) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.3 0.7 0 Building off of that, you can make the data required for Figure 6.6 like this. t &lt;- tibble(p_1 = .3, p_2 = .7, q_1 = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) head(t) ## # A tibble: 6 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.01 0.99 0.778 ## 2 0.3 0.7 0.02 0.98 0.577 ## 3 0.3 0.7 0.03 0.97 0.462 ## 4 0.3 0.7 0.04 0.96 0.383 ## 5 0.3 0.7 0.05 0.95 0.324 ## 6 0.3 0.7 0.06 0.94 0.276 Now we have the data, plotting Figure 6.6 is a just geom_line() with stylistic flourishes. t %&gt;% ggplot(aes(x = q_1, y = d_kl)) + geom_vline(xintercept = .3, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1.5) + annotate(geom = &quot;text&quot;, x = .4, y = 1.5, label = &quot;q = p&quot;, color = carto_pal(7, &quot;BurgYl&quot;)[5], family = &quot;Courier&quot;, size = 3.5) + labs(x = &quot;q[1]&quot;, y = &quot;Divergence of q from p&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.2.3.1 Rethinking: Divergence depends upon direction. Here we see \\(H(p, q) \\neq H(q, p)\\). That is, direction matters. tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 2 x 6 ## direction p_1 q_1 p_2 q_2 d_kl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 ## 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 The \\(D_{\\text{KL}}\\) was double when applying Martian estimates to Terran estimates. 6.2.4 From divergence to deviance. The point of all the preceding material about information theory and divergence is to establish both: How to measure the distance of a model from our target. Information theory gives us the distance measure we need, the K-L divergence. How to estimate the divergence. Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks. (p. 181) Now we’ll start working on item #2. We define deviance as: \\[D(q) = -2 \\sum_i \\text{log}(p_i)\\] In the formula, \\(i\\) indexes each case and \\(q_i\\) is the likelihood for each case. Here’s the deviance from model b6.1. lm(data = d, brain ~ mass) %&gt;% logLik() * -2 ## &#39;log Lik.&#39; 94.92499 (df=3) 6.2.4.1 Overthinking: Computing deviance. To follow along with the text, we’ll need to standardize mass before we compute deviance. d &lt;- d %&gt;% mutate(mass_s = (mass - mean(mass)) / sd(mass)) Open brms. library(brms) Now we’ll specify the initial values and fit the model. # Here we specify our starting values inits &lt;- list(Intercept = mean(d$brain), mass_s = 0, sigma = sd(d$brain)) inits_list &lt;-list(inits, inits, inits, inits) # The model b6.8 &lt;- brm(data = d, family = gaussian, brain ~ 1 + mass_s, prior = c(prior(normal(0, 1000), class = Intercept), prior(normal(0, 1000), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list) # Here we put our start values in the `brm()` function print(b6.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: brain ~ 1 + mass_s ## Data: d (Number of observations: 7) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 707.15 104.52 499.72 915.95 2665 1.00 ## mass_s 221.84 112.93 -0.11 449.05 2677 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 264.34 89.85 151.07 490.80 1764 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Details about inits: You don’t have to specify your inits lists outside of the brm() function the way we did, here. This is just how I currently prefer. When you specify start values for the parameters in your Stan models, you need to do so with a list of lists. You need as many lists as HMC chains—four in this example. And then you put your—in this case—four lists inside a list. Lists within lists. Also, we were lazy and specified the same start values across all our chains. You can mix them up across chains if you want. Anyway, the brms function log_lik() returns a matrix. Each occasion gets a column and each HMC chain iteration gets a row. dfLL &lt;- b6.8 %&gt;% log_lik() %&gt;% as_tibble() dfLL %&gt;% glimpse() ## Observations: 4,000 ## Variables: 7 ## $ V1 &lt;dbl&gt; -6.728868, -6.675144, -5.946516, -7.738674, -8.629742, -7.624607, -7.274391, -7.1552... ## $ V2 &lt;dbl&gt; -6.700834, -6.635752, -6.097825, -7.697466, -8.665618, -7.582775, -7.237944, -7.0634... ## $ V3 &lt;dbl&gt; -6.793648, -6.660125, -7.637703, -7.167247, -7.695637, -7.092361, -7.009797, -6.7144... ## $ V4 &lt;dbl&gt; -6.743621, -6.656155, -5.940528, -7.385453, -7.714939, -7.303445, -7.151864, -7.0446... ## $ V5 &lt;dbl&gt; -6.849665, -6.634574, -6.915150, -6.807039, -6.530817, -6.792465, -6.958994, -6.8380... ## $ V6 &lt;dbl&gt; -6.853959, -6.610865, -7.386137, -6.749870, -6.730088, -6.754827, -6.939791, -6.7357... ## $ V7 &lt;dbl&gt; -7.723946, -8.375804, -10.382664, -7.729027, -8.616605, -7.805456, -7.742376, -7.622... Deviance is the sum of the occasion-level LLs multiplied by -2. dfLL &lt;- dfLL %&gt;% mutate(sums = rowSums(.), deviance = -2*sums) Because we used HMC, deviance is a distribution rather than a single number. library(tidybayes) dfLL %&gt;% ggplot(aes(x = deviance, y = 0)) + geom_halfeyeh(fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7], point_interval = median_qi, .width = .95) + scale_x_continuous(breaks = quantile(dfLL$deviance, c(.025, .5, .975)), labels = quantile(dfLL$deviance, c(.025, .5, .975)) %&gt;% round(1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;The deviance distribution&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) But notice our deviance distribution was centered right around the sole value McElreath reported in the text. 6.2.5 From deviance to out-of-sample. Deviance is a principled way to measure distance from the target. But deviance as computed in the previous section has the same flaw as \\(R^2\\): It always improves as the model gets more complex, at least for the types of models we have considered so far. Just like \\(R^2\\), deviance in-sample is a measure of retrodictive accuracy, not predictive accuracy. In the next subsection, we’ll see this in a simulation. 6.2.5.1 Overthinking: Simulated training and testing. I find the rethinking::sim.train.test() function opaque. If you’re curious, you can find McElreath’s code here. Let’s simulate and see what happens. library(rethinking) N &lt;- 20 kseq &lt;- 1:5 # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 n_cores &lt;- 4 # here&#39;s our dev object based on `N &lt;- 20` dev_20 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = N, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) # here&#39;s our dev object based on N &lt;- 100 N &lt;- 100 dev_100 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = N, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) If you didn’t quite catch it, the simulation yields dev_20 and dev_100. We’ll want to convert them to tibbles, bind them together, and wrangle extensively before we’re ready to plot. dev_tibble &lt;- dev_20 %&gt;% as_tibble() %&gt;% bind_rows( dev_100 %&gt;% as_tibble() ) %&gt;% mutate(N = rep(c(&quot;N = 20&quot;, &quot;N = 100&quot;), each = 4), statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 2)) %&gt;% gather(n_par, value, -N, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(N = factor(N, levels = c(&quot;N = 20&quot;, &quot;N = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .075, n_par + .075)) head(dev_tibble) ## # A tibble: 6 x 5 ## N sample n_par mean sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 N = 100 in 0.925 283. 14.1 ## 2 N = 100 in 1.92 279. 13.9 ## 3 N = 100 in 2.92 263. 11.1 ## 4 N = 100 in 3.92 263. 11.2 ## 5 N = 100 in 4.92 262. 11.2 ## 6 N = 100 out 1.08 285. 14.4 Now we’re ready to make Figure 6.7. # this intermediary tibble will make `geom_text()` easier dev_text &lt;- dev_tibble %&gt;% filter(n_par &gt; 1.5, n_par &lt; 2.5) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .2, n_par + .28)) # the plot dev_tibble %&gt;% ggplot(aes(x = n_par, y = mean, ymin = mean - sd, ymax = mean + sd, group = sample, color = sample, fill = sample)) + geom_pointrange(shape = 21) + geom_text(data = dev_text, aes(label = sample)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[5], carto_pal(7, &quot;BurgYl&quot;)[7])) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~N, scale = &quot;free_y&quot;) Even with a substantially smaller \\(N\\), our simulation results matched up well with those in the text. 6.3 Regularization The root of overfitting is a model’s tendency to get overexcited by the training sample… One way to prevent a model from getting too excited by the training sample is to give it a skeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample. (p. 186) In case you were curious, here’s how you might do Figure 6.8 with ggplot2. All the action is in the geom_ribbon() portions. tibble(x = seq(from = - 3.5, to = 3.5, by = .01)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 0.2)), fill = carto_pal(7, &quot;BurgYl&quot;)[7], alpha = 1/2) + geom_ribbon(aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 0.5)), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2) + geom_ribbon(aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = carto_pal(7, &quot;BurgYl&quot;)[5], alpha = 1/2) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;parameter value&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) In our version of the plot, darker purple = more regularizing. But to prepare for Figure 6.9, let’s simulate. This time we’ll wrap the basic simulation code we used before into a function we’ll call make_sim(). Our make_sim() function has two parameters, N and b_sigma, both of which come from McElreath’s simulation code. So you’ll note that instead of hard coding the values for N and b_sigma within the simulation, we’re leaving them adjustable (i.e., sim.train.test(N = N, k = k, b_sigma = b_sigma)). Also notice that instead of saving the simulation results as objects, like before, we’re just converting them to tibbles with the as_tibble() function at the bottom. Our goal is to use make_sim() within a purrr::map2() statement. The result will be a nested tibble into which we’ve saved the results of 6 simulations based off of two sample sizes (i.e., N = c(20, 100)) and three values of \\(\\sigma\\) for our Gaussian \\(\\beta\\) prior (i.e., b_sigma = c(1, .5, .2)). library(rethinking) # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 make_sim &lt;- function(N, b_sigma){ sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = N, k = k, b_sigma = b_sigma), # this is an augmented line of code mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) %&gt;% # this is a new line of code as_tibble() } s &lt;- tibble(N = rep(c(20, 100), each = 3), b_sigma = rep(c(1, .5, .2), times = 2)) %&gt;% mutate(sim = map2(N, b_sigma, make_sim)) %&gt;% unnest() We’ll follow the same principles for wrangling these data as we did those from the previous simulation, dev_tibble. And after wrangling, we’ll feed the data directly into the code for our version of Figure 6.9. # wrangle the simulation data s %&gt;% mutate(statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 3 * 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 3 * 2)) %&gt;% gather(n_par, value, -N, -b_sigma, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(N = str_c(&quot;N = &quot;, N) %&gt;% factor(., levels = c(&quot;N = 20&quot;, &quot;N = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% # now plot ggplot(aes(x = n_par, y = mean, group = interaction(sample, b_sigma))) + geom_line(aes(color = sample, size = b_sigma %&gt;% as.character())) + # this function contains the data from the previous simulation geom_point(data = dev_tibble, aes(x = n_par, y = mean, group = sample, fill = sample), color = &quot;black&quot;, shape = 21, size = 2.5, stroke = .1) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_size_manual(values = c(1, .5, .2)) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~N, scale = &quot;free_y&quot;) Our results don’t perfectly align with those in the text. I suspect his is because we used 1e3 iterations, rather than the 1e4 of the text. If you’d like to wait all night long for the simulation to yield more stable results, be my guest. Regularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. So to use them effectively, you need some way to tune them. Tuning them isn’t always easy. (p. 187) For more on this how to choose your priors, consider Gelman, Simpson, and Betancourt’s The prior can generally only be understood in the context of the likelihood, a paper that will probably make more sense after Chapter 8. And if you’re feeling feisty, also check out Simpson’s related blog post (It’s never a) Total Eclipse of the Prior. 6.3.0.1 Rethinking: Ridge regression. Within the brms framework, you can do something like this with the horseshoe prior via the horseshoe() function. You can learn all about it from the horseshoe section of the brms reference manual (version 2.5.0). Here’s an extract from the section: The horseshoe prior is a special shrinkage prior initially proposed by Carvalho et al. (2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is nonzero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(&quot;horseshoe(1)&quot;). (p. 68) 6.4 Information criteria The data from our initial simulation isn’t formatted well to plot Figure 6.10. We’ll have to wrangle a little. ( dev_tibble &lt;- dev_tibble %&gt;% select(-sd) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par + .075, n_par - .075)) %&gt;% spread(key = sample, value = mean) %&gt;% mutate(height = (out - `in`) %&gt;% round(digits = 1) %&gt;% as.character(), dash = `in` + 2 * n_par) ) ## # A tibble: 10 x 6 ## N n_par `in` out height dash ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N = 20 1 55.6 57.4 1.8 57.6 ## 2 N = 20 2 54.4 58.1 3.7 58.4 ## 3 N = 20 3 50.6 55.9 5.3 56.6 ## 4 N = 20 4 49.5 57.7 8.1 57.5 ## 5 N = 20 5 49.0 58.8 9.8 59.0 ## 6 N = 100 1 283. 285. 2.6 285. ## 7 N = 100 2 279. 283. 3.6 283. ## 8 N = 100 3 263. 268. 5.2 269. ## 9 N = 100 4 263. 269. 6 271. ## 10 N = 100 5 262. 270. 8 272. Now we’re ready to plot. dev_tibble %&gt;% ggplot(aes(x = n_par)) + geom_line(aes(y = dash), linetype = 2, color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_point(aes(y = `in`), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 2) + geom_point(aes(y = out), color = carto_pal(7, &quot;BurgYl&quot;)[5], size = 2) + geom_errorbar(aes(x = n_par + .15, ymin = `in`, ymax = out), width = .1, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_text(aes(x = n_par + .4, y = (out + `in`) / 2, label = height), family = &quot;Courier&quot;, size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) + facet_wrap(~N, scale = &quot;free_y&quot;) Again, our numbers aren’t the exact same as McElreath’s because a) this is a simulation and b) our number of simulations was an order of magnitude smaller than his. But the overall pattern is the same. More to the point, the distances between the in- and out-of-sample points are nearly the same, for each model, at both \\(N = 20\\) (left) and \\(N = 100\\) (right). Each distance is nearly twice the number of parameters, as labeled on the horizontal axis. The dashed lines show exactly the [dark purple] points plus twice the number of parameters, tracing closely along the average out-of-sample deviance for each model. This is the phenomenon behind information criteria. (p. 189) In the text, McElreach focused on the DIC and WAIC. As you’ll see, the LOO has increased in popularity since he published the text. Going forward, we’ll juggle the WAIC and the LOO in this project. But we will respect the text and work in a little DIC talk. 6.4.1 DIC. The DIC has been widely used for some time, now. For a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. If we define \\(D\\) as the deviance’s posterior distribution, \\(\\bar{D}\\) as its mean and \\(\\hat{D}\\) as the deviance when computed at the posterior mean, then we define the DIC as \\[\\text{DIC} = \\bar{D} + (\\bar{D} + \\hat{D}) + \\bar{D} + p_D\\] And \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, I’m not aware of a way to that brms or the loo package–to be introduced shortly–offer convenience functions that yield the DIC. 6.4.2 WAIC. It’s okay that the brms and loo packages don’t yield the DIC because even better than the DIC is the Widely Applicable Information Criterion (WAIC)… Define \\(\\text{Pr} (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters sampled from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, lppd: \\[\\text{lppd} = \\sum_{i = 1}^N \\text{log Pr} (y_i)\\] You might say this out loud as: The log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation. … The second piece of WAIC is the effect number of parameters \\(p_{\\text{WAIC}}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood for observation \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_{\\text{WAIC}}\\) is defined as: \\[p_{\\text{WAIC}} = \\sum_{i=1}^N V (y_i)\\] Now WAIC is defined as: \\[\\text{WAIC} = -2 (\\text{lppd} - p_{\\text{WAIC}})\\] And this value is yet another estimate of out-of-sample deviance. (pp. 191–192) You’ll see how to compute the WAIC in brms in just a bit. 6.4.2.1 Overthinking: WAIC calculation. Here is how to fit the pre-WAIC model in brms. data(cars) b &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 30), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) print(b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: dist ~ 1 + speed ## Data: cars (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -17.36 7.04 -31.08 -3.98 2025 1.00 ## speed 3.92 0.43 3.07 4.75 1869 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 15.74 1.61 12.88 19.15 1405 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In brms, you return the loglikelihood with log_lik(). dfLL &lt;- b %&gt;% log_lik() %&gt;% as_tibble() Computing the lppd, the “Bayesian deviance”, takes a bit of leg work. dfmean &lt;- dfLL %&gt;% exp() %&gt;% summarise_all(mean) %&gt;% gather(key, means) %&gt;% select(means) %&gt;% log() ( lppd &lt;- dfmean %&gt;% sum() ) ## [1] -206.6651 Comupting the effective number of parameters, \\(p_{\\text{WAIC}}\\), isn’t much better. dfvar &lt;- dfLL %&gt;% summarise_all(var) %&gt;% gather(key, vars) %&gt;% select(vars) pwaic &lt;- dfvar %&gt;% sum() pwaic ## [1] 3.207354 Finally, here’s what we’ve been working so hard for: our hand calculated WAIC value. Compare it to the value returned by the brms waic() function. -2 * (lppd - pwaic) ## [1] 419.7449 waic(b) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -209.9 6.4 ## p_waic 3.2 1.2 ## waic 419.7 12.7 Here’s how we get the WAIC standard error. dfmean %&gt;% mutate(waic_vec = -2 * (means - dfvar$vars)) %&gt;% summarise(waic_se = (var(waic_vec) * nrow(dfmean)) %&gt;% sqrt()) ## # A tibble: 1 x 1 ## waic_se ## &lt;dbl&gt; ## 1 12.7 6.4.3 DIC and WAIC as estimates of deviance. Once again, we’ll wrap McElreath’s sim.train.test()-based simulation code within a custom function, make_sim(). This time we’ve adjusted make_sim() to take one argument, b_sigma. We will then feed that value into the same-named argument within sim.train.test(). Also notice that within sim.train.test(), we’ve specified TRUE for the information criteria and deviance arguments. Be warned: it takes extra time to compute the WAIC. Because we do that for every model, this simulation takes longer than the previous ones. To get a taste, try running it with something like n_sim &lt;- 5 first. n_sim &lt;- 1e3 make_sim &lt;- function(b_sigma){ sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = 20, k = k, b_sigma = b_sigma, DIC = T, WAIC = T, devbar = T, devbarout = T), mc.cores = n_cores); c(dev_in = mean(r[1, ]), dev_out = mean(r[2, ]), DIC = mean(r[3, ]), WAIC = mean(r[4, ]), devbar = mean(r[5, ]), devbarout = mean(r[6, ])) } ) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% rename(statistic = rowname) } s &lt;- tibble(b_sigma = c(100, .5)) %&gt;% mutate(sim = purrr::map(b_sigma, make_sim)) %&gt;% unnest() Here we wrangle and plot. s %&gt;% gather(n_par, value, -b_sigma, -statistic) %&gt;% mutate(n_par = str_remove(n_par, &quot;X&quot;) %&gt;% as.double()) %&gt;% filter(statistic != &quot;devbar&quot; &amp; statistic != &quot;devbarout&quot;) %&gt;% spread(key = statistic, value = value) %&gt;% gather(ic, value, -b_sigma, -n_par, -dev_in, -dev_out) %&gt;% gather(sample, deviance, -b_sigma, -n_par, -ic, -value) %&gt;% filter(sample == &quot;dev_out&quot;) %&gt;% mutate(b_sigma = b_sigma %&gt;% as.character()) %&gt;% ggplot(aes(x = n_par)) + geom_point(aes(y = deviance, color = b_sigma), size = 2.5) + geom_line(aes(y = value, group = b_sigma, color = b_sigma)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + # scale_color_manual(values = c(&quot;steelblue&quot;, &quot;black&quot;)) + labs(subtitle = &quot;N = 20&quot;, x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), legend.position = &quot;none&quot;) + facet_wrap(~ic, ncol = 1) And again, our results don’t perfectly match those in the text because a) we’re simulating and b) we used fewer iterations than McElreath did. But the overall pattern remains. 6.5 Using information criteria In contrast to model selection, “this section provides a brief example of model comparison and averaging” (p. 195, emphasis in the original). 6.5.1 Model comparison. Load the milk data from earlier in the text. library(rethinking) data(milk) d &lt;- milk %&gt;% filter(complete.cases(.)) rm(milk) d &lt;- d %&gt;% mutate(neocortex = neocortex.perc / 100) The dimensions of d are: dim(d) ## [1] 17 9 Load brms. detach(package:rethinking, unload = T) library(brms) We’re ready to fit the competing kcal.per.g models. Note our use of update() in the last two models. inits &lt;- list(Intercept = mean(d$kcal.per.g), sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.11 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, sigma = sd(d$kcal.per.g)) b6.12 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(-1000, 1000), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list) inits &lt;- list(Intercept = mean(d$kcal.per.g), `log(mass)` = 0, sigma = sd(d$kcal.per.g)) b6.13 &lt;- update(b6.12, newdata = d, formula = kcal.per.g ~ 1 + log(mass), inits = inits_list) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, `log(mass)` = 0, sigma = sd(d$kcal.per.g)) b6.14 &lt;- update(b6.13, newdata = d, formula = kcal.per.g ~ 1 + neocortex + log(mass), inits = inits_list) 6.5.1.1 Comparing WAIC values. In brms, you can get a model’s WAIC value with either WAIC() or waic(). WAIC(b6.14) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 8.2 2.6 ## p_waic 3.3 0.9 ## waic -16.4 5.2 ## Warning: 2 (11.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. waic(b6.14) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 8.2 2.6 ## p_waic 3.3 0.9 ## waic -16.4 5.2 ## Warning: 2 (11.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. Note the warning messages. Statisticians have made notable advances in Bayesian information criteria since McElreath published Statistical Rethinking. I won’t go into detail here, but the “We recommend trying loo instead” part of the message is designed to prompt us to use a different information criteria, the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; aka, the LOO). In brms this is available with the loo() function, which you can learn more about in this vignette from the makers of the loo package. For now, back to the WAIC. There are two basic ways to compare WAIC values from multiple models. In the first, you add more model names into the waic() function. waic(b6.11, b6.12, b6.13, b6.14) ## WAIC SE ## b6.11 -8.67 3.74 ## b6.12 -6.96 3.23 ## b6.13 -8.73 4.22 ## b6.14 -16.42 5.20 ## b6.11 - b6.12 -1.71 1.15 ## b6.11 - b6.13 0.06 2.30 ## b6.11 - b6.14 7.75 4.99 ## b6.12 - b6.13 1.77 2.91 ## b6.12 - b6.14 9.46 5.06 ## b6.13 - b6.14 7.69 3.62 Alternatively, you first save each model’s waic() output in its own object, and then feed to those objects into compare_ic(). w_b6.11 &lt;- waic(b6.11) w_b6.12 &lt;- waic(b6.12) w_b6.13 &lt;- waic(b6.13) w_b6.14 &lt;- waic(b6.14) compare_ic(w_b6.11, w_b6.12, w_b6.13, w_b6.14) ## WAIC SE ## b6.11 -8.67 3.74 ## b6.12 -6.96 3.23 ## b6.13 -8.73 4.22 ## b6.14 -16.42 5.20 ## b6.11 - b6.12 -1.71 1.15 ## b6.11 - b6.13 0.06 2.30 ## b6.11 - b6.14 7.75 4.99 ## b6.12 - b6.13 1.77 2.91 ## b6.12 - b6.14 9.46 5.06 ## b6.13 - b6.14 7.69 3.62 If you want to get those WAIC weights, you can use the brms::model_weights() function like so: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b6.11 b6.12 b6.13 b6.14 ## 0.02 0.01 0.02 0.95 That last round() line was just to limit the decimal-place precision. If you really wanted to go through the trouble, you could make yourself a little table like this: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% as_tibble() %&gt;% rename(weight = value) %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), weight = weight %&gt;% round(digits = 2)) %&gt;% select(model, weight) %&gt;% arrange(desc(weight)) %&gt;% knitr::kable() model weight b6.14 0.95 b6.11 0.02 b6.13 0.02 b6.12 0.01 Our table didn’t quite reproduce the table McElreath’s compare() function returns. However, José Roberto has provided the code that replicates McElreath’s compare(). It gives us a new function, which he calls compare_waic() # ... are the fitted models compare_waic &lt;- function (..., sort = &quot;WAIC&quot;, func = &quot;WAIC&quot;) { mnames &lt;- as.list(substitute(list(...)))[-1L] L &lt;- list(...) if (is.list(L[[1]]) &amp;&amp; length(L) == 1) {L &lt;- L[[1]]} classes &lt;- as.character(sapply(L, class)) if (any(classes != classes[1])) { warning(&quot;Not all model fits of same class.\\nThis is usually a bad idea, because it implies they were fit by different algorithms.\\nCheck yourself, before you wreck yourself.&quot;) } nobs_list &lt;- try(sapply(L, nobs)) if (any(nobs_list != nobs_list[1])) { nobs_out &lt;- paste(mnames, nobs_list, &quot;\\n&quot;) nobs_out &lt;- concat(nobs_out) warning(concat(&quot;Different numbers of observations found for at least two models.\\nInformation criteria only valid for comparing models fit to exactly same observations.\\nNumber of observations for each model:\\n&quot;, nobs_out)) } dSE.matrix &lt;- matrix(NA, nrow = length(L), ncol = length(L)) if (func == &quot;WAIC&quot;) { WAIC.list &lt;- lapply(L, function(z) WAIC(z, pointwise = TRUE)) ### The next three lines have been augmented from Roberto&#39;s code p.list &lt;- sapply(WAIC.list, function(x) x$estimates[&quot;p_waic&quot;, &quot;Estimate&quot;]) se.list &lt;- sapply(WAIC.list, function(x) x$estimates[&quot;waic&quot;, &quot;SE&quot;]) IC.list &lt;- sapply(WAIC.list, function(x) x$estimates[&quot;waic&quot;, &quot;Estimate&quot;]) #mnames &lt;- sapply(WAIC.list, function(x) x$model_name) colnames(dSE.matrix) &lt;- mnames rownames(dSE.matrix) &lt;- mnames for (i in 1:(length(L) - 1)) { for (j in (i + 1):length(L)) { waic_ptw1 &lt;- WAIC.list[[i]]$pointwise[ , 3] waic_ptw2 &lt;- WAIC.list[[j]]$pointwise[ , 3] dSE.matrix[i, j] &lt;- as.numeric(sqrt(length(waic_ptw1) * var(waic_ptw1 - waic_ptw2))) dSE.matrix[j, i] &lt;- dSE.matrix[i, j] } } } #if (!(the_func %in% c(&quot;DIC&quot;, &quot;WAIC&quot;, &quot;LOO&quot;))) { # IC.list &lt;- lapply(L, function(z) func(z)) #} IC.list &lt;- unlist(IC.list) dIC &lt;- IC.list - min(IC.list) w.IC &lt;- rethinking::ICweights(IC.list) if (func == &quot;WAIC&quot;) { topm &lt;- which(dIC == 0) dSEcol &lt;- dSE.matrix[, topm] result &lt;- data.frame(WAIC = IC.list, pWAIC = p.list, dWAIC = dIC, weight = w.IC, SE = se.list, dSE = dSEcol) } #if (!(the_func %in% c(&quot;DIC&quot;, &quot;WAIC&quot;, &quot;LOO&quot;))) { # result &lt;- data.frame(IC = IC.list, dIC = dIC, weight = w.IC) #} rownames(result) &lt;- mnames if (!is.null(sort)) { if (sort != FALSE) { if (sort == &quot;WAIC&quot;) sort &lt;- func result &lt;- result[order(result[[sort]]), ] } } new(&quot;compareIC&quot;, output = result, dSE = dSE.matrix) } Our new compare_waic() returns the following: compare_waic(b6.11, b6.12, b6.13, b6.14) ## WAIC pWAIC dWAIC weight SE dSE ## b6.14 -16.4 3.3 0.0 0.95 5.20 NA ## b6.13 -8.7 2.1 7.7 0.02 4.22 3.62 ## b6.11 -8.7 1.4 7.7 0.02 3.74 4.99 ## b6.12 -7.0 2.0 9.5 0.01 3.23 5.06 Without Roberto’s compare_waic(), I’m not aware of a convenient way to plot the WAIC comparisons of brms models the way McElreath does with rethinking. However, one can get the basic comparison plot with a little data processing. It helps to examine the structure of your WAIC objects. For example: glimpse(w_b6.11) ## List of 9 ## $ estimates : num [1:3, 1:2] 4.33 1.36 -8.67 1.87 0.3 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:3] &quot;elpd_waic&quot; &quot;p_waic&quot; &quot;waic&quot; ## .. ..$ : chr [1:2] &quot;Estimate&quot; &quot;SE&quot; ## $ pointwise : num [1:17, 1:3] 0.265 0.144 0.572 -0.175 -0.441 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:3] &quot;elpd_waic&quot; &quot;p_waic&quot; &quot;waic&quot; ## $ elpd_waic : num 4.33 ## $ p_waic : num 1.36 ## $ waic : num -8.67 ## $ se_elpd_waic: num 1.87 ## $ se_p_waic : num 0.3 ## $ se_waic : num 3.74 ## $ model_name : chr &quot;b6.11&quot; ## - attr(*, &quot;dims&quot;)= int [1:2] 4000 17 ## - attr(*, &quot;class&quot;)= chr [1:3] &quot;ic&quot; &quot;waic&quot; &quot;loo&quot; ## - attr(*, &quot;yhash&quot;)= chr &quot;1a471e77d65d7aa4da83df3672e7305b2621b59a&quot; We can index the point estimate for model b6.11’s WAIC as w.b6.11$estimates[&quot;waic&quot;, &quot;Estimate&quot;] and the standard error as w.b6.11$estimates[&quot;waic&quot;, &quot;SE&quot;]. w_b6.11$estimates[&quot;waic&quot;, &quot;Estimate&quot;] ## [1] -8.669658 w_b6.11$estimates[&quot;waic&quot;, &quot;SE&quot;] ## [1] 3.738341 Alternatively, you could do w.b6.11$estimates[3, 1] and w.b6.11$estimates[3, 2]. But instead of indexing the elements piecemeal, we might make a function that converts waic()$estimates to a tidy tibble formatted towards the end of making plotting easy. We’ll call the function waic_tibble(). waic_tibble &lt;- function(...){ tibble(model_name = rep(waic(...)$model_name, times = 3)) %&gt;% bind_cols( waic(...)$estimates %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% rename(statistic = rowname, estimate = Estimate, se = SE) ) } # for example waic_tibble(b6.11) ## # A tibble: 3 x 4 ## model_name statistic estimate se ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b6.11 elpd_waic 4.33 1.87 ## 2 b6.11 p_waic 1.36 0.300 ## 3 b6.11 waic -8.67 3.74 Now we’ll use waic_tibble() and bind_rows() to make a multi-model tibble suitable for plotting the WAIC estimates. waic_tibble(b6.11) %&gt;% bind_rows( waic_tibble(b6.12), waic_tibble(b6.13), waic_tibble(b6.14) ) %&gt;% filter(statistic == &quot;waic&quot;) %&gt;% ggplot(aes(x = model_name, y = estimate, ymin = estimate - se, ymax = estimate + se)) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;My custom WAIC plot&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) However, with Roberto’s compare_waic(), you can simply do: compare_waic(b6.14, b6.13, b6.12, b6.11) %&gt;% plot() That yielded a base R plot. If you’re stubborn and would like to stick with ggplot2, you could do something like: compare_waic(b6.11, b6.12, b6.13, b6.14)@output %&gt;% rownames_to_column() %&gt;% mutate(model = rowname) %&gt;% ggplot() + geom_pointrange(aes(x = model, y = WAIC, ymin = WAIC - SE, ymax = WAIC + SE), shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;Another custom WAIC plot&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) We briefly discussed the alternative information criteria, the LOO, above. Here’s how to use it in brms. LOO(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_loo 4.3 1.9 ## p_loo 1.4 0.3 ## looic -8.6 3.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. loo(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_loo 4.3 1.9 ## p_loo 1.4 0.3 ## looic -8.6 3.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. The Pareto \\(k\\) values are a useful model fit diagnostic tool, which we’ll discuss later. But for now, realize that brms uses functions from the loo package to compute its WAIC and LOO values. In addition to the vignette, above, this vignette demonstrates the LOO with these very same examples from McElreath’s text. And if you’d like to dive a little deeper, check out Aki Vehtari’s GPSS2017 workshop. Before we forget, McElreath gave some perspective difference between the models with the highest and lowest WAIC values (p. 200). (w_d &lt;- waic(b6.11, b6.14)) ## WAIC SE ## b6.11 -8.67 3.74 ## b6.14 -16.42 5.20 ## b6.11 - b6.14 7.75 4.99 If you use str() to look at the structure of our w_d object, you’ll see we can subset to the difference score portion with w_d$ic_diffs__. And since w_d$ic_diffs__ is a numeric vector, we can extract the two numerals with the [] notation. set.seed(6.26) diff &lt;- tibble(diff = rnorm(1e5 , mean = w_d$ic_diffs__[1], sd = w_d$ic_diffs__[2])) diff %&gt;% summarise(the_probability_a_difference_is_negative = sum(diff &lt; 0) / n()) ## # A tibble: 1 x 1 ## the_probability_a_difference_is_negative ## &lt;dbl&gt; ## 1 0.0601 In case you’re curious, this is a graphic version of what we just did. ggplot(data = tibble(diff = -20:30), aes(x = diff)) + geom_ribbon(aes(ymin = 0, ymax = dnorm(diff, w_d$ic_diffs__[1], w_d$ic_diffs__[2])), fill = carto_pal(7, &quot;BurgYl&quot;)[7]) + geom_ribbon(data = tibble(diff = -20:0), aes(ymin = 0, ymax = dnorm(diff, w_d$ic_diffs__[1], w_d$ic_diffs__[2])), fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_vline(xintercept = 0, linetype = 3, color = carto_pal(7, &quot;BurgYl&quot;)[3]) + scale_y_continuous(NULL, breaks = NULL) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.5.1.2 Comparing estimates. The brms package doesn’t have anything like rethinking’s coeftab() function. However, one can get that information with a little ingenuity. Here we’ll employ the broom::tidy() function, which will save the summary statistics for our model parameters. For example, this is what it will produce for the full model, b6.14. tidy(b6.14) ## term estimate std.error lower upper ## 1 b_Intercept -1.10778838 0.59753389 -2.07797336 -0.11671016 ## 2 b_neocortex 2.82815806 0.92990864 1.27706360 4.31866798 ## 3 b_logmass -0.09724996 0.02888818 -0.14293482 -0.04860256 ## 4 sigma 0.13995088 0.03072319 0.09899014 0.19629633 ## 5 lp__ -19.26708438 1.64611968 -22.48058767 -17.30314877 Note, tidy() also grabs the log posterior (i.e., “lp__“), which we’ll exclude for our purposes. With a rbind() and a little indexing, we can save the summaries for all four models in a single tibble. my_coef_tab &lt;- rbind(tidy(b6.11), tidy(b6.12), tidy(b6.13), tidy(b6.14)) %&gt;% mutate(model = c(rep(&quot;b6.11&quot;, times = nrow(tidy(b6.11))), rep(&quot;b6.12&quot;, times = nrow(tidy(b6.12))), rep(&quot;b6.13&quot;, times = nrow(tidy(b6.13))), rep(&quot;b6.14&quot;, times = nrow(tidy(b6.14)))) ) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% select(model, everything()) head(my_coef_tab) ## model term estimate std.error lower upper ## 1 b6.11 b_Intercept 0.6575194 0.04799216 0.5789882 0.7355497 ## 2 b6.11 sigma 0.1872545 0.03723175 0.1380815 0.2568375 ## 3 b6.12 b_Intercept 0.3715280 0.58935641 -0.5836231 1.3094683 ## 4 b6.12 b_neocortex 0.4225006 0.87013282 -0.9525745 1.8584446 ## 5 b6.12 sigma 0.1936972 0.04016163 0.1404194 0.2651822 ## 6 b6.13 b_Intercept 0.7036371 0.05920601 0.6071793 0.8007512 Just a little more work and we’ll have a table analogous to the one McElreath produced with his coef_tab() function. my_coef_tab %&gt;% # Learn more about dplyr::complete() here: https://rdrr.io/cran/tidyr/man/expand.html complete(term = distinct(., term), model) %&gt;% select(model, term, estimate) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) ## term b6.11 b6.12 b6.13 b6.14 ## 1 b_Intercept 0.66 0.37 0.70 -1.11 ## 2 b_logmass NA NA -0.03 -0.10 ## 3 b_neocortex NA 0.42 NA 2.83 ## 4 sigma 0.19 0.19 0.18 0.14 I’m also not aware of an efficient way in brms to reproduce Figure 6.12 for which McElreath nested his coeftab() argument in a plot() argument. However, one can build something similar by hand with a little data wrangling. p11 &lt;- posterior_samples(b6.11) p12 &lt;- posterior_samples(b6.12) p13 &lt;- posterior_samples(b6.13) p14 &lt;- posterior_samples(b6.14) # This block is just for intermediary information # colnames(p11) # colnames(p12) # colnames(p13) # colnames(p14) # Here we put it all together tibble(mdn = c(NA, median(p11[, 1]), median(p12[, 1]), median(p13[, 1]), median(p14[, 1]), NA, NA, median(p12[, 2]), NA, median(p14[, 2]), NA, median(p11[, 2]), NA, median(p13[, 2]), NA, NA, median(p11[, 2]), median(p12[, 3]), median(p13[, 3]), median(p14[, 4])), sd = c(NA, sd(p11[, 1]), sd(p12[, 1]), sd(p13[, 1]), sd(p14[, 1]), NA, NA, sd(p12[, 2]), NA, sd(p14[, 2]), NA, sd(p11[, 2]), NA, sd(p13[, 2]), NA, NA, sd(p11[, 2]), sd(p12[, 3]), sd(p13[, 3]), sd(p14[, 4])), order = c(20:1)) %&gt;% ggplot(aes(x = mdn, y = order)) + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(aes(x = order, y = mdn, ymin = mdn - sd, ymax = mdn + sd), shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + scale_x_continuous(breaks = 20:1, labels = c(&quot;intercept&quot;, 11:14, &quot;neocortex&quot;, 11:14, &quot;logmass&quot;, 11:14, &quot;sigma&quot;, 11:14)) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;My custom coeftab() plot&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Making that plot entailed a lot of hand typing values in the tibble, which just begs for human error. If possible, it’s better to use functions in a principled way to produce the results. Below is such an attempt. # data wrangling my_coef_tab &lt;- my_coef_tab %&gt;% complete(term = distinct(., term), model) %&gt;% rbind( tibble( model = NA, term = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;), estimate = NA, std.error = NA, lower = NA, upper = NA)) %&gt;% mutate(axis = ifelse(is.na(model), term, model), model = factor(model, levels = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;)), term = factor(term, levels = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;, NA))) %&gt;% arrange(term, model) %&gt;% mutate(axis_order = letters[1:20], axis = ifelse(str_detect(axis, &quot;b6.&quot;), str_c(&quot; &quot;, axis), axis)) # plot ggplot(data = my_coef_tab, aes(x = axis_order, y = estimate, ymin = lower, ymax = upper)) + theme_classic() + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + scale_x_discrete(NULL, labels = my_coef_tab$axis) + ggtitle(&quot;My other coeftab() plot&quot;) + coord_flip() + theme(text = element_text(family = &quot;Courier&quot;), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) I’m sure there are better ways to do this. Have at it–but do share you code when you figure it out. 6.5.1.3 Rethinking: Barplots suck. Man, I agree. “The only problem with barplots is that they have bars” (p. 203). You can find alternatives here, here, here, here, and a whole bunch here. 6.5.2 Model averaging. Within the current brms framework, you can do model-averaged predictions with the pp_average() function. The default weighting scheme is with the LOO. Here we’ll use the weights = &quot;waic&quot; argument to match McElreath’s method in the text. Because pp_average() yields a matrix, we’ll want to convert it to a tibble before feeding it into ggplot2. # we need new data for both the `fitted()` and `pp_average()` functions nd &lt;- tibble(neocortex = seq(from = .5, to = .8, length.out = 30), mass = rep(4.5, times = 30)) # we&#39;ll get the `b6.14`-implied trajectory with `fitted()` fitd_b6.14 &lt;- fitted(b6.14, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # the model-average trajectory comes from `pp_average()` pp_average(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;, method = &quot;fitted&quot;, # for new data predictions, use `method = &quot;predict&quot;` newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% # plot Figure 6.13 ggplot(aes(x = neocortex, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/4) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_ribbon(data = fitd_b6.14, aes(ymin = Q2.5, ymax = Q97.5), color = carto_pal(7, &quot;BurgYl&quot;)[5], fill = &quot;transparent&quot;, linetype = 2) + geom_line(data = fitd_b6.14, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_point(data = d, aes(x = neocortex, y = kcal.per.g), size = 2, color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(y = &quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(d$neocortex), ylim = range(d$kcal.per.g)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.6 Summary Bonus: \\(R^2\\) talk At the beginning of the chapter (pp. 167–168), McElreath briefly introduced \\(R^2\\) as a popular way to assess the variance explained in a model. He pooh-poohed it because of its tendency to overfit. It’s also limited in that it doesn’t generalize well outside of the single-level Gaussian framework. However, if you should find yourself in a situation where \\(R^2\\) suits your purposes, the brms bayes_R2() function might be of use. Simply feeding a model brm fit object into bayes_R2() will return the posterior mean, \\(SD\\), and 95% intervals. For example: bayes_R2(b6.14) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.502 0.133 0.163 0.669 With just a little data processing, you can get a tibble table of each of models’ \\(R^2\\) ‘Estimate’. rbind(bayes_R2(b6.11), bayes_R2(b6.12), bayes_R2(b6.13), bayes_R2(b6.14)) %&gt;% as_tibble() %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), r_square_posterior_mean = round(Estimate, digits = 2)) %&gt;% select(model, r_square_posterior_mean) ## # A tibble: 4 x 2 ## model r_square_posterior_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 b6.11 0 ## 2 b6.12 0.08 ## 3 b6.13 0.15 ## 4 b6.14 0.5 If you want the full distribution of the \\(R^2\\), you’ll need to add a summary = F argument. Note how this returns a numeric vector. r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) r2_b6.13 %&gt;% glimpse() ## num [1:4000, 1] 0.20441 0.00859 0.01229 0.39208 0.21113 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; If you want to use these in ggplot2, you’ll need to put them in tibbles or data frames. Here we do so for two of our model fits. # model b6.13 r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_13 = R2) # model b6.14 r2_b6.14 &lt;- bayes_R2(b6.14, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_14 = R2) # Let&#39;s put them in the same data object r2_combined &lt;- bind_cols(r2_b6.13, r2_b6.14) %&gt;% mutate(dif = r2_14 - r2_13) # Plot their densities r2_combined %&gt;% ggplot() + geom_density(aes(x = r2_13), fill = carto_pal(7, &quot;BurgYl&quot;)[4], alpha = 3/4, size = 0, ) + geom_density(aes(x = r2_14), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 3/4, size = 0, ) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + labs(x = NULL, title = expression(paste(italic(&quot;R&quot;)^{2}, &quot; distributions&quot;)), subtitle = &quot;Going from left to right, these are\\nfor models b6.13 and b6.14.&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If you do your work in a field where folks use \\(R^2\\) change, you might do that with a simple difference score, which we computed above with mutate(dif = R2.14 - R2.13). Here’s the \\(\\Delta R^2\\) (i.e., dif) plot: r2_combined %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7], point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(Delta, italic(&quot;R&quot;)^{2})), subtitle = &quot;This is how much more variance, in\\nterms of %, model b6.14 explained\\ncompared to model b6.13.&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) The brms package did not get these \\(R^2\\) values by traditional method used in, say, ordinary least squares estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out the paper by Gelman, Goodrich, Gabry, and Ali. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rstan_2.17.3 StanHeaders_2.17.2 tidybayes_1.0.1 brms_2.5.0 Rcpp_0.12.18 ## [6] gridExtra_2.3 broom_0.4.5 ggrepel_0.8.0 rcartocolor_0.0.22 forcats_0.3.0 ## [11] stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 ## [16] tibble_1.4.2 ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 mnormt_1.5-5 ## [22] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 ## [25] xfun_0.3 R6_2.2.2 markdown_0.8 ## [28] HDInterval_0.2.0 reshape_0.8.7 assertthat_0.2.0 ## [31] promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [34] gtable_0.2.0 rethinking_1.59 rlang_0.2.1 ## [37] extrafontdb_1.0 lazyeval_0.2.1 inline_0.3.15 ## [40] yaml_2.1.19 reshape2_1.4.3 abind_1.4-5 ## [43] modelr_0.1.2 threejs_0.3.1 crosstalk_1.0.0 ## [46] backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 ## [52] psych_1.8.4 RColorBrewer_1.1-2 ggridges_0.5.0 ## [55] plyr_1.8.4 base64enc_0.1-3 progress_1.2.0 ## [58] prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 ## [64] mvtnorm_1.0-8 matrixStats_0.54.0 hms_0.4.2 ## [67] shinyjs_1.0 mime_0.5 evaluate_0.10.1 ## [70] arrayhelpers_1.0-20160527 xtable_1.8-2 shinystan_2.5.0 ## [73] readxl_1.1.0 rstantools_1.5.0 compiler_3.5.1 ## [76] maps_3.3.0 crayon_1.3.4 htmltools_0.3.6 ## [79] later_0.7.3 lubridate_1.7.4 MASS_7.3-50 ## [82] Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [85] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [88] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 ## [91] vipor_0.4.5 rvest_0.3.2 digest_0.6.15 ## [94] rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [97] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [100] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 ## [103] pillar_1.2.3 lattice_0.20-35 loo_2.0.0 ## [106] httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["interactions.html", "7 Interactions 7.1 Building an interaction. 7.2 Symmetry of the linear interaction. 7.3 Continuous interactions 7.4 Interactions in design formulas 7.5 Summary Bonus: marginal_effects() Reference Session info", " 7 Interactions Every model so far in [McElreath’s text] has assumed that each predictor has an independent association with the mean of the outcome. What if we want to allow the association to be conditional?… To model deeper conditionality—where the importance of one predictor depends upon another predictor—we need interaction. Interaction is a kind of conditioning, a way of allowing parameters (really their posterior distributions) to be conditional on further aspects of the data. (p. 210) 7.1 Building an interaction. “Africa is special” (p. 211). Let’s load the rugged data to see one of the reasons why. library(rethinking) data(rugged) d &lt;- rugged And here we switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) rm(rugged) We’ll continue to use tidyverse-style syntax to wrangle the data. library(tidyverse) # make the log version of criterion d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) # extract countries with GDP data dd &lt;- d %&gt;% filter(complete.cases(rgdppc_2000)) # split countries into Africa and not-Africa d.A1 &lt;- dd %&gt;% filter(cont_africa == 1) d.A0 &lt;- dd %&gt;% filter(cont_africa == 0) The first two models predicting log_gdp are univariable. b7.1 &lt;- brm(data = d.A1, family = gaussian, log_gdp ~ 1 + rugged, prior = c(prior(normal(8, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4) b7.2 &lt;- update(b7.1, newdata = d.A0) In the text, McElreath more or less dared us to figure out how to make Figure 7.2. Here’s the brms-relevant data wrangling. nd &lt;- tibble(rugged = seq(from = 0, to = 6.3, length.out = 30)) fitd_b7.1 &lt;- fitted(b7.1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) fitd_b7.2 &lt;- fitted(b7.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # Here we&#39;ll put both in a single data object, with `fitd_b7.1` stacked atop `fitd_b7.2` fitd_both &lt;- full_join(fitd_b7.1, fitd_b7.2) %&gt;% mutate(cont_africa = rep(c(&quot;Africa&quot;, &quot;not Africa&quot;), each = 30)) For this chapter, we’ll take our plot theme from the ggthemes package, which you can learn more about here. # install.packages(&quot;ggthemes&quot;, dependencies = T) library(ggthemes) Here’s the plot code for our version of Figure 7.2. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged)) + geom_ribbon(data = fitd_both, aes(ymin = Q2.5, ymax = Q97.5, fill = cont_africa), alpha = 1/4) + geom_line(data = fitd_both, aes(y = Estimate, color = cont_africa)) + geom_point(aes(y = log_gdp, color = cont_africa), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) 7.1.1 Adding a dummy variable doesn’t work. Here’s our model with all the countries, but without the cont_africa dummy. b7.3 &lt;- update(b7.1, newdata = dd) Now we’ll add the dummy. b7.4 &lt;- update(b7.3, newdata = dd, formula = log_gdp ~ 1 + rugged + cont_africa) And here we can compare the two models with information criteria. waic(b7.3, b7.4) ## WAIC SE ## b7.3 539.48 12.99 ## b7.4 475.94 14.83 ## b7.3 - b7.4 63.54 14.61 loo(b7.3, b7.4) ## LOOIC SE ## b7.3 539.49 12.99 ## b7.4 475.97 14.84 ## b7.3 - b7.4 63.52 14.61 Happily, the WAIC and the LOO are in agreement. The model with the dummy, b7.4, fit the data much better. Here are the WAIC model weights. model_weights(b7.3, b7.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 ## 0 1 As in the text, almost all the weight goes to the multivariable model, b7.4. Before we can plot that model, we need to wrangle a bit. nd &lt;- tibble(rugged = seq(from = 0, to = 6.3, length.out = 30) %&gt;% rep(., times = 2), cont_africa = rep(0:1, each = 30)) fitd_b7.4 &lt;- fitted(b7.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) Behold our Figure 7.3. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged)) + geom_ribbon(data = fitd_b7.4, aes(ymin = Q2.5, ymax = Q97.5, fill = cont_africa, group = cont_africa), alpha = 1/4) + geom_line(data = fitd_b7.4, aes(y = Estimate, color = cont_africa, group = cont_africa)) + geom_point(aes(y = log_gdp, color = cont_africa), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = c(.69, .94), legend.title = element_blank(), legend.direction = &quot;horizontal&quot;) 7.1.2 Adding a linear interaction does work. Yes, it sure does. But before we fit, here’s the equation: \\[ \\begin{eqnarray} \\text{log_gdp}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = &amp; \\beta_1 + \\beta_3 \\text{cont_africa}_i \\end{eqnarray} \\] Fit the model. b7.5 &lt;- update(b7.4, formula = log_gdp ~ 1 + rugged*cont_africa) For kicks, we’ll just use the LOO to compare the last three models. loo(b7.3, b7.4, b7.5, cores = 4) # We can speed things up using the `cores` argument ## LOOIC SE ## b7.3 539.49 12.99 ## b7.4 475.97 14.84 ## b7.5 469.30 14.57 ## b7.3 - b7.4 63.52 14.61 ## b7.3 - b7.5 70.20 14.68 ## b7.4 - b7.5 6.68 5.86 And we can weigh the models based on the LOO rather than the WAIC, too. model_weights(b7.3, b7.4, b7.5, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 b7.5 ## 0.000 0.034 0.966 7.1.2.1 Overthinking: Conventional form of interaction. The conventional equation for the interaction model might look like: \\[ \\begin{eqnarray} \\text{log_gdp}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i \\end{eqnarray} \\] Instead of the y ~ 1 + x1*x2 approach, which will work fine with brm(), you can use this more explicit syntax. b7.5b &lt;- update(b7.5, formula = log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa) From here on, I will default to this style of syntax for interactions. Since this is the same model, it yields the same information criteria estimates within simulation error. waic(b7.5, b7.5b) ## WAIC SE ## b7.5 469.15 14.55 ## b7.5b 469.28 14.53 ## b7.5 - b7.5b -0.13 0.14 loo(b7.5, b7.5b) ## LOOIC SE ## b7.5 469.30 14.57 ## b7.5b 469.42 14.55 ## b7.5 - b7.5b -0.12 0.14 7.1.3 Plotting the interaction. Here’s our prep work for the figure. fitd_b7.5 &lt;- fitted(b7.5, newdata = nd) %&gt;% # we can use the same `nd` data from last time as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) And here’s the code for our version of Figure 7.4. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged)) + geom_ribbon(data = fitd_b7.5, aes(ymin = Q2.5, ymax = Q97.5, fill = cont_africa, group = cont_africa), alpha = 1/4) + geom_line(data = fitd_b7.5, aes(y = Estimate, color = cont_africa, group = cont_africa)) + geom_point(aes(y = log_gdp, color = cont_africa), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) 7.1.4 Interpreting an interaction estimate. Interpreting interaction estimates is tricky. It’s trickier than interpreting ordinary estimates. And for this reason, I usually advise against trying to understand an interaction from tables of numbers along. Plotting implied predictions does far more for both our own understanding and for our audience’s. 7.1.4.1 Parameters change meaning. In a simple linear regression with no interactions, each coefficient says how much the average outcome, \\(\\mu\\), changes when the predictor changes by one unit. And since all of the parameters have independent influences on the outcome, there’s no trouble in interpreting each parameter separately. Each slope parameter gives us a direct measure of each predictor variable’s influence. Interaction models ruin this paradise. (p. 220) Return the parameter estimates. posterior_summary(b7.5) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 9.1823382 0.13894772 8.90659529 9.45228293 ## b_rugged -0.1832468 0.07735517 -0.33161340 -0.03199895 ## b_cont_africa -1.8407667 0.22221419 -2.25894413 -1.40785674 ## b_rugged:cont_africa 0.3433508 0.13148168 0.08390145 0.59336056 ## sigma 0.9514394 0.05071302 0.85997921 1.05643893 ## lp__ -244.3853233 1.53536617 -248.18162453 -242.34387796 “Since \\(\\gamma\\) (gamma) doesn’t appear in this table—it wasn’t estimated—we have to compute it ourselves” (p. 221). Like in the text, we’ll do so first by working with the point estimates. # within Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 1 ## [1] 0.160104 # outside Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 0 ## [1] -0.1832468 7.1.4.2 Incorporating uncertainty. To get some idea of the uncertainty around those \\(\\gamma\\) values, we’ll need to use the whole posterior. Since \\(\\gamma\\) depends upon parameters, and those parameters have a posterior distribution, \\(\\gamma\\) must also have a posterior distribution. Read the previous sentence again a few times. It’s one of the most important concepts in processing Bayesian model fits. Anything calculated using parameters has a distribution. (p. 212) Like McElreath, we’ll avoid integral calcus in favor of working with the posterior_samples(). post &lt;- posterior_samples(b7.5) post %&gt;% transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% gather(key, value) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) ## # A tibble: 2 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 gamma_Africa 0.160 ## 2 gamma_notAfrica -0.183 And here is our version of Figure 7.5. post %&gt;% transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% gather(key, value) %&gt;% ggplot(aes(x = value, group = key, color = key, fill = key)) + geom_density(alpha = 1/4) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(expression(gamma), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Terraine Ruggedness slopes&quot;, subtitle = &quot;Blue = African nations, Green = others&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) What proportion of these differences is below zero? post %&gt;% mutate(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% mutate(diff = gamma_Africa -gamma_notAfrica) %&gt;% summarise(Proportion_of_the_difference_below_0 = sum(diff &lt; 0) / length(diff)) ## Proportion_of_the_difference_below_0 ## 1 0.00675 7.2 Symmetry of the linear interaction. Consider for example the GDP and terrain ruggedness problem. The interaction there has two equally valid phrasings. How much does the influence of ruggedness (on GDP) depend upon whether the nation is in Africa? How much does the influence of being in Africa (on GDP) depend upon ruggedness? While these two possibilities sound different to most humans, your golem thinks they are identical. (p. 223) 7.2.1 Buridan’s interaction. Recall the original equation. \\[ \\begin{eqnarray} \\text{log_gdp}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = &amp; \\beta_1 + \\beta_3 \\text{cont_africa}_i \\end{eqnarray} \\] Next McElreath replaced \\(\\gamma_i\\) with the expression for \\(\\mu_i\\). \\[ \\begin{eqnarray} \\mu_i &amp; = &amp; \\alpha + (\\beta_1 + \\beta_3 \\text{cont_africa}_i) \\times \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ &amp; = &amp; \\alpha + \\beta_1 \\text{rugged}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i + \\beta_2 \\text{cont_africa}_i \\end{eqnarray} \\] And new we’ll factor together the terms containing \\(\\text{cont_africa}_i\\). \\[\\mu_i = \\alpha + \\beta_1 \\text{rugged}_i + \\underbrace{(\\beta_2 + \\beta_3 \\text{rugged}_i)}_G \\times \\text{cont_africa}_i\\] And just as in the text, our \\(G\\) term looks a lot like the original \\(\\gamma_i\\) term. 7.2.2 Africa depends upon ruggedness. Here is our version of McElreath’s Figure 7.6. # new predictor data for `fitted()` nd &lt;- tibble(rugged = rep(range(dd$rugged), times = 2), cont_africa = rep(0:1, each = 2)) # `fitted()` fitd_b7.5 &lt;- fitted(b7.5, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(ox = rep(c(-0.05, 0.05), times = 2)) # augment the `dd` data a bit dd %&gt;% mutate(ox = ifelse(rugged &gt; median(rugged), 0.05, -0.05), cont_africa = cont_africa + ox) %&gt;% select(cont_africa, everything()) %&gt;% # plot ggplot(aes(x = cont_africa)) + geom_ribbon(data = fitd_b7.5, aes(ymin = Q2.5, ymax = Q97.5, fill = factor(ox), group = factor(ox)), alpha = 1/4) + geom_line(data = fitd_b7.5, aes(y = Estimate, color = factor(ox), group = factor(ox), linetype = factor(ox))) + geom_point(aes(y = log_gdp, color = factor(ox)), alpha = 1/2, shape = 1) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Continent&quot;, breaks = 0:1, labels = c(&quot;other&quot;, &quot;Africa&quot;)) + coord_cartesian(xlim = c(-.2, 1.2)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) 7.3 Continuous interactions Though continuous interactions can be more challenging to interpret, they’re just as easy to fit as interactions including dummies. 7.3.1 The data. Look at the tulips. library(rethinking) data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 7.3.2 The un-centered models. The likelihoods for the next two models are \\[ \\begin{eqnarray} \\text{blooms}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{water}_i + \\beta_2 \\text{shade}_i \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim &amp; \\text{Uniform} (0, 100) \\\\ \\end{eqnarray} \\] and \\[ \\begin{eqnarray} \\text{blooms}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{water} + \\beta_2 \\text{shade}_i + \\beta_3 \\text{water}_i \\times \\text{shade}_i\\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_3 &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim &amp; \\text{Uniform} (0, 100) \\\\ \\end{eqnarray} \\] Load brms. detach(package:rethinking, unload = T) library(brms) rm(tulips) Here we continue with McElreath’s very-flat priors for the multivariable and interaction models. b7.6 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water + shade, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4) ## Warning: There were 44 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: Examine the pairs() plot to diagnose sampling problems b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade) ## Warning: There were 6 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: Examine the pairs() plot to diagnose sampling problems Much like in the text, these models yielded divergent transitions. Here, we’ll try to combat them by following Stan’s advice and “[increase] adapt_delta above 0.8.” While we’re at it, we’ll put better priors on \\(\\sigma\\). b7.6 &lt;- update(b7.6, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), control = list(adapt_delta = 0.9)) b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade) Increasing adapt_delta did the trick. Instead of coeftab(), we can also use posterior_summary(), which gets us most of the way there. posterior_summary(b7.6) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 60.98 41.53 -22.32 140.61 ## b_water 74.12 14.22 46.06 101.96 ## b_shade -40.98 14.70 -69.54 -11.86 ## sigma 61.58 9.07 46.80 81.61 ## lp__ -169.76 1.49 -173.40 -167.86 posterior_summary(b7.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -106.32 64.25 -224.44 26.34 ## b_water 159.54 29.81 96.06 214.18 ## b_shade 43.50 29.84 -17.30 99.20 ## b_water:shade -42.98 13.71 -68.31 -14.15 ## sigma 49.85 7.57 37.48 66.87 ## lp__ -170.69 1.70 -174.88 -168.39 This is an example where HMC yielded point estimates notably different from MAP. However, look at the size of those posterior standard deviations (i.e., ‘Est.Error’ column)! The MAP estimates are well within a fraction of those \\(SD\\)s. Anyway, let’s look at WAIC. waic(b7.6, b7.7) ## WAIC SE ## b7.6 304.02 7.63 ## b7.7 293.84 8.06 ## b7.6 - b7.7 10.18 5.31 Why not compute the WAIC weights? model_weights(b7.6, b7.7, weights = &quot;waic&quot;) ## b7.6 b7.7 ## 0.006122731 0.993877269 As in the text, almost all the weight went to the interaction model, b7.7. 7.3.3 Center and re-estimate. Here’s a tidyverse way to center the predictors. d &lt;- d %&gt;% mutate(shade_c = shade - mean(shade), water_c = water - mean(water)) Now refit the models with our shiny new centered predictors. b7.8 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water_c + shade_c, prior = c(prior(normal(130, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9)) b7.9 &lt;- update(b7.8, formula = blooms ~ 1 + water_c + shade_c + water_c:shade_c) posterior_summary(b7.8) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 128.97 11.65 106.14 152.10 ## b_water_c 74.14 14.00 46.46 101.42 ## b_shade_c -40.86 14.46 -69.78 -12.13 ## sigma 61.27 8.65 47.27 81.31 ## lp__ -168.85 1.45 -172.60 -167.03 posterior_summary(b7.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 128.87 9.34 110.17 146.90 ## b_water_c 74.71 11.55 51.74 97.84 ## b_shade_c -41.01 11.88 -63.99 -17.91 ## b_water_c:shade_c -51.77 14.45 -80.74 -23.92 ## sigma 49.65 7.48 37.57 66.62 ## lp__ -168.56 1.70 -172.72 -166.30 And okay fine, if you really want a coeftab()-like summary, here’s a grotesque way to do it. # First, we reformat `b7.8` posterior_summary(b7.8) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% # Now we bind to it the rows of `b7.9` bind_rows( posterior_summary(b7.9) %&gt;% data.frame() %&gt;% rownames_to_column() ) %&gt;% # Miscellaneous wrangling mutate(model = c(rep(&quot;b7.8&quot;, times = 5), rep(&quot;b7.9&quot;, times = 6))) %&gt;% select(rowname, Estimate, model) %&gt;% filter(rowname != &quot;lp__&quot;) %&gt;% spread(key = model, value = Estimate) %&gt;% rename(parameter = rowname) %&gt;% mutate_if(is.double, round, digits = 2) ## parameter b7.8 b7.9 ## 1 b_Intercept 128.97 128.87 ## 2 b_shade_c -40.86 -41.01 ## 3 b_water_c 74.14 74.71 ## 4 b_water_c:shade_c NA -51.77 ## 5 sigma 61.27 49.65 Anyway, centering helped a lot. Now, not only do the results in the text match up better than those from Stan, but the ‘Est.Error’ values are uniformly smaller. 7.3.3.1 Estimation worked better. Nothing to add, here. 7.3.3.2 Estimates changed less across models. k &lt;- fixef(b7.7) k[1] + k[2]*2 + k[3]*2 + k[4]*2*2 ## [1] 127.8318 k &lt;- fixef(b7.9) k[1] + k[2]*0 + k[3]*0 + k[4]*0*0 ## [1] 128.8718 print(b7.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: blooms ~ water_c + shade_c + water_c:shade_c ## Data: d (Number of observations: 27) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 128.87 9.34 110.17 146.90 4000 1.00 ## water_c 74.71 11.55 51.74 97.84 4000 1.00 ## shade_c -41.01 11.88 -63.99 -17.91 4000 1.00 ## water_c:shade_c -51.77 14.45 -80.74 -23.92 4000 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 49.65 7.48 37.57 66.62 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 7.3.4 Plotting implied predictions. Now we’re ready for the bottom row of Figure 7.7. Here’s our variation on McElreath’s tryptych loop code, adjusted for brms and ggplot2. # loop over values of water.c and plot predictions shade_seq &lt;- -1:1 for(w in -1:1){ # defining the subset of the original data dt &lt;- d[d$water_c == w, ] # defining our new data nd &lt;- tibble(water_c = w, shade_c = shade_seq) # using our sampling skills, like before fitd_7.9 &lt;- fitted(b7.9, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # specifying our custom plot fig &lt;- ggplot() + geom_ribbon(data = fitd_7.9, aes(x = shade_c, ymin = Q2.5, ymax = Q97.5), fill = &quot;#CC79A7&quot;, alpha = 1/5) + geom_line(data = fitd_7.9, aes(x = shade_c, y = Estimate), color = &quot;#CC79A7&quot;) + geom_point(data = dt, aes(x = shade_c, y = blooms), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + labs(&quot;Blooms&quot;, title = paste(&quot;Water (centered) =&quot;, w)) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;)) # plotting that joint plot(fig) } But we don’t necessarily need a loop. We can achieve all of McElreath’s Figure 7.7 with fitted(), some data wrangling, and a little help from ggplot2::facet_grid(). # fitted() for model b7.8 fitted(b7.8) %&gt;% as_tibble() %&gt;% # adding fitted() for model b7.9 bind_rows( fitted(b7.9) %&gt;% as_tibble() ) %&gt;% # We&#39;ll want to index the models mutate(fit = rep(c(&quot;b7.8&quot;, &quot;b7.9&quot;), each = 27)) %&gt;% # Here we add the data, `d` bind_cols(bind_rows(d, d)) %&gt;% # These will come in handy for `ggplot2::facet_grid()` mutate(x_grid = paste(&quot;water_c =&quot;, water_c), y_grid = paste(&quot;model: &quot;, fit)) %&gt;% ggplot(aes(x = shade_c)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;#CC79A7&quot;, alpha = 1/5) + geom_line(aes(y = Estimate), color = &quot;#CC79A7&quot;) + geom_point(aes(y = blooms, group = x_grid), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + ylab(&quot;Blooms&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), panel.background = element_rect(color = &quot;black&quot;)) + facet_grid(y_grid ~ x_grid) 7.4 Interactions in design formulas The brms syntax generally follows the design formulas typical of lm(). Hopefully this is all old hat. 7.5 Summary Bonus: marginal_effects() The brms package includes the marginal_effects() function as a convenient way to look at simple effects and two-way interactions. Recall the simple univariable model, b7.3: b7.3$formula ## log_gdp ~ 1 + rugged We can look at the regression line and its percentile-based intervals like so: marginal_effects(b7.3) If we nest marginal_effects() within plot() with a points = T argument, we can add the original data to the figure. plot(marginal_effects(b7.3), points = T) We can further customize the plot. For example, we can replace the intervals with a spaghetti plot. While we’re at it, we can use point_args to adjust the geom_jitter() parameters. plot(marginal_effects(b7.3, spaghetti = T, nsamples = 200), points = T, point_args = c(alpha = 1/2, size = 1)) With multiple predictors, things get more complicated. Consider our multivariable, non-interaction model, b7.4. b7.4$formula ## log_gdp ~ rugged + cont_africa marginal_effects(b7.4) We got one plot for each predictor, controlling the other predictor at zero. Note how the plot for cont_africa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set: b7.4$data %&gt;% glimpse() ## Observations: 170 ## Variables: 3 ## $ log_gdp &lt;dbl&gt; 7.492609, 8.216929, 9.933263, 9.407032, 7.792343, 9.212541, 10.143191, 10.2... ## $ rugged &lt;dbl&gt; 0.858, 3.427, 0.769, 0.775, 2.688, 0.006, 0.143, 3.513, 1.672, 1.780, 0.388... ## $ cont_africa &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... One way to fix that is to adjust the data set and refit the model. d_factor &lt;- b7.4$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.4_factor &lt;- update(b7.4, newdata = d_factor) Using the update() syntax often speeds up the re-fitting process. marginal_effects(b7.4_factor) Now our second marginal plot more clearly expresses the cont_africa predictor as categorical. Things get more complicated with the interaction model, b7.5. b7.5$formula ## log_gdp ~ rugged + cont_africa + rugged:cont_africa marginal_effects(b7.5) The marginal_effects() function defaults to expressing interactions such that the first variable in the term–in this case, rugged–is on the x axis and the second variable in the term–cont_africa, treated as an integer–is depicted in three lines corresponding its mean and its mean +/- one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is, you guessed it, to refit the model after adjusting the data. d_factor &lt;- b7.5$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.5_factor &lt;- update(b7.5, newdata = d_factor) Just for kicks, we’ll use probs = c(.25, .75) to return 50% intervals, rather than the conventional 95%. marginal_effects(b7.5_factor, probs = c(.25, .75)) With the effects argument, we can just return the interaction effect, which is where all the action’s at. While we’re at it, we’ll use plot() to change some of the settings. plot(marginal_effects(b7.5_factor, effects = &quot;rugged:cont_africa&quot;, spaghetti = T, nsamples = 150), points = T, point_args = c(alpha = 2/3, size = 1), mean = F) Note, the ordering of the variables matters for the interaction term. Consider our interaction model for the tulips data. b7.9$formula ## blooms ~ water_c + shade_c + water_c:shade_c The plot tells a slightly different story, depending on whether you specify effects = &quot;shade_c:water_c&quot; or effects = &quot;water_c:shade_c&quot;. plot(marginal_effects(b7.9, effects = &quot;shade_c:water_c&quot;), points = T) plot(marginal_effects(b7.9, effects = &quot;water_c:shade_c&quot;), points = T) One might want to evaluate the effects of the second term in the interaction–water_c, in this case–at values other than the mean and the mean +/- one standard deviation. When we reproduced the bottom row of Figure 7.7, we expressed the interaction based on values -1, 0, and 1 for water_c. We can do that, here, by using the int_conditions argument. It expects a list, so we’ll put our desired water_c values in just that. ic &lt;- list(water.c = c(-1, 0, 1)) plot(marginal_effects(b7.9, effects = &quot;shade_c:water_c&quot;, int_conditions = ic), points = T) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggthemes_3.5.0 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 ## [6] readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 tidyverse_1.2.1 brms_2.5.0 ## [11] Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 ggplot2_3.0.0 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 mnormt_1.5-5 ## [22] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 ## [25] xfun_0.3 R6_2.2.2 markdown_0.8 ## [28] HDInterval_0.2.0 reshape_0.8.7 assertthat_0.2.0 ## [31] promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [34] gtable_0.2.0 rlang_0.2.1 extrafontdb_1.0 ## [37] lazyeval_0.2.1 broom_0.4.5 inline_0.3.15 ## [40] yaml_2.1.19 reshape2_1.4.3 abind_1.4-5 ## [43] modelr_0.1.2 threejs_0.3.1 crosstalk_1.0.0 ## [46] backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 ## [52] psych_1.8.4 RColorBrewer_1.1-2 ggridges_0.5.0 ## [55] plyr_1.8.4 base64enc_0.1-3 progress_1.2.0 ## [58] prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 ## [64] mvtnorm_1.0-8 matrixStats_0.54.0 hms_0.4.2 ## [67] shinyjs_1.0 mime_0.5 evaluate_0.10.1 ## [70] arrayhelpers_1.0-20160527 xtable_1.8-2 shinystan_2.5.0 ## [73] readxl_1.1.0 gridExtra_2.3 rstantools_1.5.0 ## [76] compiler_3.5.1 maps_3.3.0 crayon_1.3.4 ## [79] htmltools_0.3.6 later_0.7.3 lubridate_1.7.4 ## [82] MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 ## [85] bindr_0.1.1 igraph_1.2.1 pkgconfig_2.0.1 ## [88] foreign_0.8-70 xml2_1.2.0 svUnit_0.7-12 ## [91] dygraphs_1.1.1.5 vipor_0.4.5 rvest_0.3.2 ## [94] digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 ## [97] shiny_1.1.0 gtools_3.8.1 nlme_3.1-137 ## [100] jsonlite_1.5 bindrcpp_0.2.2 mapproj_1.2.6 ## [103] viridisLite_0.3.0 pillar_1.2.3 lattice_0.20-35 ## [106] loo_2.0.0 httr_1.3.1 glue_1.2.0 ## [109] xts_0.10-2 shinythemes_1.1.1 pander_0.6.2 ## [112] stringi_1.2.3 "],
["markov-chain-monte-carlo.html", "8 Markov Chain Monte Carlo 8.1 Good King Markov and His island kingdom 8.2 Markov chain Monte Carlo 8.3 Easy HMC: map2stan brm() 8.4 Care and feeding of your Markov chain. Reference Session info", " 8 Markov Chain Monte Carlo “This chapter introduces one of the more marvelous examples of how Fortuna and Minerva cooperate: the estimation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC) estimation” (p. 241). Though we’ve been using MCMC via the brms package for chapters, now, this chapter should clarify some of the details. 8.1 Good King Markov and His island kingdom In this version of the code, we’ve added set.seed(), which helps make the exact results reproducible. set.seed(103) num_weeks &lt;- 1e5 positions &lt;- rep(0, num_weeks) current &lt;- 10 for (i in 1:num_weeks) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around the archipelago if (proposal &lt; 1) proposal &lt;- 10 if (proposal &gt; 10) proposal &lt;- 1 # move? prob_move &lt;- proposal / current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } In this document, we’ll borrow a theme, theme_ipsum(), from the hrbrthemes package. # install.packages(&quot;hrbrthemes&quot;, dependencies = T) library(hrbrthemes) Figure 8.2.a. library(tidyverse) tibble(week = 1:1e5, island = positions) %&gt;% ggplot(aes(x = week, y = island)) + geom_point(shape = 1) + scale_x_continuous(breaks = seq(from = 0, to = 100, by = 20)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + coord_cartesian(xlim = 0:100, ylim = 1:10) + labs(title = &quot;Behold: The Metropolis algorithm in action!&quot;, subtitle = &quot;The dots show the king&#39;s path over the first 100 weeks.&quot;) + theme_ipsum() Figure 8.2.b. tibble(week = 1:1e5, island = positions) %&gt;% mutate(island = factor(island)) %&gt;% ggplot(aes(x = island)) + geom_bar() + labs(title = &quot;Old Metropolis shines in the long run.&quot;, subtitle = &quot;Sure enough, the time the king spent on each island was\\nproportional to its population size.&quot;) + theme_ipsum() 8.2 Markov chain Monte Carlo “The metropolis algorithm is the grandparent of several different strategies for getting samples from unknown posterior distributions” (p. 245). If you’re interested, Robert and Casells wrote a good historical overview of MCMC. 8.3 Easy HMC: map2stan brm() Here we load the rugged data. library(rethinking) data(rugged) d &lt;- rugged Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(rugged) It takes just a sec to do a little data manipulation. d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) dd &lt;- d %&gt;% filter(complete.cases(rgdppc_2000)) In the context of this chapter, it doesn’t make sense to translate McElreath’s m8.1 map() code to brm() code. Below, we’ll just go directly to the brm() variant of his m8.1stan. 8.3.1 Preparation. When working with brms, you don’t need to do the data processing McElreath did on pages 248 and 249. If you wanted to, however, here’s how you might do it within the tidyverse. dd.trim &lt;- dd %&gt;% select(log_gdp, rugged, cont_africa) str(dd.trim) 8.3.2 Estimation. Finally, we get to work that sweet HMC. b8.1 &lt;- brm(data = dd, family = gaussian, log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma))) Now we have officially ditched the uniform distribution for \\(\\sigma\\). We’ll only see it again in special cases for pedagogical purposes. Here’s the posterior: print(b8.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.96 9.50 2570 1.00 ## rugged -0.20 0.08 -0.36 -0.04 2495 1.00 ## cont_africa -1.95 0.23 -2.40 -1.50 2228 1.00 ## rugged:cont_africa 0.39 0.13 0.14 0.65 2111 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.86 1.06 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do note a couple things: If you look closely at the summary information at the top, you’ll see that the brms::brm() function defaults to chains = 4. If you check the manual, you’ll see it also defaults to cores = 1. You’ll also note it defaults to iter = 2000, warmup = 1000. Also of note, McElreath’s rethinking::precis() returns highest posterior density intervals (HPDIs) when summarizing map2stan() models. Not so with brms. If you want HPDIs, you’ll have to use the convenience functions from the tidybayes package. library(tidybayes) post &lt;- posterior_samples(b8.1) post %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) # note our rare use of 89% intervals ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_cont_africa -1.95 -2.30 -1.58 0.89 mean hdi ## 2 b_Intercept 9.22 9.00 9.44 0.89 mean hdi ## 3 b_rugged -0.203 -0.324 -0.0741 0.89 mean hdi ## 4 b_rugged:cont_africa 0.394 0.186 0.612 0.89 mean hdi ## 5 lp__ -248. -250. -246. 0.89 mean hdi ## 6 sigma 0.950 0.870 1.03 0.89 mean hdi 8.3.3 Sampling again, in parallel. Here we sample in parallel by adding cores = 4. b8.1_4chains_4cores &lt;- update(b8.1, cores = 4) This model sampled so fast that it really didn’t matter if we sampled in parallel or not. It will for others. print(b8.1_4chains_4cores) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.94 9.49 2526 1.00 ## rugged -0.20 0.08 -0.35 -0.05 2875 1.00 ## cont_africa -1.94 0.23 -2.38 -1.49 2560 1.00 ## rugged:cont_africa 0.39 0.13 0.13 0.65 2496 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.86 1.06 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 8.3.4 Visualization. Unlike the way rethinking’s extract.samples() yields a list, brms’s posterior_samples() returns a data frame. post &lt;- posterior_samples(b8.1) str(post) ## &#39;data.frame&#39;: 4000 obs. of 6 variables: ## $ b_Intercept : num 9.24 9.11 9.18 9.29 9.35 ... ## $ b_rugged : num -0.146 -0.205 -0.211 -0.225 -0.26 ... ## $ b_cont_africa : num -2.03 -1.95 -1.92 -2.1 -1.88 ... ## $ b_rugged:cont_africa: num 0.385 0.43 0.505 0.403 0.391 ... ## $ sigma : num 0.86 1.035 0.876 0.887 0.987 ... ## $ lp__ : num -248 -249 -248 -247 -248 ... As with McElreath’s rethinking, brms allows users to put the post data frame or the brmsfit object directly in pairs(). pairs(b8.1, off_diag_args = list(size = 1/5, alpha = 1/5)) Another nice way to customize your pairs plot is with the GGally package. library(GGally) post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs() Since GGally returns a ggplot2 object, you can customize it as you please. post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs() + labs(subtitle = &quot;My custom pairs plot&quot;) + theme_ipsum() For more ideas on customizing a GGally pairs plot, go here. 8.3.5 Using the samples. Older versions of brms allowed users to include information criteria as a part of the model summary by adding loo = T and/or waic = T in the summary() function (e.g., summary(b8.1, loo = T, waic = T). However, this is no longer the case. E.g., summary(b8.1, loo = T, waic = T) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.22 0.14 8.96 9.50 2570 1.00 ## rugged -0.20 0.08 -0.36 -0.04 2495 1.00 ## cont_africa -1.95 0.23 -2.40 -1.50 2228 1.00 ## rugged:cont_africa 0.39 0.13 0.14 0.65 2111 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.95 0.05 0.86 1.06 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Although R didn’t bark at us for adding loo = T, waic = T, they didn’t do anything. Nowadays, if you want that information, you’ll have to use the waic() and/or loo() functions, which you can save as objects as needed. waic(b8.1) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_waic -234.5 7.4 ## p_waic 5.0 0.9 ## waic 469.0 14.8 ## Warning: 2 (1.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. (l_b8.1 &lt;- loo(b8.1)) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.6 7.4 ## p_loo 5.0 0.9 ## looic 469.2 14.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. We don’t work directly with it in this project, but it’s worth pointing out the elpd values returned by loo() and waic() are the expected log pointwise predictive density for new data. It follows the formula \\[\\text{elpd} = \\sum_{i = 1}^n \\int p_t (\\tilde{y}_i) \\text{log} p (\\tilde{y}_i | y) d \\tilde{y}_i,\\] where \\(p_t (\\tilde{y}_i)\\) is the distributino representing the true data-generating process for \\(\\tilde{y}_i\\). The \\(p_t (\\tilde{y}_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distriubtions are also implicitly conditioned on any predictors in the model. (Vehtari, Gelman, &amp; Gabry, 2016, p. 2). Later in the paper, we learn the elpd_loo (i.e., the Bayesian LOO estimate of out-of-dampel predictive fit) is defined as \\[\\text{elpd}_{\\text{loo}} = \\sum_{i = 1}^n \\text{log } p (y_i | y - _i),\\] where \\[p (y_i | y - _i) = \\int p (y_i | \\theta) p (\\theta | y - _i) d \\theta\\] “is the leave-one-out predictive density given the data without the \\(i\\)th data point” (p. 3). To learn more about the \\(\\text{elpd}\\), read the rest of the paper and the other works referenced by the loo package team. 8.3.6 Checking the chain. Using plot() for a brm() fit returns both density and trace lots for the parameters. plot(b8.1) The bayesplot package allows a little more control. Here, we use bayesplot’s mcmc_trace() to show only trace plots with our custom theme. Note that mcmc_trace() works with data frames, not brmfit objects. There’s a further complication. Recall how we made post (i.e., post &lt;- posterior_samples(b8.1)). Our post data frame carries no information on chains. To retain that information, we’ll need to add an add_chain = T argument to our posterior_samples() function. library(bayesplot) post &lt;- posterior_samples(b8.1, add_chain = T) mcmc_trace(post[, c(1:5, 7)], # We need to include column 7 because that contains the chain info facet_args = list(ncol = 3), size = .15) + labs(title = &quot;My custom trace plots&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.95, .2)) The bayesplot package offers a variety of diagnostic plots. Here we make autocorrelation plots for all model parameters, one for each HMC chain. mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;b_rugged&quot;, &quot;b_cont_africa&quot;, &quot;b_rugged:cont_africa&quot;, &quot;sigma&quot;), lags = 5) + scale_color_ipsum() + theme_ipsum() That’s just what we like to see–nice L-shaped autocorrelation plots. Those are the kinds of shapes you’d expect when you have reasonably large effective samples. Anyway… 8.3.6.1 Overthinking: Raw Stan model code. The stancode() function works in brms much like it does in rethinking. brms::stancode(b8.1) ## // generated with brms 2.5.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## int Kc = K - 1; ## matrix[N, K - 1] Xc; // centered version of X ## vector[K - 1] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // population-level effects ## real temp_Intercept; // temporary intercept ## real&lt;lower=0&gt; sigma; // residual SD ## } ## transformed parameters { ## } ## model { ## vector[N] mu = temp_Intercept + Xc * b; ## // priors including all constants ## target += normal_lpdf(b | 0, 10); ## target += normal_lpdf(temp_Intercept | 0, 100); ## target += cauchy_lpdf(sigma | 0, 2) ## - 1 * cauchy_lccdf(0 | 0, 2); ## // likelihood including all constants ## if (!prior_only) { ## target += normal_lpdf(Y | mu, sigma); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = temp_Intercept - dot_product(means_X, b); ## } You can also get that information with b8.1$model or b8.1$fit@stanmodel. 8.4 Care and feeding of your Markov chain. Markov chain Monte Carlo is a highly technical and usually automated procedure. Most people who use it don’t really understand what it is doing. That’s okay, up to a point. Science requires division of labor, and if every one of us had to write our own Markov chains from scratch, a lot less research would get done in the aggregate. (p. 255) 8.4.1 How many samples do you need? The brms defaults for iter and warmup match those of McElreath’s rethinking. If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255) 8.4.2 How many chains do you need? “Using 3 or 4 chains is conventional, and quite often more than enough to reassure us that the sampling is working properly” (p. 257). 8.4.2.1 Convergence diagnostics. The default diagnostic output from Stan includes two metrics, n_eff and Rhat. The first is a measure of the effective number of samples. The second is the Gelman-Rubin convergence diagnostic, \\(\\hat{R}\\). When n_eff is much lower than the actual number of iterations (minus warmup) of your chains, it means the chains are inefficient, but possibly still okay. When Rhat is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples. If you draw more iterations, it could be fine, or it could never converge. See the Stan user manual for more details. It’s important however not to rely too much on these diagnostics. Like all heuristics, there are cases in which they provide poor advice. (p. 257) For more on n_eff and Rhat, you might also check out Gabry and Modrák’s vignette, Visual MCMC diagnostics using the bayesplot package. 8.4.3 Taming a wild chain. As with rethinking, brms can take data in the form of a list. Recall however, that in order to specify starting values, you need to specify a list of lists with an inits argument, rather than with start, as in rethinking. b8.2 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(uniform(-1e10, 1e10), class = Intercept), prior(uniform(0, 1e10), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2) Those were some silly flat priors. Here’s the damage. post &lt;- posterior_samples(b8.2, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.a.&quot;, subtitle = &quot;These trace plots do not look like the fuzzy caterpillars we usually hope for.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Let’s peek at the summary. print(b8.2) ## Warning: There were 1285 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 103583182.95 372634423.73 -47596907.77 1233403820.81 35 1.05 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 318449913.44 1056592781.72 3318.58 3458739759.26 40 1.04 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Holy smokes, those parameters are a mess! Plus we got a nasty warning message, too. Watch our reasonable priors save the day. b8.3 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2) print(b8.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.09 1.56 -3.57 3.01 1036 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.95 2.02 0.59 6.25 1451 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As in the text, no more warning signs and no more silly estimates. The trace plots look great, too. post &lt;- posterior_samples(b8.3, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.b&quot;, subtitle = &quot;Oh man. This looks so much better.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Now behold our version of Figure 8.6.a. post %&gt;% select(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = min(post$b_Intercept), to = max(post$b_Intercept), length.out = 50)), aes(x = x, y = dnorm(x = x, mean = 0, sd = 10)), color = ipsum_pal()(1), linetype = 2) + theme_ipsum() Here’s our version of Figure 8.6.b. post %&gt;% select(sigma) %&gt;% ggplot(aes(x = sigma)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = 0, to = max(post$sigma), length.out = 50)), aes(x = x, y = dcauchy(x = x, location = 0, scale = 1)*2), color = ipsum_pal()(2)[2], linetype = 2) + coord_cartesian(xlim = c(0, 10)) + theme_ipsum() 8.4.3.1 Overthinking: Cauchy distribution. Behold the beautiful Cauchy probability density: \\[p(x|x_0, \\gamma) = \\Bigg ( \\pi \\gamma \\Bigg [ 1 + \\Big ( \\frac{x - x_0}{\\gamma} \\Big ) ^2 \\Bigg ] \\Bigg ) ^{-1}\\] The Cauchy has no mean and variance, but \\(x_0\\) is the location and \\(\\gamma\\) is the scale. Here’s our version of the simulation. Note our use of the cummean() function. N &lt;- 1e4 set.seed(1e4) tibble(y = rcauchy(N, 0, 5), mu = cummean(y), index = 1:N) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line() + theme_ipsum() The whole thing is quite remarkible. Just for kicks, here we do it again, this time with eight simulations. N &lt;- 1e4 set.seed(1) tibble(a = rcauchy(N, 0, 5), b = rcauchy(N, 0, 5), c = rcauchy(N, 0, 5), d = rcauchy(N, 0, 5), e = rcauchy(N, 0, 5), f = rcauchy(N, 0, 5), g = rcauchy(N, 0, 5), h = rcauchy(N, 0, 5)) %&gt;% gather() %&gt;% group_by(key) %&gt;% mutate(mu = cummean(value)) %&gt;% ungroup() %&gt;% mutate(index = rep(1:N, times = 8)) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line(aes(color = key)) + scale_color_manual(values = ipsum_pal()(8)) + scale_x_continuous(breaks = c(0, 5000, 10000)) + theme_ipsum() + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, ncol = 4, scales = &quot;free&quot;) 8.4.4 Non-identifiable parameters. It appears that the only way to get a brms version of McElreath’s m8.4 and m8.5 is to augment the data. In addition to the Gaussian y vector, we’ll add two constants to the data, intercept_1 = 1 and intercept_2 = 1. set.seed(8.4) y &lt;- rnorm(100, mean = 0, sd = 1) b8.4 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(uniform(-1e10, 1e10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8.4) Our model results don’t perfectly mirror McElreath’s, but they’re identical in spirit. print(b8.4) ## Warning: The model has not converged (some Rhats are &gt; 1.1). Do not analyse the results! ## We recommend running more iterations and/or setting stronger priors. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept_1 1116.48 944.73 -762.62 2439.62 2 2.50 ## intercept_2 -1116.57 944.74 -2439.65 762.38 2 2.50 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.06 0.08 0.91 1.22 5 1.16 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note the frightening warning message. Those results are a mess! Let’s try again. b8.5 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2) print(b8.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept_1 0.20 7.11 -14.03 14.12 1192 1.00 ## intercept_2 -0.29 7.11 -14.22 13.96 1191 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.09 0.08 0.95 1.25 2116 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Much better. Now we’ll do the preparatory work for Figure 8.7. Instead of showing the plots, here, we’ll save them as objects, left_column and right_column, in order to combine them below. post &lt;- posterior_samples(b8.4, add_chain = T) left_column &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) post &lt;- posterior_samples(b8.5, add_chain = T) right_column &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) library(gridExtra) grid.arrange(left_column, right_column, ncol = 2) The central message in the text, default to weakly-regularizing priors, holds for brms just as it does in rethinking. For more on the topic, see the recommendations from the Stan team. If you want to dive deeper, check out Dan Simpson’s post on Gelman’s blog and their corresponding paper with Michael Betancourt. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 bayesplot_1.6.0 GGally_1.4.0 tidybayes_1.0.1 brms_2.5.0 ## [6] Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 forcats_0.3.0 stringr_1.3.1 ## [11] dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ## [16] ggplot2_3.0.0 tidyverse_1.2.1 hrbrthemes_0.5.0.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] labeling_0.3 mnormt_1.5-5 bridgesampling_0.4-0 ## [22] rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 ## [28] reshape_0.8.7 assertthat_0.2.0 promises_1.0.1 ## [31] scales_0.5.0 beeswarm_0.2.3 gtable_0.2.0 ## [34] rethinking_1.59 rlang_0.2.1 extrafontdb_1.0 ## [37] lazyeval_0.2.1 broom_0.4.5 inline_0.3.15 ## [40] yaml_2.1.19 reshape2_1.4.3 abind_1.4-5 ## [43] modelr_0.1.2 threejs_0.3.1 crosstalk_1.0.0 ## [46] backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 ## [52] psych_1.8.4 RColorBrewer_1.1-2 ggridges_0.5.0 ## [55] plyr_1.8.4 base64enc_0.1-3 progress_1.2.0 ## [58] prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 ## [64] mvtnorm_1.0-8 matrixStats_0.54.0 hms_0.4.2 ## [67] shinyjs_1.0 mime_0.5 evaluate_0.10.1 ## [70] arrayhelpers_1.0-20160527 xtable_1.8-2 shinystan_2.5.0 ## [73] readxl_1.1.0 rstantools_1.5.0 compiler_3.5.1 ## [76] maps_3.3.0 crayon_1.3.4 htmltools_0.3.6 ## [79] later_0.7.3 lubridate_1.7.4 MASS_7.3-50 ## [82] Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [85] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [88] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 ## [91] vipor_0.4.5 rvest_0.3.2 digest_0.6.15 ## [94] rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [97] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [100] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 ## [103] pillar_1.2.3 lattice_0.20-35 loo_2.0.0 ## [106] httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["big-entropy-and-the-generalized-linear-model.html", "9 Big Entropy and the Generalized Linear Model 9.1 Maximum entropy 9.2 Generalized linear models Reference Session info", " 9 Big Entropy and the Generalized Linear Model …Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention. To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice. When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. (p. 267) 9.1 Maximum entropy In Chapter 6, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution \\(p\\) with probabilities \\(p_i\\) for each possible event \\(i\\) turns out to be just the average log-probability: \\[H(p) = - \\sum_i p_i \\text{ log } p_i\\] This function is known as information entropy. (p. 268, emphasis in the original) Let’s execute the code for the pebbles-in-buckets example. library(tidyverse) d &lt;- tibble(a = c(0, 0, 10, 0, 0), b = c(0, 1, 8, 1, 0), c = c(0, 2, 6, 2, 0), d = c(1, 2, 4, 2, 1), e = 2) # this is our analogue to McElreath&#39;s `lapply()` code d %&gt;% mutate_all(funs(. / sum(.))) %&gt;% # the next few lines constitute our analogue to his `sapply()` code gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) ## # A tibble: 5 x 2 ## key h ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0 ## 2 b 0.639 ## 3 c 0.950 ## 4 d 1.47 ## 5 e 1.61 We’re almost ready to plot. Which brings us to color. For the plots in this chapter, we’ll be taking our color palettes from the ghibli package, which provides palettes based on scenes from anime films by the Studio Ghibli. library(ghibli) The main function is ghibli_palette() which you can use to both preview the palettes before using them and also index in order to use specific colors. For example, we’ll play with “MarnieMedium1”, first. ghibli_palette(&quot;MarnieMedium1&quot;) ghibli_palette(&quot;MarnieMedium1&quot;)[1:7] ## [1] &quot;#7BA46C&quot; &quot;#602D31&quot; &quot;#008D91&quot; &quot;#0A789F&quot; &quot;#C6A28A&quot; &quot;#61B8D3&quot; &quot;#EACF9E&quot; Now we’re ready to plot five of the six panels of Figure 9.1. d %&gt;% mutate(bucket = 1:5) %&gt;% gather(letter, pebbles, - bucket) %&gt;% ggplot(aes(x = bucket, y = pebbles)) + geom_col(width = 1/5, fill = ghibli_palette(&quot;MarnieMedium1&quot;)[2]) + geom_text(aes(y = pebbles + 1, label = pebbles)) + geom_text(data = tibble( letter = letters[1:5], bucket = 5.5, pebbles = 10, label = str_c(c(1, 90, 1260, 37800, 113400), rep(c(&quot; way&quot;, &quot; ways&quot;), times = c(1, 4)))), aes(label = label), hjust = 1) + scale_y_continuous(breaks = c(0, 5, 10)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6]), strip.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[1])) + facet_wrap(~letter, ncol = 2) We might plot the final panel like so. d %&gt;% # the next four lines are the same from above mutate_all(funs(. / sum(.))) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) %&gt;% # here&#39;s the R code 9.4 stuff mutate(n_ways = c(1, 90, 1260, 37800, 113400)) %&gt;% group_by(key) %&gt;% mutate(log_ways = log(n_ways) / 10, text_y = ifelse(key &lt; &quot;c&quot;, h + .15, h - .15)) %&gt;% # plot ggplot(aes(x = log_ways, y = h)) + geom_abline(intercept = 0, slope = 1.37, color = &quot;white&quot;) + geom_point(size = 2.5, color = ghibli_palette(&quot;MarnieMedium1&quot;)[7]) + geom_text(aes(y = text_y, label = key)) + labs(x = &quot;log(ways) per pebble&quot;, y = &quot;entropy&quot;) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6])) “The distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution” (p. 271). Among the pebbles, the maximum entropy distribution was e (i.e., the uniform). 9.1.1 Gaussian. Behold the probability distribution for the generalized normal distribution: \\[\\text{Pr} (y | \\mu, \\alpha, \\beta) = \\frac{\\beta}{2 \\alpha \\Gamma \\bigg (\\frac{1}{\\beta} \\bigg )} e ^ {- \\bigg (\\frac{|y - \\mu|}{\\alpha} \\bigg ) ^ {\\beta}}\\] In this formulation, \\(\\alpha =\\) the scale, \\(\\beta =\\) the shape, \\(\\mu =\\) the location, and \\(\\Gamma =\\) the gamma function. If you read closely in the text, you’ll discover that the densities in the right panel of Figure 9.2 were all created with the constraint \\(\\sigma^2 = 1\\). But \\(\\sigma^2 \\neq \\alpha\\) and there’s no \\(\\sigma\\) in the equations in the text. However, it appears the variance for the generalized normal distribution follows the form: \\[\\sigma^2 = \\frac{\\alpha^2 \\Gamma (3/\\beta)}{\\Gamma (1/\\beta)}\\] So if you do the algebra, you’ll see that you can compute \\(\\alpha\\) for a given \\(\\sigma^2\\) and \\(\\beta\\) like so: \\[\\alpha = \\sqrt{ \\frac{\\sigma^2 \\Gamma (1/\\beta)}{\\Gamma (3/\\beta)} }\\] I go the formula from Wikipedia.com. Don’t judge. We can wrap that formula in a custom function, alpha_per_beta(), use it to solve for the desired \\(\\beta\\) values, and plot. But one more thing: McElreath didn’t tell us exactly which \\(\\beta\\) values the left panel of Figure 9.2 was based on. So the plot below is my best guess. alpha_per_beta &lt;- function(variance, beta){ sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } tibble(mu = 0, variance = 1, # I arrived at these values by trial and error beta = c(1, 1.5, 2, 4)) %&gt;% mutate(alpha = map2(variance, beta, alpha_per_beta)) %&gt;% unnest() %&gt;% expand(nesting(mu, beta, alpha), value = seq(from = -5, to = 5, by = .1)) %&gt;% # behold the formula for the generalized normal distribution in code mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% ggplot(aes(x = value, y = density, group = beta)) + geom_line(aes(color = beta == 2, size = beta == 2)) + scale_color_manual(values = c(ghibli_palette(&quot;MarnieMedium2&quot;)[2], ghibli_palette(&quot;MarnieMedium2&quot;)[4])) + scale_size_manual(values = c(1/4, 1.25)) + ggtitle(NULL, subtitle = &quot;Guess which color denotes the Gaussian.&quot;) + coord_cartesian(xlim = -4:4) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) Here’s Figure 9.2’s right panel. tibble(mu = 0, variance = 1, # this time we need a more densely-packed sequence of `beta` values beta = seq(from = 1, to = 4, length.out = 100)) %&gt;% mutate(alpha = map2(variance, beta, alpha_per_beta)) %&gt;% unnest() %&gt;% expand(nesting(mu, beta, alpha), value = -8:8) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% # this is just an abbreviated version of the formula we used in our first code block summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_vline(xintercept = 2, color = &quot;white&quot;) + geom_line(size = 2, color = ghibli_palette(&quot;MarnieMedium2&quot;)[6]) + coord_cartesian(ylim = c(1.34, 1.42)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) If you look closely, you’ll see our version doesn’t quite match up with McElreath’s. Over x-axis values of 2 to 4, they match up pretty well. But as you go from 2 to 1, you’ll see our line drops off more steeply than his did. [And no, coord_cartesian() isn’t the problem.] If you can figure out why our numbers diverged, please share the answer. But getting back on track: The take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian. (p. 274) 9.1.2 Binomial. The binomial likelihood entials counting the numbers of ways that a given observation could arise, according to assumptions… If only two things can happen (blue or white marble, for example), and there’s a constant chance \\(p\\) of each across \\(n\\) trials, then the probability of observing \\(y\\) events of type 1 and \\(n - y\\) events of type 2 is: \\[\\text{Pr} (y | n, p) = \\frac{n!}{y! (n - y)!} p^y (1 - p)^{n - y}\\] It may help to note that the fraction with the factorials is just saying how many different ordered sequences of \\(n\\) outcomes have a count of \\(y\\). (p. 275) For me, that last sentence made more sense when I walked it out in a example. To do so, lets wrap that fraction of factorials into a function. count_ways &lt;- function(n, y){ # n = the total number of trials (i.e., the number of rows in your vector) # y = the total number of 1s (i.e., successes) in your vector (factorial(n) / (factorial(y) * factorial(n - y))) } Now consider three sequences: 0, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 0\\)) 1, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 1\\)) 1, 1, 0, 0 (i.e., \\(n = 4\\) and \\(y = 2\\)) We can organize that information in a little tibble and then demo our count_ways() function. tibble(sequence = 1:3, n = 4, y = c(0, 1, 2)) %&gt;% mutate(n_ways = map2(n, y, count_ways)) %&gt;% unnest() ## # A tibble: 3 x 4 ## sequence n y n_ways ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 0 1 ## 2 2 4 1 4 ## 3 3 4 2 6 Here’s the pre-Figure 9.3 data McElreath presented at the bottom of page 275. ( d &lt;- tibble(distribution = letters[1:4], ww = c(1/4, 2/6, 1/6, 1/8), bw = c(1/4, 1/6, 2/6, 4/8), wb = c(1/4, 1/6, 2/6, 2/8), bb = c(1/4, 2/6, 1/6, 1/8)) ) ## # A tibble: 4 x 5 ## distribution ww bw wb bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.25 0.25 0.25 0.25 ## 2 b 0.333 0.167 0.167 0.333 ## 3 c 0.167 0.333 0.333 0.167 ## 4 d 0.125 0.5 0.25 0.125 Those data take just a tiny bit of wrangling before they’re ready to plot with. d %&gt;% gather(key, value, -distribution) %&gt;% mutate(key = factor(key, levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) %&gt;% ggplot(aes(x = key, y = value, group = 1)) + geom_point(size = 2, color = ghibli_palette(&quot;PonyoMedium&quot;)[4]) + geom_line(color = ghibli_palette(&quot;PonyoMedium&quot;)[5]) + coord_cartesian(ylim = 0:1) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[2]), strip.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[6])) + facet_wrap(~distribution) If we go step by step, we might count the expected value for each distribution like follows. d %&gt;% gather(sequence, probability, -distribution) %&gt;% # `str_count()` will count the number of times &quot;b&quot; occurs within a given row of `sequence` mutate(n_b = str_count(sequence, &quot;b&quot;)) %&gt;% mutate(product = probability * n_b) %&gt;% group_by(distribution) %&gt;% summarise(expected_value = sum(product)) ## # A tibble: 4 x 2 ## distribution expected_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 1 ## 3 c 1 ## 4 d 1 We can use the same gather() and group_by() strategies on the way to computing the entropies. d %&gt;% gather(sequence, probability, -distribution) %&gt;% group_by(distribution) %&gt;% summarise(entropy = -sum(probability * log(probability))) ## # A tibble: 4 x 2 ## distribution entropy ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.39 ## 2 b 1.33 ## 3 c 1.33 ## 4 d 1.21 Like in the text, distribution == &quot;a&quot; had the largest entropy of the four. In the next example, the \\(\\text{expected value} = 1.4\\) and \\(p = .7\\). p &lt;- 0.7 ( a &lt;- c((1 - p)^2, p * (1 - p), (1 - p) * p, p^2) ) ## [1] 0.09 0.21 0.21 0.49 Here’s the entropy for our distribution a. -sum(a * log(a)) ## [1] 1.221729 I’m going to alter McElreath’s simulation function from R code block 9.9 to take a seed argument. In addition, I altered the names of the objects within the function and changed the output to a tibble that will also include the conditions “ww”, “bw”, “wb”, and “bb”. sim_p &lt;- function(seed, g = 1.4) { set.seed(seed) x_123 &lt;- runif(3) x_4 &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g) z &lt;- sum(c(x_123, x_4)) p &lt;- c(x_123, x_4) / z tibble(h = -sum(p * log(p)), p = p, key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) } For a given seed and g value, our augmented sim_p() function returns a \\(4 \\times 3\\) tibble. sim_p(seed = 9.9, g = 1.4) ## # A tibble: 4 x 3 ## h p key ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1.02 0.197 ww ## 2 1.02 0.0216 bw ## 3 1.02 0.184 wb ## 4 1.02 0.597 bb So the next step is to determine how many replications we’d like, create a tibble with seed values ranging from 1 to that number, and then feed those seed values into sim_p() via purrr::map2(), which will return a nested tibble. We’ll then unnest() and take a peek. # how many replications would you like? n_rep &lt;- 1e5 d &lt;- tibble(seed = 1:1e5) %&gt;% mutate(sim = map2(seed, 1.4, sim_p)) %&gt;% unnest() head(d) ## # A tibble: 6 x 4 ## seed h p key ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1.21 0.108 ww ## 2 1 1.21 0.151 bw ## 3 1 1.21 0.233 wb ## 4 1 1.21 0.508 bb ## 5 2 1.21 0.0674 ww ## 6 2 1.21 0.256 bw In order to intelligently choose which four replications we want to highlight in Figure 9.4, we’ll want to rank order them by entropy, h. ranked_d &lt;- d %&gt;% group_by(seed) %&gt;% arrange(desc(h)) %&gt;% ungroup() %&gt;% # here&#39;s the rank order step mutate(rank = rep(1:n_rep, each = 4)) head(ranked_d) ## # A tibble: 6 x 5 ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 ## 5 71132 1.22 0.0902 ww 2 ## 6 71132 1.22 0.210 bw 2 And we’ll also want a subset of the data to correspond to McElreath’s “A” through “D” distributions. subset_d &lt;- ranked_d %&gt;% # I arrived at these `rank` values by trial and error filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% # I arrived at the `height` values by trial and error, too mutate(height = rep(c(8, 2.25, .75, .5), each = 4), distribution = rep(letters[1:4], each = 4)) head(subset_d) ## # A tibble: 6 x 7 ## seed h p key rank height distribution ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 55665 1.22 0.0903 ww 1 8 a ## 2 55665 1.22 0.209 bw 1 8 a ## 3 55665 1.22 0.210 wb 1 8 a ## 4 55665 1.22 0.490 bb 1 8 a ## 5 50981 1.000 0.0459 ww 87373 2.25 b ## 6 50981 1.000 0.0459 bw 87373 2.25 b We’re finally ready to plot the left panel of Figure 9.4. d %&gt;% ggplot(aes(x = h)) + geom_density(size = 0, fill = ghibli_palette(&quot;LaputaMedium&quot;)[3], adjust = 1/4) + # note the data statements for the next two geoms geom_linerange(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(ymin = 0, ymax = height), color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_text(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(y = height + .5, label = distribution)) + scale_x_continuous(&quot;Entropy&quot;, breaks = seq(from = .7, to = 1.2, by = .1)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7])) Did you notice how our adjust = 1/4 with geom_density() served a similar function to the adj=0.1 in McElreath’s dens() code. Anyways, here’s the right panel. ranked_d %&gt;% filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% mutate(distribution = rep(letters[1:4], each = 4)) %&gt;% ggplot(aes(x = key, y = p, group = 1)) + geom_line(color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_point(size = 2, color = ghibli_palette(&quot;LaputaMedium&quot;)[4]) + coord_cartesian(ylim = 0:1) + labs(x = NULL, y = NULL) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7]), strip.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[6])) + facet_wrap(~distribution) Because we were simulating, our values won’t match up identically with those in the text. But we’re pretty close, eh? Since we saved our sim_p() output in a nested tibble, which we then unnested(), there’s no need to separate the entropy values from the distributional values the way McElreath did in R code 9.11. If we wanted to determine our highest entropy value–and the corresponding seed and p values, while we’re at it–, we might use max(h) within slice(). ranked_d %&gt;% group_by(key) %&gt;% slice(max(h)) ## # A tibble: 4 x 5 ## # Groups: key [4] ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 That maximum h value matched up nicely with the one in the text. If you look at the p column, you’ll see our values approximated McElreath’s distribution values, too. In both cases, they’re real close to the a values we computed, above. a ## [1] 0.09 0.21 0.21 0.49 9.2 Generalized linear models For an outcome variable that is continuous and far from any theoretical maximum or minimum, [a simple] Gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. (p. 280) I winged the values for our Figure 9.5. tibble(x = seq(from = -1, to = 3, by = .01)) %&gt;% mutate(probability = .35 + x * .5) %&gt;% ggplot(aes(x = x, y = probability)) + geom_rect(aes(xmin = -1, xmax = 3, ymin = 0, ymax = 1), fill = ghibli_palette(&quot;MononokeMedium&quot;)[5]) + geom_hline(yintercept = 0:1, linetype = 2, color = ghibli_palette(&quot;MononokeMedium&quot;)[7]) + geom_line(aes(linetype = probability &gt; 1, color = probability &gt; 1), size = 1) + geom_segment(x = 1.3, xend = 3, y = 1, yend = 1, size = 2/3, color = ghibli_palette(&quot;MononokeMedium&quot;)[3]) + scale_color_manual(values = c(ghibli_palette(&quot;MononokeMedium&quot;)[3], ghibli_palette(&quot;MononokeMedium&quot;)[7])) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(xlim = 0:2, ylim = c(0, 1.2)) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MononokeMedium&quot;)[1])) For a count outcome \\(y\\) for which each observation arises from \\(n\\) trials and with constant expected value \\(np\\), the binomial distribution has maximum entropy. So it’s the least informative distribution that satisfies our prior knowledge of the outcomes \\(y\\). (p. 281) The binomial model follows the basic form \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{Binomial} (n, p_i) \\\\ f(p_i) &amp; = &amp; \\alpha + \\beta x_i \\end{eqnarray} \\] The \\(f()\\) portion of the second line represents the link function. We need the link function because, though the shape of the Binomial distribution is determined by two parameters–\\(n\\) and \\(p\\)–, neither is equivalent to the Gaussian mean \\(\\mu\\). The mean outcome, rather, is \\(np\\)–a function of both. The link function also ensures the model doesn’t make probability predictions outside of the boundary \\([0, 1]\\). Let’s get more general. 9.2.1 Meet the family. Here are the Gamma and Exponential panels for Figure 9.6. length_out &lt;- 100 tibble(x = seq(from = 0, to = 5, length.out = length_out)) %&gt;% mutate(Gamma = dgamma(x, 2, 2), Exponential = dexp(x)) %&gt;% gather(key, density, -x) %&gt;% mutate(label = rep(c(&quot;y %~% Gamma(lambda, kappa)&quot;, &quot;y %~% Exponential(lambda)&quot;), each = n()/2)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~label, scales = &quot;free_y&quot;, labeller = label_parsed) The Gaussian: length_out &lt;- 100 tibble(x = seq(from = -5, to = 5, length.out = length_out)) %&gt;% mutate(density = dnorm(x), strip = &quot;y %~% Normal(mu, sigma)&quot;) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -4:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Here is the Poisson. length_out &lt;- 100 tibble(x = 0:20) %&gt;% mutate(density = dpois(x, lambda = 2.5), strip = &quot;y %~% Poisson(lambda)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Finally, the Binomial: length_out &lt;- 100 tibble(x = 0:10) %&gt;% mutate(density = dbinom(x, size = 10, prob = .85), strip = &quot;y %~% Binomial(n, p)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) 9.2.1.1 Rethinking: A likelihood is a prior. In traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” However, likelihoods are themselves prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon context. (p. 284) For a little more in this, check out McElreath’s great lecture, Bayesian Statistics without Frequentist Language. This subsection also reminds me of the title of one of Gelman’s blog posts, “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”. The title, which itself is a quote, comes from one of his papers, which he linked to in the blog, along with several related papers. It’s taken some time for the weight of that quote to sink in with me, and indeed it’s still sinking. Perhaps you’ll benefit from it, too. 9.2.2 Linking linear models to distributions. To build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a link function to prevent mathematical accidents like negative distances or probability masses that exceed 1. (p. 284) These models generally follow the form: \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{Some distribution} (\\theta_i, \\phi) \\\\ f(\\theta_i) &amp; = &amp; \\alpha + \\beta x_i \\end{eqnarray} \\] where \\(\\theta_i\\) is a parameter of central interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not of primary substantive interest (e.g., \\(\\sigma\\) in work-a-day Gaussian models). And as stated earlier, \\(f()\\) is the link function. Speaking of, the logit link maps a parameter that is defined as a probability mass and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this: \\[ \\begin{eqnarray}y_i&amp;\\sim&amp;\\text{Binomial}(n, p_i)\\\\\\text{logit}(p_i)&amp;=&amp;\\alpha+\\beta x_i \\end{eqnarray}\\] And the logit function itself is defined as the log-odds: \\[\\text{logit} (p_i) = \\text{log} \\frac{p_i}{1 - p_i}\\] The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is: \\[\\text{log} \\frac{p_i}{1 - p_i} = \\alpha + \\beta x_i\\] If we do the final algebraic manipulation on page 285, we can solve for \\(p_i\\) in terms of the linear model: \\[p_i = \\frac{\\text{exp} (\\alpha + \\beta x_i)}{1 + \\text{exp} (\\alpha + \\beta x_i)}\\] As we’ll see later, we will make great use of this formula via the brms::inv_logit_scaled() when making sense of logistic regression models. Now we have that last formula in hand, we can make the data necessary for Figure 9.7. # first, we&#39;ll make data for the horizontal lines alpha &lt;- 0 beta &lt;- 4 lines &lt;- tibble(x = seq(from = -1, to = 1, by = .25)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we&#39;re ready to make the primary data beta &lt;- 2 d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-odds`)) + geom_hline(data = lines, aes(yintercept = `log-odds`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = probability)) + geom_hline(data = lines, aes(yintercept = probability), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory library(gridExtra) grid.arrange(p1, p2, ncol = 2) The key lesson for now is just that no regression coefficient, such as \\(\\beta\\), from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 7) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change… The second very common link function is the log link. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation of \\(\\sigma\\) of a Gaussian distribution so it is a function of a predictor variable \\(x\\). The parameter \\(\\sigma\\) must be positive, because a standard deviation cannot be negative no can it be zero. The model might look like: \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{Normal} (\\mu, \\sigma_i) \\\\ \\text{log} (\\sigma_i) &amp; = &amp; \\alpha + \\beta x_i \\end{eqnarray} \\] In this model, the mean \\(\\mu\\) is constant, but the standard deviation scales with the value \\(x_i\\). (p. 268) This kind of model is trivial in the brms framework, which you can learn more about in Bürkner’s vignette Estimating Distributional Models with brms. Before moving on with the text, let’s detour and see how we might fit such a model. First, let’s simulate some continuous data y for which the \\(SD\\) is effected by a dummy variable x. set.seed(100) ( d &lt;- tibble(x = rep(0:1, each = 100)) %&gt;% mutate(y = rnorm(n = n(), mean = 100, sd = 10 + x * 10)) ) ## # A tibble: 200 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 0 95.0 ## 2 0 101. ## 3 0 99.2 ## 4 0 109. ## 5 0 101. ## 6 0 103. ## 7 0 94.2 ## 8 0 107. ## 9 0 91.7 ## 10 0 96.4 ## # ... with 190 more rows We can view what data like these look like with aid from tidybayes::geom_halfeyeh(). library(tidybayes) d %&gt;% mutate(x = x %&gt;% as.character()) %&gt;% ggplot(aes(x = y, y = x, fill = x)) + geom_halfeyeh(color = ghibli_palette(&quot;KikiMedium&quot;)[2], point_interval = mean_qi, .width = .68) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[4], ghibli_palette(&quot;KikiMedium&quot;)[6])) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) Even though the means of y are the same for both levels of the x dummy, the variance for x == 1 is substantially larger than that for x == 0. Let’s open brms. library(brms) For such a model, we have two formulas: one for \\(\\mu\\) and one for \\(\\sigma\\). We wrap both within the bf() function. b9.1 &lt;- brm(data = d, family = gaussian, bf(y ~ 1, sigma ~ 1 + x), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = Intercept, dpar = sigma), prior(normal(0, 10), class = b, dpar = sigma))) Do note our use of the dpar arguments in the prior statements. Here’s the summary. print(b9.1) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 1 ## sigma ~ 1 + x ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 100.12 0.84 98.49 101.79 3908 1.00 ## sigma_Intercept 2.33 0.07 2.19 2.48 4000 1.00 ## sigma_x 0.44 0.10 0.25 0.64 3511 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we get an intercept for both \\(\\mu\\) and \\(\\sigma\\), with the intercept for sigma identified as sigma_Intercept. And note the coefficient for \\(\\sigma\\) was named sigma_x. Also notice the scale the sigma_ coefficients are on. These are not in the original metric, but rather based on log(). You can confirm that by the second line of the print() output: Links: mu = identity; sigma = log. So if you want to get a sense of the effects of x on the \\(\\sigma\\) for y, you have to exponentiate the formula. Here we’ll do so with the posterior_samples(). post &lt;- posterior_samples(b9.1) head(post) ## b_Intercept b_sigma_Intercept b_sigma_x lp__ ## 1 100.96575 2.343610 0.3280349 -803.0574 ## 2 99.11466 2.427497 0.3172929 -803.3017 ## 3 99.16897 2.427403 0.3362887 -803.1968 ## 4 99.60756 2.304547 0.4648463 -801.7567 ## 5 100.67528 2.317253 0.4149594 -801.9135 ## 6 99.55635 2.224778 0.7179760 -805.5759 With the samples in hand, we’ll use the model formula to compute the model-implied standard deviations of y based on the x dummy and then examine them in a plot. post %&gt;% transmute(`x == 0` = exp(b_sigma_Intercept + b_sigma_x * 0), `x == 1` = exp(b_sigma_Intercept + b_sigma_x * 1)) %&gt;% gather(key, sd) %&gt;% ggplot(aes(x = sd, y = key, fill = key)) + geom_halfeyeh(color = ghibli_palette(&quot;KikiMedium&quot;)[2], point_interval = median_qi, .width = .95) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[4], ghibli_palette(&quot;KikiMedium&quot;)[6])) + labs(x = NULL, y = NULL, subtitle = expression(paste(&quot;Model-implied &quot;, italic(SD), &quot;s by group x&quot;))) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) And if we looked back at the data, those \\(SD\\) estimates are just what we’d expect. d %&gt;% group_by(x) %&gt;% summarise(sd = sd(y) %&gt;% round(digits = 1)) ## # A tibble: 2 x 2 ## x sd ## &lt;int&gt; &lt;dbl&gt; ## 1 0 10.2 ## 2 1 15.9 For more on models like this, check out Christakis’ 2014: What scientific idea is ready for retirement? or The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright. But getting back to the text, what the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving \\(\\text{log} (\\sigma_i) = \\alpha + \\beta x_i\\) for \\(\\sigma_i\\) yields the inverse link: \\[\\sigma_i = \\text{exp} (\\alpha + \\beta x_i)\\] The impact of this assumption can be seen in [our version of] Figure 9.8. (pp. 286—287) # first, we&#39;ll make data that&#39;ll be make the horizontal lines alpha &lt;- 0 beta &lt;- 2 lines &lt;- tibble(`log-measurement` = -3:3) %&gt;% mutate(`original measurement` = exp(`log-measurement`)) # now we&#39;re ready to make the primary data d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-measurement` = alpha + x * beta, `original measurement` = exp(alpha + x * beta)) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-measurement`)) + geom_hline(data = lines, aes(yintercept = `log-measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = `original measurement`)) + geom_hline(data = lines, aes(yintercept = `original measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1, ylim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory grid.arrange(p1, p2, ncol = 2) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.5.0 Rcpp_0.12.18 tidybayes_1.0.1 gridExtra_2.3 ghibli_0.1.1 forcats_0.3.0 ## [7] stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ## [13] ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 rstan_2.17.3 ## [22] mnormt_1.5-5 bridgesampling_0.4-0 rprojroot_1.3-2 ## [25] coda_0.19-1 xfun_0.3 R6_2.2.2 ## [28] markdown_0.8 HDInterval_0.2.0 reshape_0.8.7 ## [31] assertthat_0.2.0 promises_1.0.1 scales_0.5.0 ## [34] beeswarm_0.2.3 gtable_0.2.0 rethinking_1.59 ## [37] rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [40] broom_0.4.5 inline_0.3.15 yaml_2.1.19 ## [43] reshape2_1.4.3 abind_1.4-5 modelr_0.1.2 ## [46] threejs_0.3.1 crosstalk_1.0.0 backports_1.1.2 ## [49] httpuv_1.4.4.2 rsconnect_0.8.8 extrafont_0.17 ## [52] tools_3.5.1 bookdown_0.7 psych_1.8.4 ## [55] RColorBrewer_1.1-2 ggridges_0.5.0 plyr_1.8.4 ## [58] base64enc_0.1-3 progress_1.2.0 prettyunits_1.0.2 ## [61] zoo_1.8-2 LaplacesDemon_16.1.1 haven_1.1.2 ## [64] magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 ## [67] matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 ## [70] mime_0.5 evaluate_0.10.1 arrayhelpers_1.0-20160527 ## [73] xtable_1.8-2 shinystan_2.5.0 readxl_1.1.0 ## [76] rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [79] crayon_1.3.4 StanHeaders_2.17.2 htmltools_0.3.6 ## [82] later_0.7.3 lubridate_1.7.4 MASS_7.3-50 ## [85] Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [88] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [91] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 ## [94] vipor_0.4.5 rvest_0.3.2 digest_0.6.15 ## [97] rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [100] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [103] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 ## [106] pillar_1.2.3 lattice_0.20-35 loo_2.0.0 ## [109] httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [112] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["counting-and-classification.html", "10 Counting and Classification 10.1 Binomial regression 10.2 Poisson regression 10.3 Other count regressions Reference Session info", " 10 Counting and Classification All over the world, every day, scientists throw away information. Sometimes this is through the removal of “outliers,” cases in the data that offend the model and are exiled. More routinely, counted things are converted to proportions before analysis. Why does analysis of proportions throw away information? Because 10/20 and ½ are the same proportion, one-half, but have very different sample sizes. Once converted to proportions, and treated as outcomes in a linear regression, the information about sample size has been destroyed. It’s easy to retain the information about sample size. All that is needed is to model what has actually been observed, the counts instead of the proportions. (p. 291) In this chapter, we focus on the two most common types of count models: the binomial and the Poisson. Side note: For a nice Bayesian way to accommodate outliers in your Gaussian models, check out my project Robust Linear Regression with Student’s T Distribution. 10.1 Binomial regression The basic binomial model follows the form \\[y \\sim \\text{Binomial} (n, p)\\] where \\(y\\) is some count variable, \\(n\\) is the number of trials, and \\(p\\) it the probability a given trial was a 1, which is sometimes termed a success. When \\(n = 1\\), then \\(y\\) is a vector of 0s and 1s. Presuming the logit link, models of this type are commonly termed logistic regression. When \\(n &gt; 1\\), and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model. 10.1.1 Logistic regression: Prosocial chimpanzees. Load the chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(chimpanzees) We start with the simple intercept-only logistic regression model. \\[ \\begin{eqnarray} \\text{pulled_left}_i &amp; \\sim &amp; \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] b10.1 &lt;- brm(data = d, family = binomial, pulled_left ~ 1, prior(normal(0, 10), class = Intercept)) You might use fixef() to get a focused summary of the intercept. library(tidyverse) fixef(b10.1) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.32 0.09 0.14 0.5 The brms::inv_logit_scaled() function will be our alternative to the logistic() function in rethinking. c(.18, .46) %&gt;% inv_logit_scaled() ## [1] 0.5448789 0.6130142 fixef(b10.1) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.5791349 0.5231598 0.5347185 0.6229277 The next two chimp models add predictors in the usual way. b10.2 &lt;- brm(data = d, family = binomial, pulled_left ~ 1 + prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b))) b10.3 &lt;- update(b10.2, newdata = d, formula = pulled_left ~ 1 + prosoc_left + condition:prosoc_left) Before comparing our models, we’ll first save their WAIC estimates as objects. These will come in handy in just a bit. w_b10.1 &lt;- waic(b10.1) w_b10.2 &lt;- waic(b10.2) w_b10.3 &lt;- waic(b10.3) compare_ic(w_b10.1, w_b10.2, w_b10.3) ## WAIC SE ## b10.1 688.05 7.10 ## b10.2 680.66 9.32 ## b10.3 682.57 9.45 ## b10.1 - b10.2 7.40 6.08 ## b10.1 - b10.3 5.49 6.28 ## b10.2 - b10.3 -1.91 0.87 For this chapter, we’ll take our color scheme from the wesanderson package’s Moonrise2 palette. # install.packages(&quot;wesanderson&quot;, dependencies = T) library(wesanderson) wes_palette(&quot;Moonrise2&quot;) wes_palette(&quot;Moonrise2&quot;)[1:4] ## [1] &quot;#798E87&quot; &quot;#C27D38&quot; &quot;#CCC591&quot; &quot;#29211F&quot; We’ll also take a few formatting cues from Edward Tufte, curtesy of the ggthemes package. The theme_tufte() function will change the default font and remove some chart junk. The theme_set() function, below, will make these adjustments the default for all subsequent ggplot2 plots. To undo this, just execute theme_set(theme_default()). library(ggthemes) library(bayesplot) theme_set(theme_default() + theme_tufte() + theme(plot.background = element_rect(fill = wes_palette(&quot;Moonrise2&quot;)[3], color = wes_palette(&quot;Moonrise2&quot;)[3]))) Finally, here’s our WAIC plot. tibble(model = c(&quot;b10.1&quot;, &quot;b10.2&quot;, &quot;b10.3&quot;), waic = c(w_b10.1$estimates[3, 1], w_b10.2$estimates[3, 1], w_b10.3$estimates[3, 1]), se = c(w_b10.1$estimates[3, 2], w_b10.2$estimates[3, 2], w_b10.3$estimates[3, 2])) %&gt;% ggplot() + geom_pointrange(aes(x = reorder(model, -waic), y = waic, ymin = waic - se, ymax = waic + se, color = model), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1:2, 4)]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;WAIC&quot;) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) The full model, b10.3, did not have the lowest WAIC value. Though note how wide those standard error bars are relative to the point estimates. There’s a lot of model uncertainty there. Here are the WAIC weights. model_weights(b10.1, b10.2, b10.3, weights = &quot;waic&quot;) ## b10.1 b10.2 b10.3 ## 0.01756844 0.70935342 0.27307814 Let’s look at the parameter summaries for the theory-based model. print(b10.3) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left ~ prosoc_left + prosoc_left:condition ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.05 0.13 -0.21 0.31 3811 1.00 ## prosoc_left 0.62 0.23 0.17 1.05 3001 1.00 ## prosoc_left:condition -0.11 0.26 -0.61 0.41 2870 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the odds are multiplied by: fixef(b10.3)[2] %&gt;% exp() ## [1] 1.855592 Given an estimated value of 4, the probability of a pull, all else equal, would be: inv_logit_scaled(4) ## [1] 0.9820138 Adding the coefficient, fixef(b10.3)[2], would yield: (4 + fixef(b10.3)[2]) %&gt;% inv_logit_scaled() ## [1] 0.990226 For our variant of Figure 10.2, we use brms::pp_average() in place of rethinking::ensemble(). # the combined `fitted()` results of the three models weighted by their WAICs pp_a &lt;- pp_average(b10.1, b10.2, b10.3, weights = &quot;waic&quot;, method = &quot;fitted&quot;) %&gt;% as_tibble() %&gt;% bind_cols(b10.3$data) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # the empirically-based summaries d_plot &lt;- d %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(pulled_left = mean(pulled_left)) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # the plot pp_a %&gt;% ggplot(aes(x = x_axis)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = 0), fill = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(y = pulled_left, group = 0)) + geom_line(data = d_plot, aes(y = pulled_left, group = actor), color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/3) + scale_x_discrete(expand = c(.03, .03)) + coord_cartesian(ylim = 0:1) + labs(x = &quot;prosoc_left/condition&quot;, y = &quot;proportion pulled left&quot;) + theme(axis.ticks.x = element_blank()) McElreath didn’t show the actual pairs plot in the text. Here’s ours using mcmc_pairs(). # this helps us set our custom color scheme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[3], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) # the actual plot mcmc_pairs(x = posterior_samples(b10.3), pars = c(&quot;b_Intercept&quot;, &quot;b_prosoc_left&quot;, &quot;b_prosoc_left:condition&quot;), off_diag_args = list(size = 1/10, alpha = 1/6), diag_fun = &quot;dens&quot;) As McElreath observed, the posterior looks multivariate Gaussian. In equations, the next model follows the form \\[ \\begin{eqnarray} \\text{pulled_left}_i &amp; \\sim &amp; \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{actor}} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] Enclosing the actor variable within factor() will produce the indexing we need to get actor-specific intercepts. Also notice that we used the 0 + factor(actor) part of the model formula to suppress the brms default intercept. As such, the priors for all parameters in the model are of class = b. And since we’re using the same Gaussian prior for each, we only need one line for the prior argument. b10.4 &lt;- brm(data = d, family = binomial, pulled_left ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left , prior(normal(0, 10), class = b), iter = 2500, warmup = 500, chains = 2, cores = 2, control = list(adapt_delta = 0.9)) Within the tidyverse, distinct() yields the information you’d otherwise get from unique(). d %&gt;% distinct(actor) ## actor ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 We have no need to use something like depth=2 for our posterior summary. print(b10.4) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left ## Data: d (Number of observations: 504) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## factoractor1 -0.74 0.28 -1.29 -0.20 4000 1.00 ## factoractor2 10.94 5.34 3.95 23.49 1432 1.00 ## factoractor3 -1.05 0.27 -1.58 -0.52 4000 1.00 ## factoractor4 -1.05 0.27 -1.60 -0.52 4000 1.00 ## factoractor5 -0.74 0.27 -1.28 -0.22 3315 1.00 ## factoractor6 0.21 0.27 -0.30 0.75 4000 1.00 ## factoractor7 1.80 0.39 1.06 2.62 4000 1.00 ## prosoc_left 0.85 0.26 0.35 1.36 2400 1.00 ## prosoc_left:condition -0.14 0.29 -0.71 0.44 3714 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Correspondingly, brms::posterior_samples() returns an object for b10.4 that doesn’t quite follow the same structure as from rethinking::extract.samples(). We just have a typical 2-dimensional data frame. post &lt;- posterior_samples(b10.4) post %&gt;% glimpse() ## Observations: 4,000 ## Variables: 10 ## $ b_factoractor1 &lt;dbl&gt; -0.6355844, -0.7254224, -0.5764440, -0.9344404, -0.3579917, -0.4891183,... ## $ b_factoractor2 &lt;dbl&gt; 5.475166, 5.340297, 8.574469, 5.661893, 12.682989, 14.878802, 12.233435... ## $ b_factoractor3 &lt;dbl&gt; -0.8977912, -1.0710743, -1.0648612, -1.2356516, -0.7725446, -0.8693108,... ## $ b_factoractor4 &lt;dbl&gt; -1.1187303, -0.9678019, -0.8924143, -1.0182037, -0.6519190, -0.6077649,... ## $ b_factoractor5 &lt;dbl&gt; -0.8956251, -1.0444115, -0.6307956, -0.5598369, -1.0749834, -0.9830991,... ## $ b_factoractor6 &lt;dbl&gt; 0.259730435, 0.164825979, 0.146007705, 0.699094631, -0.034953154, -0.38... ## $ b_factoractor7 &lt;dbl&gt; 1.060955, 1.450292, 1.508535, 1.756391, 1.410583, 1.454120, 1.642576, 1... ## $ b_prosoc_left &lt;dbl&gt; 0.8008632, 1.0265301, 1.1981122, 0.6532315, 0.9707405, 0.9510254, 1.032... ## $ `b_prosoc_left:condition` &lt;dbl&gt; -0.23110218, -0.16912781, -0.94886273, 0.33701244, -0.30203147, -0.3763... ## $ lp__ &lt;dbl&gt; -288.8091, -287.2721, -290.4306, -290.0048, -291.6453, -293.0462, -290.... Our variant of Figure 10.3: post %&gt;% ggplot(aes(x = b_factoractor2)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + scale_y_continuous(NULL, breaks = NULL) + labs(x = NULL, title = &quot;Actor 2&#39;s large and uncertain intercept&quot;, subtitle = &quot;Once your log-odds are above, like, 4, it&#39;s all\\npretty much a probability of 1.&quot;) Figure 10.4. shows the idiographic trajectories for four of our chimps. # subset the `d_plot` data d_plot_4 &lt;- d_plot %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor)) # compute the model-implied estimates with `fitted()` and wrangle ftd &lt;- fitted(b10.4) %&gt;% as_tibble() %&gt;% bind_cols(b10.4$data) %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left, actor) %&gt;% select(actor, everything()) %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor)) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # plot ftd %&gt;% ggplot(aes(x = x_axis, y = pulled_left, group = actor)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line() + geom_line(data = d_plot_4, color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1.25) + scale_x_discrete(expand = c(.03, .03)) + coord_cartesian(ylim = 0:1) + labs(x = &quot;prosoc_left/condition&quot;, y = &quot;proportion pulled left&quot;) + theme(axis.ticks.x = element_blank(), # color came from: http://www.color-hex.com/color/ccc591 panel.background = element_rect(fill = &quot;#d1ca9c&quot;, color = &quot;transparent&quot;)) + facet_wrap(~actor) 10.1.1.1 Overthinking: Using the by group_by() function. Let’s work within the tidyverse, instead. If you wanted to compute the proportion of trials pulled_left == 1 for each combination of prosoc_left, condition, and chimp actor, you’d put those last three variables within group_by() and then compute the mean() of pulled_left within summarise(). d %&gt;% group_by(prosoc_left, condition, actor) %&gt;% summarise(`proportion pulled_left` = mean(pulled_left)) ## # A tibble: 28 x 4 ## # Groups: prosoc_left, condition [?] ## prosoc_left condition actor `proportion pulled_left` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0 1 0.333 ## 2 0 0 2 1 ## 3 0 0 3 0.278 ## 4 0 0 4 0.333 ## 5 0 0 5 0.333 ## 6 0 0 6 0.778 ## 7 0 0 7 0.778 ## 8 0 1 1 0.278 ## 9 0 1 2 1 ## 10 0 1 3 0.167 ## # ... with 18 more rows And since we’re working within the tidyverse, that operation returns a tibble rather than a list. 10.1.2 Aggregated binomial: Chimpanzees again, condensed. With the tidyverse, we use group_by() and summarise() to achieve what McElreath did with aggregate(). d_aggregated &lt;- d %&gt;% select(-recipient, -block, -trial, -chose_prosoc) %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(x = sum(pulled_left)) d_aggregated %&gt;% filter(actor %in% c(1, 2)) ## # A tibble: 8 x 4 ## # Groups: actor, condition [4] ## actor condition prosoc_left x ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 6 ## 2 1 0 1 9 ## 3 1 1 0 5 ## 4 1 1 1 10 ## 5 2 0 0 18 ## 6 2 0 1 18 ## 7 2 1 0 18 ## 8 2 1 1 18 To fit an aggregated binomial model in brms, we use the &lt;criterion&gt; | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing \\(n\\). We’ll see an example of the latter in just a bit. b10.5 &lt;- brm(data = d_aggregated, family = binomial, x | trials(18) ~ 1 + prosoc_left + condition:prosoc_left , prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2) We might compare b10.3 with b10.5 like this. fixef(b10.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.13 -0.21 0.31 ## prosoc_left 0.62 0.23 0.17 1.05 ## prosoc_left:condition -0.11 0.26 -0.61 0.41 fixef(b10.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.13 -0.20 0.30 ## prosoc_left 0.62 0.23 0.19 1.06 ## prosoc_left:condition -0.11 0.26 -0.64 0.40 The two are close within rounding error. 10.1.3 Aggregated binomial: Graduate school admissions. Load the infamous UCBadmit data: # detach(package:brms) library(rethinking) data(UCBadmit) d &lt;- UCBadmit Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(UCBadmit) d ## dept applicant.gender admit reject applications ## 1 A male 512 313 825 ## 2 A female 89 19 108 ## 3 B male 353 207 560 ## 4 B female 17 8 25 ## 5 C male 120 205 325 ## 6 C female 202 391 593 ## 7 D male 138 279 417 ## 8 D female 131 244 375 ## 9 E male 53 138 191 ## 10 E female 94 299 393 ## 11 F male 22 351 373 ## 12 F female 24 317 341 Now compute our newly-constructed dummy variable, male. d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0)) The univariable logistic model with male as the sole predictor of admit follows the form \\[ \\begin{eqnarray} n_{\\text{admit}_i} &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha + \\beta \\text{male}_i \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] The second model omits the male predictor. b10.6 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male , prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2) b10.7 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 10), class = Intercept), iter = 2500, warmup = 500, cores = 2, chains = 2) Here’s the WAIC comparison. waic(b10.6, b10.7) ## WAIC SE ## b10.6 990.44 326.67 ## b10.7 1053.35 330.32 ## b10.6 - b10.7 -62.91 166.49 Bonus: Information criteria digression. Let’s see what happens if we switch to the LOO. l_b10.6 &lt;- loo(b10.6) ## Warning: Found 5 observations with a pareto_k &gt; 0.7 in model &#39;b10.6&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 5 times to compute the ELPDs for the problematic observations directly. l_b10.7 &lt;- loo(b10.7) ## Warning: Found 5 observations with a pareto_k &gt; 0.7 in model &#39;b10.7&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 5 times to compute the ELPDs for the problematic observations directly. If you just ape the text and use the WAIC, everything appears fine. But holy smokes look at those nasty warning messages from the loo()! One of the frightening but ultimately handy things about working with the PSIS-LOO is that it requires we estimate a Pareto \\(k\\) parameter, which you can learn all about in the loo-package section of the loo reference manual. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)s are low. The makers of the loo package get worried when those \\(k\\)s exceed 0.7 and as a result, loo() spits out a warning message when they do. First things first, if you explicitly open the loo package, you’ll have access to some handy diagnostic functions. library(loo) Using the loo-object for model b10.6, which we’ve named l_b10.6, let’s take a look at the pareto_k_table() function. pareto_k_table(l_b10.6) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 5 41.7% 348 ## (0.5, 0.7] (ok) 2 16.7% 115 ## (0.7, 1] (bad) 1 8.3% 24 ## (1, Inf) (very bad) 4 33.3% 1 You may have noticed that this same table pops out when you just do something like loo(b10.6). Recall that this data set has 12 observations (i.e., execute count(d)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like nice and low \\(k\\)s. In this example, our observations are all over the place, with 4 in the “bad” \\(k\\) range We can take a closer look like this: plot(l_b10.6) So when you plot() a loo object, you get a nice diagnostic plot for those \\(k\\) values, ordered by observation number. Our plot indicates cases 1, 3, 11, and 12 had “very bad” \\(k\\) values for this model. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function. pareto_k_ids(l_b10.6, threshold = 1) ## [1] 1 3 11 12 Note our use of the threshold argument. Play around with it to see how it works. If you want an explicit look at those \\(k\\) values, you do: l_b10.6$diagnostics ## $pareto_k ## [1] 2.50987677 0.82273016 1.72566672 0.04319393 0.36866076 0.48872544 0.67582978 0.32487845 0.41154145 ## [10] 0.60205403 1.87952629 1.57030593 ## ## $n_eff ## [1] 1.210837 24.165654 1.885488 3160.176521 1314.469403 348.263540 114.581537 1100.519473 ## [9] 825.291622 265.031447 1.629926 2.293286 The pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_{i}\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in this paper and in this presentation by Aki Vehtari. Anyway, the implication of all this is these values suggest model b10.6 isn’t a great fit for these data. Part of the warning message for model b10.6 read: It is recommended to set ‘reloo = TRUE’ in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model [\\(n\\)] times to compute the ELPDs for the problematic observations directly. Let’s do that: l_b10.6_reloo &lt;- loo(b10.6, reloo = T) l_b10.6_reloo ## ## Computed from 4000 by 12 log-likelihood matrix ## ## Estimate SE ## elpd_loo -519.2 172.1 ## p_loo 133.8 51.7 ## looic 1038.3 344.1 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 10 83.3% 1 ## (0.5, 0.7] (ok) 2 16.7% 115 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Now that looks better. We’ll do the same thing for model b10.7. l_b10.7_reloo &lt;- loo(b10.7, reloo = T) Okay, let’s compare our PSIS-LOO values before and after adjusting loo() with reloo = T. compare_ic(l_b10.6, l_b10.7) ## LOOIC SE ## b10.6 966.29 314.47 ## b10.7 1035.96 322.62 ## b10.6 - b10.7 -69.67 154.92 compare_ic(l_b10.6_reloo, l_b10.7_reloo) ## LOOIC SE ## b10.6 1038.34 344.13 ## b10.7 1066.97 336.42 ## b10.6 - b10.7 -28.63 164.31 In this case, the results are kinda similar. But holy smokes, look at the size of those SEs! Anyway, watch out for this in your real-world data. You also might check out this vignette on how the loo package’s Pareto \\(k\\) can help detect outliers. But this has all been a tangent from the central thrust of this section. Back from our information criteria digression. Let’s get back on track. Here’s a look at b10.6, the unavailable model: print(b10.6) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 1 + male ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.83 0.05 -0.93 -0.73 1952 1.00 ## male 0.61 0.06 0.48 0.73 2664 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the relative difference in admission odds. fixef(b10.6)[2] %&gt;% exp() %&gt;% round(digits = 2) ## [1] 1.84 And now we’ll compute difference in admission probabilities. post &lt;- posterior_samples(b10.6) post %&gt;% mutate(p_admit_male = inv_logit_scaled(b_Intercept + b_male), p_admit_female = inv_logit_scaled(b_Intercept), diff_admit = p_admit_male - p_admit_female) %&gt;% summarise(`2.5%` = quantile(diff_admit, probs = .025), `50%` = median(diff_admit), `97.5%` = quantile(diff_admit, probs = .975)) ## 2.5% 50% 97.5% ## 1 0.1125606 0.1410549 0.167903 Instead of the summarise() code, we could have also used tidybayes::median_qi(diff_admit). It’s good to have options. Here’s our version of Figure 10.5. d &lt;- d %&gt;% mutate(case = factor(1:12)) p_10.6 &lt;- predict(b10.6) %&gt;% as_tibble() %&gt;% bind_cols(d) d_text &lt;- d %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) ggplot(data = d, aes(x = case, y = admit / applications)) + geom_pointrange(data = p_10.6, aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = d_text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + coord_cartesian(ylim = 0:1) + labs(y = &quot;Proportion admitted&quot;, title = &quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) As alluded to in all that LOO/pareto_k talk, above, this is not a great fit. So we’ll ditch the last model paradigm for one that answers the new question “What is the average difference in probability of admission between females and males within departments?” (p. 307). The statistical formula for the full model follows the form \\[ \\begin{eqnarray} n_{\\text{admit}_i} &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{dept}_i} + \\beta \\text{male}_i \\\\ \\alpha_{\\text{dept}} &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] We don’t need to coerce an index like McElreath did in the text. But here are the models. b10.8 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 0 + dept, prior(normal(0, 10), class = b), iter = 2500, warmup = 500, cores = 2, chains = 2) b10.9 &lt;- update(b10.8, newdata = d, formula = admit | trials(applications) ~ 0 + dept + male) Here we compare all four models by the LOO. loos &lt;- loo(b10.6, b10.7, b10.8, b10.9, reloo = T, cores = 2) loos ## LOOIC SE ## b10.6 1035.19 344.13 ## b10.7 1067.84 337.84 ## b10.8 135.45 36.28 ## b10.9 142.81 35.46 ## b10.6 - b10.7 -32.66 165.07 ## b10.6 - b10.8 899.73 334.36 ## b10.6 - b10.9 892.38 336.80 ## b10.7 - b10.8 932.39 321.46 ## b10.7 - b10.9 925.03 323.55 ## b10.8 - b10.9 -7.36 5.08 Here are the LOO weights. model_weights(b10.6, b10.7, b10.8, b10.9, weights = &quot;loo&quot;) ## b10.6 b10.7 b10.8 b10.9 ## 6.211826e-187 4.611571e-202 8.069492e-01 1.930508e-01 The parameters summaries for our multivariable model, b10.9, look like this: fixef(b10.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## deptA 0.68 0.10 0.49 0.87 ## deptB 0.64 0.11 0.42 0.87 ## deptC -0.58 0.07 -0.73 -0.44 ## deptD -0.61 0.08 -0.78 -0.45 ## deptE -1.06 0.10 -1.25 -0.87 ## deptF -2.62 0.16 -2.94 -2.33 ## male -0.10 0.08 -0.26 0.05 And on the proportional odds scale, the posterior mean for b_male is: fixef(b10.9)[7, 1] %&gt;% exp() ## [1] 0.9048756 Since we’ve been using brms, there’s no need to fit our version of McElreath’s m10.9stan. We already have that in our b10.9. But just for kicks and giggles, here’s another way to get the model summary. b10.9$fit ## Inference for Stan model: 3a12d1ca81dcc4f1abc237903458aea3. ## 2 chains, each with iter=2500; warmup=1250; thin=1; ## post-warmup draws per chain=1250, total post-warmup draws=2500. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_deptA 0.68 0.00 0.10 0.49 0.62 0.68 0.75 0.87 1137 1 ## b_deptB 0.64 0.00 0.11 0.42 0.56 0.64 0.72 0.87 1288 1 ## b_deptC -0.58 0.00 0.07 -0.73 -0.63 -0.58 -0.53 -0.44 1802 1 ## b_deptD -0.61 0.00 0.08 -0.78 -0.67 -0.61 -0.56 -0.45 1662 1 ## b_deptE -1.06 0.00 0.10 -1.25 -1.13 -1.06 -1.00 -0.87 2500 1 ## b_deptF -2.62 0.00 0.16 -2.94 -2.73 -2.62 -2.52 -2.33 2500 1 ## b_male -0.10 0.00 0.08 -0.26 -0.15 -0.10 -0.05 0.05 890 1 ## lp__ -70.61 0.05 1.79 -74.96 -71.66 -70.31 -69.27 -68.00 1139 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Sep 24 08:02:26 2018. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Here’s our version of Figure 10.6, the posterior validation check. predict(b10.9) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = d_text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + coord_cartesian(ylim = 0:1) + labs(y = &quot;Proportion admitted&quot;, title = &quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) The model precisions are imperfect, but way more valid than before. The posterior looks reasonably multivariate Gaussian. pairs(b10.9, off_diag_args = list(size = 1/10, alpha = 1/6)) 10.1.3.1 Overthinking: WAIC and aggregated binomial models. McElreath wrote: The WAIC function in rethinking detects aggregated binomial models and automatically splits them apart into 0/1 Bernoulli trials, for the purpose of calculating WAIC. It does this, because WAIC is computed point by point (see Chapter 6). So what you define as a “point” affects WAIC’s value. In an aggregated binomial each “point” is a bunch of independent trials that happen to share the same predictor values. In order for the disaggregated and aggregated models to agree, it makes sense to use the disaggregated representation. To my knowledge, brms::waic() and brms::loo() do not do this, which might well be why some of our values didn’t match up with those in the text. If you have additional insight on this, please share with the rest of the class. 10.1.4 Fitting binomial regressions with glm(). We’re not here to learn frequentist code, so we’re going to skip most of this section. But model b.good is worth fitting. Here are the data. # outcome and predictor almost perfectly associated y &lt;- c(rep(0, 10), rep(1, 10)) x &lt;- c(rep(-1, 9), rep(1, 11)) Fit the b.good model. b.good &lt;- brm(data = list(y = y, x = x), family = binomial, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b))) Our model summary will differ a bit from the one in the text. It seems this is because of the MAP/HMC contrast and our choice of priors. print(b.good) ## Family: binomial ## Links: mu = logit ## Formula: y ~ 1 + x ## Data: list(y = y, x = x) (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -5.50 4.37 -15.68 0.36 300 1.01 ## x 8.30 4.38 2.44 18.54 304 1.01 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You might experiment with different prior \\(SD\\)s to see how they influence the posterior \\(SD\\)s. Anyways, here’s the pairs() plot McElreath excluded from the text: pairs(b.good, off_diag_args = list(size = 1/10, alpha = 1/6)) That posterior, my friends, is not multivariate Gaussian. The plot deserves and extensive quote from McElreath. Inspecting the pairs plot (not shown) demonstrates just how subtle even simple models can be, once we start working with GLMs. I don’t say this to scare the reader. But it’s true that even simple models can behave in complicated ways. How you fit the model is part of the model, and in principle no GLM is safe for MAP estimation. (p. 311) 10.2 Poisson regression We’ll simulate our sweet count data. set.seed(10.2) # make the results reproducible tibble(y = rbinom(1e5, 1000, 1/1000)) %&gt;% summarise(y_mean = mean(y), y_variance = var(y)) ## # A tibble: 1 x 2 ## y_mean y_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.994 0.995 Yes, those statistics are virtually the same. When dealing with Poisson data, \\(\\mu = \\sigma^2\\). When you have a number of trials for which \\(n\\) is unknown or much larger than seen in the data, the Poisson likelihood is a useful tool. We define it like this \\[y \\sim \\text{Poisson} (\\lambda)\\] As \\(\\lambda\\) expresses both mean and variance because, within this model, the variance scales right along with the mean. Since \\(\\lambda\\) is constrained to be positive, we typically use the log link. Thus the basic Poisson regression model is \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{Poisson} (\\lambda_i) \\\\ \\text{log} (\\lambda_i) &amp; = &amp; \\alpha + \\beta x_i \\end{eqnarray} \\] 10.2.1 Example: Oceanic tool complexity. Load the Kline data. library(rethinking) data(Kline) d &lt;- Kline Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(Kline) d ## culture population contact total_tools mean_TU ## 1 Malekula 1100 low 13 3.2 ## 2 Tikopia 1500 low 22 4.7 ## 3 Santa Cruz 3600 low 24 4.0 ## 4 Yap 4791 high 43 5.0 ## 5 Lau Fiji 7400 high 33 5.0 ## 6 Trobriand 8000 high 19 4.0 ## 7 Chuuk 9200 high 40 3.8 ## 8 Manus 13000 low 28 6.6 ## 9 Tonga 17500 high 55 5.4 ## 10 Hawaii 275000 low 71 6.6 Here are our new columns. d &lt;- d %&gt;% mutate(log_pop = log(population), contact_high = ifelse(contact == &quot;high&quot;, 1, 0)) Our statistical model will follow the form \\[ \\begin{eqnarray} \\text{total_tools}_i &amp; \\sim &amp; \\text{Poisson} (\\lambda_i) \\\\ \\text{log} (\\lambda_i) &amp; = &amp; \\alpha + \\beta_1 \\text{log_pop}_i + \\beta_2 \\text{contact_high}_i + \\beta_3 \\text{contact_high}_i \\times \\text{log_pop}_i \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\beta_3 &amp; \\sim &amp; \\text{Normal} (0, 1) \\end{eqnarray} \\] The only new thing in our model code is family = poisson. brms defaults to the log() link. b10.10 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4) print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.94 0.36 0.21 1.64 3915 1.00 ## log_pop 0.26 0.03 0.20 0.33 4244 1.00 ## contact_high -0.10 0.83 -1.71 1.54 2701 1.00 ## log_pop:contact_high 0.04 0.09 -0.14 0.22 2662 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the lower triangle of the correlation matrix for the parameters. post &lt;- posterior_samples(b10.10) post %&gt;% select(-lp__) %&gt;% rename(b_interaction = `b_log_pop:contact_high`) %&gt;% psych::lowerCor() ## b_Int b_lg_ b_cn_ b_ntr ## b_Intercept 1.00 ## b_log_pop -0.98 1.00 ## b_contact_high -0.12 0.12 1.00 ## b_interaction 0.06 -0.08 -0.99 1.00 And here’s the coefficient plot via bayesplot::mcmc_intervals(): # We&#39;ll set a renewed color theme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[4], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) post %&gt;% select(-lp__) %&gt;% rename(b_interaction = `b_log_pop:contact_high`) %&gt;% mcmc_intervals(prob = .5, prob_outer = .95) + theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) How plausible is it a high-contact island will have more tools than a low-contact island? post &lt;- post %&gt;% mutate(lambda_high = exp(b_Intercept + b_contact_high + (b_log_pop + `b_log_pop:contact_high`)*8), lambda_low = exp(b_Intercept + b_log_pop*8)) %&gt;% mutate(diff = lambda_high - lambda_low) post %&gt;% summarise(sum = sum(diff &gt; 0)/length(diff)) ## sum ## 1 0.95725 Quite, it turns out. Behold the corresponding Figure 10.8.a. post %&gt;% ggplot(aes(x = diff)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + geom_vline(xintercept = 0, linetype = 2, color = wes_palette(&quot;Moonrise2&quot;)[2]) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &quot;lambda_high - lambda_low&quot;) I’m not happy with how clunky this solution is, but one way to get those marginal dot and line plots for the axes is to make intermediary tibbles. Anyway, here’s a version of Figure 10.8.b. # Intermediary tibbles for our the dot and line portoin of the plot point_tibble &lt;- tibble(x = c(median(post$b_contact_high), min(post$b_contact_high)), y = c(min(post$`b_log_pop:contact_high`), median(post$`b_log_pop:contact_high`))) line_tibble &lt;- tibble(parameter = rep(c(&quot;b_contact_high&quot;, &quot;b_log_pop:contact_high&quot;), each = 2), x = c(quantile(post$b_contact_high, probs = c(.025, .975)), rep(min(post$b_contact_high), times = 2)), y = c(rep(min(post$`b_log_pop:contact_high`), times = 2), quantile(post$`b_log_pop:contact_high`, probs = c(.025, .975)))) # the plot post %&gt;% ggplot(aes(x = b_contact_high, y = `b_log_pop:contact_high`)) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) + geom_point(data = point_tibble, aes(x = x, y = y)) + geom_line(data = line_tibble, aes(x = x, y = y, group = parameter)) Here we deconstruct model b10.10, bit by bit. # no interaction b10.11 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop + contact_high) # no contact rate b10.12 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop) # no log-population b10.13 &lt;- update(b10.10, formula = total_tools ~ 1 + contact_high) # intercept only b10.14 &lt;- update(b10.10, formula = total_tools ~ 1) I know we got all excited with the LOO, above. Let’s just be lazy and go WAIC, here. [Though beware, the LOO opens up a similar can of worms, here, to what we dealt with above.] w_b10.10 &lt;- waic(b10.10) w_b10.11 &lt;- waic(b10.11) w_b10.12 &lt;- waic(b10.12) w_b10.13 &lt;- waic(b10.13) w_b10.14 &lt;- waic(b10.14) compare_ic(w_b10.10, w_b10.11, w_b10.12, w_b10.13, w_b10.14) ## WAIC SE ## b10.10 79.94 11.78 ## b10.11 79.15 11.66 ## b10.12 84.32 9.37 ## b10.13 150.91 47.79 ## b10.14 141.72 33.36 ## b10.10 - b10.11 0.79 1.35 ## b10.10 - b10.12 -4.38 7.84 ## b10.10 - b10.13 -70.97 47.81 ## b10.10 - b10.14 -61.77 34.84 ## b10.11 - b10.12 -5.17 8.33 ## b10.11 - b10.13 -71.76 47.41 ## b10.11 - b10.14 -62.56 34.46 ## b10.12 - b10.13 -66.59 47.55 ## b10.12 - b10.14 -57.40 33.28 ## b10.13 - b10.14 9.19 17.75 tibble(model = c(&quot;b10.10&quot;, &quot;b10.11&quot;, &quot;b10.12&quot;, &quot;b10.13&quot;, &quot;b10.14&quot;), waic = c(w_b10.10$estimates[3, 1], w_b10.11$estimates[3, 1], w_b10.12$estimates[3, 1], w_b10.13$estimates[3, 1], w_b10.14$estimates[3, 1]), se = c(w_b10.10$estimates[3, 2], w_b10.11$estimates[3, 2], w_b10.12$estimates[3, 2], w_b10.13$estimates[3, 2], w_b10.14$estimates[3, 2])) %&gt;% ggplot() + geom_pointrange(aes(x = reorder(model, -waic), y = waic, ymin = waic - se, ymax = waic + se, color = model), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1, 2, 1, 1, 1)]) + coord_flip() + labs(x = NULL, y = NULL, title = &quot;WAIC&quot;) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) Here’s our version of Figure 10.9. Recall, to do an “ensemble” posterior prediction in brms, one uses the pp_average() function. I know we were just lazy and focused on the WAIC. But let’s play around, a bit. Here we’ll weight the models based on the LOO by adding a weights = &quot;loo&quot; argument to the pp_average() function. If you check the corresponding section of the brms reference manual, you’ll find several weighting schemes. nd &lt;- tibble(log_pop = seq(from = 6.5, to = 13, length.out = 50) %&gt;% rep(., times = 2), contact_high = rep(0:1, each = 50)) ppa_10.9 &lt;- pp_average(b10.10, b10.11, b10.12, weights = &quot;loo&quot;, method = &quot;fitted&quot;, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ppa_10.9 %&gt;% ggplot(aes(x = log_pop, group = contact_high)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = contact_high), alpha = 1/4) + geom_line(aes(y = Estimate, color = contact_high)) + geom_text(data = d, aes(y = total_tools, label = total_tools, color = contact_high), size = 3.5) + coord_cartesian(xlim = c(7.1, 12.4), ylim = c(12, 70)) + labs(x = &quot;log population&quot;, y = &quot;total tools&quot;, subtitle = &quot;Blue is the high contact rate; black is the low.&quot;) + theme(legend.position = &quot;none&quot;, panel.border = element_blank()) In case you were curious, here are those LOO weights: model_weights(b10.10, b10.11, b10.12, weights = &quot;loo&quot;) ## b10.10 b10.11 b10.12 ## 0.43316920 0.52547604 0.04135476 10.2.2 MCMC islands. We fit our analogue to m10.10stan, b10.10, some time ago. print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.94 0.36 0.21 1.64 3915 1.00 ## log_pop 0.26 0.03 0.20 0.33 4244 1.00 ## contact_high -0.10 0.83 -1.71 1.54 2701 1.00 ## log_pop:contact_high 0.04 0.09 -0.14 0.22 2662 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Center log_pop. d &lt;- d %&gt;% mutate(log_pop_c = log_pop - mean(log_pop)) Now fit the log_pop-centered model. b10.10_c &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4) print(b10.10_c) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.31 0.09 3.14 3.48 6236 1.00 ## log_pop_c 0.26 0.03 0.20 0.33 6393 1.00 ## contact_high 0.28 0.12 0.05 0.51 6825 1.00 ## log_pop_c:contact_high 0.07 0.17 -0.27 0.40 7004 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll use mcmc_pairs(), again, for Figure 10.10.a. # this helps us set our custom color scheme color_scheme_set(c(wes_palette(&quot;Moonrise2&quot;)[3], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[2], wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[1])) # the actual plot mcmc_pairs(x = posterior_samples(b10.10), pars = c(&quot;b_Intercept&quot;, &quot;b_log_pop&quot;, &quot;b_contact_high&quot;, &quot;b_log_pop:contact_high&quot;), off_diag_args = list(size = 1/10, alpha = 1/10), diag_fun = &quot;dens&quot;) And now behold Figure 10.10.b. mcmc_pairs(x = posterior_samples(b10.10_c), pars = c(&quot;b_Intercept&quot;, &quot;b_log_pop_c&quot;, &quot;b_contact_high&quot;, &quot;b_log_pop_c:contact_high&quot;), off_diag_args = list(size = 1/10, alpha = 1/10), diag_fun = &quot;dens&quot;) If you really want the correlation point estimates, use psych::lowerCorr(). psych::lowerCor(posterior_samples(b10.10)[, 1:4]) ## b_Int b_lg_ b_cn_ b__:_ ## b_Intercept 1.00 ## b_log_pop -0.98 1.00 ## b_contact_high -0.12 0.12 1.00 ## b_log_pop:contact_high 0.06 -0.08 -0.99 1.00 psych::lowerCor(posterior_samples(b10.10_c)[, 1:4]) ## b_Int b_l__ b_cn_ b___: ## b_Intercept 1.00 ## b_log_pop_c -0.46 1.00 ## b_contact_high -0.77 0.35 1.00 ## b_log_pop_c:contact_high 0.09 -0.20 -0.25 1.00 10.2.3 Example: Exposure and the offset. For the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained on page 312 [of the text], is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset. (p. 321, emphasis in the original) Here we simulate our data. set.seed(3838) num_days &lt;- 30 y &lt;- rpois(num_days, 1.5) num_weeks &lt;- 4 y_new &lt;- rpois(num_weeks, 0.5*7) Let’s make them tidy and add log_days. ( d &lt;- tibble(y = c(y, y_new), days = c(rep(1, num_days), rep(7, num_weeks)), monastery = c(rep(0, num_days), rep(1, num_weeks))) %&gt;% mutate(log_days = log(days)) ) ## # A tibble: 34 x 4 ## y days monastery log_days ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 ## 2 2 1 0 0 ## 3 1 1 0 0 ## 4 1 1 0 0 ## 5 1 1 0 0 ## 6 2 1 0 0 ## 7 0 1 0 0 ## 8 1 1 0 0 ## 9 1 1 0 0 ## 10 0 1 0 0 ## # ... with 24 more rows With the brms package, you use the offset() syntax, in which you put a pre-processed variable like log_days or the log of a variable, such as log(days). b10.15 &lt;- brm(data = d, family = poisson, y ~ 1 + offset(log_days) + monastery, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2) The model summary: print(b10.15) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + offset(log_days) + monastery ## Data: d (Number of observations: 34) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.17 0.16 -0.15 0.48 2112 1.00 ## monastery -0.91 0.30 -1.51 -0.34 2313 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The mode summary helps clarify that when you use offset(), brm() fixes the value. Thus there is no parameter estimate for the offset(). It’s a fixed part of the model not unlike the \\(\\nu\\) parameter of the Student-\\(t\\) distribution gets fixed to infinity when you use the Gaussian likelihood. Here we’ll compute the posterior means and 89% HDIs with tidybayes::mean_hdi(). library(tidybayes) posterior_samples(b10.15) %&gt;% transmute(lambda_old = exp(b_Intercept), lambda_new = exp(b_Intercept + b_monastery)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;lambda_old&quot;, &quot;lambda_new&quot;))) %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lambda_old 1.2 0.88 1.49 0.89 mean hdi ## 2 lambda_new 0.5 0.290 0.69 0.89 mean hdi As McElreath pointed out in the text, “Your estimates will be slightly different, because you got different randomly simulated data” (p. 322). 10.3 Other count regressions The next two of the remaining four models are maximum entropy distributions for certain problem types. The last two are mixtures, of which we’ll see more in the next chapter. 10.3.1 Multinomial. When more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. [We] already met the multinomial, implicitly, in Chapter 9 when we tossed pebbles into buckets as an introduction to maximum entropy. The binomial is really a special case of this distribution. And so its distribution formula resembles the binomial, just extrapolated out to three or more types of events. If there are \\(K\\) types of events with probabilities \\(p_1, …, p_K\\), then the probability of observing \\(y_1, …, y_K\\) events of each type out of \\(n\\) trials is (p. 323): \\[\\text{Pr} (y_1, ..., y_K | n, p_1, ..., p_K) = \\frac{n!}{\\prod_i y_i!} \\prod_{i = 1}^K p_i^{y_i}\\] Compare that equation with the simpler version in section 2.3.1 (page 33 in the text). 10.3.1.1 Explicit multinomial models. “The conventional and natural link is this context is the multinomial logit. This link function takes a vector of scores, one for each \\(K\\) event types, and computed the probability of a particular type of event \\(K\\) as” (p. 323, emphasis in the original) \\[\\text{Pr} (k |s_1, s_2, ..., s_K) = \\frac{\\text{exp} (s_k)}{\\sum_{i = 1}^K \\text{exp} (s_i)}\\] Let’s simulate the data. detach(package:brms) library(rethinking) # simulate career choices among 500 individuals N &lt;- 500 # number of individuals income &lt;- 1:3 # expected income of each career score &lt;- 0.5 * income # scores for each career, based on income # next line converts scores to probabilities p &lt;- softmax(score[1], score[2], score[3]) # now simulate choice # outcome career holds event type values, not counts career &lt;- rep(NA, N) # empty vector of choices for each individual set.seed(2078) # sample chosen career for each individual for(i in 1:N) career[i] &lt;- sample(1:3, size = 1, prob = p) Here’s what the data look like. career %&gt;% as_tibble() %&gt;% ggplot(aes(x = value %&gt;% as.factor())) + geom_bar(size = 0, fill = wes_palette(&quot;Moonrise2&quot;)[2]) Switch out rethinking for brms. detach(package:rethinking) library(brms) Here’s my naive attempt to fit the model in brms. b10.16 &lt;- brm(data = list(career = career), family = categorical(link = &quot;logit&quot;), career ~ 1, prior(normal(0, 5), class = Intercept), iter = 2500, warmup = 500, cores = 2, chains = 2) This differs from McElreath’s m10.16. Most obviously, this has two parameters. McElreath’s m10.16 only has one. If you have experience with these models and know how to reproduce McElreath’s results in brms, please share your code. print(b10.16) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 ## Data: list(career = career) (Number of observations: 500) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## mu2_Intercept 0.29 0.13 0.03 0.55 1343 1.00 ## mu3_Intercept 0.96 0.12 0.73 1.20 1318 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the second data simulation, this time based on McElreath’s R code 10.58. detach(package:brms) library(rethinking) N &lt;- 100 set.seed(2078) # simulate family incomes for each individual family_income &lt;- runif(N) # assign a unique coefficient for each type of event b &lt;- (1:-1) career &lt;- rep(NA, N) # empty vector of choices for each individual for (i in 1:N) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } Switch out rethinking for brms. detach(package:rethinking) library(brms) Here’s the brms version of McElreath’s m10.17. b10.17 &lt;- brm(data = list(career = career, # note how we used a list instead of a tibble family_income = family_income), family = categorical(link = &quot;logit&quot;), career ~ 1 + family_income, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2) Happily, these results cohere with the rethinking model. print(b10.17) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 + family_income ## Data: list(career = career, family_income = family_incom (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## mu2_Intercept 1.84 0.58 0.73 3.02 2504 1.00 ## mu3_Intercept 1.52 0.56 0.48 2.70 2387 1.00 ## mu2_family_income -3.96 1.04 -6.11 -1.99 2877 1.00 ## mu3_family_income -2.52 0.92 -4.36 -0.81 2686 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). McElreath described the parameters as “on a scale that is very hard to interpret” (p. 325). Indeed. 10.3.1.2 Multinomial in disguise as Poisson. Here we fit a multinomial likelihood by refactoring it to a series of Poissons. Let’s retrieve the Berkeley data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit rm(UCBadmit) detach(package:rethinking) library(brms) Fit the models. # binomial model of overall admission probability b_binom &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3) # Poisson model of overall admission rate and rejection rate b_pois &lt;- brm(data = d %&gt;% mutate(rej = reject), # &#39;reject&#39; is a reserved word family = poisson, cbind(admit, rej) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3) Note, the cbind() syntax made b_pois a multivariate Poisson model. Starting with version 2.0.0., brms supports a variety of multivariate models. Anyway, here are the implications of b_pois. # extract the samples post &lt;- posterior_samples(b_pois) # wrangle post %&gt;% transmute(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% gather() %&gt;% # plot ggplot(aes(x = value, y = key, fill = key)) + geom_halfeyeh(point_interval = median_qi, .width = .95, color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = c(wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2])) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank()) The model summaries: print(b_binom) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 1 ## Data: d (Number of observations: 12) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.46 0.03 -0.52 -0.39 978 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b_pois) ## Family: MV(poisson, poisson) ## Links: mu = log ## mu = log ## Formula: admit ~ 1 ## rej ~ 1 ## Data: d %&gt;% mutate(rej = reject) (Number of observations: 12) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## admit_Intercept 4.98 0.02 4.94 5.03 2886 1.00 ## rej_Intercept 5.44 0.02 5.40 5.48 2853 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the posterior mean for the probability of admission, based on b_binom. fixef(b_binom)[ ,&quot;Estimate&quot;] %&gt;% inv_logit_scaled() ## [1] 0.3879565 Happily, we get the same value within simulation error from model b_pois. k &lt;- fixef(b_pois) %&gt;% as.numeric() exp(k[1]) / (exp(k[1]) + exp(k[2])) ## [1] 0.3877586 The formula for what we just did in code is \\[p_{\\text{admit}} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{\\text{exp} (\\alpha_1)}{\\text{exp} (\\alpha_1) + \\text{exp} (\\alpha_2)}\\] 10.3.2 Geometric. Sometimes a count variable is a number of events up until something happened. Call this “something” the terminating event. Often we want to model the probability of that event, a kind of analysis known as event history analysis or survival analysis. When the probability of the terminating event is constant through time (or distance), and the units of time (or distance) are discrete, a common likelihood function is the geometric distribution. This distribution has the form: \\[\\text{Pr} (y | p) = p (1 - p) ^{y - 1}\\] where \\(y\\) is the number of time steps (events) until the terminating event occurred and \\(p\\) is the probability of that event in each time step. This distribution has maximum entropy for unbounded counts with constant expected value. (pp. 327–328) Here we simulate exemplar data. # simulate N &lt;- 100 set.seed(10.32) x &lt;- runif(N) set.seed(10.32) y &lt;- rgeom(N, prob = inv_logit_scaled(-1 + 2*x)) In case you’re curious, here are the data. list(y = y, x = x) %&gt;% as_tibble() %&gt;% ggplot(aes(x = x, y = y)) + geom_point(size = 3/5, alpha = 2/3) Our geometric model: b10.18 &lt;- brm(data = list(y = y, x = x), family = geometric(link = log), y ~ 0 + intercept + x, prior = c(prior(normal(0, 10), class = b, coef = intercept), prior(normal(0, 1), class = b)), iter = 2500, warmup = 500, chains = 2, cores = 2) The results: print(b10.18, digits = 2) ## Family: geometric ## Links: mu = log ## Formula: y ~ 0 + intercept + x ## Data: list(y = y, x = x) (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 0.75 0.25 0.27 1.23 1104 1.00 ## x -1.62 0.53 -2.63 -0.59 1161 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It turns out brms uses a different parameterization for the geometric distribution than rethinking does. It follows the form \\[f(y_i) = {y_i \\choose y_i} \\bigg (\\frac{\\mu_i}{\\mu_i + 1} \\bigg )^{y_i} \\bigg (\\frac{1}{\\mu_i + 1} \\bigg )\\] Even though the parameters brms yielded look different from those in the text, their predictions describe the data well. Here’s the marginal_effects() plot: plot(marginal_effects(b10.18), points = T, point_args = c(size = 3/5, alpha = 2/3), line_args = c(color = wes_palette(&quot;Moonrise2&quot;)[1], fill = wes_palette(&quot;Moonrise2&quot;)[1])) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.5.0 tidybayes_1.0.1 loo_2.0.0 bayesplot_1.6.0 ggthemes_3.5.0 ## [6] wesanderson_0.3.6 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 ## [11] readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 tidyverse_1.2.1 Rcpp_0.12.18 ## [16] rstan_2.17.3 StanHeaders_2.17.2 ggplot2_3.0.0 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 tidyselect_0.2.4 ## [5] htmlwidgets_1.2 munsell_0.5.0 codetools_0.2-15 nleqslv_3.3.2 ## [9] DT_0.4 miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 rstudioapi_0.7 ## [17] stats4_3.5.1 Rttf2pt1_1.3.7 labeling_0.3 mnormt_1.5-5 ## [21] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 reshape_0.8.7 ## [29] assertthat_0.2.0 promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [33] gtable_0.2.0 rethinking_1.59 rlang_0.2.1 extrafontdb_1.0 ## [37] lazyeval_0.2.1 broom_0.4.5 inline_0.3.15 yaml_2.1.19 ## [41] reshape2_1.4.3 abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [45] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 psych_1.8.4 ## [53] RColorBrewer_1.1-2 ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [57] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 ## [65] matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 mime_0.5 ## [69] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 shinystan_2.5.0 ## [73] readxl_1.1.0 gridExtra_2.3 rstantools_1.5.0 compiler_3.5.1 ## [77] maps_3.3.0 crayon_1.3.4 htmltools_0.3.6 later_0.7.3 ## [81] lubridate_1.7.4 MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 ## [85] bindr_0.1.1 igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [89] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 ## [93] rvest_0.3.2 digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 ## [97] shiny_1.1.0 gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [101] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 pillar_1.2.3 ## [105] lattice_0.20-35 httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["monsters-and-mixtures.html", "11 Monsters and Mixtures 11.1 Ordered categorical outcomes 11.2 Zero-inflated outcomes 11.3 Over-dispersed outcomes Reference Session info", " 11 Monsters and Mixtures [Of these majestic creatures], we’ll consider two common and useful examples. The first type is the ordered categorical model, useful for categorical outcomes with a fixed ordering. This model is built by merging a categorical likelihood function with a special kind of link function, usually a cumulative link. The second type is a family of zero-inflated and zero-augmented models, each of which mixes a binary event within an ordinary GLM likelihood like a Poisson or binomial. Both types of models help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models. (p. 331) 11.1 Ordered categorical outcomes It is very common in the social sciences, and occasional in the natural sciences, to have an outcome variable that is discrete, like a count, but in which the values merely indicate different ordered levels along some dimension. For example, if I were to ask you how much you like to eat fish, on a scale from 1 to 7, you might say 5. If I were to ask 100 people the same question, I’d end up with 100 values between 1 and 7. In modeling each outcome value, I’d have to keep in mind that these values are ordered because 7 is greater than 6, which is greater than 5, and so on. But unlike a count, the differences in values are not necessarily equal. In principle, an ordered categorical variable is just a multinomial prediction problem (page 323). But the constraint that the categories be ordered demands special treatment… The conventional solution is to use a cumulative link function. The cumulative probability of a value is the probability of that value or any smaller value. (pp. 331–332, emphasis in the original) 11.1.1 Example: Moral intuition. Let’s get the Trolley data from rethinking. library(rethinking) data(Trolley) d &lt;- Trolley Unload rethinking and load brms. rm(Trolley) detach(package:rethinking, unload = T) library(brms) Use the tidyverse to get a sense of the dimensions of the data. library(tidyverse) glimpse(d) ## Observations: 9,930 ## Variables: 12 ## $ case &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbox, fkbur, fkcar, fkspe, fks... ## $ response &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 4, 3, 4, ... ## $ order &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, 30, 5, 1, 13, 20, 17, 28, 1... ## $ id &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434,... ## $ age &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,... ## $ male &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ edu &lt;fct&gt; Middle School, Middle School, Middle School, Middle School, Middle School, Middle Schoo... ## $ action &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ... ## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ contact &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ## $ story &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, swi, boa, car, che, sha, sw... ## $ action2 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ... Though we have 9,930 rows, we only have 331 unique individuals. d %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 331 11.1.2 Describing an ordered distribution with intercepts. Before we get to plotting, in this chapter we’ll use theme settings and a color palette from the ggthemes package, which you might learn more about here. library(ggthemes) We’ll take our basic theme settings from the theme_hc() function. We’ll use the “Green fields” color palette, which we can inspect with the canva_pal() function and a little help from scales::show_col(). scales::show_col(canva_pal(&quot;Green fields&quot;)(4)) canva_pal(&quot;Green fields&quot;)(4) ## [1] &quot;#919636&quot; &quot;#524a3a&quot; &quot;#fffae1&quot; &quot;#5a5f37&quot; canva_pal(&quot;Green fields&quot;)(4)[3] ## [1] &quot;#fffae1&quot; Now we’re ready to make our ggplot2 version of the simple histogram, Figure 11.1.a. ggplot(data = d, aes(x = response, fill = ..x..)) + geom_histogram(binwidth = 1/4, size = 0) + scale_x_continuous(breaks = 1:7) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) Our cumulative proportion plot, Figure 11.1.b, will require some pre-plot wrangling. d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) %&gt;% ggplot(aes(x = response, y = cum_pr_k, fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 1)) + labs(y = &quot;cumulative proportion&quot;) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) In order to make the next plot, we’ll need McElreath’s logit() function. Here it is, the logarithm of cumulative odds plot, Figure 11.1.c. # McElreath&#39;s convenience function from page 335 logit &lt;- function(x) log(x / (1 - x)) d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) %&gt;% filter(response &lt; 7) %&gt;% # We can do the logit() conversion right in ggplot2 ggplot(aes(x = response, y = logit(cum_pr_k), fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + coord_cartesian(xlim = c(1, 7)) + labs(y = &quot;log-cumulative-odds&quot;) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) The code for Figure 11.2 is itself something of a monster. d_plot &lt;- d %&gt;% group_by(response) %&gt;% count() %&gt;% mutate(pr_k = n / nrow(d)) %&gt;% ungroup() %&gt;% mutate(cum_pr_k = cumsum(pr_k)) ggplot(data = d_plot, aes(x = response, y = cum_pr_k, color = cum_pr_k, fill = cum_pr_k)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + geom_linerange(aes(ymin = 0, ymax = cum_pr_k), alpha = 1/2, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + # There are probably more elegant ways to do this part. geom_linerange(data = . %&gt;% mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k)), aes(x = response + .025, ymin = ifelse(response == 1, 0, discrete_probability), ymax = cum_pr_k), color = &quot;black&quot;) + geom_text(data = tibble(text = 1:7, response = seq(from = 1.25, to = 7.25, by = 1), cum_pr_k = d_plot$cum_pr_k - .065), aes(label = text), size = 4) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = c(0, 1)) + labs(y = &quot;cumulative proportion&quot;) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks.x = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) McElreath’s convention for this first type of statistical model is \\[ \\begin{eqnarray} R_i &amp; \\sim &amp; \\text{Ordered} (\\mathbf p) \\\\ \\text{logit} (p_k) &amp; = &amp; \\alpha_k \\\\ \\alpha_k &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] The Ordered distribution is really just a categorical distribution that takes a vector \\(\\mathbf p = {p_1, p_2, p_3, p_4, p_5, p_6}\\) of probabilities of each response value below the maximum response (7 in this example). Each response value \\(k\\) in this vector is defined by its link to an intercept parameter, \\(\\alpha_k\\). Finally, some weakly regularizing priors are placed on these intercepts. (p. 335) Whereas in rethinking::map() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(phi , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s the intercepts-only model: # Here are our starting values, which we specify with the `inits` argument in brm() inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits) b11.1 &lt;- brm(data = d, family = cumulative, response ~ 1, prior(normal(0, 10), class = Intercept), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = inits_list) # Here we add our start values McElreath needed to include the depth=2 argument in the rethinking::precis() function to show the threshold parameters from his m11.1stan model. With a brm() fit, we just use print() or summary() as usual. print(b11.1) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept[1] -1.92 0.03 -1.97 -1.86 1607 1.00 ## Intercept[2] -1.27 0.02 -1.31 -1.22 2000 1.00 ## Intercept[3] -0.72 0.02 -0.76 -0.68 2000 1.00 ## Intercept[4] 0.25 0.02 0.21 0.29 2000 1.00 ## Intercept[5] 0.89 0.02 0.85 0.94 2000 1.00 ## Intercept[6] 1.77 0.03 1.71 1.83 2000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). What McElreath’s m11.1stan summary termed cutpoints[k], ours termed Intercept[k]. In both cases, these are the \\(\\alpha_k\\) parameters from the equations, above. The summaries look like those in the text, number of effective samples are high, and the \\(\\hat{R}\\) values are great. The model looks good. Recall we use the brms::inv_logit_scaled() function in place of McElreath’s logistic() function to get these into the probability metric. b11.1 %&gt;% fixef() %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept[1] 0.1281804 0.5073397 0.1218530 0.1345840 ## Intercept[2] 0.2198212 0.5061516 0.2118180 0.2280529 ## Intercept[3] 0.3278527 0.5052970 0.3189295 0.3371387 ## Intercept[4] 0.5617107 0.5050998 0.5515601 0.5711353 ## Intercept[5] 0.7088937 0.5056750 0.6998509 0.7181421 ## Intercept[6] 0.8543631 0.5072888 0.8471596 0.8614790 But recall that the posterior \\(SD\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the posterior_samples(). posterior_samples(b11.1) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) ## # A tibble: 6 x 5 ## key mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] 0.128 0.00328 0.122 0.135 ## 2 b_Intercept[2] 0.220 0.00422 0.212 0.228 ## 3 b_Intercept[3] 0.328 0.00467 0.319 0.337 ## 4 b_Intercept[4] 0.562 0.00502 0.552 0.571 ## 5 b_Intercept[5] 0.709 0.00468 0.700 0.718 ## 6 b_Intercept[6] 0.854 0.00363 0.847 0.861 11.1.3 Adding predictor variables. Now we define the linear model as \\(\\phi_i = \\beta x_i\\). Accordingly, the formula for our cumulative logit model becomes \\[ \\begin{eqnarray} \\text{log} \\frac{\\text{Pr} (y_i \\leq k)}{1 - \\text{Pr} (y_i \\leq k)} &amp; = &amp; \\alpha_k - \\phi_i\\\\ \\phi_i &amp; = &amp; \\beta x_i \\end{eqnarray} \\] I’m not aware that brms has an equivalent to the rethinking::dordlogit() function. So here we’ll make it by hand. The code comes from McElreath’s GitHub page. # First, we needed to specify the `logistic()` function, which is apart of the `dordlogit()` function logistic &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) p &lt;- ifelse(x == Inf, 1, p) p } # Now we get down to it dordlogit &lt;- function(x, phi, a, log = FALSE) { a &lt;- c(as.numeric(a), Inf) p &lt;- logistic(a[x] - phi) na &lt;- c(-Inf, a) np &lt;- logistic(na[x] - phi) p &lt;- p - np if (log == TRUE) p &lt;- log(p) p } The dordlogit() function works like this: (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1])) ## [1] 0.1281804 0.0916408 0.1080315 0.2338580 0.1471830 0.1454693 0.1456369 Note the slight difference in how we used dordlogit() with a brm() fit summarized by fixef() than the way McElreath did with a map2stan() fit summarized by coef(). McElreath just put coef(m11.1) into dordlogit(). We, however, more specifically placed fixef(b11.1)[, 1] into the function. With the [, 1] part, we specified that we were working with the posterior means (i.e., Estimate) and neglecting the other summaries (i.e., the posterior SDs and 95% intervals). If you forget to subset, chaos ensues. Next, as McElreath further noted in the text, “these probabilities imply an average outcome of:” sum(pk * (1:7)) ## [1] 4.199178 I found that a bit abstract. Here’s the thing in a more elaborate tibble format. ( explicit_example &lt;- tibble(probability_of_a_response = pk) %&gt;% mutate(the_response = 1:7) %&gt;% mutate(their_product = probability_of_a_response * the_response) ) ## # A tibble: 7 x 3 ## probability_of_a_response the_response their_product ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.128 1 0.128 ## 2 0.0916 2 0.183 ## 3 0.108 3 0.324 ## 4 0.234 4 0.935 ## 5 0.147 5 0.736 ## 6 0.145 6 0.873 ## 7 0.146 7 1.02 explicit_example %&gt;% summarise(average_outcome_value = sum(their_product)) ## # A tibble: 1 x 1 ## average_outcome_value ## &lt;dbl&gt; ## 1 4.20 Side note This made me wonder how this would compare if we were lazy and ignored the categorical nature of the response. Here we refit the model with the typical Gaussian likelihood. brm(data = d, family = gaussian, response ~ 1, # In this case, 4 (i.e., the middle response) seems to be the conservative place to put the mean prior = c(prior(normal(4, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4) %&gt;% print() ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 4.20 0.02 4.16 4.24 3401 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.91 0.01 1.88 1.93 4000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Happily, this yielded a mean estimate of 4.2, much like our average_outcome_value, above. End side note Now we’ll try it by subtracting .5 from each. # The probabilities of a given response (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1] - .5)) ## [1] 0.08187471 0.06407742 0.08235179 0.20905389 0.15892905 0.18432566 0.21938747 # The average rating sum(pk * (1:7)) ## [1] 4.729612 So the rule is we subtract the linear model from each interecept. Let’s fit our multivariable models. # Start values for b11.2 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0) b11.2 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + intention + contact, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = list(inits, inits)) # Start values for b11.3 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0, `action:intention` = 0, `contact:intention` = 0) b11.3 &lt;- update(b11.2, formula = response ~ 1 + action + intention + contact + action:intention + contact:intention, inits = list(inits, inits)) We don’t have a coeftab() function in brms like for rethinking. But as we did for Chapter 6, we can reproduce it with help from the broom package and a bit of data wrangling. library(broom) tidy(b11.1) %&gt;% mutate(model = &quot;b11.1&quot;) %&gt;% bind_rows(tidy(b11.2) %&gt;% mutate(model = &quot;b11.2&quot;)) %&gt;% bind_rows(tidy(b11.3) %&gt;% mutate(model = &quot;b11.3&quot;)) %&gt;% select(model, term, estimate) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% complete(term = distinct(., term), model) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) %&gt;% # this last step isn&#39;t necessary, but it orders the rows to match the text slice(c(6:11, 1, 4, 3, 2, 5)) ## term b11.1 b11.2 b11.3 ## 1 b_Intercept[1] -1.92 -2.84 -2.64 ## 2 b_Intercept[2] -1.27 -2.16 -1.94 ## 3 b_Intercept[3] -0.72 -1.57 -1.34 ## 4 b_Intercept[4] 0.25 -0.55 -0.31 ## 5 b_Intercept[5] 0.89 0.12 0.36 ## 6 b_Intercept[6] 1.77 1.02 1.27 ## 7 b_action NA -0.71 -0.47 ## 8 b_intention NA -0.72 -0.28 ## 9 b_contact NA -0.96 -0.33 ## 10 b_action:intention NA NA -0.45 ## 11 b_intention:contact NA NA -1.28 If you really wanted that last nobs row at the bottom, you could elaborate on this code: b11.1$data %&gt;% count(). Also, if you want a proper coeftab() function for brms, McElreath’s code lives here. Give it a whirl. Anyway, here are the WAIC comparisons. Caution: This took some time to compute. waic(b11.1, b11.2, b11.3) ## WAIC SE ## b11.1 37854.52 57.59 ## b11.2 37089.56 76.29 ## b11.3 36929.48 81.27 ## b11.1 - b11.2 764.97 56.01 ## b11.1 - b11.3 925.04 62.72 ## b11.2 - b11.3 160.07 25.79 model_weights(b11.1, b11.2, b11.3, weights = &quot;waic&quot;) ## b11.1 b11.2 b11.3 ## 1.350890e-201 1.741863e-35 1.000000e+00 McElreath made Figure 11.3 by extracting the samples of his m11.3, saving them as post, and working some hairy base R plot() code. We’ll take a different route and use brms::fitted(). This will take substantial data wrangling, but hopefully it’ll be instructive. Let’s first take a look at the initial fitted() output for the beginnings of Figure 11.3.a. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% glimpse() ## Observations: 100 ## Variables: 14 ## $ `1.1` &lt;dbl&gt; 0.07050793, 0.07331175, 0.06556431, 0.07556150, 0.06974222, 0.06728931, 0.06258419, 0.06675... ## $ `2.1` &lt;dbl&gt; 0.09022314, 0.08797921, 0.08204221, 0.08942581, 0.08508239, 0.08195592, 0.08165978, 0.08186... ## $ `1.2` &lt;dbl&gt; 0.06035870, 0.06120538, 0.05837122, 0.06129558, 0.06332180, 0.05933344, 0.05723077, 0.05918... ## $ `2.2` &lt;dbl&gt; 0.07424959, 0.07134540, 0.07064424, 0.07059550, 0.07485515, 0.07015684, 0.07181866, 0.07039... ## $ `1.3` &lt;dbl&gt; 0.07949822, 0.09019671, 0.07580081, 0.08899270, 0.07998488, 0.08829395, 0.08117568, 0.08038... ## $ `2.3` &lt;dbl&gt; 0.09384552, 0.10181182, 0.08854330, 0.09952609, 0.09145436, 0.10091865, 0.09748015, 0.09247... ## $ `1.4` &lt;dbl&gt; 0.2165909, 0.2222326, 0.2184516, 0.2182741, 0.2254301, 0.2178642, 0.2166278, 0.2165743, 0.2... ## $ `2.4` &lt;dbl&gt; 0.2351114, 0.2351932, 0.2367288, 0.2302369, 0.2406349, 0.2326015, 0.2375560, 0.2326468, 0.2... ## $ `1.5` &lt;dbl&gt; 0.1595335, 0.1613757, 0.1708593, 0.1600106, 0.1680163, 0.1623742, 0.1630913, 0.1644984, 0.1... ## $ `2.5` &lt;dbl&gt; 0.1562134, 0.1581071, 0.1681601, 0.1573111, 0.1645432, 0.1596454, 0.1599497, 0.1622035, 0.1... ## $ `1.6` &lt;dbl&gt; 0.1984142, 0.1870130, 0.1893513, 0.1878230, 0.1836006, 0.1948381, 0.2020022, 0.1962142, 0.1... ## $ `2.6` &lt;dbl&gt; 0.1770649, 0.1713022, 0.1712098, 0.1734930, 0.1670322, 0.1778831, 0.1790553, 0.1790553, 0.1... ## $ `1.7` &lt;dbl&gt; 0.2150965, 0.2046648, 0.2216015, 0.2080425, 0.2099041, 0.2100068, 0.2172881, 0.2163944, 0.2... ## $ `2.7` &lt;dbl&gt; 0.1732920, 0.1742611, 0.1826715, 0.1794117, 0.1763978, 0.1768387, 0.1724805, 0.1813547, 0.1... Hopefully by now it’s clear why we needed the nd tibble, which we made use of in the newdata = nd argument. Because we set summary = F, we get draws from the posterior instead of summaries. With max_iter, we controlled how many of those posterior draws we wanted. McElreath used 100, which he indicated at the top of page 341, so we followed suit. It took me a minute to wrap my head around the meaning of the 14 vectors, which were named by brms::fitted() default. Notice how each column is named by two numerals, separated by a period. That first numeral indicates which if the two intention values the draw is based on (i.e., 1 stands for intention == 0, 2, stands for intention == 1). The numbers on the right of the decimals are the seven response options for response. For each posterior draw, you get one of those for each value of intention. Finally, it might not be immediately apparent, but the values are in the probability scale, just like pk on page 338. Now we know what we have in hand, it’s just a matter of careful wrangling to get those probabilities into a more useful format to insert into ggplot2. I’ve extensively annotated the code, below. If you lose track of happens in a given step, just run the code up till that point. Go step by step. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% # We convert the data to the long format gather() %&gt;% # We need an variable to index which posterior iteration we&#39;re working with mutate(iter = rep(1:max_iter, times = 14)) %&gt;% # This step isn’t technically necessary, but I prefer my iter index at the far left. select(iter, everything()) %&gt;% # Here we extract the `intention` and `response` information out of the `key` vector and spread it into two vectors. separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% # That step produced two character vectors. They’ll be more useful as numbers mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% # Here we convert `intention` into its proper 0:1 metric mutate(intention = intention -1) %&gt;% # This isn&#39;t necessary, but it helps me understand exactly what metric the values are currently in rename(pk = value) %&gt;% # This step is based on McElreath&#39;s R code 11.10 on page 338 mutate(`pk:rating` = pk * rating) %&gt;% # I’m not sure how to succinctly explain this. You’re just going to have to trust me. group_by(iter, intention) %&gt;% # This is very important for the next step. arrange(iter, intention, rating) %&gt;% # Here we take our `pk` values and make culmulative sums. Why? Take a long hard look at Figure 11.2. mutate(probability = cumsum(pk)) %&gt;% # `rating == 7` is unnecessary. These `probability` values are by definition 1. filter(rating &lt; 7) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + # Note how we made a new data object for `geom_text()` geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;, x = &quot;intention&quot;) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) Boom! Okay, that pile of code is a bit of a mess and you’re not going to want to repeatedly cut and paste all that. Let’s condense it into a homemade function, make_Figure_11.3_data(). make_Figure_11.3_data &lt;- function(action, contact, max_iter){ nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(iter = rep(1:max_iter, times = 14)) %&gt;% select(iter, everything()) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% rename(pk = value) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% arrange(iter, intention, rating) %&gt;% mutate(probability = cumsum(pk)) %&gt;% filter(rating &lt; 7) } Now we’ll use our sweet homemade function to make our plots. # Figure 11.3.a p1 &lt;- make_Figure_11.3_data(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;, x = &quot;intention&quot;) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Figure 11.3.b p2 &lt;- make_Figure_11.3_data(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.12, .24, .35, .50, .68, .80, .92)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 1,\\ncontact = 0&quot;, x = &quot;intention&quot;) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Figure 11.3.c p3 &lt;- make_Figure_11.3_data(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.15, .34, .44, .56, .695, .8, .92)), aes(label = text), size = 3) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;action = 0,\\ncontact = 1&quot;, x = &quot;intention&quot;) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # here we stitch them together with `grid.arrange()` library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) If you’d like to learn more about these kinds of models and how to fit them in brms, check out Bürkner and Vuorre’s Ordinal Regression Models in Psychology: A Tutorial. 11.1.4 Bonus: Figure 11.3 alternative. I have a lot of respect for McElreath. But man, Figure 11.3 is the worst. I’m in clinical psychology and there’s no way a working therapist is going to look at a figure like that and have any sense of what’s going on. Nobody’s got time for that. We’ve have clients to serve! Happily, we can go further. Look back at McElreath’s R code 11.10 on page 338. See how he multiplied the elements of pk by their respective response values and then just summed them up to get an average outcome value? With just a little amendment to our custom make_Figure_11.3_data() function, we can wrangle our fitted() output to express average response values for each of our conditions of interest. Here’s the adjusted function: make_data_for_an_alternative_fiture &lt;- function(action, contact, max_iter){ nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(iter = rep(1:max_iter, times = 14)) %&gt;% select(iter, everything()) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% rename(pk = value) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% # Everything above this point is identical to the previous custom function. # All we do is replace the last few lines with this one line of code. summarise(mean_rating = sum(`pk:rating`)) } Our handy homemade but monstrously-named make_data_for_an_alternative_fiture() function works very much like its predecessor. You’ll see. # Alternative to Figure 11.3.a p1 &lt;- make_data_for_an_alternative_fiture(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;, x = &quot;intention&quot;, y = &quot;response&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Alternative to Figure 11.3.b p2 &lt;- make_data_for_an_alternative_fiture(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 1,\\ncontact = 0&quot;, x = &quot;intention&quot;, y = &quot;response&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) # Alternative to Figure 11.3.c p3 &lt;- make_data_for_an_alternative_fiture(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 0:1) + scale_y_continuous(breaks = 1:7) + coord_cartesian(ylim = 1:7) + labs(subtitle = &quot;action = 0,\\ncontact = 1&quot;, x = &quot;intention&quot;, y = &quot;response&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) grid.arrange(p1, p2, p3, ncol = 3) Finally; now those are plots I can sell in a clinical psychology journal! 11.2 Zero-inflated outcomes Very often, the things we can measure are not emissions from any pure process. Instead, they are mixtures of multiple processes. Whenever there are different causes for the same observation, then a mixture model may be useful. A mixture model uses more than one simple probability distribution to model a mixture of causes. In effect, these models use more than one likelihood for the same outcome variable. Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A “zero” means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started. (p. 342, emphasis in the original) In his Rethinking: Breaking the law box, McElreath discussed how advances in computing have made it possible for working scientists to define their own data generating models. If you’d like to dive deeper into the topic, check out Bürkner’s vignette, Define Custom Response Distributions with brms. We’ll even make use of it a little further down. 11.2.1 Example: Zero-inflated Poisson. Here we simulate our drunk monk data. # define parameters prob_drink &lt;- 0.2 # 20% of days rate_work &lt;- 1 # average 1 manuscript per day # sample one year of production N &lt;- 365 # simulate days monks drink set.seed(0.2) drink &lt;- rbinom(N, 1, prob_drink) # simulate manuscripts completed y &lt;- (1 - drink) * rpois(N, rate_work) We’ll put those data in a tidy tibble before plotting. d &lt;- tibble(Y = y) %&gt;% arrange(Y) %&gt;% mutate(zeros = c(rep(&quot;zeros_drink&quot;, times = sum(drink)), rep(&quot;zeros_work&quot;, times = sum(y == 0 &amp; drink == 0)), rep(&quot;nope&quot;, times = N - sum(y == 0)) )) ggplot(data = d, aes(x = Y)) + geom_histogram(aes(fill = zeros), binwidth = 1, color = &quot;grey92&quot;) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[1], canva_pal(&quot;Green fields&quot;)(4)[2], canva_pal(&quot;Green fields&quot;)(4)[1])) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) With these data, the likelihood of observing zero on y, (i.e., the likelihood zero manuscripts were completed on a given occasion) is \\[ \\begin{eqnarray} \\text{Pr} (0 | p, \\lambda) &amp; = &amp; \\text{Pr} (\\text{drink} | p) + \\text{Pr} (\\text{work} | p) \\times \\text{Pr} (0 | \\lambda) \\\\ &amp; = &amp; p + (1 - p) \\text{ exp} (- \\lambda) \\end{eqnarray} \\] And since the Poisson likelihood of \\(y\\) is \\(\\text{Pr} (y | \\lambda) = \\lambda^y \\text{exp} (- \\lambda) / y!\\), the likelihood of \\(y = 0\\) is just \\(\\text{exp} (- \\lambda)\\). The above is just the mathematics for: The probability of observing a zero is the probability that the monks didn’t drink OR (\\(+\\)) the probability that the monks worked AND (\\(\\times\\)) failed to finish anything. And the likelihood of a non-zero value \\(y\\) is: \\[ \\begin{eqnarray} \\text{Pr} (y | p, \\lambda) &amp; = &amp; \\text{Pr} (\\text{drink} | p) (0) + \\text{Pr} (\\text{work} | p) \\text{Pr} (y | \\lambda) \\\\ &amp; = &amp; (1 - p) \\frac {\\lambda^y \\text{ exp} (- \\lambda)}{y!} \\end{eqnarray} \\] Since drinking monks never produce \\(y &gt; 0\\), the expression above is just the chance the monks both work \\(1 - p\\), and finish \\(y\\) manuscripts. (p. 344, emphasis in the original) So letting \\(p\\) be the probability \\(y\\) is zero and \\(\\lambda\\) be the shape of the distribution, the zero-inflated Poisson (ZIPoisson) regression model takes the basic form \\[ \\begin{eqnarray} y_i &amp; \\sim &amp; \\text{ZIPoisson} (p_i, \\lambda_i)\\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_p + \\beta_p x_i \\\\ \\text{log} (\\lambda_i) &amp; = &amp; \\alpha_\\lambda + \\beta_\\lambda x_i \\end{eqnarray} \\] One last thing to note is that in brms, \\(p_i\\) is denoted zi. So the intercept [and zi] only zero-inflated Poisson model in brms looks like this. b11.4 &lt;- brm(data = d, family = zero_inflated_poisson, Y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(beta(2, 2), class = zi)), # the brms default is beta(1, 1) cores = 4) print(b11.4) ## Family: zero_inflated_poisson ## Links: mu = log; zi = identity ## Formula: Y ~ 1 ## Data: d (Number of observations: 365) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.07 0.08 -0.09 0.22 1422 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## zi 0.17 0.05 0.07 0.27 1477 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The zero-inflated Poisson is parameterized in brms a little differently than it is in rethinking. The different parameterization did not influence the estimate for the Intercept, \\(\\lambda\\). In both here and in the text, \\(\\lambda\\) was about 0.06. However, it did influence the summary of zi. Note how McElreath’s logistic(-1.39) yielded 0.1994078. Seems rather close to our zi estimate of 0.17. First off, because he didn’t set his seed in the text before simulating, we couldn’t exactly reproduce his simulated drunk monk data. So our results will vary a little due to that alone. But after accounting for simulation variance, hopefully it’s clear that zi in brms is already in the probability metric. There’s no need to convert it. In the prior argument, we used beta(2, 2) for zi and also mentioned in the margin that the brms default is beta(1, 1). To give you a sense of the priors, let’s plot them. tibble(`zi prior`= seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(`beta(1, 1)` = dbeta(`zi prior`, 1, 1), `beta(2, 2)` = dbeta(`zi prior`, 2, 2)) %&gt;% gather(prior, density, -`zi prior`) %&gt;% ggplot(aes(x = `zi prior`, ymin = 0, ymax = density)) + geom_ribbon(aes(fill = prior)) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[4], canva_pal(&quot;Green fields&quot;)(4)[2])) + theme_hc() + scale_x_continuous(breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) + facet_wrap(~prior) Hopefully this clarifies that the brms default is flat, whereas our prior regularized a bit toward .5. Anyway, here’s that exponentiated \\(\\lambda\\). fixef(b11.4)[1, ] %&gt;% exp() ## Estimate Est.Error Q2.5 Q97.5 ## 1.0717283 1.0838109 0.9178318 1.2508876 11.2.1.1 Overthinking: Zero-inflated Poisson distribution function. dzip &lt;- function(x, p, lambda, log = TRUE) { ll &lt;- ifelse( x == 0, p + (1 - p) * exp(-lambda), (1 - p) * dpois(x, lambda, log = FALSE) ) if (log == TRUE) ll &lt;- log(ll) return(ll) } We can use McElreath’s dzip() to do a posterior predictive check for our model. To work with our estimates for \\(p\\) and \\(\\lambda\\) directly, we’ll set log = F. p_b11.4 &lt;- posterior_summary(b11.4)[2, 1] lambda_b11.4 &lt;- posterior_summary(b11.4)[1, 1] %&gt;% exp() tibble(x = 0:4) %&gt;% mutate(density = dzip(x = x, p = p_b11.4, lambda = lambda_b11.4, log = F)) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) If you look up to the histogram we made at the beginning of this section, you’ll see this isn’t a terrible approximation. 11.3 Over-dispersed outcomes All statistical models omit something. The question is only whether that something is necessary for making useful inferences. One symptom that something important has been omitted from a count model is over-dispersion. The variance of a variable is sometimes called its dispersion. For a counting process like a binomial, the variance is a function of the same parameters as the expected value. For example, the expected value of a binomial is \\(np\\) and its variance is \\(np (1 - p)\\). When the observed variance exceeds this amount—after conditioning on all the predictor variables—this implies that some omitted variable is producing additional dispersion in the observed counts. What could go wrong, if we ignore the over-dispersion? Ignoring it can lead to all of the same problems as ignoring any predictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or producing spurious inferences. (p, 346, emphasis in the original) In this chapter we’ll cope with the problem using continuous mixture models—first the beta-binomial and then the gamma-Poisson (a.k.a. negative binomial). 11.3.1 Beta-binomial. A beta-binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success. And predictor variables change the shape of this distribution, instead of directly determining the probability of each success. (p, 347, emphasis in the original) Unfortunately, we need to digress. As it turns out, there are multiple ways to parameterize the beta distribution and we’ve run square into two. In the text, McElreath wrote the beta distribution has two parameters, an average probability \\(\\overline{p}\\) and a shape parameter \\(\\theta\\). In his R code 11.24, which we’ll reproduce in a bit, he demonstrated that parameterization with the rethinking::dbeta2() function. The nice thing about this parameterization is how intuitive the pbar parameter is. If you want a beta with an average of .2, you set pbar &lt;- .2. If you want the distribution to be more or less certain, make the theta argument more or less large, respectively. However, the beta density is typically defined in terms of \\(\\alpha\\) and \\(\\beta\\). If you denote the data as \\(y\\), this follows the form \\[\\text{Beta} (y | \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{\\text B (\\alpha, \\beta)}\\] which you can verify in the Continuous Distributions on [0, 1] section of the Stan reference manual. In the formula, \\(\\text B\\) stands for the Beta function, which computes a normalizing constant, which you can learn about in the Mathematical Functions of the Stan reference manual. This is all important to be aware of because when we defined that beta prior for zi in the last model, it was using this parameterization. Also, if you look at the base R dbeta() function, you’ll learn it takes two parameters, shape1 and shape2. Those uncreatively-named parameters are the same \\(\\alpha\\) and \\(\\beta\\) from the density, above. They do not correspond to the pbar and theta parameters of McEreath’s rethinking::dbeta2(). McElreath had good reason for using dbeta2(). Beta’s typical \\(\\alpha\\) and \\(\\beta\\) parameters aren’t the most intuitive to use; the parameters in McElreath’s dbeta2() are much nicer. But if you’re willing to dive deeper, it turns out you can find the mean of a beta distribution in terms of \\(\\alpha\\) and \\(\\beta\\) like this \\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\] We can talk about the spread of the distribution, sometimes called \\(\\kappa\\), in terms \\(\\alpha\\) and \\(\\beta\\) like this \\[\\kappa = \\alpha + \\beta\\] With \\(\\mu\\) and \\(\\kappa\\) in hand, we can even find the \\(SD\\) of a beta distribution with \\[\\sigma = \\sqrt{\\mu (1 - \\mu) / (\\kappa + 1)}\\] I’m explicate all this because McElreath’s pbar is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) and his theta is \\(\\kappa = \\alpha + \\beta\\). This is great news because it means that we can understand what McElreath did with his beta2() function in terms of base R’s dbeta() function. Which also means that we can understand the distribution of the beta parameters used in brms::brm(). To demonstrate, let’s walk through McElreath’s R code 11.25. pbar &lt;- 0.5 theta &lt;- 5 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01))) + geom_ribbon(aes(x = x, ymin = 0, ymax = rethinking::dbeta2(x, pbar, theta)), fill = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(paste(&quot;The &quot;, beta, &quot; distribution&quot;)), subtitle = expression(paste(&quot;Defined in terms of &quot;, mu, &quot; (i.e., pbar) and &quot;, kappa, &quot; (i.e., theta)&quot;)), x = &quot;probability space&quot;, y = &quot;density&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) In his 2014 text, Doing Bayesian Data Analysis, Kruschke provided code for a convenience function that will take pbar and theta as inputs and return the corresponding \\(\\alpha\\) and \\(\\beta\\) values. Here’s the function: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now we can use Kruschke’s betaABfromMeanKappa() to find the \\(\\alpha\\) and \\(\\beta\\) values corresponding to pbar and theta. betaABfromMeanKappa(mean = pbar, kappa = theta) ## $a ## [1] 2.5 ## ## $b ## [1] 2.5 And finally, we can double check that all of this works. Here’s the same distribution but defined in terms of \\(\\alpha\\) and \\(\\beta\\). ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01))) + geom_ribbon(aes(x = x, ymin = 0, ymax = dbeta(x, 2.5, 2.5)), fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(paste(&quot;The &quot;, beta, &quot; distribution&quot;)), subtitle = expression(paste(&quot;This time defined in terms of &quot;, alpha, &quot; and &quot;, beta)), x = &quot;probability space&quot;, y = &quot;density&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) McElreath encouraged us to “explore different values for pbar and theta” (p. 348). Here’s a grid of plots with pbar = c(.25, .5, .75) and theta = c(5, 10, 15) # data tibble(pbar = c(.25, .5, .75)) %&gt;% expand(pbar, theta = c(5, 15, 30)) %&gt;% expand(nesting(pbar, theta), x = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(density = rethinking::dbeta2(x, pbar, theta), mu = str_c(&quot;mu == &quot;, pbar %&gt;% str_remove(., &quot;0&quot;)), kappa = str_c(&quot;kappa == &quot;, theta)) %&gt;% mutate(kappa = factor(kappa, levels = c(&quot;kappa == 30&quot;, &quot;kappa == 15&quot;, &quot;kappa == 5&quot;))) %&gt;% # plot ggplot() + geom_ribbon(aes(x = x, ymin = 0, ymax = density), fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(breaks = c(0, .5, 1)) + scale_y_continuous(NULL, labels = NULL) + labs(x = &quot;probability space&quot;, y = &quot;density&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), axis.ticks.y = element_blank()) + facet_grid(kappa ~ mu, labeller = label_parsed) If you’d like to see how to make a similar plot in terms of \\(\\alpha\\) and \\(\\beta\\), see the chapter 6 document of my project recoding Kruschke’s text into tidyverse and brms code. But remember, we’re not fitting a beta model. We’re using the beta-binomial. “We’re going to bind our linear model to \\(\\overline p\\), so that changes in predictor variables change the central tendency of the distribution” (p. 348). The statistical model we’ll be fitting follows the form \\[ \\begin{eqnarray} \\text{admit}_i &amp; \\sim &amp; \\text{BetaBinomial} (n_i, \\overline p_i, \\theta)\\\\ \\text{logit} (\\overline p_i) &amp; = &amp; \\alpha \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 2) \\\\ \\theta &amp; \\sim &amp; \\text{Exponential} (1) \\end{eqnarray} \\] Here the size \\(n = \\text{applications}\\). Before we fit, we have an additional complication. The beta-binomial distribution is not implemented in brms at this time. However, brms versions 2.2.0 and above allow users to define custom distributions. You can find the handy vignette here. Happily, Bürkner even used the beta-binomial distribution as the exemplar in the vignette. Before we get carried away, let’s load the data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Unload rethinking and load brms. rm(UCBadmit) detach(package:rethinking, unload = T) library(brms) I’m not going to go into great detail explaining the ins and outs of making custom distributions for brm(). You’ve got Bürkner’s vignette for that. For our purposes, we need two preparatory steps. First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some relevant Stan functions. beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 0), type = &quot;int&quot;, vars = &quot;trials[n]&quot; ) stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int T) { return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int T) { return beta_binomial_rng(T, mu * phi, (1 - mu) * phi); } &quot; With that out of the way, we’re almost ready to test this baby out. Before we do, a point of clarification: What McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar{p}\\), Bürkner simply called \\(\\mu\\). b11.5 &lt;- brm(data = d, family = beta_binomial2, # Here&#39;s our custom likelihood admit | trials(applications) ~ 1, prior = c(prior(normal(0, 2), class = Intercept), prior(exponential(1), class = phi)), iter = 4000, warmup = 1000, cores = 2, chains = 2, stan_funs = stan_funs) Success, our results look a lot like those in the text! print(b11.5) ## Family: beta_binomial2 ## Links: mu = logit; phi = identity ## Formula: admit | trials(applications) ~ 1 ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.38 0.31 -0.99 0.23 4518 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## phi 2.78 0.97 1.26 5.00 3626 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the corresponding posterior_samples() data object looks like. post &lt;- posterior_samples(b11.5) head(post) ## b_Intercept phi lp__ ## 1 -0.6521994 2.756449 -70.47105 ## 2 -0.2172964 2.655298 -70.19371 ## 3 -0.5630268 2.947290 -70.22836 ## 4 -0.4356159 2.894025 -70.05242 ## 5 -0.4290582 3.209413 -70.11186 ## 6 -0.5040190 2.353383 -70.28866 Here’s our median and percentile-based 95% interval. post %&gt;% tidybayes::median_qi(inv_logit_scaled(b_Intercept)) %&gt;% mutate_if(is.double, round, digits = 3) ## inv_logit_scaled(b_Intercept) .lower .upper .width .point .interval ## 1 0.407 0.271 0.557 0.95 median qi With our post object in hand, here’s our Figure 11.5.a. tibble(x = 0:1) %&gt;% ggplot(aes(x = x)) + stat_function(fun = rethinking::dbeta2, args = list(prob = mean(inv_logit_scaled(post[, 1])), theta = mean(post[, 2])), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1.5) + mapply(function(prob, theta) { stat_function(fun = rethinking::dbeta2, args = list(prob = prob, theta = theta), alpha = .2, color = canva_pal(&quot;Green fields&quot;)(4)[4]) }, # Enter `prob` and `theta`, here prob = inv_logit_scaled(post[1:100, 1]), theta = post[1:100, 2]) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = 0:3) + labs(x = &quot;probability admit&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) I got the idea to nest stat_function() within mapply() from shadow’s answer to this Stack Overflow question. Before we can do our variant of Figure 11.5.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, fitted_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it. expose_functions(b11.5, vectorize = TRUE) # Required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, draws) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi N &lt;- draws$data$trials[i] y &lt;- draws$data$Y[i] beta_binomial2_lpmf(y, mu, phi, N) } predict_beta_binomial2 &lt;- function(i, draws, ...) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi N &lt;- draws$data$trials[i] beta_binomial2_rng(mu, phi, N) } # Required to use `fitted()` fitted_beta_binomial2 &lt;- function(draws) { mu &lt;- draws$dpars$mu trials &lt;- draws$data$trials trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } With those intermediary steps out of the way, we’re ready to make Figure 11.5.b. # The prediction intervals predict(b11.5) %&gt;% as_tibble() %&gt;% transmute(ll = Q2.5, ul = Q97.5) %&gt;% # The fitted intervals bind_cols( fitted(b11.5) %&gt;% as_tibble() ) %&gt;% # The original data used to fit the model bind_cols(b11.5$data) %&gt;% mutate(case = 1:12) %&gt;% # plot ggplot(aes(x = case)) + geom_linerange(aes(ymin = ll / applications, ymax = ul / applications), color = canva_pal(&quot;Green fields&quot;)(4)[1], size = 2.5, alpha = 1/4) + geom_pointrange(aes(ymin = Q2.5 / applications, ymax = Q97.5 / applications, y = Estimate/applications), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1/2, shape = 1) + geom_point(aes(y = admit/applications), color = canva_pal(&quot;Green fields&quot;)(4)[2], size = 2) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:1) + labs(subtitle = &quot;Posterior validation check&quot;, y = &quot;Admittance probability&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) As in the text, the raw data are consistent with the prediction intervals. But those intervals are so incredibly wide, they’re hardly an endorsement of the model. Once we learn about hierarchical models, we’ll be able to do much better. 11.3.2 Negative-binomial or gamma-Poisson. Recall the Poisson distribution presumes \\(\\sigma^2\\) scales with \\(\\mu\\). The negative binomial distribution relaxes this assumption and presumes “each Poisson count observation has its own rate. It estimates the shape of a gamma distribution to describe the Poisson rates across cases” (p. 350). Here’s a look at the \\(\\gamma\\) distribution. mu &lt;- 3 theta &lt;- 1 ggplot(data = tibble(x = seq(from = 0, to = 12, by = .01)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = rethinking::dgamma2(x, mu, theta)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_vline(xintercept = mu, linetype = 3, color = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(NULL, breaks = c(0, mu, 10)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + ggtitle(expression(paste(&quot;Our sweet &quot;, gamma, &quot;(3, 1)&quot;))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) 11.3.2.1 Bonus: Let’s fit a negative-binomial model. McElreath didn’t give an example of negative-binomial regression in the text. Here’s one with the UCBadmit data. brm(data = d, family = negbinomial, admit ~ 1 + applicant.gender, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(gamma(0.01, 0.01), class = shape)), # this is the brms default iter = 4000, warmup = 1000, cores = 2, chains = 2) %&gt;% print() ## Family: negbinomial ## Links: mu = log; shape = identity ## Formula: admit ~ 1 + applicant.gender ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 4.68 0.40 3.98 5.57 4534 1.00 ## applicant.gendermale 0.59 0.49 -0.42 1.56 4586 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## shape 1.23 0.48 0.51 2.30 3999 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since the negative-binomial model uses the log link, you need to exponentiate to get the estimates back into the count metric. E.g., exp(4.7) ## [1] 109.9472 Also, you may have noticed we used the brms default prior(gamma(0.01, 0.01), class = shape) for the shape parameter. Here’s what that prior looks like. ggplot(data = tibble(x = seq(from = 0, to = 60, by = .1)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 0.01, 0.01)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:50) + ggtitle(expression(paste(&quot;Our brms default &quot;, gamma, &quot;(0.01, 0.01) prior&quot;))) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) 11.3.3 Over-dispersion, entropy, and information criteria. Both the beta-binomial and the gamma-Poisson models are maximum entropy for the same constraints as the regular binomial and Poisson. They just try to account for unobserved heterogeneity in probabilities and rates. So while they can be a lot harder to fit to data, they can be usefully conceptualized much like ordinary binomial and Poisson GLMs. So in terms of model comparison using information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson (negative-binomial) is a Poisson model. (pp. 350–351) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 broom_0.4.5 ggthemes_3.5.0 forcats_0.3.0 stringr_1.3.1 ## [6] dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ## [11] tidyverse_1.2.1 brms_2.5.0 Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 ## [16] ggplot2_3.0.0 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 tidyselect_0.2.4 ## [5] htmlwidgets_1.2 munsell_0.5.0 codetools_0.2-15 nleqslv_3.3.2 ## [9] DT_0.4 miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 rstudioapi_0.7 ## [17] stats4_3.5.1 Rttf2pt1_1.3.7 bayesplot_1.6.0 labeling_0.3 ## [21] mnormt_1.5-5 bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 ## [25] xfun_0.3 R6_2.2.2 markdown_0.8 HDInterval_0.2.0 ## [29] reshape_0.8.7 assertthat_0.2.0 promises_1.0.1 scales_0.5.0 ## [33] beeswarm_0.2.3 gtable_0.2.0 rethinking_1.59 rlang_0.2.1 ## [37] extrafontdb_1.0 lazyeval_0.2.1 inline_0.3.15 yaml_2.1.19 ## [41] reshape2_1.4.3 abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [45] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 ## [49] extrafont_0.17 tools_3.5.1 bookdown_0.7 psych_1.8.4 ## [53] RColorBrewer_1.1-2 ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [57] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 ## [61] haven_1.1.2 magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 ## [65] tidybayes_1.0.1 matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 ## [69] mime_0.5 evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [73] shinystan_2.5.0 readxl_1.1.0 rstantools_1.5.0 compiler_3.5.1 ## [77] maps_3.3.0 crayon_1.3.4 htmltools_0.3.6 later_0.7.3 ## [81] lubridate_1.7.4 MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 ## [85] bindr_0.1.1 igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [89] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 ## [93] rvest_0.3.2 digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 ## [97] shiny_1.1.0 gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [101] bindrcpp_0.2.2 mapproj_1.2.6 viridisLite_0.3.0 pillar_1.2.3 ## [105] lattice_0.20-35 loo_2.0.0 httr_1.3.1 glue_1.2.0 ## [109] xts_0.10-2 shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["multilevel-models.html", "12 Multilevel Models 12.1 Example: Multilevel tadpoles 12.2 Varying effects and the underfitting/overfitting trade-off 12.3 More than one type of cluster 12.4 Multilevel posterior predictions Reference Session info", " 12 Multilevel Models Multilevel models… remember features of each cluster in the data as they learn about all of the clusters. Depending upon the variation among clusters, which is learned from the data as well, the model pools information across clusters. This pooling tends to improve estimates about each cluster. This improved estimation leads to several, more pragmatic sounding, benefits of the multilevel approach. (p. 356) These benefits include: improved estimates for repeated sampling (i.e., in longitudinal data) improved estimates when there are imbalances among subsamples estimates of the variation across subsamples avoiding simplistic averaging by retaining variation across subsamples All of these benefits flow out of the same strategy and model structure. You learn one basic design and you get all of this for free. When it comes to regression, multilevel regression deserves to be the default approach. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous. It is better to begin to build a multilevel analysis, and then realize it’s unnecessary, than to overlook it. And once you grasp the basic multilevel stragety, it becomes much easier to incorporate related tricks such as allowing for measurement error in the data and even model missing data itself (Chapter 14). (p. 356) I’m totally on board with this. After learning about the multilevel model, I see it everywhere. For more on the sentiment it should be the default, check out McElreath’s blog post, Multilevel Regression as Default. 12.1 Example: Multilevel tadpoles Let’s get the reedfrogs data from rethinking. library(rethinking) data(reedfrogs) d &lt;- reedfrogs Detach rethinking and load brms. rm(reedfrogs) detach(package:rethinking, unload = T) library(brms) Go ahead and acquaint yourself with the reedfrogs. library(tidyverse) d %&gt;% glimpse() ## Observations: 48 ## Variables: 5 ## $ density &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25, 25, 25, 25, 25, ... ## $ pred &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pred, pred, pred, pred, no, no, ... ## $ size &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, big, big, small, small, small,... ## $ surv &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23, 22, 25, 23, 23, 23, 21, 6, 13... ## $ propsurv &lt;dbl&gt; 0.9000000, 1.0000000, 0.7000000, 1.0000000, 0.9000000, 0.9000000, 1.0000000, 0.9000000, ... Making the tank cluster variable is easy. d &lt;- d %&gt;% mutate(tank = 1:nrow(d)) Here’s the formula for the un-pooled model in which each tank gets its own intercept. \\[ \\begin{eqnarray} \\text{surv}_i &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim &amp; \\text{Normal} (0, 5) \\end{eqnarray} \\] And \\(n_i = \\text{density}_i\\). Now we’ll fit this simple aggregated binomial model much like we practiced in Chapter 10. b12.1 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 0 + factor(tank), prior(normal(0, 5), class = b), iter = 2000, warmup = 500, chains = 4, cores = 4) The formula for the multilevel alternative is \\[ \\begin{eqnarray} \\text{surv}_i &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim &amp; \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\] You specify the corresponding multilevel model like this. b12.2 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 1 + (1 | tank), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, chains = 4, cores = 4) The syntax for the varying effects follows the lme4 style, ( &lt;varying predictor(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(cauchy(0, 1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. Instead of computing the information criteria for each model, saving the results as objects and then placing those objects in compare_ic(), we can also just put both fit objects in waic() or loo(). waic(b12.1, b12.2) ## WAIC SE ## b12.1 201.24 9.47 ## b12.2 200.86 7.25 ## b12.1 - b12.2 0.38 4.56 loo(b12.1, b12.2) ## Warning: Found 40 observations with a pareto_k &gt; 0.7 in model &#39;b12.1&#39;. With this many problematic ## observations, it may be more appropriate to use &#39;kfold&#39; with argument &#39;K = 10&#39; to perform 10-fold cross- ## validation rather than LOO. ## Warning: Found 41 observations with a pareto_k &gt; 0.7 in model &#39;b12.2&#39;. With this many problematic ## observations, it may be more appropriate to use &#39;kfold&#39; with argument &#39;K = 10&#39; to perform 10-fold cross- ## validation rather than LOO. ## LOOIC SE ## b12.1 229.25 10.60 ## b12.2 229.97 8.87 ## b12.1 - b12.2 -0.72 6.42 Note those “pareto_k &gt; 0.7” warnings. We can follow the advice and use the kfold() function, instead. We’ll also go ahead and specify K = 10, as recommended. But beware, this takes a few minutes. kf &lt;- kfold(b12.1, b12.2, K = 10, cores = 4) kf ## KFOLDIC SE ## b12.1 323.89 13.19 ## b12.2 264.51 13.12 ## b12.1 - b12.2 59.39 8.35 The \\(K\\)-fold cross-validation difference of 59, with a standard error around 8, suggests that model b12.2 is the clear favorite relative to b12.1. For more on the kfold() function, see the brms reference manual. But here’s our prep work for Figure 12.1. post &lt;- posterior_samples(b12.2) postMdn &lt;- coef(b12.2, robust = T)$tank[, , ] %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(postMdn = inv_logit_scaled(Estimate)) postMdn ## # A tibble: 48 x 11 ## Estimate Est.Error Q2.5 Q97.5 density pred size surv propsurv tank postMdn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2.07 0.854 0.588 4.02 10 no big 9 0.9 1 0.887 ## 2 2.95 1.08 1.16 5.50 10 no big 10 1 2 0.950 ## 3 0.969 0.656 -0.266 2.39 10 no big 7 0.7 3 0.725 ## 4 2.94 1.06 1.15 5.49 10 no big 10 1 4 0.950 ## 5 2.05 0.859 0.588 4.01 10 no small 9 0.9 5 0.886 ## 6 2.07 0.862 0.610 4.04 10 no small 9 0.9 6 0.888 ## 7 2.95 1.10 1.20 5.48 10 no small 10 1 7 0.950 ## 8 2.06 0.851 0.609 4.03 10 no small 9 0.9 8 0.887 ## 9 -0.173 0.601 -1.38 1.01 10 pred big 4 0.4 9 0.457 ## 10 2.05 0.853 0.578 4.01 10 pred big 9 0.9 10 0.886 ## # ... with 38 more rows For kicks and giggles, let’s use a FiveThirtyEight-like theme for our plots. An easy way to do so is with help from the ggthemes package. # install.packages(&quot;ggthemes&quot;, dependencies = T) library(ggthemes) Finally, here’s the ggplot2 code to reproduce Figure 12.1. postMdn %&gt;% ggplot(aes(x = tank, y = postMdn)) + geom_hline(yintercept = inv_logit_scaled(median(post$b_Intercept)), linetype = 2, size = 1/4) + geom_vline(xintercept = c(16.5, 32.5), size = 1/4) + geom_point(aes(y = propsurv), color = &quot;orange2&quot;) + geom_point(shape = 1) + coord_cartesian(ylim = c(0, 1)) + scale_x_continuous(breaks = c(1, 16, 32, 48)) + labs(title = &quot;Multilevel shrinkage!&quot;, subtitle = &quot;The empirical proportions are in orange while the model-\\nimplied proportions are the black circles. The dashed line is\\nthe model-implied average survival proportion.&quot;) + annotate(&quot;text&quot;, x = c(8, 16 + 8, 32 + 8), y = 0, label = c(&quot;small tanks&quot;, &quot;medium tanks&quot;, &quot;large tanks&quot;)) + theme_fivethirtyeight() + theme(panel.grid = element_blank()) Here is our version of Figure 12.2.a. tibble(x = c(-4, 5)) %&gt;% ggplot(aes(x = x)) + mapply(function(mean, sd) { stat_function(fun = dnorm, args = list(mean = mean, sd = sd), alpha = .2, color = &quot;orange2&quot;) }, # Enter means and standard deviations here mean = post[1:100, 1], sd = post[1:100, 2] ) + labs(title = &quot;Population survival distribution&quot;, subtitle = &quot;The Gaussians are on the log-odds scale.&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-3, 4)) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 13), plot.subtitle = element_text(size = 10)) Note the uncertainty in terms of both location \\(\\alpha\\) and scale \\(\\sigma\\). Now here’s the code for Figure 12.2.b. ggplot(data = post, aes(x = rnorm(n = nrow(post), mean = post[, 1], sd = post[, 2]) %&gt;% inv_logit_scaled())) + geom_density(size = 0, fill = &quot;orange2&quot;) + labs(title = &quot;Probability of survival&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme_fivethirtyeight() Note how we sampled 12,000 imaginary tanks rather than McElreath’s 8,000. This is because we had 12,000 HMC iterations (i.e., execute nrow(post)). The aes() code, above, was a bit much. To get a sense of how it worked, consider this: rnorm(n = 1, mean = post[, 1], sd = post[, 2]) %&gt;% inv_logit_scaled() ## [1] 0.8743237 First, we took one random draw from a normal distribution with a mean of the first row in post[, 1] and a standard deviation of the value from the first row in post[, 2], and passed it through the inv_logit_scaled() function. By replacing the 1 with nrow(post), we do this nrow(post) times (i.e., 12,000). So our orange density is the summary of that process. 12.1.0.1 Overthinking: Prior for variance components. Yep, you can use the exponential distribution for your priors in brms. Here it is for model b12.2. b12.2.e &lt;- update(b12.2, prior = c(prior(normal(0, 1), class = Intercept), prior(exponential(1), class = sd))) The model summary: print(b12.2.e) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.61 0.21 1.24 2.07 2030 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.30 0.25 0.81 1.78 1490 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you’re curious how the exponential prior compares to the posterior, you might just plot. tibble(x = seq(from = 0, to = 6, by = .01)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dexp(x, rate = 1)), # the prior fill = &quot;orange2&quot;, alpha = 1/3) + geom_density(data = posterior_samples(b12.2.e), # the posterior aes(x = sd_tank__Intercept), fill = &quot;orange2&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 5)) + labs(title = &quot;Bonus prior/posterior plot\\nfor sd_tank__Intercept&quot;, subtitle = &quot;The prior is the semitransparent ramp in the\\nbackground. The posterior is the solid orange\\nmound.&quot;) + theme_fivethirtyeight() 12.2 Varying effects and the underfitting/overfitting trade-off Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp… A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provides better estimates is that they do a better job trading off underfitting and overfitting. (p. 364) In this section, we explicate this by contrasting three perspectives: Complete pooling (i.e., a single-\\(\\alpha\\) model) No pooling (i.e., the single-level \\(\\alpha_{\\text{tank}_i}\\) model) Partial pooling (i.e., the multilevel model for which \\(\\alpha_{\\text{tank}} \\sim \\text{Normal} (\\alpha, \\sigma)\\)) To demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 365) 12.2.1 The model. The simulation formula should look familiar. \\[ \\begin{eqnarray} \\text{surv}_i &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{pond}_i} \\\\ \\alpha_{\\text{pond}} &amp; \\sim &amp; \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\] 12.2.2 Assign values to the parameters. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(1222) # make results reproducible ( dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) ) ## # A tibble: 60 x 3 ## pond ni true_a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 1.95 ## 2 2 5 0.249 ## 3 3 5 0.521 ## 4 4 5 1.20 ## 5 5 5 -3.34 ## 6 6 5 0.184 ## 7 7 5 1.63 ## 8 8 5 0.336 ## 9 9 5 1.83 ## 10 10 5 -0.865 ## # ... with 50 more rows 12.2.3 Sumulate survivors. Each pond \\(i\\) has \\(n_i\\) potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival \\(p_i\\). This probability \\(p_i\\) is implied by the model definition, and is equal to: \\[p_i = \\frac{\\text{exp} (\\alpha_i)}{1 + \\text{exp} (\\alpha_i)}\\] The model uses a logit link, and so the probability is defined by the [inv_logit_scaled()] function. (p. 367) set.seed(1223) ( dsim &lt;- dsim %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) ) ## # A tibble: 60 x 4 ## pond ni true_a si ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 5 1.95 4 ## 2 2 5 0.249 4 ## 3 3 5 0.521 4 ## 4 4 5 1.20 4 ## 5 5 5 -3.34 0 ## 6 6 5 0.184 2 ## 7 7 5 1.63 5 ## 8 8 5 0.336 2 ## 9 9 5 1.83 4 ## 10 10 5 -0.865 0 ## # ... with 50 more rows 12.2.4 Compute the no-pooling estimates. The no-pooling estimates (i.e., \\(\\alpha_{\\text{tank}_i}\\)) are a function of simple algebra. ( dsim &lt;- dsim %&gt;% mutate(p_nopool = si / ni) ) ## # A tibble: 60 x 5 ## pond ni true_a si p_nopool ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 1.95 4 0.8 ## 2 2 5 0.249 4 0.8 ## 3 3 5 0.521 4 0.8 ## 4 4 5 1.20 4 0.8 ## 5 5 5 -3.34 0 0 ## 6 6 5 0.184 2 0.4 ## 7 7 5 1.63 5 1 ## 8 8 5 0.336 2 0.4 ## 9 9 5 1.83 4 0.8 ## 10 10 5 -0.865 0 0 ## # ... with 50 more rows “These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induct no regularization” (p. 367). 12.2.5 Compute the partial-pooling estimates. To follow along with McElreath, set chains = 1, cores = 1 to fit with one chain. b12.3 &lt;- brm(data = dsim, family = binomial, si | trials(ni) ~ 1 + (1 | pond), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 10000, warmup = 1000, chains = 1, cores = 1) print(b12.3) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.43 0.23 1.04 1.93 3111 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.54 0.22 1.13 1.98 3085 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not aware that you can use McElreath’s depth=2 trick in brms for summary() or print(). But can get that information with the coef() function. coef(b12.3)$pond[c(1:2, 59:60), , ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.58 0.92 -0.08 3.52 ## 2 1.58 0.92 -0.08 3.55 ## 59 2.68 0.63 1.58 4.05 ## 60 0.99 0.38 0.27 1.76 Note how we just peeked at the top and bottom two rows with the c(1:2, 59:60) part of the code, there. Somewhat discouragingly, coef() doesn’t return the ‘Eff.Sample’ or ‘Rhat’ columns as in McElreath’s output. We can still extract that information, though. For \\(\\hat{R}\\), the solution is simple; use the brms::rhat() function. rhat(b12.3) ## b_Intercept sd_pond__Intercept r_pond[1,Intercept] r_pond[2,Intercept] r_pond[3,Intercept] ## 1.0000059 0.9998954 0.9998894 0.9998926 1.0000145 ## r_pond[4,Intercept] r_pond[5,Intercept] r_pond[6,Intercept] r_pond[7,Intercept] r_pond[8,Intercept] ## 0.9998937 0.9998977 0.9998892 0.9999315 0.9999297 ## r_pond[9,Intercept] r_pond[10,Intercept] r_pond[11,Intercept] r_pond[12,Intercept] r_pond[13,Intercept] ## 0.9999757 0.9999514 0.9999386 0.9999411 0.9998939 ## r_pond[14,Intercept] r_pond[15,Intercept] r_pond[16,Intercept] r_pond[17,Intercept] r_pond[18,Intercept] ## 0.9999678 0.9999174 1.0000529 1.0000404 0.9999700 ## r_pond[19,Intercept] r_pond[20,Intercept] r_pond[21,Intercept] r_pond[22,Intercept] r_pond[23,Intercept] ## 0.9998949 0.9998985 0.9998891 0.9998928 0.9999322 ## r_pond[24,Intercept] r_pond[25,Intercept] r_pond[26,Intercept] r_pond[27,Intercept] r_pond[28,Intercept] ## 0.9999352 0.9999363 0.9998922 0.9998911 0.9999259 ## r_pond[29,Intercept] r_pond[30,Intercept] r_pond[31,Intercept] r_pond[32,Intercept] r_pond[33,Intercept] ## 0.9999971 1.0000482 0.9998963 0.9999102 0.9998930 ## r_pond[34,Intercept] r_pond[35,Intercept] r_pond[36,Intercept] r_pond[37,Intercept] r_pond[38,Intercept] ## 0.9999403 0.9998928 0.9999676 0.9999138 0.9998943 ## r_pond[39,Intercept] r_pond[40,Intercept] r_pond[41,Intercept] r_pond[42,Intercept] r_pond[43,Intercept] ## 0.9999134 0.9999159 0.9999320 1.0000600 0.9999650 ## r_pond[44,Intercept] r_pond[45,Intercept] r_pond[46,Intercept] r_pond[47,Intercept] r_pond[48,Intercept] ## 0.9999229 0.9998911 0.9998898 0.9998894 0.9999701 ## r_pond[49,Intercept] r_pond[50,Intercept] r_pond[51,Intercept] r_pond[52,Intercept] r_pond[53,Intercept] ## 0.9999082 0.9998923 0.9999195 0.9998889 0.9998916 ## r_pond[54,Intercept] r_pond[55,Intercept] r_pond[56,Intercept] r_pond[57,Intercept] r_pond[58,Intercept] ## 0.9998930 0.9998973 0.9999618 0.9999326 0.9998965 ## r_pond[59,Intercept] r_pond[60,Intercept] lp__ ## 0.9998951 0.9999115 0.9999012 Extracting the ‘Eff.Sample’ values is a little more complicated. There is no effsamples() function. However, we do have neff_ratio(). neff_ratio(b12.3) ## b_Intercept sd_pond__Intercept r_pond[1,Intercept] r_pond[2,Intercept] r_pond[3,Intercept] ## 0.3427542 0.3456433 1.0000000 1.0000000 1.0000000 ## r_pond[4,Intercept] r_pond[5,Intercept] r_pond[6,Intercept] r_pond[7,Intercept] r_pond[8,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[9,Intercept] r_pond[10,Intercept] r_pond[11,Intercept] r_pond[12,Intercept] r_pond[13,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[14,Intercept] r_pond[15,Intercept] r_pond[16,Intercept] r_pond[17,Intercept] r_pond[18,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[19,Intercept] r_pond[20,Intercept] r_pond[21,Intercept] r_pond[22,Intercept] r_pond[23,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[24,Intercept] r_pond[25,Intercept] r_pond[26,Intercept] r_pond[27,Intercept] r_pond[28,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[29,Intercept] r_pond[30,Intercept] r_pond[31,Intercept] r_pond[32,Intercept] r_pond[33,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[34,Intercept] r_pond[35,Intercept] r_pond[36,Intercept] r_pond[37,Intercept] r_pond[38,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[39,Intercept] r_pond[40,Intercept] r_pond[41,Intercept] r_pond[42,Intercept] r_pond[43,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[44,Intercept] r_pond[45,Intercept] r_pond[46,Intercept] r_pond[47,Intercept] r_pond[48,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[49,Intercept] r_pond[50,Intercept] r_pond[51,Intercept] r_pond[52,Intercept] r_pond[53,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[54,Intercept] r_pond[55,Intercept] r_pond[56,Intercept] r_pond[57,Intercept] r_pond[58,Intercept] ## 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## r_pond[59,Intercept] r_pond[60,Intercept] lp__ ## 1.0000000 1.0000000 0.2037963 The brms::neff_ratio() function returns ratios of the effective samples over the total number of post-warmup iterations. So if we know the neff_ratio() values and the number of post-warmup iterations, the ‘Eff.Sample’ values are just a little algebra away. A quick solution is to look at the ‘total post-warmup samples’ line at the top of our print() output. Another way is to extract that information from our brm() fit object. I’m not aware of a way to do that directly, but we can extract the iter value (i.e., b12.2$fit@sim$iter), the warmup value (i.e., b12.2$fit@sim$warmup), and the number of chains (i.e., b12.2$fit@sim$chains). With those values in hand, simple algebra will return the ‘total post-warmup samples’ value. E.g., ( n_iter &lt;- (b12.3$fit@sim$iter - b12.3$fit@sim$warmup) * b12.3$fit@sim$chains ) ## [1] 9000 And now we have n_iter, we can calculate the ‘Eff.Sample’ values. neff_ratio(b12.3) %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% rename(parameter = rowname, neff_ratio = &quot;.&quot;) %&gt;% mutate(eff_sample = (neff_ratio * n_iter) %&gt;% round(digits = 0)) %&gt;% head() ## parameter neff_ratio eff_sample ## 1 b_Intercept 0.3427542 3085 ## 2 sd_pond__Intercept 0.3456433 3111 ## 3 r_pond[1,Intercept] 1.0000000 9000 ## 4 r_pond[2,Intercept] 1.0000000 9000 ## 5 r_pond[3,Intercept] 1.0000000 9000 ## 6 r_pond[4,Intercept] 1.0000000 9000 Digressions aside, let’s get ready for the diagnostic plot of Figure 12.3. dsim %&gt;% glimpse() ## Observations: 60 ## Variables: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2... ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,... ## $ true_a &lt;dbl&gt; 1.9473216, 0.2487733, 0.5214389, 1.1987112, -3.3372853, 0.1843209, 1.6260145, 0.3359535,... ## $ si &lt;int&gt; 4, 4, 4, 4, 0, 2, 5, 2, 4, 0, 1, 5, 5, 5, 5, 10, 0, 5, 10, 10, 7, 10, 7, 6, 10, 9, 6, 10... ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 0.80, 0.80, 0.00, 0.40, 1.00, 0.40, 0.80, 0.00, 0.20, 1.00, 1.00, 1.00, 1.00... # we could have included this step in the block of code below, if we wanted to p_partpool &lt;- coef(b12.3)$pond[, , ] %&gt;% as_tibble() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) dsim &lt;- dsim %&gt;% bind_cols(p_partpool) %&gt;% mutate(p_true = inv_logit_scaled(true_a)) %&gt;% mutate(nopool_error = abs(p_nopool - p_true), partpool_error = abs(p_partpool - p_true)) dsim %&gt;% glimpse() ## Observations: 60 ## Variables: 9 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,... ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1... ## $ true_a &lt;dbl&gt; 1.9473216, 0.2487733, 0.5214389, 1.1987112, -3.3372853, 0.1843209, 1.6260145, 0.33... ## $ si &lt;int&gt; 4, 4, 4, 4, 0, 2, 5, 2, 4, 0, 1, 5, 5, 5, 5, 10, 0, 5, 10, 10, 7, 10, 7, 6, 10, 9,... ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 0.80, 0.80, 0.00, 0.40, 1.00, 0.40, 0.80, 0.00, 0.20, 1.00, 1.00, 1.00... ## $ p_partpool &lt;dbl&gt; 0.8294316, 0.8285394, 0.8288654, 0.8293981, 0.2394738, 0.5428037, 0.9304361, 0.542... ## $ p_true &lt;dbl&gt; 0.8751543, 0.5618746, 0.6274842, 0.7682954, 0.0343140, 0.5459502, 0.8356229, 0.583... ## $ nopool_error &lt;dbl&gt; 0.075154292, 0.238125444, 0.172515824, 0.031704565, 0.034314001, 0.145950213, 0.16... ## $ partpool_error &lt;dbl&gt; 0.045722683, 0.266664853, 0.201381248, 0.061102621, 0.205159786, 0.003146557, 0.09... Here is our code for Figure 12.3. The extra data processing for dfline is how we get the values necessary for the horizontal summary lines. dfline &lt;- dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key, ni) %&gt;% summarise(mean_error = mean(value)) %&gt;% mutate(x = c( 1, 16, 31, 46), xend = c(15, 30, 45, 60)) dsim %&gt;% ggplot(aes(x = pond)) + geom_vline(xintercept = c(15.5, 30.5, 45.4), color = &quot;white&quot;, size = 2/3) + geom_point(aes(y = nopool_error), color = &quot;orange2&quot;) + geom_point(aes(y = partpool_error), shape = 1) + geom_segment(data = dfline, aes(x = x, xend = xend, y = mean_error, yend = mean_error), color = rep(c(&quot;orange2&quot;, &quot;black&quot;), each = 4), linetype = rep(1:2, each = 4)) + labs(y = &quot;absolute error&quot;, title = &quot;Estimate error by model type&quot;, subtitle = &quot;The horizontal axis displays pond number. The vertical axis measures\\nthe absolute error in the predicted proportion of survivors, compared to\\nthe true value used in the simulation. The higher the point, the worse\\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\\nThe orange and dashed black lines show the average error for each kind\\nof estimate, across each initial density of tadpoles (pond size). Smaller\\nponds produce more error, but the partial pooling estimates are better\\non average, especially in smaller ponds.&quot;) + scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) + annotate(&quot;text&quot;, x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, label = c(&quot;tiny (5)&quot;, &quot;small (10)&quot;, &quot;medium (25)&quot;, &quot;large (35)&quot;)) + theme_fivethirtyeight() + theme(panel.grid = element_blank(), plot.subtitle = element_text(size = 10)) If you wanted to quantify the difference in simple summaries, you might do something like this: dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key) %&gt;% summarise(mean_error = mean(value) %&gt;% round(digits = 3), median_error = median(value) %&gt;% round(digits = 3)) ## # A tibble: 2 x 3 ## key mean_error median_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nopool_error 0.073 0.045 ## 2 partpool_error 0.06 0.045 I originally learned about the multilevel model within the context of longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, took me some time to wrap my head around. If you’re struggling with this, be patient and keep chipping away. When McElreath was lecturing on this topic in 2015, he traced partial pooling to statistician James Stein. In 1977, Efron and Morris wrote the now classic paper, Stein’s Paradox in Statistics, which did a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you’d like more practice seeing how partial pooling works–or if you just like baseball–, check out my project on that example, James-Stein and Bayesian partial pooling. 12.2.5.1 Overthinking: Repeating the pond simulation. Within the brms workflow, we reuse a compiled model with update(). But first, we’ll simulate new data. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(12.251) # for new data, set a new seed new_dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) %&gt;% mutate(p_nopool = si / ni) glimpse(new_dsim) ## Observations: 60 ## Variables: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2... ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,... ## $ true_a &lt;dbl&gt; -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1.59646315, 0.99155593, 0.92697693, 0... ## $ si &lt;int&gt; 2, 5, 4, 1, 2, 3, 3, 3, 3, 4, 2, 1, 1, 5, 5, 4, 9, 8, 8, 7, 9, 10, 9, 6, 4, 7, 8, 7, 9, ... ## $ p_nopool &lt;dbl&gt; 0.40, 1.00, 0.80, 0.20, 0.40, 0.60, 0.60, 0.60, 0.60, 0.80, 0.40, 0.20, 0.20, 1.00, 1.00... Fit the new model. b12.3_new &lt;- update(b12.3, newdata = new_dsim, iter = 10000, warmup = 1000, chains = 1, cores = 1) print(b12.3_new) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: new_dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.16 0.16 0.89 1.50 3274 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.14 0.18 0.80 1.49 4647 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Why not plot the first simulation versus the second one? posterior_samples(b12.3) %&gt;% bind_rows(posterior_samples(b12.3_new)) %&gt;% mutate(model = rep(c(&quot;b12.3&quot;, &quot;b12.3_new&quot;), each = n()/2)) %&gt;% ggplot(aes(x = b_Intercept, y = sd_pond__Intercept)) + stat_density_2d(geom = &quot;raster&quot;, aes(fill = stat(density)), contour = F) + geom_vline(xintercept = a, color = &quot;orange3&quot;, linetype = 3) + geom_hline(yintercept = sigma, color = &quot;orange3&quot;, linetype = 3) + scale_fill_gradient(low = &quot;grey25&quot;, high = &quot;orange3&quot;) + ggtitle(&quot;Our simulation posteriors contrast a bit&quot;, subtitle = expression(paste(alpha, &quot; is on the x and &quot;, sigma, &quot; is on the y, both in log-odds. The dotted lines intersect at the true values.&quot;))) + coord_cartesian(xlim = c(.7, 2), ylim = c(.8, 1.9)) + theme_fivethirtyeight() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~model, ncol = 2) If you’d like the stanfit portion of your brm() object, subset with $fit. Take b12.3, for example. You might check out its structure via b12.3$fit %&gt;% str(). Here’s the actual Stan code. b12.3$fit@ stanmodel ## S4 class stanmodel &#39;0e126f378ddfd8d0780d3e099c4e7266&#39; coded as follows: ## // generated with brms 2.5.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## int Y[N]; // response variable ## int trials[N]; // number of trials ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; J_1[N]; ## int&lt;lower=1&gt; N_1; ## int&lt;lower=1&gt; M_1; ## vector[N] Z_1_1; ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real temp_Intercept; // temporary intercept ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## vector[N_1] z_1[M_1]; // unscaled group-level effects ## } ## transformed parameters { ## // group-level effects ## vector[N_1] r_1_1 = sd_1[1] * (z_1[1]); ## } ## model { ## vector[N] mu = temp_Intercept + rep_vector(0, N); ## for (n in 1:N) { ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; ## } ## // priors including all constants ## target += normal_lpdf(temp_Intercept | 0, 1); ## target += cauchy_lpdf(sd_1 | 0, 1) ## - 1 * cauchy_lccdf(0 | 0, 1); ## target += normal_lpdf(z_1[1] | 0, 1); ## // likelihood including all constants ## if (!prior_only) { ## target += binomial_logit_lpmf(Y | trials, mu); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = temp_Intercept; ## } ## And you can get the data of a given brm() fit object like so. b12.3$data %&gt;% head() ## si ni pond ## 1 4 5 1 ## 2 4 5 2 ## 3 4 5 3 ## 4 4 5 4 ## 5 0 5 5 ## 6 2 5 6 12.3 More than one type of cluster “We can use and often should use more than one type of cluster in the same model” (p. 370). 12.3.1 Multilevel chimpanzees. The initial multilevel update from model b10.4 from the last chapter follows the statistical formula \\[ \\begin{eqnarray} \\text{left_pull}_i &amp; \\sim &amp; \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha + \\alpha_{\\text{actor}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim &amp; \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\] Notice that \\(\\alpha\\) is inside the linear model, not inside the Gaussian prior for \\(\\alpha_\\text{actor}\\). This is mathematically equivalent to what [we] did with the tadpoles earlier in the chapter. You can always take the mean out of a Gaussian distribution and treat that distribution as a constant plus a Gaussian distribution centered on zero. This might seem a little weird at first, so it might help train your intuition by experimenting in R. (p. 371) Behold our two identical Gaussians in a tidy tibble. set.seed(241) two_gaussians &lt;- tibble(y1 = rnorm(n = 1e4, mean = 10, sd = 1), y2 = 10 + rnorm(n = 1e4, mean = 0, sd = 1)) Let’s follow McElreath’s advice to make sure they are same by superimposing the density of one on the other. two_gaussians %&gt;% ggplot() + geom_density(aes(x = y1), size = 0, fill = &quot;orange1&quot;, alpha = 1/3) + geom_density(aes(x = y2), size = 0, fill = &quot;orange4&quot;, alpha = 1/3) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Our simulated Gaussians&quot;) + theme_fivethirtyeight() Yep, those Gaussians look about the same. Let’s get the chimpanzees data from rethinking. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Detach rethinking and reload brms. rm(chimpanzees) detach(package:rethinking, unload = T) library(brms) For our brms model with varying intercepts for actor but not block, we employ the pulled_left ~ 1 + ... + (1 | actor) syntax, specifically omitting a (1 | block) section. b12.4 &lt;- brm(data = d, family = binomial, pulled_left ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, # I&#39;m using 4 cores, instead of the `cores=3` in McElreath&#39;s code control = list(adapt_delta = 0.95)) The initial solutions came with a few divergent transitions. Increasing adapt_delta to 0.95 solved the problem. You can also solve the problem with more strongly regularizing priors such as normal(0, 2) on the intercept and slope parameters (see recommendations from the Stan team). Consider trying both methods and comparing the results. They’re similar. Here we add the actor-level deviations to the fixed intercept, the grand mean. post &lt;- posterior_samples(b12.4) post %&gt;% select(`r_actor[1,Intercept]`:`r_actor[7,Intercept]`) %&gt;% gather() %&gt;% # This is how we add the grand mean to the actor-level deviations mutate(value = value + post$b_Intercept) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value) %&gt;% round(digits = 2)) ## # A tibble: 7 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 r_actor[1,Intercept] -0.71 ## 2 r_actor[2,Intercept] 4.6 ## 3 r_actor[3,Intercept] -1.02 ## 4 r_actor[4,Intercept] -1.02 ## 5 r_actor[5,Intercept] -0.71 ## 6 r_actor[6,Intercept] 0.23 ## 7 r_actor[7,Intercept] 1.76 Here’s another way to get at the same information, this time using coef() and a little formatting help from the stringr::str_c() function. Just for kicks, we’ll throw in the 95% intervals, too. coef(b12.4)$actor[ , c(1, 3:4), 1] %&gt;% as_tibble() %&gt;% round(digits = 2) %&gt;% # Here we put the credible intervals in an APA-6-style format mutate(`95% CIs` = str_c(&quot;[&quot;, Q2.5, &quot;, &quot;, Q97.5, &quot;]&quot;)) %&gt;% mutate(actor = str_c(&quot;chimp #&quot;, 1:7)) %&gt;% rename(mean = Estimate) %&gt;% select(actor, mean, `95% CIs`) %&gt;% knitr::kable() actor mean 95% CIs chimp #1 -0.71 [-1.24, -0.18] chimp #2 4.60 [2.57, 8.54] chimp #3 -1.02 [-1.57, -0.48] chimp #4 -1.02 [-1.59, -0.48] chimp #5 -0.71 [-1.24, -0.19] chimp #6 0.23 [-0.3, 0.77] chimp #7 1.76 [1.06, 2.55] If you prefer the posterior median to the mean, just add a robust = T argument inside the coef() function. 12.3.2 Two types of cluster. The full statistical model follows the form \\[\\begin{eqnarray} \\text{left_pull}_i &amp; \\sim &amp; \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim &amp; \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha_{\\text{block}} &amp; \\sim &amp; \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\sigma_{\\text{block}} &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray}\\] Our brms model with varying intercepts for both actor and block now employs the ... (1 | actor) + (1 | block) syntax. b12.5 &lt;- update(b12.4, newdata = d, formula = pulled_left ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) + (1 | block), iter = 6000, warmup = 1000, cores = 4, chains = 4, control = list(adapt_delta = 0.99)) This time we increased adapt_delta to 0.99 to avoid divergent transitions. We can look at the primary coefficients with print(). McElreath encouraged us to inspect the trace plots. Here they are. library(bayesplot) color_scheme_set(&quot;orange&quot;) post &lt;- posterior_samples(b12.5, add_chain = T) post %&gt;% select(-lp__, -iter) %&gt;% mcmc_trace(facet_args = list(ncol = 4)) + scale_x_continuous(breaks = c(0, 2500, 5000)) + theme_fivethirtyeight() + theme(legend.position = c(.75, .06)) The trace plots look great. We may as well examine the \\(n_\\text{eff} / N\\) ratios, too. neff_ratio(b12.5) %&gt;% mcmc_neff() + theme_fivethirtyeight() About half of them are lower than we might like, but none are in the embarrassing \\(n_\\text{eff} / N \\leq .1\\) range. Let’s look at the summary of the main parameters. print(b12.5) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left ~ prosoc_left + (1 | actor) + (1 | block) + prosoc_left:condition ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 6000; warmup = 1000; thin = 1; ## total post-warmup samples = 20000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 2.27 0.93 1.13 4.58 5159 1.00 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 0.22 0.18 0.01 0.67 7999 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.43 0.93 -1.34 2.43 4822 1.00 ## prosoc_left 0.83 0.26 0.31 1.34 15231 1.00 ## prosoc_left:condition -0.13 0.30 -0.72 0.46 15826 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time, we’ll need to use brms::ranef() to get those depth=2-type estimates in the same metric displayed in the text. With ranef(), you get the group-specific estimates in a deviance metric. The coef() function, in contrast, yields the group-specific estimates in what you might call the natural metric. We’ll get more language for this in the next chapter. ranef(b12.5)$actor[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.15 0.94 -3.15 0.63 ## 2 4.19 1.64 1.80 8.14 ## 3 -1.46 0.94 -3.48 0.33 ## 4 -1.46 0.94 -3.48 0.32 ## 5 -1.15 0.94 -3.18 0.64 ## 6 -0.20 0.94 -2.19 1.59 ## 7 1.34 0.97 -0.71 3.22 ranef(b12.5)$block[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.17 0.23 -0.74 0.13 ## 2 0.04 0.19 -0.32 0.47 ## 3 0.05 0.19 -0.29 0.49 ## 4 0.01 0.18 -0.37 0.41 ## 5 -0.03 0.18 -0.44 0.34 ## 6 0.11 0.21 -0.20 0.62 We might make the coefficient plot of Figure 12.4.a like this: stanplot(b12.5, pars = c(&quot;^r_&quot;, &quot;^b_&quot;, &quot;^sd_&quot;)) + theme_fivethirtyeight() + theme(axis.text.y = element_text(hjust = 0)) Once we get the posterior samples, it’s easy to compare the random variances as in Figure 12.4.b. post %&gt;% ggplot(aes(x = sd_actor__Intercept)) + theme_fivethirtyeight() + geom_density(size = 0, fill = &quot;orange1&quot;, alpha = 3/4) + geom_density(aes(x = sd_block__Intercept), size = 0, fill = &quot;orange4&quot;, alpha = 3/4) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 4)) + labs(title = expression(sigma)) + annotate(&quot;text&quot;, x = 2/3, y = 2, label = &quot;block&quot;, color = &quot;orange4&quot;) + annotate(&quot;text&quot;, x = 2, y = 3/4, label = &quot;actor&quot;, color = &quot;orange1&quot;) We might compare our models by their PSIS-LOO values. l.b12.4 &lt;- loo(b12.4, cores = 4) l.b12.5 &lt;- loo(b12.5, cores = 4) compare_ic(l.b12.4, l.b12.5) ## LOOIC SE ## b12.4 531.61 19.50 ## b12.5 532.75 19.68 ## b12.4 - b12.5 -1.14 1.72 And you can get the LOO version of the p_waic, the p_loo, like so. l.b12.4$ estimates ## Estimate SE ## elpd_loo -265.806641 9.7518758 ## p_loo 8.236901 0.4396798 ## looic 531.613283 19.5037517 l.b12.5$ estimates ## Estimate SE ## elpd_loo -266.37486 9.8389698 ## p_loo 10.39833 0.5325361 ## looic 532.74972 19.6779396 And if you peek at the structure of the loo objects, you’ll see you can call the p_loo values directly with something like l.b12.5$ estimates[&quot;p_loo&quot;, 1]. The results are quite similar to those in the text. Anyways, the two models yield nearly-equivalent information criteria values. Yet recall what McElreath wrote: “There is nothing to gain here by selecting either model. The comparison of the two models tells a richer story” (p. 367). 12.4 Multilevel posterior predictions … producing implied predictions from a fit model, is very helpful for understanding what the model means. Every model is a merger of sense and nonsense. When we understand a model, we can find its sense and control its nonsense. But as models get more complex, it is very difficult to impossible to understand them just by inspecting tables of posterior means and intervals. Exploring implied posterior predictions helps much more… … The introduction of varying effects does introduce nuance, however. First, we should no longer expect the model to exactly retrodict the sample, because adaptive regularization has as its goal to trade off poorer fit in sample for better inference and hopefully better fit out of sample. This is what shrinkage does for us… Second, “prediction” in the context of a multilevel model requires additional choices. If we wish to validate a model against the specific clusters used to fit the model, that is one thing. But if we instead wish to compute predictions for new clusters, other than the one observed in the sample, that is quite another. We’ll consider each of these in turn, continuing to use the chimpanzees model from the previous section. (p. 376) 12.4.1 Posterior prediction for same clusters. Like McElreath did in the text, we’ll do this two ways. Recall we use brms::fitted() in place of rethinking::link(). chimp &lt;- 2 nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1), actor = chimp) ( chimp_2_fitted &lt;- fitted(b12.4, newdata = nd) %&gt;% as_tibble() %&gt;% mutate(condition = factor(c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 5 ## Estimate Est.Error Q2.5 Q97.5 condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.981 0.0195 0.929 1.000 0/0 ## 2 0.991 0.00949 0.965 1.000 1/0 ## 3 0.981 0.0195 0.929 1.000 0/1 ## 4 0.990 0.0107 0.961 1.000 1/1 ( chimp_2_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 1 ## 2 0 0/1 1 ## 3 1 1/0 1 ## 4 1 1/1 1 McElreath didn’t show the corresponding plot in the text. It might look like this. chimp_2_fitted %&gt;% # if you want to use `geom_line()` or `geom_ribbon()` with a factor on the x axis, # you need to code something like `group = 1` in `aes()` ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_2_d, aes(x = condition, y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #2&quot;, subtitle = &quot;The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.&quot;) + coord_cartesian(ylim = c(.75, 1)) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Do note how severely we’ve restricted the y-axis range. But okay, now let’s do things by hand. We’ll need to extract the posterior samples and look at the structure of the data. post &lt;- posterior_samples(b12.4) glimpse(post) ## Observations: 16,000 ## Variables: 12 ## $ b_Intercept &lt;dbl&gt; 2.84660836, 3.31678210, 0.92818980, 0.97523659, 1.06284714, -1.57315237... ## $ b_prosoc_left &lt;dbl&gt; 0.8027592, 0.6465404, 1.0693015, 1.0108346, 1.0995699, 0.5916456, 0.628... ## $ `b_prosoc_left:condition` &lt;dbl&gt; -0.304525935, -0.366690285, -0.183819656, -0.367542935, -0.434902624, 0... ## $ sd_actor__Intercept &lt;dbl&gt; 3.947720, 3.301699, 4.001243, 3.917649, 3.835672, 2.342017, 1.791982, 2... ## $ `r_actor[1,Intercept]` &lt;dbl&gt; -3.9548165, -3.7774344, -1.7728771, -1.7907886, -1.8501038, 0.9077583, ... ## $ `r_actor[2,Intercept]` &lt;dbl&gt; 6.114119, 4.679728, 2.563180, 3.707954, 2.191760, 5.983084, 6.056797, 6... ## $ `r_actor[3,Intercept]` &lt;dbl&gt; -4.0365853, -3.9685120, -2.1543374, -1.9182331, -2.1817965, 0.7542737, ... ## $ `r_actor[4,Intercept]` &lt;dbl&gt; -3.86370546, -4.37117423, -2.05761046, -2.29637822, -2.19367525, 0.7843... ## $ `r_actor[5,Intercept]` &lt;dbl&gt; -3.40039529, -3.90678422, -1.71947109, -1.79589150, -1.87454021, 0.9572... ## $ `r_actor[6,Intercept]` &lt;dbl&gt; -2.6719766, -2.8518619, -0.8761091, -0.5816997, -0.9458346, 1.5937625, ... ## $ `r_actor[7,Intercept]` &lt;dbl&gt; -0.988928592, -1.515283707, 1.451833859, 0.271678419, 1.194536366, 3.22... ## $ lp__ &lt;dbl&gt; -280.3395, -281.0325, -278.8489, -278.2605, -278.8375, -280.8820, -283.... McElreath didn’t show what his R code 12.29 dens( post$a_actor[,5] ) would look like. But here’s our analogue. post %&gt;% transmute(actor_5 =`r_actor[5,Intercept]`) %&gt;% ggplot(aes(x = actor_5)) + geom_density(size = 0, fill = &quot;blue&quot;) + scale_y_continuous(breaks = NULL) + ggtitle(&quot;Chimp #5&#39;s density&quot;) + theme_fivethirtyeight() McElreath built his own link() function. Here we’ll build an alternative to fitted(). # our hand-made `brms::fitted()` alternative my_fitted &lt;- function(prosoc_left, condition){ post %&gt;% transmute(fitted = (b_Intercept + `r_actor[5,Intercept]` + b_prosoc_left * prosoc_left + `b_prosoc_left:condition` * prosoc_left * condition) %&gt;% inv_logit_scaled()) } # the posterior summaries ( chimp_5_my_fitted &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) %&gt;% mutate(post = map2(prosoc_left, condition, my_fitted)) %&gt;% unnest() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(condition) %&gt;% tidybayes::mean_qi(fitted) ) ## # A tibble: 4 x 7 ## condition fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0/0 0.332 0.224 0.452 0.95 mean qi ## 2 1/0 0.527 0.382 0.670 0.95 mean qi ## 3 0/1 0.332 0.224 0.452 0.95 mean qi ## 4 1/1 0.495 0.351 0.637 0.95 mean qi # the empirical summaries chimp &lt;- 5 ( chimp_5_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 0.333 ## 2 0 0/1 0.278 ## 3 1 1/0 0.556 ## 4 1 1/1 0.5 Okay, let’s see how good we are at retrodicting the pulled_left probabilities for actor == 5. chimp_5_my_fitted %&gt;% ggplot(aes(x = condition, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_5_d, aes(x = condition, y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #5&quot;, subtitle = &quot;This plot is like the last except\\nwe did more by hand.&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Not bad. 12.4.2 Posterior prediction for new clusters. By average actor, McElreath referred to a chimp with an intercept exactly at the population mean \\(\\alpha\\). So this time we’ll only be working with the population parameters or what are also sometimes called the fixed effects. When you work with brms::posterior_samples() output, this would mean working with columns beginning with the b_ prefix (i.e., b_Intercept, b_prosoc_left, and b_prosoc_left:condition). post_average_actor &lt;- post %&gt;% # here we use the linear regression formula to get the log_odds for the 4 conditions transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% # with `mutate_all()` we can convert the estimates to probabilities in one fell swoop mutate_all(inv_logit_scaled) %&gt;% # putting the data in the long format and grouping by condition (i.e., `key`) gather() %&gt;% mutate(key = factor(key, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(key) %&gt;% # here we get the summary values for the plot summarise(m = mean(value), # note we&#39;re using 80% intervals ll = quantile(value, probs = .1), ul = quantile(value, probs = .9)) post_average_actor ## # A tibble: 4 x 4 ## key m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.587 0.344 0.822 ## 2 1/0 0.744 0.539 0.914 ## 3 0/1 0.587 0.344 0.822 ## 4 1/1 0.721 0.506 0.903 Figure 12.5.a. p1 &lt;- post_average_actor %&gt;% ggplot(aes(x = key, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p1 If we want to depict the variability across the chimps, we need to include sd_actor__Intercept into the calculations. In the first block of code, below, we simulate a bundle of new intercepts defined by \\[\\alpha_\\text{actor} \\sim \\text{Normal} (0, \\sigma_\\text{actor})\\] # the random effects set.seed(12.42) ran_ef &lt;- tibble(random_effect = rnorm(n = 1000, mean = 0, sd = post$sd_actor__Intercept)) %&gt;% # with the `., ., ., .` syntax, we quadruple the previous line bind_rows(., ., ., .) # the fixed effects (i.e., the population parameters) fix_ef &lt;- post %&gt;% slice(1:1000) %&gt;% transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% gather() %&gt;% rename(condition = key, fixed_effect = value) %&gt;% mutate(condition = factor(condition, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # combine them ran_and_fix_ef &lt;- bind_cols(ran_ef, fix_ef) %&gt;% mutate(intercept = fixed_effect + random_effect) %&gt;% mutate(prob = inv_logit_scaled(intercept)) # to simplify things, we&#39;ll reduce them to summaries ( marginal_effects &lt;- ran_and_fix_ef %&gt;% group_by(condition) %&gt;% summarise(m = mean(prob), ll = quantile(prob, probs = .1), ul = quantile(prob, probs = .9)) ) ## # A tibble: 4 x 4 ## condition m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.552 0.0772 0.970 ## 2 1/0 0.661 0.159 0.986 ## 3 0/1 0.552 0.0772 0.970 ## 4 1/1 0.647 0.142 0.984 Behold Figure 12.5.b. p2 &lt;- marginal_effects %&gt;% ggplot(aes(x = condition, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p2 Figure 12.5.c just takes a tiny bit more wrangling. p3 &lt;- ran_and_fix_ef %&gt;% mutate(iter = rep(1:1000, times = 4)) %&gt;% filter(iter %in% c(1:50)) %&gt;% ggplot(aes(x = condition, y = prob, group = iter)) + theme_fivethirtyeight() + geom_line(alpha = 1/2, color = &quot;orange3&quot;) + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p3 For the finale, we’ll stitch the three plots together. library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) 12.4.2.1 Bonus: Let’s use fitted() this time. We just made those plots using various wrangled versions of post, the data frame returned by posterior_samples(b.12.4). If you followed along closely, part of what made that a great exercise is that it forced you to consider what the various vectors in post meant with respect to the model formula. But it’s also handy to see how to do that from a different perspective. So in this section, we’ll repeat that process by relying on the fitted() function, instead. We’ll go in the same order, starting with the average actor. nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) ( fitd_b12.4 &lt;- fitted(b12.4, newdata = nd, re_formula = NA, probs = c(.1, .9)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.587 0.182 0.344 0.822 0 0/0 ## 2 0.744 0.152 0.539 0.914 1 1/0 ## 3 0.587 0.182 0.344 0.822 0 0/1 ## 4 0.721 0.159 0.506 0.903 1 1/1 You should notice a few things. Since b12.4 is a multilevel model, it had three predictors: prosoc_left, condition, and actor. However, our nd data only included the first two of those predictors. The reason fitted() permitted that was because we set re_formula = NA. When you do that, you tell fitted() to ignore group-level effects (i.e., focus only on the fixed effects). This was our fitted() version of ignoring the r_ vectors returned by posterior_samples(). Here’s the plot. p4 &lt;- fitd_b12.4 %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p4 For marginal of actor, we can continue using the same nd data. This time we’ll be sticking with the default re_formula setting, which will accommodate the multilevel nature of the model. However, we’ll also be adding allow_new_levels = T and sample_new_levels = &quot;gaussian&quot;. The former will allow us to marginalize across the specific actors in our data and the latter will instruct fitted() to use the multivariate normal distribution implied by the random effects. It’ll make more sense why I say multivariate normal by the end of the next chapter. For now, just go with it. ( fitd_b12.4 &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.556 0.330 0.0679 0.970 0 0/0 ## 2 0.665 0.312 0.144 0.987 1 1/0 ## 3 0.556 0.330 0.0679 0.970 0 0/1 ## 4 0.648 0.316 0.124 0.985 1 1/1 Here’s our fitted()-based marginal of actor plot. p5 &lt;- fitd_b12.4 %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p5 For the simulated actors plot, we’ll just amend our process from the last one. This time we’re setting summary = F, in order to keep the iteration-specific results, and setting nsamples = n_sim. n_sim is just a name for the number of actors we’d like to simulate (i.e., 50, as in the text). # how many simulated actors would you like? n_sim &lt;- 50 ( fitd_b12.4 &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;, summary = F, nsamples = n_sim) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n_sim) %&gt;% gather(key, value, -iter) %&gt;% bind_cols(nd %&gt;% transmute(condition = str_c(prosoc_left, &quot;/&quot;, condition) %&gt;% factor(., levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% expand(condition, iter = 1:n_sim)) ) ## # A tibble: 200 x 5 ## iter key value condition iter1 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 V1 0.00971 0/0 1 ## 2 2 V1 0.498 0/0 2 ## 3 3 V1 0.670 0/0 3 ## 4 4 V1 0.615 0/0 4 ## 5 5 V1 0.896 0/0 5 ## 6 6 V1 0.436 0/0 6 ## 7 7 V1 0.220 0/0 7 ## 8 8 V1 0.510 0/0 8 ## 9 9 V1 0.945 0/0 9 ## 10 10 V1 0.326 0/0 10 ## # ... with 190 more rows p6 &lt;- fitd_b12.4 %&gt;% ggplot(aes(x = condition, y = value, group = iter)) + theme_fivethirtyeight() + geom_line(alpha = 1/2, color = &quot;blue&quot;) + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p6 Here they are altogether. grid.arrange(p4, p5, p6, ncol = 3) 12.4.3 Focus and multilevel prediction. First, let’s load the Kline data. # prep data library(rethinking) data(Kline) d &lt;- Kline Switch out the packages, once again. detach(package:rethinking, unload = T) library(brms) rm(Kline) The statistical formula for our multilevel count model is \\[ \\begin{eqnarray} \\text{total_tools}_i &amp; \\sim &amp; \\text{Poisson} (\\mu_i) \\\\ \\text{log} (\\mu_i) &amp; = &amp; \\alpha + \\alpha_{\\text{culture}_i} + \\beta \\text{log} (\\text{population}_i) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\alpha_{\\text{culture}} &amp; \\sim &amp; \\text{Normal} (0, \\sigma_{\\text{culture}}) \\\\ \\sigma_{\\text{culture}} &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\end{eqnarray} \\] With brms, we don’t actually need to make the logpop or society variables. We’re ready to fit the multilevel Kline model with the data in hand. b12.6 &lt;- brm(data = d, family = poisson, total_tools ~ 0 + intercept + log(population) + (1 | culture), prior = c(prior(normal(0, 10), class = b, coef = intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 3, chains = 3) Note how we used the special 0 + intercept syntax rather than using the default Intercept. This is because our predictor variable was not mean centered. For more info, see here. Though we used the 0 + intercept syntax for the fixed effect, it was not necessary for the random effect. Both ways work. Here is the data-processing work for our variant of Figure 12.6. nd &lt;- tibble(population = seq(from = 1000, to = 400000, by = 5000), # To &quot;simulate counterfactual societies, using the hyper-parameters&quot; (p. 383), # we&#39;ll plug a new island into the `culture` variable culture = &quot;my_island&quot;) pred_12.6 &lt;- predict(b12.6, # This allows us to simulate values for our counterfactual island, &quot;my_island&quot; allow_new_levels = T, # Here we explicitly tell brms we want to include the group-level effects re_formula = ~ (1 | culture), # From the brms manual, this uses the &quot;(multivariate) normal distribution implied by # the group-level standard deviations and correlations&quot;, which appears to be # what McElreath did in the text. sample_new_levels = &quot;gaussian&quot;, newdata = nd, probs = c(.015, .055, .165, .835, .945, .985)) %&gt;% as_tibble() %&gt;% bind_cols(nd) pred_12.6 %&gt;% glimpse() ## Observations: 80 ## Variables: 10 ## $ Estimate &lt;dbl&gt; 19.69167, 31.10667, 36.57911, 40.37367, 43.45178, 45.96167, 48.30811, 50.49456, 52.206... ## $ Est.Error &lt;dbl&gt; 9.576842, 14.402249, 17.682183, 20.466952, 22.705786, 24.992111, 27.111403, 29.735646,... ## $ Q1.5 &lt;dbl&gt; 5.000, 10.000, 13.000, 14.000, 15.000, 16.000, 17.000, 18.000, 18.000, 19.000, 19.000,... ## $ Q5.5 &lt;dbl&gt; 8, 15, 18, 20, 21, 23, 23, 25, 26, 26, 27, 27, 28, 29, 29, 29, 30, 30, 30, 31, 31, 32,... ## $ Q16.5 &lt;dbl&gt; 11, 20, 24, 26, 28, 30, 31, 33, 34, 35, 35, 36, 37, 38, 39, 39, 40, 40, 41, 41, 42, 43... ## $ Q83.5 &lt;dbl&gt; 27.000, 41.000, 48.000, 53.000, 57.000, 60.000, 63.000, 66.000, 68.000, 70.000, 72.000... ## $ Q94.5 &lt;dbl&gt; 35.000, 53.000, 62.000, 68.000, 73.000, 77.055, 81.055, 86.000, 88.000, 91.000, 95.000... ## $ Q98.5 &lt;dbl&gt; 47.000, 70.000, 80.000, 91.000, 96.000, 104.015, 110.000, 116.000, 122.015, 127.015, 1... ## $ population &lt;dbl&gt; 1000, 6000, 11000, 16000, 21000, 26000, 31000, 36000, 41000, 46000, 51000, 56000, 6100... ## $ culture &lt;chr&gt; &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_isla... For a detailed discussion on this way of using brms::predict(), see Andrew MacDonald’s great blogpost on this very figure. Here’s what we’ve been working for: pred_12.6 %&gt;% ggplot(aes(x = log(population), y = Estimate)) + geom_ribbon(aes(ymin = Q1.5, ymax = Q98.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q16.5, ymax = Q83.5), fill = &quot;orange2&quot;, alpha = 1/3) + coord_cartesian(ylim = range(d$total_tools)) + geom_line(color = &quot;orange4&quot;) + geom_text(data = d, aes(y = total_tools, label = culture), size = 2.33, color = &quot;blue&quot;) + ggtitle(&quot;Total tools as a function of log(population)&quot;) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 12, hjust = .5)) Glorious. The envelope of predictions is a lot wider here than it was back in Chapter 10. This is a consequene of the varying intercepts, combined with the fact that there is much more variation in the data than a pure-Poisson model anticipates. (p. 384) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 bayesplot_1.6.0 ggthemes_3.5.0 forcats_0.3.0 stringr_1.3.1 ## [6] dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ## [11] tidyverse_1.2.1 brms_2.5.0 Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 ## [16] ggplot2_3.0.0 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 tidyselect_0.2.4 ## [5] htmlwidgets_1.2 munsell_0.5.0 codetools_0.2-15 nleqslv_3.3.2 ## [9] DT_0.4 miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 rstudioapi_0.7 ## [17] stats4_3.5.1 Rttf2pt1_1.3.7 labeling_0.3 mnormt_1.5-5 ## [21] bridgesampling_0.4-0 rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 reshape_0.8.7 ## [29] assertthat_0.2.0 promises_1.0.1 scales_0.5.0 beeswarm_0.2.3 ## [33] gtable_0.2.0 rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [37] broom_0.4.5 inline_0.3.15 yaml_2.1.19 reshape2_1.4.3 ## [41] abind_1.4-5 modelr_0.1.2 threejs_0.3.1 crosstalk_1.0.0 ## [45] backports_1.1.2 httpuv_1.4.4.2 rsconnect_0.8.8 extrafont_0.17 ## [49] tools_3.5.1 bookdown_0.7 psych_1.8.4 RColorBrewer_1.1-2 ## [53] ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 progress_1.2.0 ## [57] prettyunits_1.0.2 zoo_1.8-2 LaplacesDemon_16.1.1 haven_1.1.2 ## [61] magrittr_1.5 colourpicker_1.0 mvtnorm_1.0-8 tidybayes_1.0.1 ## [65] matrixStats_0.54.0 hms_0.4.2 shinyjs_1.0 mime_0.5 ## [69] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 shinystan_2.5.0 ## [73] readxl_1.1.0 rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [77] crayon_1.3.4 htmltools_0.3.6 later_0.7.3 lubridate_1.7.4 ## [81] MASS_7.3-50 Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [85] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 xml2_1.2.0 ## [89] svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 rvest_0.3.2 ## [93] digest_0.6.15 rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [97] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 bindrcpp_0.2.2 ## [101] mapproj_1.2.6 viridisLite_0.3.0 pillar_1.2.3 lattice_0.20-35 ## [105] loo_2.0.0 httr_1.3.1 glue_1.2.0 xts_0.10-2 ## [109] shinythemes_1.1.1 pander_0.6.2 stringi_1.2.3 "],
["adventures-in-covariance.html", "13 Adventures in Covariance 13.1 Varying slopes by construction 13.2 Example: Admission decisions and gender 13.3 Example: Cross-classified chimpanzees with varying slopes 13.4 Continuous categories and the Gaussian process 13.5 Summary Bonus: Another Berkley-admissions-data-like example. Reference Session info", " 13 Adventures in Covariance In this chapter, you’ll see how to… specify varying slopes in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. It will also improve estimates of intercepts, by borrowing information across parameter types. Essentially, varying slopes models are massive interaction machines. They allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling. When the variation in slopes is large, the average slope is of less interest. Sometimes, the pattern of variation in slopes provides hints about omitted variables that explain why some units respond more or less. We’ll see an example in this chapter. The machinery that makes such complex varying effects possible will be used later in the chapter to extend the varying effects strategy to more subtle model types, including the use of continuous categories, using Gaussian process. (p. 388) 13.1 Varying slopes by construction How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 389) In the Rethinking: Why Gaussian? box, McElreath discussed how researchers might use other multivariate distributions to model multiple random effects. The only one he named as an alternative to the Gaussian was the multivariate Student’s \\(t\\). As it turns out, brms does currently allow users to use multivariate Student’s \\(t\\) in this way. For details, check out this discussion in the brms GitHub page. Bürkner’s exemplar syntax from his comment on May 13, 2018, was y ~ x + (x | gr(g, dist = &quot;student&quot;)). I haven’t experimented with this, but if you do, do consider commenting on how it went. 13.1.1 Simulate the population. If you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done some nice work along these lines, too. a &lt;- 3.5 # average morning wait time b &lt;- -1 # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- -.7 # correlation between intercepts and slopes # The next three lines of code simply combine the terms, above mu &lt;- c(a, b) cov_ab &lt;- sigma_a * sigma_b * rho sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) If you haven’t used matirx() before, you might get a sense of the elements like so. matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 This next block of code will finally yield our café data. library(tidyverse) sigmas &lt;- c(sigma_a, sigma_b) # standard deviations rho &lt;- matrix(c(1, rho, # correlation matrix rho, 1), nrow = 2) # now matrix multiply to get covariance matrix sigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas) # how many cafes would you like? n_cafes &lt;- 20 set.seed(5) # used to replicate example vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) vary_effects &lt;- vary_effects %&gt;% as_tibble() %&gt;% rename(a_cafe = V1, b_cafe = V2) head(vary_effects) ## # A tibble: 6 x 2 ## a_cafe b_cafe ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.22 -1.61 ## 2 2.01 -0.752 ## 3 4.57 -1.95 ## 4 3.34 -1.19 ## 5 1.70 -0.586 ## 6 4.13 -1.14 Let’s make sure we’re keeping this all straight. a_cafe = our café-specific intercepts; b_cafe = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask What’s the distribution of a_cafe and b_cafe? Our variant of Figure 13.2 contains the answer. For our plots in this chapter, we’ll use a custom theme. The color palette will come from the “pearl_earring” palette of the dutchmasters package. You can learn more about the original painting, Vermeer’s Girl with a Pearl Earring, here. # devtools::install_github(&quot;EdwinTh/dutchmasters&quot;) library(dutchmasters) dutchmasters$pearl_earring ## red(lips) skin blue(scarf1) blue(scarf2) white(colar) ## &quot;#A65141&quot; &quot;#E7CDC2&quot; &quot;#80A0C7&quot; &quot;#394165&quot; &quot;#FCF9F0&quot; ## gold(dress) gold(dress2) black(background) grey(scarf3) yellow(scarf4) ## &quot;#B1934A&quot; &quot;#DCA258&quot; &quot;#100F14&quot; &quot;#8B9DAF&quot; &quot;#EEDA9D&quot; ## ## &quot;#E8DCCF&quot; We’ll name our custom theme theme_pearl_earring. theme_pearl_earring &lt;- theme(text = element_text(color = &quot;#E8DCCF&quot;, family = &quot;Courier&quot;), strip.text = element_text(color = &quot;#E8DCCF&quot;, family = &quot;Courier&quot;), axis.text = element_text(color = &quot;#E8DCCF&quot;), axis.ticks = element_line(color = &quot;#E8DCCF&quot;), line = element_line(color = &quot;#E8DCCF&quot;), plot.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), panel.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;#E8DCCF&quot;), strip.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), panel.grid = element_blank(), legend.background = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), legend.key = element_rect(fill = &quot;#100F14&quot;, color = &quot;transparent&quot;), axis.line = element_blank()) Now we’re ready to plot Figure 13.2. vary_effects %&gt;% ggplot(aes(x = a_cafe, y = b_cafe)) + geom_point(color = &quot;#80A0C7&quot;) + geom_rug(color = &quot;#8B9DAF&quot;, size = 1/7) + theme_pearl_earring Again, these are not “data.” This is a distribution of parameters. 13.1.2 Simulate observations. Here we put those simulated parameters to use. n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes set.seed(5) # used to replicate example d &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = a_cafe + b_cafe * afternoon) %&gt;% mutate(wait = rnorm(n = n(), mean = mu, sd = sigma)) We might peek at the data. d %&gt;% head() ## # A tibble: 6 x 7 ## cafe a_cafe b_cafe visit afternoon mu wait ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.22 -1.61 1 0 4.22 3.80 ## 2 1 4.22 -1.61 2 1 2.61 3.31 ## 3 1 4.22 -1.61 3 0 4.22 3.60 ## 4 1 4.22 -1.61 4 1 2.61 2.65 ## 5 1 4.22 -1.61 5 0 4.22 5.08 ## 6 1 4.22 -1.61 6 1 2.61 2.31 Now we’ve finally simulated our data, we are ready to make our version of Figure 13.1, from way back on page 388. d %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;M&quot;, &quot;A&quot;), day = rep(rep(1:5, each = 2), times = n_cafes)) %&gt;% filter(cafe %in% c(3, 5)) %&gt;% mutate(cafe = ifelse(cafe == 3, &quot;cafe #3&quot;, &quot;cafe #5&quot;)) %&gt;% ggplot(aes(x = visit, y = wait, group = day)) + geom_point(aes(color = afternoon), size = 2) + geom_line(color = &quot;#8B9DAF&quot;) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(breaks = 1:10, labels = rep(c(&quot;M&quot;, &quot;A&quot;), times = 5)) + coord_cartesian(ylim = 0:8) + labs(x = NULL, y = &quot;wait time in minutes&quot;) + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.x = element_blank()) + facet_wrap(~cafe, ncol = 1) 13.1.3 The varying slopes model. The statistical formula for our varying-slopes model follows the form \\[ \\begin{eqnarray} \\text{wait}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha_{\\text{cafe}_i} + \\beta_{\\text{cafe}_i} \\text{afternoon}_i \\\\ \\begin{bmatrix} \\alpha_\\text{cafe} \\\\ \\beta_\\text{cafe} \\end{bmatrix} &amp; \\sim &amp; \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = &amp; \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\alpha &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\beta &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\mathbf R &amp; \\sim &amp; \\text{LKJcorr} (2) \\end{eqnarray} \\] Of the notable new parts, \\(\\mathbf S\\) is the covariance matrix and \\(\\mathbf R\\) is the corresponding correlation matrix, which we might more fully express as \\[\\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix}\\] And according to our prior, \\(\\mathbf R\\) is distributed as \\(\\text{LKJcorr} (2)\\). We’ll use rethinking::rlkjcorr() to get a better sense of what that even is. library(rethinking) n_sim &lt;- 1e5 set.seed(133) r_1 &lt;- rlkjcorr(n_sim, K = 2, eta = 1) %&gt;% as_tibble() set.seed(133) r_2 &lt;- rlkjcorr(n_sim, K = 2, eta = 2) %&gt;% as_tibble() set.seed(133) r_4 &lt;- rlkjcorr(n_sim, K = 2, eta = 4) %&gt;% as_tibble() Here are the \\(\\text{LKJcorr}\\) distributions of Figure 13.3. ggplot(data = r_1, aes(x = V2)) + geom_density(color = &quot;transparent&quot;, fill = &quot;#DCA258&quot;, alpha = 2/3) + geom_density(data = r_2, color = &quot;transparent&quot;, fill = &quot;#FCF9F0&quot;, alpha = 2/3) + geom_density(data = r_4, color = &quot;transparent&quot;, fill = &quot;#394165&quot;, alpha = 2/3) + geom_text(data = tibble(x = c(.83, .62, .46), y = c(.54, .74, 1), label = c(&quot;eta = 1&quot;, &quot;eta = 2&quot;, &quot;eta = 4&quot;)), aes(x = x, y = y, label = label), color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring Okay, let’s get ready to model and switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) As defined above, our first model has both varying intercepts and afternoon slopes. I should point out that the (1 + afternoon | cafe) syntax specifies that we’d like brm() to fit the random effects for 1 (i.e., the intercept) and the afternoon slope as correlated. Had we wanted to fit a model in which they were orthogonal, we’d have coded (1 + afternoon || cafe). b13.1 &lt;- brm(data = d, family = gaussian, wait ~ 1 + afternoon + (1 + afternoon | cafe), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sd), prior(cauchy(0, 2), class = sigma), prior(lkj(2), class = cor)), iter = 5000, warmup = 2000, chains = 2, cores = 2) With Figure 13.4, we assess how the posterior for the correlation of the random effects compares to its prior. post &lt;- posterior_samples(b13.1) post %&gt;% ggplot(aes(x = cor_cafe__Intercept__afternoon)) + geom_density(data = r_2, aes(x = V2), color = &quot;transparent&quot;, fill = &quot;#EEDA9D&quot;, alpha = 3/4) + geom_density(color = &quot;transparent&quot;, fill = &quot;#A65141&quot;, alpha = 9/10) + annotate(&quot;text&quot;, label = &quot;posterior&quot;, x = -0.2, y = 2.2, color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + annotate(&quot;text&quot;, label = &quot;prior&quot;, x = 0, y = 0.85, color = &quot;#EEDA9D&quot;, alpha = 2/3, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring McElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the cafe-specific intercepts and afternoon slopes with coef(), which returns a three-dimensional list. # coef(b13.1) %&gt;% glimpse() coef(b13.1) ## $cafe ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 4.076657 0.2074001 3.673043 4.485835 ## 2 1.930431 0.2083095 1.524455 2.336841 ## 3 4.817168 0.2112695 4.400361 5.229888 ## 4 3.475913 0.2072789 3.076951 3.879560 ## 5 1.775441 0.2182903 1.346382 2.204663 ## 6 4.384955 0.2062509 3.980472 4.784476 ## 7 3.246504 0.2001336 2.847035 3.646729 ## 8 4.008772 0.2107133 3.595458 4.428586 ## 9 4.236734 0.2052556 3.836461 4.636787 ## 10 3.711203 0.2078821 3.303882 4.109533 ## 11 2.168381 0.2143573 1.743727 2.582371 ## 12 4.035374 0.2089868 3.628665 4.438421 ## 13 4.086637 0.2133844 3.670318 4.497274 ## 14 3.591279 0.2054213 3.196173 3.999429 ## 15 4.260217 0.2050365 3.857916 4.659703 ## 16 3.482390 0.2064012 3.075759 3.880146 ## 17 4.040741 0.2113138 3.615844 4.457417 ## 18 5.840836 0.2091523 5.438021 6.239810 ## 19 3.774387 0.2069485 3.362714 4.179448 ## 20 3.856362 0.2105514 3.450898 4.264787 ## ## , , afternoon ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.4028304 0.2650106 -1.925897 -0.89745586 ## 2 -0.9499469 0.2730138 -1.486945 -0.42107261 ## 3 -1.8877371 0.2747891 -2.421923 -1.35021637 ## 4 -1.1999865 0.2609638 -1.706151 -0.68144455 ## 5 -0.5798662 0.2762329 -1.128942 -0.04157682 ## 6 -1.4941387 0.2634284 -2.017196 -0.97103914 ## 7 -1.0510192 0.2543420 -1.547794 -0.53539776 ## 8 -1.7245287 0.2750980 -2.278482 -1.20547927 ## 9 -1.5635515 0.2603850 -2.078395 -1.05338222 ## 10 -1.0259160 0.2656137 -1.523594 -0.48770944 ## 11 -0.4837134 0.2840094 -1.024349 0.07764580 ## 12 -1.2707149 0.2654349 -1.791585 -0.74327996 ## 13 -1.8111164 0.2771616 -2.367564 -1.27067278 ## 14 -1.6278600 0.2713188 -2.158032 -1.12148827 ## 15 -1.6767712 0.2668051 -2.207544 -1.16339863 ## 16 -0.9631167 0.2558213 -1.456668 -0.45254115 ## 17 -0.6856954 0.2855379 -1.232211 -0.11076829 ## 18 -1.5524621 0.2716130 -2.084856 -1.01864804 ## 19 -0.9174169 0.2692748 -1.438692 -0.39226069 ## 20 -0.9492934 0.2706767 -1.465136 -0.41188546 Here’s the code to extract the relevant elements from the coef() list, convert them to a tibble, and add the cafe index. partially_pooled_params &lt;- # With this line we select each of the 20 cafe&#39;s posterior mean (i.e., Estimate) # for both `Intercept` and `afternoon` coef(b13.1)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% # convert the two vectors to a tibble rename(Slope = afternoon) %&gt;% mutate(cafe = 1:nrow(.)) %&gt;% # add the `cafe` index select(cafe, everything()) # simply moving `cafe` to the leftmost position Like McElreath, we’ll compute the unpooled estimates directly from the data. # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% # With these two lines, we compute the mean value for each cafe&#39;s wait time # in the morning and then the afternoon. group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% ungroup() %&gt;% # Ungrouping allows us to alter afternoon, one of the grouping variables mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% spread(key = afternoon, value = mean) %&gt;% # use `spread()` just as in the previous block mutate(Slope = Slope - Intercept) # Finally, here&#39;s our slope! # Here we combine the partially-pooled and unpooled means into a single data object, # which will make plotting easier. params &lt;- # `bind_rows()` will stack the second tibble below the first bind_rows(partially_pooled_params, un_pooled_params) %&gt;% # index whether the estimates are pooled mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = nrow(.)/2)) # Here&#39;s a glimpse at what we&#39;ve been working for params %&gt;% slice(c(1:5, 36:40)) ## # A tibble: 10 x 4 ## cafe Intercept Slope pooled ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 4.08 -1.40 partially ## 2 2 1.93 -0.950 partially ## 3 3 4.82 -1.89 partially ## 4 4 3.48 -1.20 partially ## 5 5 1.78 -0.580 partially ## 6 16 3.42 -0.836 not ## 7 17 3.91 -0.348 not ## 8 18 5.89 -1.50 not ## 9 19 3.70 -0.733 not ## 10 20 3.79 -0.774 not Finally, here’s our code for Figure 13.5.a, showing shrinkage in two dimensions. ggplot(data = params, aes(x = Intercept, y = Slope)) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 1/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 2/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 3/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 4/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 5/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 6/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 7/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 8/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 9/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = .99, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(params$Intercept), ylim = range(params$Slope)) + theme_pearl_earring Learn more about stat_ellipse(), here. Let’s prep for Figure 13.5.b. # retrieve the partially-pooled estimates with `coef()` partially_pooled_estimates &lt;- coef(b13.1)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% # convert the two vectors to a tibble rename(morning = Intercept) %&gt;% # the Intercept is the wait time for morning (i.e., `afternoon == 0`) mutate(afternoon = morning + afternoon, # `afternoon` wait time is the `morning` wait time plus the afternoon slope cafe = 1:n()) %&gt;% # Add the `cafe` index select(cafe, everything()) # Compute unpooled estimates directly from data un_pooled_estimates &lt;- d %&gt;% # As above, with these two lines, we compute each cafe&#39;s mean wait value by time of day. group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% ungroup() %&gt;% # ungrouping allows us to alter the grouping variable, afternoon mutate(afternoon = ifelse(afternoon == 0, &quot;morning&quot;, &quot;afternoon&quot;)) %&gt;% spread(key = afternoon, value = mean) # this seperates out the values into morning and afternoon columns estimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) The code for Figure 13.5.b. ggplot(data = estimates, aes(x = morning, y = afternoon)) + # Nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` # functions in the previous plot mapply(function(level) { stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, # Enter the levels here level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(estimates$morning), ylim = range(estimates$afternoon)) + labs(x = &quot;morning wait (mins)&quot;, y = &quot;afternoon wait (mins)&quot;) + theme_pearl_earring 13.2 Example: Admission decisions and gender Let’s revisit the infamous UCB admissions data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Here we detach rethinking, reload brms, and augment the data a bit. detach(package:rethinking, unload = T) library(brms) rm(UCBadmit) d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0), dept_id = rep(1:6, each = 2)) 13.2.1 Varying intercepts. The statistical formula for our varying-intercepts logistic regression model follows the form \\[ \\begin{eqnarray} \\text{admit}_i &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{dept_id}_i} + \\beta \\text{male}_i \\\\ \\alpha_\\text{dept_id} &amp; \\sim &amp; \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 2) \\\\ \\end{eqnarray} \\] Since there’s only one left-hand term in our (1 | dept_id) code, there’s only one random effect. b13.2 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd)), iter = 4500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = 0.99)) Since we don’t have a depth=2 argument in brms::summary(), we’ll have to get creative. One way to look at the parameters is with b13.2$fit: b13.2$fit ## Inference for Stan model: 986d730d4cab1fb3046dbb2c44ac3aae. ## 3 chains, each with iter=4500; warmup=500; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept -0.57 0.01 0.65 -1.85 -0.94 -0.56 -0.19 0.76 1882 1 ## b_male -0.10 0.00 0.08 -0.25 -0.15 -0.10 -0.04 0.06 5114 1 ## sd_dept_id__Intercept 1.49 0.01 0.58 0.80 1.10 1.37 1.73 3.00 2251 1 ## r_dept_id[1,Intercept] 1.24 0.01 0.65 -0.09 0.86 1.23 1.62 2.54 1887 1 ## r_dept_id[2,Intercept] 1.20 0.01 0.65 -0.13 0.82 1.19 1.58 2.49 1896 1 ## r_dept_id[3,Intercept] -0.02 0.01 0.65 -1.32 -0.39 -0.02 0.36 1.28 1887 1 ## r_dept_id[4,Intercept] -0.05 0.01 0.65 -1.37 -0.42 -0.06 0.32 1.23 1881 1 ## r_dept_id[5,Intercept] -0.49 0.01 0.65 -1.82 -0.87 -0.50 -0.11 0.78 1900 1 ## r_dept_id[6,Intercept] -2.04 0.01 0.66 -3.38 -2.42 -2.04 -1.65 -0.75 1945 1 ## lp__ -61.77 0.05 2.49 -67.54 -63.22 -61.44 -59.97 -57.89 2375 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Sep 24 19:28:16 2018. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). However, notice that the group-specific parameters don’t match up with those in the text. Though our r_dept_id[1,Intercept] had a posterior mean of 1.27, the number for a_dept[1] in the text is 0.67. This is because the brms package presented the random effects in the non-centered metric. The rethinking package, in contrast, presented the random effects in the centered metric. On page 399, McElreath wrote: Remember, the values above are the \\(\\alpha_{DEPT}\\) estimates, and so they are deviations from the global mean \\(\\alpha\\), which in this case has posterior mean -0.58. So department A, “[1]” in the table, has the highest average admission rate. Department F, “[6]” in the table, has the lowest. Here’s another fun fact: # Numbers taken from the mean column on page 399 in the text c(0.67, 0.63, -0.59, -0.62, -1.06, -2.61) %&gt;% mean() ## [1] -0.5966667 The average of the rethinking-based centered random effects is within rounding error of the global mean, -0.58. If you want the random effects in the centered metric from brms, you can use the coef() function: coef(b13.2) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6748743 0.09869596 0.4799627 0.8678702 ## 2 0.6306516 0.11550916 0.4044213 0.8592874 ## 3 -0.5832062 0.07462316 -0.7304812 -0.4367849 ## 4 -0.6157081 0.08498400 -0.7829412 -0.4495225 ## 5 -1.0597297 0.09859278 -1.2584943 -0.8702506 ## 6 -2.6065066 0.15474597 -2.9195257 -2.3113986 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.09530202 0.07988608 -0.2528348 0.06242025 ## 2 -0.09530202 0.07988608 -0.2528348 0.06242025 ## 3 -0.09530202 0.07988608 -0.2528348 0.06242025 ## 4 -0.09530202 0.07988608 -0.2528348 0.06242025 ## 5 -0.09530202 0.07988608 -0.2528348 0.06242025 ## 6 -0.09530202 0.07988608 -0.2528348 0.06242025 And just to confirm, the average of the posterior means of the Intercept random effects with brms::coef() is also the global mean within rounding error: mean(coef(b13.2)$dept_id[ , &quot;Estimate&quot;, &quot;Intercept&quot;]) ## [1] -0.5932708 Note how coef() returned a three-dimensional list. coef(b13.2) %&gt;% str() ## List of 1 ## $ dept_id: num [1:6, 1:4, 1:2] 0.675 0.631 -0.583 -0.616 -1.06 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;male&quot; If you just want the parameter summaries for the random intercepts, you have to use three-dimensional indexing. coef(b13.2)$dept_id[ , , &quot;Intercept&quot;] # this also works: coef(b13.2)$dept_id[ , , 1] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6748743 0.09869596 0.4799627 0.8678702 ## 2 0.6306516 0.11550916 0.4044213 0.8592874 ## 3 -0.5832062 0.07462316 -0.7304812 -0.4367849 ## 4 -0.6157081 0.08498400 -0.7829412 -0.4495225 ## 5 -1.0597297 0.09859278 -1.2584943 -0.8702506 ## 6 -2.6065066 0.15474597 -2.9195257 -2.3113986 So to get our brms summaries in a similar format to those in the text, we’ll have to combine coef() with fixef() and VarCorr(). coef(b13.2)$dept_id[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% bind_rows(fixef(b13.2) %&gt;% as_tibble()) %&gt;% bind_rows(VarCorr(b13.2)$dept_id$sd %&gt;% as_tibble()) ## # A tibble: 9 x 4 ## Estimate Est.Error Q2.5 Q97.5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.675 0.0987 0.480 0.868 ## 2 0.631 0.116 0.404 0.859 ## 3 -0.583 0.0746 -0.730 -0.437 ## 4 -0.616 0.0850 -0.783 -0.450 ## 5 -1.06 0.0986 -1.26 -0.870 ## 6 -2.61 0.155 -2.92 -2.31 ## 7 -0.567 0.649 -1.85 0.756 ## 8 -0.0953 0.0799 -0.253 0.0624 ## 9 1.49 0.579 0.795 3.00 And a little more data wrangling will make the summaries easier to read: coef(b13.2)$dept_id[, , &quot;Intercept&quot;] %&gt;% as_tibble() %&gt;% bind_rows(fixef(b13.2) %&gt;% as_tibble()) %&gt;% bind_rows(VarCorr(b13.2)$dept_id$sd %&gt;% as_tibble()) %&gt;% mutate(parameter = c(paste(&quot;Intercept [&quot;, 1:6, &quot;]&quot;, sep = &quot;&quot;), &quot;Intercept&quot;, &quot;male&quot;, &quot;sigma&quot;)) %&gt;% select(parameter, everything()) %&gt;% mutate_if(is_double, round, digits = 2) ## # A tibble: 9 x 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Intercept [1] 0.67 0.1 0.48 0.87 ## 2 Intercept [2] 0.63 0.12 0.4 0.86 ## 3 Intercept [3] -0.580 0.07 -0.73 -0.44 ## 4 Intercept [4] -0.62 0.08 -0.78 -0.45 ## 5 Intercept [5] -1.06 0.1 -1.26 -0.87 ## 6 Intercept [6] -2.61 0.15 -2.92 -2.31 ## 7 Intercept -0.570 0.65 -1.85 0.76 ## 8 male -0.1 0.08 -0.25 0.06 ## 9 sigma 1.49 0.580 0.8 3 I’m not aware of a slick and easy way to get the n_eff and Rhat summaries into the mix. But if you’re fine with working with the brms-default non-centered parameterization, b13.2$fit gets you those just fine. One last thing. The broom package offers a very handy way to get those brms random effects. Just throw the model brm() fit into the tidy() function. library(broom) tidy(b13.2) %&gt;% mutate_if(is.numeric, round, digits = 2) # This line just rounds the output ## term estimate std.error lower upper ## 1 b_Intercept -0.57 0.65 -1.59 0.46 ## 2 b_male -0.10 0.08 -0.23 0.04 ## 3 sd_dept_id__Intercept 1.49 0.58 0.86 2.57 ## 4 r_dept_id[1,Intercept] 1.24 0.65 0.22 2.27 ## 5 r_dept_id[2,Intercept] 1.20 0.65 0.17 2.23 ## 6 r_dept_id[3,Intercept] -0.02 0.65 -1.04 1.02 ## 7 r_dept_id[4,Intercept] -0.05 0.65 -1.09 0.97 ## 8 r_dept_id[5,Intercept] -0.49 0.65 -1.53 0.54 ## 9 r_dept_id[6,Intercept] -2.04 0.66 -3.09 -1.02 ## 10 lp__ -61.77 2.49 -66.32 -58.36 But note how, just as with b13.2$fit, this approach summarizes the posterior with the non-centered parameterization. Which is a fine parameterization. It’s just a little different from what you’ll get when using precis( m13.2 , depth=2 ), as in the text. 13.2.2 Varying effects of being male. Now we’re ready to allow our male dummy to varies, too, the statistical model follows the form \\[ \\begin{eqnarray} \\text{admit}_i &amp; \\sim &amp; \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_{\\text{dept_id}_i} + \\beta_{\\text{dept_id}_i} \\text{male}_i \\\\ \\begin{bmatrix} \\alpha_\\text{dept_id} \\\\ \\beta_\\text{dept_id} \\end{bmatrix} &amp; \\sim &amp; \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = &amp; \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim &amp; \\text{Normal} (0, 1) \\\\ (\\sigma_\\alpha, \\sigma_\\beta) &amp; \\sim &amp; \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R &amp; \\sim &amp; \\text{LKJcorr} (2) \\end{eqnarray} \\] Fit the model. b13.3 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 + male | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(2), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99, max_treedepth = 12)) McElreath encouraged us to make sure the chains look good. Instead of relying on convenience functions, let’s do it by hand. post &lt;- posterior_samples(b13.3, add_chain = T) post %&gt;% select(-lp__) %&gt;% gather(key, value, -chain, -iter) %&gt;% mutate(chain = as.character(chain)) %&gt;% ggplot(aes(x = iter, y = value, group = chain, color = chain)) + geom_line(size = 1/15) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#B1934A&quot;, &quot;#A65141&quot;, &quot;#EEDA9D&quot;)) + labs(x = NULL, y = NULL) + scale_x_continuous(breaks = c(1001, 5000)) + theme_pearl_earring + theme(legend.position = c(.825, .06), legend.direction = &quot;horizontal&quot;) + facet_wrap(~key, ncol = 3, scales = &quot;free_y&quot;) Our chains look great. While we’re at it, let’s examine the \\(\\hat{R}\\) vales in a handmade plot, too. rhat(b13.3) %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% filter(rowname != &quot;lp__&quot;) %&gt;% ggplot(aes(x = `.`, y = reorder(rowname, `.`))) + geom_segment(aes(xend = 1, yend = rowname), color = &quot;#EEDA9D&quot;) + geom_point(aes(color = `.` &gt; 1), size = 2) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(x = NULL, y = NULL) + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Them are some respectable \\(\\hat{R}\\) values. The plot accentuates their differences, but they’re all basically 1 (e.g., see what happens is you set coord_cartesian(xlim = c(0.99, 1.01))). Here are the random effects in the centered metric: coef(b13.3) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.3060287 0.25584336 0.8242594 1.8202385 ## 2 0.7420184 0.32818266 0.1035647 1.3976491 ## 3 -0.6470283 0.08457924 -0.8135330 -0.4827617 ## 4 -0.6172350 0.10497566 -0.8239028 -0.4132265 ## 5 -1.1315001 0.11332819 -1.3580756 -0.9143988 ## 6 -2.6007380 0.20242186 -3.0135856 -2.2216006 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.79382902 0.2692380 -1.3324602 -0.2744163 ## 2 -0.21194501 0.3305760 -0.8767613 0.4424014 ## 3 0.08194407 0.1391819 -0.1814187 0.3592572 ## 4 -0.09240970 0.1394929 -0.3666799 0.1801527 ## 5 0.12153994 0.1866106 -0.2342076 0.4933623 ## 6 -0.12447792 0.2722629 -0.6711310 0.3969950 We may as well keep our doing-things-by-hand kick going. Instead relying on bayesplog::mcmc_intervals() or tidybayes::pointintervalh() to make our coefficient plot, we’ll combine geom_pointrange() and coord_flip(). But we will need to wrangle a bit to get those brms-based centered random effects into a usefully-formatted tidy tibble. # As far as I can tell, because `coef()` yields a list, you have to take out the two # random effects one at a time, convert them to tibbles, and reassemble them with `bind_rows()` coef(b13.3)$dept_id[, , 1] %&gt;% as_tibble() %&gt;% bind_rows( coef(b13.3)$dept_id[, , 2] %&gt;% as_tibble() ) %&gt;% mutate(param = c(paste(&quot;Intercept&quot;, 1:6), paste(&quot;male&quot;, 1:6)), reorder = c(6:1, 12:7)) %&gt;% # plot ggplot(aes(x = reorder(param, reorder))) + geom_hline(yintercept = 0, linetype = 3, color = &quot;#8B9DAF&quot;) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, y = Estimate, color = reorder &lt; 7), shape = 20, size = 3/4) + scale_color_manual(values = c(&quot;#394165&quot;, &quot;#A65141&quot;)) + xlab(NULL) + coord_flip() + theme_pearl_earring + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Just like in the text, our male slopes are much less dispersed than our intercepts. 13.2.3 Shrinkage. Figure 13.6.a depicts the correlation between the full UCB model’s varying intercepts and slopes. library(tidybayes) post &lt;- posterior_samples(b13.3) post %&gt;% ggplot(aes(x = cor_dept_id__Intercept__male, y = 0)) + geom_halfeyeh(fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;, point_interval = median_qi, .width = .95) + scale_x_continuous(breaks = c(-1, median(post$cor_dept_id__Intercept__male), 1), labels = c(-1, &quot;-.35&quot;, 1)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -1:1) + labs(subtitle = &quot;The dot is at the median; the\\nhorizontal bar is the 95% CI.&quot;, x = &quot;correlation&quot;) + theme_pearl_earring Much like for Figure 13.5.b, above, it’ll take a little data processing before we’re ready to reproduce Figure 13.6.b. # Here we put the partially-pooled estimate summaries in a tibble partially_pooled_params &lt;- coef(b13.3)$dept_id[ , 1, ] %&gt;% as_tibble() %&gt;% rename(intercept = Intercept, slope = male) %&gt;% mutate(dept = 1:n()) %&gt;% select(dept, everything()) # In order to calculate the unpooled estimates from the data, we&#39;ll need a function that # can convert probabilities into the logit metric. If you do the algebra, this is just # a transformation of the `inv_logit_scaled()` function. prob_to_logit &lt;- function(x){ -log((1 / x) -1) } # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% group_by(male, dept_id) %&gt;% summarise(prob_admit = mean(admit / applications)) %&gt;% ungroup() %&gt;% mutate(male = ifelse(male == 0, &quot;intercept&quot;, &quot;slope&quot;)) %&gt;% spread(key = male, value = prob_admit) %&gt;% rename(dept = dept_id) %&gt;% mutate(intercept = prob_to_logit(intercept), # Here we put our `prob_to_logit()` function to work slope = prob_to_logit(slope)) %&gt;% mutate(slope = slope - intercept) # Here we combine the partially-pooled and unpooled means into a single data object. params &lt;- bind_rows(partially_pooled_params, un_pooled_params) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) %&gt;% mutate(dept_letter = rep(LETTERS[1:6], times = 2)) # This will help with plotting params ## # A tibble: 12 x 5 ## dept intercept slope pooled dept_letter ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1.31 -0.794 partially A ## 2 2 0.742 -0.212 partially B ## 3 3 -0.647 0.0819 partially C ## 4 4 -0.617 -0.0924 partially D ## 5 5 -1.13 0.122 partially E ## 6 6 -2.60 -0.124 partially F ## 7 1 1.54 -1.05 not A ## 8 2 0.754 -0.220 not B ## 9 3 -0.660 0.125 not C ## 10 4 -0.622 -0.0820 not D ## 11 5 -1.16 0.200 not E ## 12 6 -2.58 -0.189 not F Here’s our version of Figure 13.6.b, depicting two-dimensional shrinkage for the partially-pooled multilevel estimates (posterior means) relative to the unpooled coefficients, calculated from the data. The ggrepel::geom_text_repel() function will help us with the in-plot labels. library(ggrepel) ggplot(data = params, aes(x = intercept, y = slope)) + mapply(function(level){ stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = dept, color = pooled)) + geom_line(aes(group = dept), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + geom_text_repel(data = params %&gt;% filter(pooled == &quot;partially&quot;), aes(label = dept_letter), color = &quot;#E8DCCF&quot;, size = 4, family = &quot;Courier&quot;, seed = 13.6) + coord_cartesian(xlim = range(params$intercept), ylim = range(params$slope)) + labs(x = expression(paste(&quot;intercept (&quot;, alpha[dept_id], &quot;)&quot;)), y = expression(paste(&quot;slope (&quot;, beta[dept_id], &quot;)&quot;))) + theme_pearl_earring 13.2.4 Model comparison. Fit the no-gender model. b13.4 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 2), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99, max_treedepth = 12)) Compare the three models by the WAIC. waic(b13.2, b13.3, b13.4) ## WAIC SE ## b13.2 108.43 16.46 ## b13.3 90.88 4.61 ## b13.4 105.30 18.00 ## b13.2 - b13.3 17.55 13.45 ## b13.2 - b13.4 3.12 3.62 ## b13.3 - b13.4 -14.42 15.20 The varying slopes model, [b13.3], dominates [the other two]. This is despite the fact that the average slope in [b13.3] is nearly zero. The average isn’t what matters, however. It is the individual slopes, one for each department, that matter. If we wish to generalize to new departments, the variation in slopes suggest that it’ll be worth paying attention to gender, even if the average slope is nearly zero in the population. (pp. 402–403, emphasis in the original) 13.3 Example: Cross-classified chimpanzees with varying slopes Retrieve the chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees detach(package:rethinking, unload = T) library(brms) rm(chimpanzees) d &lt;- d %&gt;% select(-recipient) %&gt;% mutate(block_id = block) My math’s aren’t the best. But if I’m following along correctly, here’s a fuller statistical expression of our cross-classified model. \\[\\begin{eqnarray} \\text{pulled_left}_i &amp; \\sim &amp; \\text{Binomial} (n = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = &amp; \\alpha_i + (\\beta_{1i} + \\beta_{2i} \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_i &amp; = &amp; \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block_id}_i} \\\\ \\beta_{1i} &amp; = &amp; \\beta_1 + \\beta_{1, \\text{actor}_i} + \\beta_{1, \\text{block_id}_i} \\\\ \\beta_{2i} &amp; = &amp; \\beta_2 + \\beta_{2, \\text{actor}_i} + \\beta_{2, \\text{block_id}_i} \\\\ \\begin{bmatrix} \\alpha_\\text{actor} \\\\ \\beta_{1, \\text{actor}} \\\\ \\beta_{2, \\text{actor}} \\end{bmatrix} &amp; \\sim &amp; \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{actor} \\end{pmatrix} \\\\ \\begin{bmatrix} \\alpha_\\text{block_id} \\\\ \\beta_{1, \\text{block_id}} \\\\ \\beta_{2, \\text{block_id}} \\end{bmatrix} &amp; \\sim &amp; \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{block_id} \\end{pmatrix} \\\\ \\mathbf S_\\text{actor} &amp; = &amp; \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\mathbf R_\\text{actor} \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\\\ \\mathbf S_\\text{block_id} &amp; = &amp; \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\mathbf R_\\text{block_id} \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\\\ (\\sigma_{\\alpha_\\text{actor}}, \\sigma_{\\beta_{1_\\text{actor}}}, \\sigma_{\\beta_{2_\\text{actor}}}) &amp; \\sim &amp; \\text{HalfCauchy} (0, 2) \\\\ (\\sigma_{\\alpha_\\text{block_id}}, \\sigma_{\\beta_{1_\\text{block_id}}}, \\sigma_{\\beta_{2_\\text{block_id}}}) &amp; \\sim &amp; \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R_\\text{actor} &amp; \\sim &amp; \\text{LKJcorr} (4) \\\\ \\mathbf R_\\text{block_id} &amp; \\sim &amp; \\text{LKJcorr} (4) \\end{eqnarray}\\] And now each \\(\\mathbf R\\) is a \\(3 \\times 3\\) correlation matrix. Let’s fit this beast. b13.6 &lt;- brm(data = d, family = binomial, pulled_left ~ 1 + prosoc_left + condition:prosoc_left + (1 + prosoc_left + condition:prosoc_left | actor) + (1 + prosoc_left + condition:prosoc_left | block_id), prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 3, cores = 3) Even though it’s not apparent in the syntax, our model b13.6 was already fit using the non-centered parameterization. Behind the scenes, Bürkner has brms do this automatically. It’s been that way all along. If you recall from last chapter, we can compute the number of effective samples for our parameters like so. ratios_cp &lt;- neff_ratio(b13.6) neff &lt;- ratios_cp %&gt;% as_tibble %&gt;% rename(neff_ratio = value) %&gt;% mutate(neff = neff_ratio * 12000) head(neff) ## # A tibble: 6 x 2 ## neff_ratio neff ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.174 2088. ## 2 0.518 6214. ## 3 0.627 7519. ## 4 0.320 3836. ## 5 0.465 5577. ## 6 0.550 6595. Now we’re ready for our variant of Figure 13.7. The handy ggbeeswarm package and its geom_quasirandom() function will give a better sense of the distribution. library(ggbeeswarm) neff %&gt;% ggplot(aes(x = factor(0), y = neff)) + geom_boxplot(fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;) + geom_quasirandom(method = &quot;tukeyDense&quot;, size = 2/3, color = &quot;#EEDA9D&quot;, alpha = 2/3) + scale_x_discrete(NULL, breaks = NULL, expand = c(.75, .75)) + scale_y_continuous(breaks = c(0, 6000, 12000)) + coord_cartesian(ylim = 0:12000) + labs(y = &quot;effective samples&quot;, subtitle = &quot;The non-centered\\nparameterization is the\\nbrms default. No fancy\\ncoding required.&quot;) + theme_pearl_earring As in the last chapter, we’ll use the bayesplot::mcmc_neff() function to examine the ratio of n.eff and the fill number of post-warm-up iterations, N. Ideally, that ratio is closer to 1 than not. library(bayesplot) color_scheme_set(c(&quot;#DCA258&quot;, &quot;#EEDA9D&quot;, &quot;#394165&quot;, &quot;#8B9DAF&quot;, &quot;#A65141&quot;, &quot;#A65141&quot;)) mcmc_neff(ratios_cp, size = 2) + theme_pearl_earring Here are our standard deviation parameters. tidy(b13.6) %&gt;% filter(str_detect(term , &quot;sd_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 sd_actor__Intercept 2.37 0.91 1.31 4.08 ## 2 sd_actor__prosoc_left 0.45 0.37 0.04 1.12 ## 3 sd_actor__prosoc_left:condition 0.52 0.48 0.04 1.42 ## 4 sd_block_id__Intercept 0.22 0.20 0.02 0.59 ## 5 sd_block_id__prosoc_left 0.57 0.40 0.07 1.31 ## 6 sd_block_id__prosoc_left:condition 0.51 0.44 0.04 1.29 McElreath discussed rethinking::link() in the middle of page 407. He showed how his link(m13.6NC) code returned a list of four matrices, of which the p matrix was of primary interest. The brms::fitted() function doesn’t work quite the same way, here. fitted(b13.6, summary = F, nsamples = 1000) %&gt;% str() ## Using the maximum response value as the number of trials. ## Warning: Using &#39;binomial&#39; families without specifying &#39;trials&#39; on the left-hand side of the model ## formula is deprecated. ## num [1:1000, 1:504] 0.365 0.303 0.321 0.248 0.27 ... First off, recall that fitted() returns summary values, by default. If we want individual values, set summary = FALSE. It’s also the fitted() default to use all posterior iterations, which is 12,000 in this case. To match the text, we need to set nsamples = 1000. But those are just details. The main point is that fitted() only returns one matrix, which is the analogue to the p matrix in the text. Moving forward, before we can follow along with McElreath’s R code 13.27, we need to refit the simpler model from way back in Chapter 12. b12.5 &lt;- brm(data = d, family = binomial, pulled_left ~ 1 + prosoc_left + condition:prosoc_left + (1 | actor) + (1 | block_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 3, cores = 3) Now we can compare them with waic(). waic(b13.6, b12.5) ## WAIC SE ## b13.6 534.77 19.89 ## b12.5 532.76 19.69 ## b13.6 - b12.5 2.00 4.10 model_weights(b13.6, b12.5, weights = &quot;waic&quot;) ## b13.6 b12.5 ## 0.2685155 0.7314845 In this example, no matter which varying effect structure you use, you’ll find that actors vary a lot in their baseline preference for the left-hand lever. Everything else is much less important. But using the most complex model, [b13.6], tells the correct story. Because the varying slopes are adaptively regularized, the model hasn’t overfit much, relative to the simpler model that contains only the important intercept variation. (p. 408) 13.4 Continuous categories and the Gaussian process There is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works. We’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 410) 13.4.1 Example: Spatial autocorrelation in Oceanic tools. # load the distance matrix library(rethinking) data(islandsDistMatrix) # display short column names, so fits on screen d_mat &lt;- islandsDistMatrix colnames(d_mat) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) round(d_mat, 1) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 If you wanted to use color to more effectively visualize the values in the matirx, you might do something like this. d_mat %&gt;% as_tibble() %&gt;% gather() %&gt;% rename(column = key, distance = value) %&gt;% mutate(row = rep(rownames(d_mat), times = 10), row_order = rep(9:0, times = 10), column_order = rep(0:9, each = 10)) %&gt;% ggplot(aes(x = reorder(column, column_order), y = reorder(row, row_order))) + geom_raster(aes(fill = distance)) + geom_text(aes(label = round(distance, digits = 1)), size = 3, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;) + scale_x_discrete(position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + labs(x = NULL, y = NULL) + theme_pearl_earring + theme(axis.ticks = element_blank(), axis.text.y = element_text(hjust = 0)) Figure 13.8 shows the “shape of the function relating distance to the covariance \\(\\mathbf{K}_{ij}\\).” tibble( x = seq(from = 0, to = 4, by = .01), linear = exp(-1 * x), squared = exp(-1 * x^2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = linear), color = &quot;#B1934A&quot;, linetype = 2) + geom_line(aes(y = squared), color = &quot;#DCA258&quot;) + scale_x_continuous(&quot;distance&quot;, expand = c(0, 0)) + scale_y_continuous(&quot;correlation&quot;, breaks = c(0, .5, 1), labels = c(0, &quot;.5&quot;, 1)) + theme_pearl_earring data(Kline2) # load the ordinary data, now with coordinates d &lt;- Kline2 %&gt;% mutate(society = 1:10) rm(Kline2) d %&gt;% glimpse() ## Observations: 10 ## Variables: 10 ## $ culture &lt;fct&gt; Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tong... ## $ population &lt;int&gt; 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000 ## $ contact &lt;fct&gt; low, low, low, high, high, high, high, low, high, low ## $ total_tools &lt;int&gt; 13, 22, 24, 43, 33, 19, 40, 28, 55, 71 ## $ mean_TU &lt;dbl&gt; 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6 ## $ lat &lt;dbl&gt; -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9 ## $ lon &lt;dbl&gt; 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6 ## $ lon2 &lt;dbl&gt; -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4 ## $ logpop &lt;dbl&gt; 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.472... ## $ society &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Okay, it appears this is going to be a bit of a ride. It’s not entirely clear to me if we can fit a Gaussian process model in brms that’s a direct equivalent to what McElreath did with rethinking. But we can try. First, note our use of the gp() syntax in the brm() function, below. We’re attempting to tell brms that we would like to include latitude and longitude (i.e., lat and long2, respectively) in a Gaussian process. Also note how our priors are a little different than those in the text. I’ll explain, below. Let’s just move ahead and fit the model. b13.7 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(inv_gamma(2.874624, 0.393695), class = lscale), prior(cauchy(0, 1), class = sdgp)), iter = 1e4, warmup = 2000, chains = 4, cores = 4, control = list(adapt_delta = 0.999, max_treedepth = 12)) Here’s the model summary. posterior_summary(b13.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.45 1.11 -0.72 3.78 ## b_logpop 0.23 0.10 0.02 0.44 ## sdgp_gplatlon2 0.52 0.36 0.16 1.46 ## lscale_gplatlon2 0.23 0.13 0.07 0.58 ## zgp_gplatlon2[1] -0.60 0.79 -2.16 0.92 ## zgp_gplatlon2[2] 0.44 0.85 -1.26 2.08 ## zgp_gplatlon2[3] -0.63 0.70 -1.98 0.88 ## zgp_gplatlon2[4] 0.88 0.71 -0.48 2.32 ## zgp_gplatlon2[5] 0.25 0.77 -1.25 1.78 ## zgp_gplatlon2[6] -1.00 0.79 -2.56 0.59 ## zgp_gplatlon2[7] 0.13 0.72 -1.39 1.52 ## zgp_gplatlon2[8] -0.18 0.88 -1.91 1.61 ## zgp_gplatlon2[9] 0.40 0.92 -1.51 2.11 ## zgp_gplatlon2[10] -0.32 0.83 -1.95 1.33 ## lp__ -51.61 3.16 -58.80 -46.46 Our Gaussian process parameters are different than McElreath’s. From the brms reference manual, here’s the brms parameterization: \\[k(x_{i},x_{j}) = sdgp^2 \\text{exp} (-||x_{i} - x_{j}||/2lscale^2)\\] What McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^2\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp_gplatlon2 before it’s on the same scale as etasq in the text. Here it is. posterior_samples(b13.7) %&gt;% transmute(sdgp_squared = sdgp_gplatlon2^2) %&gt;% mean_hdi(sdgp_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## sdgp_squared .lower .upper .width .point .interval ## 1 0.399 0.002 0.773 0.89 mean hdi Now we’re in the ballpark. In our model brm() code, above, we just went with the flow and kept the cauchy(0, 1) prior on sdgp. Now look at the denominator of the inner part of Bürkner equation, \\(2lscale^2\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^2\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^2\\) directly as rhosq. If I’m doing the algebra correctly–and that may well be a big if–, we might expect: \\[\\rho^2 = 1/(2 \\times (lscale^2))\\] But that doesn’t appear to be the case. Sigh. posterior_samples(b13.7) %&gt;% transmute(rho_squared = 1/(2*(lscale_gplatlon2^2))) %&gt;% mean_hdi(rho_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## rho_squared .lower .upper .width .point .interval ## 1 21.995 0.513 47.817 0.89 mean hdi Oh man, that isn’t even close to the 2.67 McElreath reported in the text. The plot deepens. If you look back, you’ll see we used a very different prior for \\(lscale\\). Here is it: inv_gamma(2.874624, 0.393695). Use get_prior() to discover where that came from. get_prior(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b logpop ## 3 student_t(3, 3, 10) Intercept ## 4 normal(0, 0.5) lscale ## 5 inv_gamma(2.874624, 0.393695) lscale gp(lat,lon2) ## 6 student_t(3, 0, 10) sdgp ## 7 sdgp gp(lat,lon2) That is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt [of the Stan team]’s vignette on the topic. Though it isn’t included in this document, I also ran the model with the cauchy(0, 1) prior and the results were quite similar. So the big discrepancy between our model and the one in the text isn’t based on that prior. Now that we’ve started, we may as well keep going down the comparison train. Let’s reproduce McElreath’s model with rethinking. Switch out brms for rethinking. detach(package:brms, unload = T) library(rethinking) m13.7 &lt;- map2stan( alist( total_tools ~ dpois(lambda), log(lambda) &lt;- a + g[society] + bp*logpop, g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ), a ~ dnorm(0,10), bp ~ dnorm(0,1), etasq ~ dcauchy(0,1), rhosq ~ dcauchy(0,1) ), data=list( total_tools=d$total_tools, logpop=d$logpop, society=d$society, Dmat=islandsDistMatrix), warmup=2000 , iter=1e4 , chains=4) Alright, now we’ll work directly with the posteriors to make some visual comparisons. # rethinking-based posterior post_m13.7 &lt;- rethinking::extract.samples(m13.7)[2:5] %&gt;% as_tibble() detach(package:rethinking, unload = T) library(brms) # brms-based posterior post_b13.7 &lt;- posterior_samples(b13.7) Here’s the model intercept posterior, by package: post_m13.7[, &quot;a&quot;] %&gt;% bind_rows(post_b13.7[, &quot;b_Intercept&quot;] %&gt;% as_tibble() %&gt;% rename(a = value)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = a, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Not identical, but pretty close&quot;, x = &quot;intercept&quot;) + theme_pearl_earring The slope: post_m13.7[, &quot;bp&quot;] %&gt;% bind_rows(post_b13.7[, &quot;b_logpop&quot;] %&gt;% as_tibble() %&gt;% rename(bp = value)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = bp, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Again, pretty close&quot;, x = &quot;slope&quot;) + theme_pearl_earring This one, \\(\\eta^2\\), required a little transformation: post_m13.7[, &quot;etasq&quot;] %&gt;% bind_rows(post_b13.7[, &quot;sdgp_gplatlon2&quot;] %&gt;% as_tibble() %&gt;% mutate(value = value^2) %&gt;% rename(etasq = value)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = etasq, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Still in the same ballpark&quot;, x = expression(eta^2)) + coord_cartesian(xlim = 0:3) + theme_pearl_earring \\(\\rho^2\\) required more extensive transformation of the brms posterior: post_m13.7[, &quot;rhosq&quot;] %&gt;% bind_rows(post_b13.7[, &quot;lscale_gplatlon2&quot;] %&gt;% as_tibble() %&gt;% transmute(value = 1/(2*(value^2))) %&gt;% # transmute(value = value^2) %&gt;% rename(rhosq = value)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = rhosq, fill = package)) + geom_density(size = 0) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(title = &quot;Holy smokes are those not the same!&quot;, subtitle = &quot;Notice how differently the y axes got scaled. Also, that brms density is\\nright skewed for days.&quot;, x = expression(rho^2)) + coord_cartesian(xlim = 0:50) + theme_pearl_earring + theme(legend.position = &quot;none&quot;) + facet_wrap(~package, scales = &quot;free_y&quot;) I’m in clinical psychology. Folks in my field don’t tend to use Gaussian processes, so getting to the bottom of this is low on my to-do list. Perhaps one of y’all are more experienced with Gaussian processes and see a flaw somewhere in my code. Please hit me up if you do. Anyways, here’s our brms + ggplot2 version of Figure 13.9. ggplot(data = tibble(x = c(0, 50.2)), aes(x = x)) + mapply(function(etasq, rhosq) { stat_function(fun = function(x, etasq, rhosq) etasq*exp(-rhosq*x^2), args = list(etasq = etasq, rhosq = rhosq), size = 1/4, alpha = 1/4, color = &quot;#EEDA9D&quot;) }, etasq = post_b13.7[1:100, &quot;sdgp_gplatlon2&quot;]^2, rhosq = post_b13.7[1:100, &quot;lscale_gplatlon2&quot;]^2*.5 ) + stat_function(fun = function(x) median(post_b13.7$sdgp_gplatlon2)^2 *exp(-median(post_b13.7[1:100, &quot;lscale_gplatlon2&quot;] )^2*.5*x^2), color = &quot;#EEDA9D&quot;, size = 1.1) + coord_cartesian(ylim = 0:1) + scale_x_continuous(breaks = seq(from = 0, to = 50, by = 10), expand = c(0, 0)) + labs(x = &quot;distance (thousand km)&quot;, y = &quot;covariance&quot;) + theme_pearl_earring Do note the scale on which we placed our x axis. Our brms parameterization resulted in a gentler decline in spatial covariance. Let’s finish this up and “push the parameters back through the function for \\(\\mathbf{K}\\), the covariance matrix” (p. 415). # compute posterior median covariance among societies k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(post_b13.7$sdgp_gplatlon2^2) * exp(-median(post_b13.7$lscale_gplatlon2^2) * islandsDistMatrix[i, j]^2) diag(k) &lt;- median(post_b13.7$sdgp_gplatlon2^2) + 0.01 k %&gt;% round(2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.19 0.18 0.18 0.08 0.17 0.16 0.12 0.13 0.16 0.05 ## [2,] 0.18 0.19 0.18 0.09 0.17 0.16 0.13 0.14 0.16 0.06 ## [3,] 0.18 0.18 0.19 0.10 0.17 0.16 0.14 0.15 0.15 0.06 ## [4,] 0.08 0.09 0.10 0.19 0.06 0.14 0.17 0.17 0.04 0.02 ## [5,] 0.17 0.17 0.17 0.06 0.19 0.12 0.10 0.10 0.18 0.07 ## [6,] 0.16 0.16 0.16 0.14 0.12 0.19 0.16 0.18 0.10 0.03 ## [7,] 0.12 0.13 0.14 0.17 0.10 0.16 0.19 0.17 0.07 0.05 ## [8,] 0.13 0.14 0.15 0.17 0.10 0.18 0.17 0.19 0.08 0.03 ## [9,] 0.16 0.16 0.15 0.04 0.18 0.10 0.07 0.08 0.19 0.07 ## [10,] 0.05 0.06 0.06 0.02 0.07 0.03 0.05 0.03 0.07 0.19 And we’ll continue to follow suit and change these to a correlation matrix. # convert to correlation matrix rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;,&quot;Ti&quot;,&quot;SC&quot;,&quot;Ya&quot;,&quot;Fi&quot;,&quot;Tr&quot;,&quot;Ch&quot;,&quot;Mn&quot;,&quot;To&quot;,&quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.94 0.93 0.44 0.89 0.80 0.63 0.69 0.82 0.26 ## Ti 0.94 1.00 0.94 0.47 0.89 0.81 0.68 0.71 0.81 0.30 ## SC 0.93 0.94 1.00 0.52 0.86 0.84 0.72 0.76 0.77 0.29 ## Ya 0.44 0.47 0.52 1.00 0.29 0.74 0.86 0.85 0.20 0.12 ## Fi 0.89 0.89 0.86 0.29 1.00 0.62 0.49 0.51 0.93 0.36 ## Tr 0.80 0.81 0.84 0.74 0.62 1.00 0.83 0.92 0.51 0.16 ## Ch 0.63 0.68 0.72 0.86 0.49 0.83 1.00 0.89 0.37 0.24 ## Mn 0.69 0.71 0.76 0.85 0.51 0.92 0.89 1.00 0.40 0.15 ## To 0.82 0.81 0.77 0.20 0.93 0.51 0.37 0.40 1.00 0.34 ## Ha 0.26 0.30 0.29 0.12 0.36 0.16 0.24 0.15 0.34 1.00 The correlations in our rho matrix look a little higher than those in the text. Before we get see them in a plot, let’s consider psize. If you really want to scale the points in Figure 13.10.a like McElreath did, you can make the psize variable in a tidyverse sort of way as follows. However, if you compare the psize method and the default ggplot2 method using just logpop, you’ll see the difference is negligible. In that light, I’m going to be lazy and just use logpop in my plots. d %&gt;% transmute(psize = logpop / max(logpop)) %&gt;% transmute(psize = exp(psize * 1.5) - 2) ## psize ## 1 0.3134090 ## 2 0.4009582 ## 3 0.6663711 ## 4 0.7592196 ## 5 0.9066890 ## 6 0.9339560 ## 7 0.9834797 ## 8 1.1096138 ## 9 1.2223112 ## 10 2.4816891 As far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so. tidy_rho &lt;- rho %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(d %&gt;% select(culture, logpop, total_tools, lon2, lat)) %&gt;% gather(colname, correlation, -rowname, -culture, -logpop, -total_tools, -lon2, -lat) %&gt;% mutate(group = paste(pmin(rowname, colname), pmax(rowname, colname))) %&gt;% select(rowname, colname, group, culture, everything()) head(tidy_rho) ## rowname colname group culture logpop total_tools lon2 lat correlation ## 1 Ml Ml Ml Ml Malekula 7.003065 13 -12.5 -16.3 1.00 ## 2 Ti Ml Ml Ti Tikopia 7.313220 22 -11.2 -12.3 0.94 ## 3 SC Ml Ml SC Santa Cruz 8.188689 24 -14.0 -10.7 0.93 ## 4 Ya Ml Ml Ya Yap 8.474494 43 -41.9 9.5 0.44 ## 5 Fi Ml Fi Ml Lau Fiji 8.909235 33 -1.9 -17.7 0.89 ## 6 Tr Ml Ml Tr Trobriand 8.987197 19 -29.1 -8.7 0.80 Okay, here’s our version of Figure 13.10.a. tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_line(aes(group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(d$lon2), ylim = range(d$lat)) + theme(legend.position = &quot;none&quot;) + theme_pearl_earring Yep, as expressed by the intensity of the colors of the connecting lines, those correlations are more pronounced than those in the text. Here’s our version of Figure 13.10.b. # new data for fitted() nd &lt;- tibble(logpop = seq(from = 6, to = 14, length.out = 30), lat = median(d$lat), lon2 = median(d$lon2)) # fitted() ftd &lt;- fitted(b13.7, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # plot tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_ribbon(data = ftd, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;#394165&quot;, alpha = .5) + geom_line(data = ftd, aes(y = Estimate), color = &quot;#100F14&quot;, linetype = 1, size = 1.1) + # 80A0C7 100F14 geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(y = total_tools, size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(y = total_tools, label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(d$logpop), ylim = range(d$total_tools)) + theme(legend.position = &quot;none&quot;) + theme_pearl_earring Same deal. Our higher correlations make for a more intensely-webbed plot. To learn more on Bürkner’s thoughts on this model in brms, check out the thread on this issue. 13.5 Summary Bonus: Another Berkley-admissions-data-like example. McElreath uploaded recordings of him teaching out of his text for a graduate course during the 2017/2018 fall semester. In the beginning of lecture 13 from week 7, he discussed a paper from van der Lee and Ellemers (2015) published an article in PNAS. Their paper suggested male researchers were more likely than female researchers to get research funding in the Netherlands. In their initial analysis (p. 12350) they provided a simple \\(\\chi^2\\) test to test the null hypothesis there was no difference in success for male versus female researchers, for which they reported \\(\\chi_{df = 1}^2 = 4.01, p = .045\\). Happily, van der Lee and Ellemers provided their data values in their supplemental material (i.e., Table S1.), which McElreath also displayed in his video. Their data follows the same structure as the Berkley admissions data. In his lecture, McElreath suggested their \\(\\chi^2\\) test is an example of Simpson’s paradox, just as with the Berkley data. He isn’t the first person to raise this criticism (see Volker and SteenBeek’s critique, which McElreath also pointed to in the lecture). Here are the data: funding &lt;- tibble( discipline = rep(c(&quot;Chemical sciences&quot;, &quot;Physical sciences&quot;, &quot;Physics&quot;, &quot;Humanities&quot;, &quot;Technical sciences&quot;, &quot;Interdisciplinary&quot;, &quot;Earth/life sciences&quot;, &quot;Social sciences&quot;, &quot;Medical sciences&quot;), each = 2), gender = rep(c(&quot;m&quot;, &quot;f&quot;), times = 9), applications = c(83, 39, 135, 39, 67, 9, 230, 166, 189, 62, 105, 78, 156, 126, 425, 409, 245, 260) %&gt;% as.integer(), awards = c(22, 10, 26, 9, 18, 2, 33, 32, 30, 13, 12, 17, 38, 18, 65, 47, 46, 29) %&gt;% as.integer(), rejects = c(61, 29, 109, 30, 49, 7, 197, 134, 159, 49, 93, 61, 118, 108, 360, 362, 199, 231) %&gt;% as.integer(), male = ifelse(gender == &quot;f&quot;, 0, 1) %&gt;% as.integer() ) funding ## # A tibble: 18 x 6 ## discipline gender applications awards rejects male ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Chemical sciences m 83 22 61 1 ## 2 Chemical sciences f 39 10 29 0 ## 3 Physical sciences m 135 26 109 1 ## 4 Physical sciences f 39 9 30 0 ## 5 Physics m 67 18 49 1 ## 6 Physics f 9 2 7 0 ## 7 Humanities m 230 33 197 1 ## 8 Humanities f 166 32 134 0 ## 9 Technical sciences m 189 30 159 1 ## 10 Technical sciences f 62 13 49 0 ## 11 Interdisciplinary m 105 12 93 1 ## 12 Interdisciplinary f 78 17 61 0 ## 13 Earth/life sciences m 156 38 118 1 ## 14 Earth/life sciences f 126 18 108 0 ## 15 Social sciences m 425 65 360 1 ## 16 Social sciences f 409 47 362 0 ## 17 Medical sciences m 245 46 199 1 ## 18 Medical sciences f 260 29 231 0 Let’s fit a few models. First, we’ll fit an analogue to the initial van der Lee and Ellemers \\(\\chi^2\\) test. Since we’re Bayesian modelers, we’ll use a simple logistic regression, using male (dummy coded 0 = female, 1 = male) to predict admission (i.e., awards). b13.bonus_0 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male, # Note our continued use of weakly-regularizing priors prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 5000, warmup = 1000, chains = 4, cores = 4) If you inspect them, the chains look great. Here are the posterior summaries: tidy(b13.bonus_0) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.75 0.08 -1.88 -1.61 ## 2 b_male 0.21 0.10 0.04 0.38 Yep, the 95% intervals for male dummy exclude zero. If you wanted a one-sided Bayesian \\(p\\)-value, you might do something like: posterior_samples(b13.bonus_0) %&gt;% summarise(One_sided_Bayesian_p_value = filter(., b_male &lt;= 0) %&gt;% nrow()/nrow(.)) ## One_sided_Bayesian_p_value ## 1 0.0209375 Pretty small. But recall how Simpson’s paradox helped us understand the Berkley data. Different departments in Berkley had different acceptance rates AND different ratios of male and female applicants. Similarly, different academic disciplines in the Netherlands might have different award rates for funding AND different ratios of male and female applications. Just like in section 13.2, let’s fit two more models. The first model will allow intercepts to vary by discipline. The second model will allow intercepts and the male dummy slopes to vary by discipline. b13.bonus_1 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99)) b13.bonus_2 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 + male | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99)) We’ll compare the models with information criteria. waic(b13.bonus_0, b13.bonus_1, b13.bonus_2) ## WAIC SE ## b13.bonus_0 129.69 8.90 ## b13.bonus_1 125.74 7.35 ## b13.bonus_2 116.61 5.60 ## b13.bonus_0 - b13.bonus_1 3.95 6.29 ## b13.bonus_0 - b13.bonus_2 13.09 5.60 ## b13.bonus_1 - b13.bonus_2 9.14 2.76 The WAIC suggests the varying intercepts/varying slopes model made the best sense of the data. Here’s what the random intercepts look like in a coefficient plot. coef(b13.bonus_2)$discipline[, , 2] %&gt;% as_tibble() %&gt;% mutate(discipline = c(&quot;Chemical sciences&quot;, &quot;Physical sciences&quot;, &quot;Physics&quot;, &quot;Humanities&quot;, &quot;Technical sciences&quot;, &quot;Interdisciplinary&quot;, &quot;Earth/life sciences&quot;, &quot;Social sciences&quot;, &quot;Medical sciences&quot;)) %&gt;% ggplot(aes(x = reorder(discipline, Estimate), y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = 0, color = &quot;#E8DCCF&quot;, alpha = 1/4) + geom_hline(yintercept = fixef(b13.bonus_2)[2], linetype = 3, color = &quot;#80A0C7&quot;) + geom_pointrange(shape = 20, size = 3/4, color = &quot;#8B9DAF&quot;) + labs(title = &quot;Random slopes for the male dummy&quot;, subtitle = &quot;The vertical dotted line is the posterior mean of the fixed effect for the\\nmale dummy. The dots and horizontal lines are the posterior means and\\npercentile-based 95% intervals, respectively. The values are on the log scale.&quot;, x = NULL, y = NULL) + coord_flip(ylim = -1:1) + theme_pearl_earring + theme(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Note how the 95% intervals for all the random male slopes contain zero within their bounds. Here are the fixed effects: tidy(b13.bonus_2) %&gt;% filter(str_detect(term , &quot;b_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.63 0.14 -1.85 -1.38 ## 2 b_male 0.15 0.17 -0.13 0.42 And if you wanted a one-sided Bayesian \\(p\\)-value for the male dummy for the full model: posterior_samples(b13.bonus_2) %&gt;% summarise(One_sided_Bayesian_p_value = filter(., b_male &lt;= 0) %&gt;% nrow()/nrow(.)) ## One_sided_Bayesian_p_value ## 1 0.17975 So, the estimate of the gender bias is small and consistent with the null hypothesis. Which is good! We want gender equality for things like funding success. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.5.0 bayesplot_1.6.0 ggbeeswarm_0.6.0 ggrepel_0.8.0 tidybayes_1.0.1 ## [6] broom_0.4.5 Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 dutchmasters_0.1.0 ## [11] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 ## [16] tidyr_0.8.1 tibble_1.4.2 ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] labeling_0.3 mnormt_1.5-5 bridgesampling_0.4-0 ## [22] rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 ## [28] reshape_0.8.7 assertthat_0.2.0 promises_1.0.1 ## [31] scales_0.5.0 beeswarm_0.2.3 gtable_0.2.0 ## [34] rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [37] inline_0.3.15 yaml_2.1.19 reshape2_1.4.3 ## [40] abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 ## [46] rsconnect_0.8.8 extrafont_0.17 tools_3.5.1 ## [49] bookdown_0.7 psych_1.8.4 RColorBrewer_1.1-2 ## [52] ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [55] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 ## [58] LaplacesDemon_16.1.1 haven_1.1.2 magrittr_1.5 ## [61] colourpicker_1.0 mvtnorm_1.0-8 matrixStats_0.54.0 ## [64] hms_0.4.2 shinyjs_1.0 mime_0.5 ## [67] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [70] shinystan_2.5.0 readxl_1.1.0 gridExtra_2.3 ## [73] rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [76] crayon_1.3.4 htmltools_0.3.6 later_0.7.3 ## [79] lubridate_1.7.4 MASS_7.3-50 Matrix_1.2-14 ## [82] cli_1.0.0 bindr_0.1.1 igraph_1.2.1 ## [85] pkgconfig_2.0.1 foreign_0.8-70 xml2_1.2.0 ## [88] svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 ## [91] rvest_0.3.2 digest_0.6.15 rmarkdown_1.10 ## [94] cellranger_1.1.0 shiny_1.1.0 gtools_3.8.1 ## [97] nlme_3.1-137 jsonlite_1.5 bindrcpp_0.2.2 ## [100] mapproj_1.2.6 viridisLite_0.3.0 pillar_1.2.3 ## [103] lattice_0.20-35 loo_2.0.0 httr_1.3.1 ## [106] glue_1.2.0 xts_0.10-2 shinythemes_1.1.1 ## [109] pander_0.6.2 stringi_1.2.3 "],
["missing-data-and-other-opportunities.html", "14 Missing Data and Other Opportunities 14.1 Measurement error 14.2 Missing data Reference Session info", " 14 Missing Data and Other Opportunities For the opening example, we’re playing with the conditional probability \\[ \\text{Pr(burnt down | burnt up)} = \\frac{\\text{Pr(burnt up, burnt down)}}{\\text{Pr(burnt up)}} \\] It works out that \\[ \\text{Pr(burnt down | burnt up)} = \\frac{1/3}{1/2} = \\frac{2}{3} \\] We might express the math in the middle of page 423 in tibble form like this. library(tidyverse) p_pancake &lt;- 1/3 ( d &lt;- tibble(pancake = c(&quot;BB&quot;, &quot;BU&quot;, &quot;UU&quot;), p_burnt = c(1, .5, 0)) %&gt;% mutate(p_burnt_up = p_burnt * p_pancake) ) ## # A tibble: 3 x 3 ## pancake p_burnt p_burnt_up ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BB 1 0.333 ## 2 BU 0.5 0.167 ## 3 UU 0 0 d %&gt;% summarise(`p (burnt_down | burnt_up)` = p_pancake / sum(p_burnt_up)) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.667 I understood McElreath’s simulation better after I broke it apart. The first part of sim_pancake() takes one random draw from the integers 1, 2, and 3. It just so happens that if we set set.seed(1), the code returns a 1. set.seed(1) sample(x = 1:3, size = 1) ## [1] 1 So here’s what it looks like if we use seeds 2:11. take_sample &lt;- function(seed){ set.seed(seed) sample(x = 1:3, size = 1) } tibble(seed = 2:11) %&gt;% mutate(value_returned = map(seed, take_sample)) %&gt;% unnest() ## # A tibble: 10 x 2 ## seed value_returned ## &lt;int&gt; &lt;int&gt; ## 1 2 1 ## 2 3 1 ## 3 4 2 ## 4 5 1 ## 5 6 2 ## 6 7 3 ## 7 8 2 ## 8 9 1 ## 9 10 2 ## 10 11 1 Each of those value_returned values stands for one of the three pancakes: 1 = BB, 2 = BU, 3 = UU. In the next line, McElreath made slick use of a matrix to specify that. Here’s what the matrix looks like: matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] 1 0 0 See how the three columns are identified as [,1], [,2], and [,3]? If, say, we wanted to subset the values in the second column, we’d execute matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] ## [1] 1 0 which returns a numeric vector. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] %&gt;% str() ## num [1:2] 1 0 And that 1 0 corresponds to the pancake with one burnt (i.e., 1) and one unburnt (i.e., 0) side. So when McElreath then executed sample(sides), he randomly sampled from one of those two values. In the case of pancake == 2, he randomly sampled one the pancake with one burnt and one unburnt side. Had he sampled from pancake == 1, he would have sampled from the pancake with both sides burnt. Going forward, let’s amend McElreath’s sim_pancake() function a bit. First, we’ll add a seed argument, with will allow us to make the output reproducible. We’ll be inserting seed into set.seed() in the two places preceding the sample() function. The second major change is that we’re going to convert the output of the sim_pancake() function to a tibble and adding a side column, which will contain the values c(&quot;up&quot;, &quot;down&quot;). Just for pedagogical purposes, we’ll also add pancake_n and pancake_chr columns to help index which pancake the draws came from. # simulate a pancake and return randomly ordered sides sim_pancake &lt;- function(seed) { set.seed(seed) pancake &lt;- sample(x = 1:3, size = 1) sides &lt;- matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, pancake] set.seed(seed) sample(sides) %&gt;% as_tibble() %&gt;% mutate(side = c(&quot;up&quot;, &quot;down&quot;), pancake_n = pancake, pancake_chr = ifelse(pancake == 1, &quot;BB&quot;, ifelse(pancake == 2, &quot;BU&quot;, &quot;UU&quot;))) } Let’s take this baby for a whirl. # How many simulations would you like? n_sim &lt;- 1e4 ( d &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(r = map(seed, sim_pancake)) %&gt;% unnest() ) ## # A tibble: 20,000 x 5 ## seed value side pancake_n pancake_chr ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 up 1 BB ## 2 1 1 down 1 BB ## 3 2 1 up 1 BB ## 4 2 1 down 1 BB ## 5 3 1 up 1 BB ## 6 3 1 down 1 BB ## 7 4 0 up 2 BU ## 8 4 1 down 2 BU ## 9 5 1 up 1 BB ## 10 5 1 down 1 BB ## # ... with 19,990 more rows And now we’ll spread() and summarise() to get the value we’ve been working for. d %&gt;% spread(key = side, value = value) %&gt;% summarise(`p (burnt_down | burnt_up)` = sum(up == 1 &amp; down == 1) / ( sum(up == 1))) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.661 The results are within rounding error of the ideal 2/3. Probability theory is not difficult mathematically. It’s just counting. But it is hard to interpret and apply. Doing so often seems to require some cleverness, and authors have an incentive to solve problems in clever ways, just to show off. But we don’t need that cleverness, if we ruthlessly apply conditional probability… In this chapter, [we’ll] meet two commonplace applications of this assume-and-deduce strategy. The first is the incorporation of measurement error into our models. The second is the estimation of missing data through Bayesian imputation… In neither application do [we] have to intuit the consequences of measurement errors nor the implications of missing values in order to design the models. All [we] have to do is state [the] information about the error or about the variables with missing values. Logic does the rest. (p. 424) 14.1 Measurement error First, let’s grab our WaffleDivorce data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce rm(WaffleDivorce) Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) The brms package currently supports theme_black(), which changes the default ggplot2 theme to a black background with white lines, text, and so forth. You can find the origins of the code, here. Though I like the idea of brms including theme_black(), I’m not a fan of some of the default settings (e.g., it includes gridlines). Happily, data scientist Tyler Rinker has some nice alternative theme_black() code you can find here. The version of theme_black() used for this chapter is based on his version, with a few amendments of my own. theme_black &lt;- function(base_size=12, base_family=&quot;&quot;) { theme_grey(base_size=base_size, base_family=base_family) %+replace% theme( # Specify axis options axis.line=element_blank(), # All text colors used to be &quot;grey55&quot; axis.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9, vjust=1), axis.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9,hjust=1), axis.ticks=element_line(color=&quot;grey55&quot;, size = 0.2), axis.title.x=element_text(size=base_size, color=&quot;grey85&quot;, vjust=1, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;)), axis.title.y=element_text(size=base_size, color=&quot;grey85&quot;, angle=90, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;), vjust=0.5), axis.ticks.length=grid::unit(0.3, &quot;lines&quot;), # Specify legend options legend.background=element_rect(color=NA, fill=&quot;black&quot;), legend.key=element_rect(color=&quot;grey55&quot;, fill=&quot;black&quot;), legend.key.size=grid::unit(1.2, &quot;lines&quot;), legend.key.height=NULL, legend.key.width=NULL, legend.text=element_text(size=base_size*0.8, color=&quot;grey85&quot;), legend.title=element_text(size=base_size*0.8, face=&quot;bold&quot;,hjust=0, color=&quot;grey85&quot;), # legend.position=&quot;right&quot;, legend.position = &quot;none&quot;, legend.text.align=NULL, legend.title.align=NULL, legend.direction=&quot;vertical&quot;, legend.box=NULL, # Specify panel options panel.background=element_rect(fill=&quot;black&quot;, color = NA), panel.border=element_rect(fill=NA, color=&quot;grey55&quot;), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.spacing=grid::unit(0.25,&quot;lines&quot;), # Specify facetting options strip.background=element_rect(fill = &quot;black&quot;, color=&quot;grey10&quot;), # fill=&quot;grey30&quot; strip.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;), strip.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, angle=-90), # Specify plot options plot.background=element_rect(color=&quot;black&quot;, fill=&quot;black&quot;), plot.title=element_text(size=base_size*1.2, color=&quot;grey85&quot;, hjust = 0), # added hjust = 0 plot.subtitle=element_text(size=base_size*.9, color=&quot;grey85&quot;, hjust = 0), # added line # plot.margin=grid::unit(c(1, 1, 0.5, 0.5), &quot;lines&quot;) plot.margin=grid::unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;) ) } One way to use our theme_black() is to make it part of the code for an individual plot, such as ggplot() + geom_point() + theme_back(). Another way is to make theme_black() the default setting with bayesplot::theme_set(). That’s the method we’ll use. library(bayesplot) theme_set(theme_black()) # To reset the default ggplot2 theme to its traditional parameters, use this code: # theme_set(theme_default()) In the brms reference manual, Bürkner recommended complimenting theme_black() with color scheme “C” from the viridis package, which provides a variety of colorblind-safe color palettes. # install.packages(&quot;viridis&quot;) library(viridis) The viridis_pal() function gives a list of colors within a given palette. The colors in each palette fall on a spectrum. Within viridis_pal(), the option argument allows one to select a given spectrum, “C”, in our case. The final parentheses, (), allows one to determine how many discrete colors one would like to break the spectrum up by. We’ll choose 7. viridis_pal(option = &quot;C&quot;)(7) ## [1] &quot;#0D0887FF&quot; &quot;#5D01A6FF&quot; &quot;#9C179EFF&quot; &quot;#CC4678FF&quot; &quot;#ED7953FF&quot; &quot;#FDB32FFF&quot; &quot;#F0F921FF&quot; With a little data wrangling, we can put the colors of our palette in a tibble and display them in a plot. viridis_pal(option = &quot;C&quot;)(7) %&gt;% as_tibble() %&gt;% mutate(color_number = str_c(1:7, &quot;. &quot;, value), number = 1:7) %&gt;% ggplot(aes(x = factor(0), y = reorder(color_number, number))) + geom_tile(aes(fill = factor(number))) + geom_text(aes(color = factor(number), label = color_number)) + scale_color_manual(values = c(rep(&quot;black&quot;, times = 4), rep(&quot;white&quot;, times = 3))) + scale_fill_viridis(option = &quot;C&quot;, discrete = T, direction = -1) + scale_x_discrete(NULL, breaks = NULL) + scale_y_discrete(NULL, breaks = NULL) + ggtitle(&quot;Behold: viridis C!&quot;) Now, let’s make use of our custom theme and reproduce/reimagine Figure 14.1.a. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] d %&gt;% ggplot(aes(x = MedianAgeMarriage, y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate&quot;) Notice how viridis_pal(option = &quot;C&quot;)(7)[7] called the seventh color in the color scheme, &quot;#F0F921FF&quot;. For Figure 14.1.b, we’ll select the sixth color in the palette by coding viridis_pal(option = &quot;C&quot;)(7)[6]. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6] d %&gt;% ggplot(aes(x = log(Population), y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;log population&quot;, y = &quot;Divorce rate&quot;) Just like in the text, our plot shows states with larger populations tend to have smaller measurement error. 14.1.1 Error on the outcome. To get a better sense of what we’re about to do, imagine for a moment that each states’ divorce rate is normally distributed with a mean of Divorce and standard deviation Divorce.SE. Those distributions would be: d %&gt;% mutate(Divorce_distribution = str_c(&quot;Divorce ~ Normal(&quot;, Divorce, &quot;, &quot;, Divorce.SE, &quot;)&quot;)) %&gt;% select(Loc, Divorce_distribution) %&gt;% head() ## Loc Divorce_distribution ## 1 AL Divorce ~ Normal(12.7, 0.79) ## 2 AK Divorce ~ Normal(12.5, 2.05) ## 3 AZ Divorce ~ Normal(10.8, 0.74) ## 4 AR Divorce ~ Normal(13.5, 1.22) ## 5 CA Divorce ~ Normal(8, 0.24) ## 6 CO Divorce ~ Normal(11.6, 0.94) As in the text In [the following] example we’ll use a Gaussian distribution with mean equal to the observed value and standard deviation equal to the measurement’s standard error. This is the logical choice, because if all we know about the error is its standard deviation, then the maximum entropy distribution for it will be Gaussian… Here’s how to define the distribution for each divorce rate. For each observed value \\(D_{\\text{OBS}i}\\), there will be one parameter, \\(D_{\\text{EST}i}\\), defined by: \\[D_{\\text{OBS}i} \\sim \\text{Normal} (D_{\\text{EST}i}, D_{\\text{SE}i})\\] All this does is define the measurement \\(D_{\\text{OBS}i}\\) as having the specified Gaussian distribution centered on the unknown parameter \\(D_{\\text{EST}i}\\). So the above defines a probability for each State \\(i\\)’s observed divorce rate, given a known measurement error. (pp. 426–427) Now we’re ready to fit some models. In brms, there are at least two ways to accommodate measurement error in the criterion. The first way uses the se() syntax, following the form &lt;response&gt; | se(&lt;se_response&gt;, sigma = TRUE). With this syntax, se stands for standard error, the loose frequentist analogue to the Bayesian posterior \\(SD\\). Unless you’re fitting a meta-analysis on summary information, make sure to specify sigma = TRUE. Without that you’ll have no estimate for \\(\\sigma\\)! For more information on the se() method, go to the brms reference manual and find the Additional response information subsection of the brmsformula section. The second way uses the mi() syntax, following the form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the newer missing data capabilities for brms. We will cover that in more detail in the second half of this chapter. We’ll start off useing both methods. Our first model, b14.1_se, will follow the se() syntax; the second model, b14.1_mi, will follow the mi() syntax. # Put the data into a list dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, R = d$Marriage, A = d$MedianAgeMarriage) # Here we specify the initial (i.e., starting) values inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # Fit the models b14.1_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, control = list(adapt_delta = 0.99, max_treedepth = 12), inits = inits_list) b14.1_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, # note this line for the `mi()` model inits = inits_list) Before we dive into the model summaries, notice how the starting values (i.e., inits) differ by model. Even though we coded inits = inits_list for both models, the differ by fit@inits. b14.1_se$fit@inits ## [[1]] ## [[1]]$b ## [1] 0.2068648 -0.1441787 -1.2745083 ## ## [[1]]$sigma ## [1] 2.84125 ## ## ## [[2]] ## [[2]]$b ## [1] 1.6530769 1.6304362 0.7857997 ## ## [[2]]$sigma ## [1] 1.833162 b14.1_mi$fit@inits ## [[1]] ## [[1]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[1]]$b ## [1] -0.6843108 1.1501697 1.1976380 ## ## [[1]]$sigma ## [1] 0.4970607 ## ## ## [[2]] ## [[2]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[2]]$b ## [1] 0.6437190 -0.8064231 0.1724876 ## ## [[2]]$sigma ## [1] 0.4096365 As we explore further, it should become apparent why. Here are the primary model summaries. print(b14.1_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 21.33 6.88 6.92 34.16 1713 1.00 ## R 0.13 0.08 -0.02 0.28 2190 1.00 ## A -0.55 0.22 -0.96 -0.08 1772 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.13 0.22 0.75 1.60 2493 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.1_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 21.35 6.60 7.97 33.82 2702 1.00 ## R 0.13 0.08 -0.02 0.28 3008 1.00 ## A -0.55 0.21 -0.95 -0.11 2811 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.13 0.21 0.75 1.58 2386 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Based on the print()/summary() information, the main parameters for the models are about the same. However, the plot deepens when we summarize the models with the broom::tidy() method. library(broom) tidy(b14.1_se) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_intercept 21.33 6.88 9.83 32.39 ## 2 b_R 0.13 0.08 0.00 0.26 ## 3 b_A -0.55 0.22 -0.90 -0.17 ## 4 sigma 1.13 0.22 0.80 1.51 ## 5 lp__ -105.45 1.54 -108.38 -103.71 tidy(b14.1_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_intercept 21.35 6.60 9.93 31.88 ## 2 b_R 0.13 0.08 0.00 0.26 ## 3 b_A -0.55 0.21 -0.89 -0.18 ## 4 sigma 1.13 0.21 0.81 1.50 ## 5 Yl[1] 11.79 0.68 10.69 12.92 ## 6 Yl[2] 11.18 1.05 9.44 12.93 ## 7 Yl[3] 10.46 0.62 9.45 11.48 ## 8 Yl[4] 12.34 0.87 10.96 13.81 ## 9 Yl[5] 8.05 0.24 7.66 8.44 ## 10 Yl[6] 11.01 0.74 9.81 12.24 ## 11 Yl[7] 7.24 0.64 6.18 8.26 ## 12 Yl[8] 9.35 0.91 7.87 10.84 ## 13 Yl[9] 7.00 1.09 5.21 8.82 ## 14 Yl[10] 8.54 0.31 8.03 9.04 ## 15 Yl[11] 11.15 0.53 10.28 12.02 ## 16 Yl[12] 9.09 0.90 7.59 10.56 ## 17 Yl[13] 9.69 0.91 8.16 11.19 ## 18 Yl[14] 8.11 0.41 7.44 8.79 ## 19 Yl[15] 10.69 0.55 9.78 11.60 ## 20 Yl[16] 10.16 0.71 9.02 11.31 ## 21 Yl[17] 10.51 0.78 9.22 11.79 ## 22 Yl[18] 11.94 0.66 10.87 13.04 ## 23 Yl[19] 10.48 0.70 9.32 11.64 ## 24 Yl[20] 10.17 1.01 8.58 11.87 ## 25 Yl[21] 8.75 0.59 7.80 9.72 ## 26 Yl[22] 7.77 0.47 6.99 8.55 ## 27 Yl[23] 9.15 0.49 8.35 9.96 ## 28 Yl[24] 7.73 0.54 6.83 8.62 ## 29 Yl[25] 10.44 0.77 9.17 11.71 ## 30 Yl[26] 9.54 0.58 8.58 10.49 ## 31 Yl[27] 9.44 0.99 7.80 11.08 ## 32 Yl[28] 9.27 0.74 8.02 10.45 ## 33 Yl[29] 9.18 0.94 7.66 10.75 ## 34 Yl[30] 6.38 0.44 5.66 7.11 ## 35 Yl[31] 9.97 0.78 8.70 11.26 ## 36 Yl[32] 6.69 0.31 6.18 7.20 ## 37 Yl[33] 9.88 0.44 9.17 10.61 ## 38 Yl[34] 9.74 0.99 8.07 11.33 ## 39 Yl[35] 9.43 0.42 8.73 10.11 ## 40 Yl[36] 11.96 0.78 10.68 13.27 ## 41 Yl[37] 10.08 0.65 9.00 11.16 ## 42 Yl[38] 7.79 0.40 7.13 8.45 ## 43 Yl[39] 8.21 1.03 6.57 9.96 ## 44 Yl[40] 8.40 0.61 7.39 9.38 ## 45 Yl[41] 10.01 1.06 8.28 11.75 ## 46 Yl[42] 10.94 0.63 9.93 11.97 ## 47 Yl[43] 10.02 0.34 9.45 10.59 ## 48 Yl[44] 11.08 0.79 9.75 12.38 ## 49 Yl[45] 8.90 1.02 7.25 10.62 ## 50 Yl[46] 8.99 0.47 8.21 9.77 ## 51 Yl[47] 9.95 0.57 9.04 10.91 ## 52 Yl[48] 10.63 0.88 9.18 12.08 ## 53 Yl[49] 8.47 0.51 7.63 9.31 ## 54 Yl[50] 11.49 1.11 9.67 13.28 ## 55 lp__ -152.80 6.57 -163.98 -142.45 # you can get similar output with b14.1_mi$fit Again, from b_intercept to sigma, the output is about the same. But model b14.1_mi, based on the mi() syntax, contained posterior summaries for all 50 of the criterion values. The se() method gave us similar model result, but no posterior summaries for the 50 criterion values. The rethinking package indexed those additional 50 as div_est[i]; with the mi() method, brms indexed them as Yl[i]–no big deal. So while both brms methods accommodated measurement error, the mi() method appears to be the brms analogue to what McElreath did with his model m14.1 in the text. Thus, it’s our b14.1_mi model that follows the form \\[ \\begin{eqnarray} \\text{Divorce}_{\\text{estimated}, i} &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu &amp; = &amp; \\alpha + \\beta_1 \\text A_i + \\beta_2 \\text R_i \\\\ \\text{Divorce}_{\\text{observed}, i} &amp; \\sim &amp; \\text{Normal} (\\text{Divorce}_{\\text{estimated}, i}, \\text{Divorce}_{\\text{standard error}, i}) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 2.5) \\end{eqnarray} \\] Note. The normal(0, 10) prior McElreath used was quite informative and can lead to discrepancies between the rethinking and brms results if you’re not careful. A large issue is the default way brms handles intercept priors. From the hyperlink, Bürkner wrote: The formula for the original intercept is b_intercept = temp_intercept - dot_product(means_X, b), where means_X is the vector of means of the predictor variables and b is the vector of regression coefficients (fixed effects). That is, when transforming a prior on the intercept to an “equivalent” prior on the temporary intercept, you have to take the means of the predictors and well as the priors on the other coefficients into account. If this seems confusing, you have an alternative. The 0 + intercept part of the brm formula kept the intercept in the metric of the untransformed data, leading to similar results to those from rethinking. When your priors are vague, this might not be much of an issue. And since many of the models in Statistical Rethinking use only weakly-regularizing priors, this hasn’t been much of an issue up to this point. But this model is quite sensitive to the intercept syntax. My general recommendation for applied data analysis is this: If your predictors aren’t mean centered, default to the 0 + intercept syntax for the formula argument when using brms::brm(). Otherwise, your priors might not be doing what you think they’re doing. Anyway, since our mi()-syntax b14.1_mi model appears to be the analogue to McElreath’s m14.1, we’ll use that one for our plots. Here’s our Figure 14.2.a. data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5] data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) Before we make Figure 14.2.b, we need to fit a model that ignores measurement error. b14.1b &lt;- brm(data = dlist, family = gaussian, div_obs ~ 0 + intercept + R + A, prior = c(prior(normal(0, 50), class = b, coef = intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), chains = 2, iter = 5000, warmup = 1000, cores = 2, control = list(adapt_delta = 0.95)) print(b14.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs ~ 0 + intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 35.66 7.71 20.54 50.52 1986 1.00 ## R -0.05 0.08 -0.20 0.11 2372 1.00 ## A -0.96 0.25 -1.43 -0.48 2053 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.51 0.16 1.24 1.86 3489 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With the ignore-measurement-error fit in hand, we’re ready for Figure 14.2.b. nd &lt;- tibble(R = mean(d$Marriage), A = seq(from = 22, to = 30.2, length.out = 30), div_sd = mean(d$Divorce.SE)) # red line fitd_error &lt;- fitted(b14.1_mi, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # yellow line fitd_no_error &lt;- fitted(b14.1b, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # white dots data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(b14.1_mi$data) color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_r &lt;- viridis_pal(option = &quot;C&quot;)(7)[4] # plot fitd_error %&gt;% ggplot(aes(x = A, y = Estimate)) + geom_ribbon(data = fitd_no_error, aes(ymin = Q2.5, ymax = Q97.5), fill = color_y, alpha = 1/4) + geom_line(data = fitd_no_error, color = color_y, linetype = 2) + geom_ribbon(data = fitd_error, aes(ymin = Q2.5, ymax = Q97.5), fill = color_r, alpha = 1/3) + geom_line(data = fitd_error, color = color_r) + geom_pointrange(data = data_error, aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), color = &quot;white&quot;, shape = 20, alpha = 1/2) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(xlim = range(data_error$A), ylim = c(4, 15)) In our plot, it’s the reddish regression line that accounts for measurement error. 14.1.2 Error on both outcome and predictor. In brms, you can specify error on predictors with an me() statement in the form of me(predictor, sd_predictor) where sd_predictor is a vector in the data denoting the size of the measurement error, presumed to be in a standard-deviation metric. # The data dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, mar_obs = d$Marriage, mar_sd = d$Marriage.SE, A = d$MedianAgeMarriage) # The `inits` inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # The models b14.2_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, chains = 3, cores = 3, control = list(adapt_delta = 0.95), save_mevars = TRUE) # Note the lack if `inits`. See below. b14.2_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, inits = inits_list) We already know including inits values for our Yl[i] estimates is a waste of time for our se() model. But note how we still defined our inits values as inits &lt;- list(Yl = dlist$div_obs) for the mi() model. Although it’s easy in brms to set the starting values for our Yl[i] estimates, much the way McElreath did, that isn’t the case when you have measurement error on the predictors. The brms package uses a non-centered parameterization for these, which requires users to have a deeper understanding of the underlying Stan code. This is where I get off the train, but if you want to go further, execute stancode(b14.2_mi). Here’s the two versions of the model. print(b14.2_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 3 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 15.65 6.80 2.44 29.11 5338 1.00 ## A -0.44 0.20 -0.84 -0.05 6120 1.00 ## memar_obsmar_sd 0.27 0.11 0.07 0.49 5237 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.99 0.21 0.61 1.45 12000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.2_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## intercept 15.69 6.86 2.19 28.99 1958 1.00 ## A -0.44 0.21 -0.83 -0.03 2212 1.00 ## memar_obsmar_sd 0.27 0.11 0.07 0.49 1949 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.99 0.21 0.61 1.43 1786 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll use broom::tidy(), again, to get a sense of depth=2 summaries. tidy(b14.2_se) %&gt;% mutate_if(is.numeric, round, digits = 2) tidy(b14.2_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) Due to space concerns, I’m not going to show the results, here. You can do that on your own. Both methods yielded the posteriors for Xme_memar_obs[1], but only the b14.2_mi model based on the mi() syntax yielded posteriors for the criterion, the Yl[i] summaries. Note that you’ll need to specify save_mevars = TRUE in the brm() function order to save the posterior samples of error-adjusted variables obtained by using the me() argument. Without doing so, functions like predict() may give you trouble. Here is the code for Figure 14.3.a. data_error &lt;- fitted(b14.2_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[3] data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) To get the posterior samples for error-adjusted Marriage rate, we’ll use posterior_samples. If you examine the object with glimpse(), you’ll notice 50 Xme_memar_obsmar_sd[i] vectors, with \\(i\\) ranging from 1 to 50, each corresponding to one of the 50 states. With a little data wrangling, you can get the mean of each to put in a plot. Once we have those summaries, we can make our version of Figure 14.4.b. color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_p &lt;- viridis_pal(option = &quot;C&quot;)(7)[2] posterior_samples(b14.2_mi) %&gt;% select(starts_with(&quot;Xme&quot;)) %&gt;% gather() %&gt;% # This extracts the numerals from the otherwise cumbersome names in key and saves them as integers mutate(key = str_extract(key, &quot;\\\\d+&quot;) %&gt;% as.integer()) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) %&gt;% bind_cols(data_error) %&gt;% ggplot(aes(x = mean, y = Estimate)) + geom_segment(aes(xend = Marriage, yend = Divorce), color = &quot;white&quot;, size = 1/4) + geom_point(size = 2, alpha = 2/3, color = color_y) + geom_point(aes(x = Marriage, y = Divorce), size = 2, alpha = 2/3, color = color_p) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Marriage rate (posterior)&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(ylim = c(4, 14.5)) The yellow points are model-implied; the purple ones are of the original data. It turns out our brms model regularized more aggressively than McElreath’s rethinking model. I’m unsure of why. If you understand the difference, please share with the rest of the class. Anyway, the big take home point for this section is that when you have a distribution of values, don’t reduce it down to a single value to use in a regression. Instead, use the entire distribution. Anytime we use an average value, discarding the uncertainty around that average, we risk overconfidence and spurious inference. This doesn’t only apply to measurement error, but also to cases which data are averaged before analysis. Do not average. Instead, model. (p. 431) 14.2 Missing data Starting with the developer’s version 2.1.2, (or the official version 2.3.1 available on CRAN) brms now supports Bayesian missing data imputation using adaptations of the multivariate syntax. Bürkner’s Handle Missing Values with brms vignette is quite helpful. 14.2.1 Imputing neocortex Once again, here are the milk data. library(rethinking) data(milk) d &lt;- milk d &lt;- d %&gt;% mutate(neocortex.prop = neocortex.perc/100, logmass = log(mass)) Now we’ll switch out rethinking for brms and do a little data wrangling. detach(package:rethinking, unload = T) library(brms) rm(milk) # prep data data_list &lt;- list( kcal = d$kcal.per.g, neocortex = d$neocortex.prop, logmass = d$logmass) Here’s the structure of our data list. data_list ## $kcal ## [1] 0.49 0.51 0.46 0.48 0.60 0.47 0.56 0.89 0.91 0.92 0.80 0.46 0.71 0.71 0.73 0.68 0.72 0.97 0.79 ## [20] 0.84 0.48 0.62 0.51 0.54 0.49 0.53 0.48 0.55 0.71 ## ## $neocortex ## [1] 0.5516 NA NA NA NA 0.6454 0.6454 0.6764 NA 0.6885 0.5885 0.6169 0.6032 ## [14] NA NA 0.6997 NA 0.7041 NA 0.7340 NA 0.6753 NA 0.7126 0.7260 NA ## [27] 0.7024 0.7630 0.7549 ## ## $logmass ## [1] 0.6678294 0.7371641 0.9202828 0.4824261 0.7839015 1.6582281 1.6808279 0.9202828 ## [9] -0.3424903 -0.3856625 -2.1202635 -0.7550226 -1.1394343 -0.5108256 1.2441546 0.4382549 ## [17] 1.9572739 1.1755733 2.0719133 2.5095993 2.0268316 1.6808279 2.3721112 3.5689692 ## [25] 4.3748761 4.5821062 3.7072104 3.4998354 4.0064237 Our statistical model follows the form \\[ \\begin{eqnarray} \\text{kcal}_i &amp; \\sim &amp; \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 \\text{neocortex}_i + \\beta_2 \\text{logmass}_i \\\\ \\text{neocortex}_i &amp; \\sim &amp; \\text{Normal} (\\nu, \\sigma_\\text{neocortex}) \\\\ \\alpha &amp; \\sim &amp; \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim &amp; \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\\\ \\nu &amp; \\sim &amp; \\text{Normal} (0.5, 1) \\\\ \\sigma_\\text{neocortex} &amp; \\sim &amp; \\text{HalfCauchy} (0, 1) \\end{eqnarray} \\] When writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things get cluttered in a hurry. b_model &lt;- # Here&#39;s the primary `kcal` model bf(kcal | mi() ~ 1 + mi(neocortex) + logmass) + # Here&#39;s the model for the missing `neocortex` data bf(neocortex | mi() ~ 1) + # Here we set the residual correlations for the two models to zero set_rescor(FALSE) Note the mi(neocortex) syntax in the kcal model. This indicates that the predictor, neocortex, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model, use the get_prior() function. get_prior(data = data_list, family = gaussian, b_model) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 Intercept ## 3 b kcal ## 4 b logmass kcal ## 5 b mineocortex kcal ## 6 student_t(3, 1, 10) Intercept kcal ## 7 student_t(3, 0, 10) sigma kcal ## 8 student_t(3, 1, 10) Intercept neocortex ## 9 student_t(3, 0, 10) sigma neocortex With the one-step Bayesian imputation procedure in brms, you might need to use the resp argument when specifying non-defaut priors. Anyway, here we fit the model. b14.3 &lt;- brm(data = data_list, family = gaussian, b_model, # here we insert the model prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2) The imputed neocortex values are indexed by occasion number from the original data. tidy(b14.3) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.53 0.48 -1.31 0.28 ## 2 b_neocortex_Intercept 0.67 0.01 0.65 0.69 ## 3 b_kcal_logmass -0.07 0.02 -0.11 -0.03 ## 4 bsp_kcal_mineocortex 1.90 0.74 0.63 3.11 ## 5 sigma_kcal 0.13 0.02 0.10 0.18 ## 6 sigma_neocortex 0.06 0.01 0.05 0.08 ## 7 Ymi_neocortex[2] 0.63 0.05 0.55 0.72 ## 8 Ymi_neocortex[3] 0.63 0.05 0.54 0.71 ## 9 Ymi_neocortex[4] 0.62 0.05 0.54 0.71 ## 10 Ymi_neocortex[5] 0.65 0.05 0.57 0.73 ## 11 Ymi_neocortex[9] 0.70 0.05 0.62 0.78 ## 12 Ymi_neocortex[14] 0.66 0.05 0.58 0.74 ## 13 Ymi_neocortex[15] 0.69 0.05 0.61 0.77 ## 14 Ymi_neocortex[17] 0.70 0.05 0.62 0.78 ## 15 Ymi_neocortex[19] 0.71 0.05 0.63 0.79 ## 16 Ymi_neocortex[21] 0.65 0.05 0.57 0.73 ## 17 Ymi_neocortex[23] 0.66 0.05 0.58 0.74 ## 18 Ymi_neocortex[26] 0.70 0.05 0.61 0.77 ## 19 lp__ 40.38 4.33 32.60 46.75 Here’s the model that drops the cases with NAs on neocortex. b14.3cc &lt;- brm(data = data_list, family = gaussian, kcal ~ 1 + neocortex + logmass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 1e4, chains = 2, cores = 2) The parameters: tidy(b14.3cc) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.06 0.58 -2.00 -0.12 ## 2 b_neocortex 2.76 0.90 1.27 4.21 ## 3 b_logmass -0.10 0.03 -0.14 -0.05 ## 4 sigma 0.14 0.03 0.10 0.19 ## 5 lp__ -4.23 1.60 -7.33 -2.35 In order to make our versions of Figure 14.4, we’ll need to do a little data wrangling with fitted(). nd &lt;- tibble(neocortex = seq(from = .5, to = .85, length.out = 30), logmass = median(data_list$logmass)) f_b14.3 &lt;- fitted(b14.3, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.3 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.3304664, 0.3533606, 0.3762547, 0.3991489, 0.4220431, 0.4449372, 0... ## $ Est.Error.kcal &lt;dbl&gt; 0.12720301, 0.11846516, 0.10976767, 0.10112096, 0.09253927, 0.08404... ## $ Q2.5.kcal &lt;dbl&gt; 0.07988178, 0.11954155, 0.16020737, 0.20032220, 0.24093396, 0.28176... ## $ Q97.5.kcal &lt;dbl&gt; 0.5912997, 0.5955131, 0.6007122, 0.6050263, 0.6098755, 0.6159543, 0... ## $ Estimate.neocortex &lt;dbl&gt; 0.6713599, 0.6713599, 0.6713599, 0.6713599, 0.6713599, 0.6713599, 0... ## $ Est.Error.neocortex &lt;dbl&gt; 0.01397784, 0.01397784, 0.01397784, 0.01397784, 0.01397784, 0.01397... ## $ Q2.5.neocortex &lt;dbl&gt; 0.6436987, 0.6436987, 0.6436987, 0.6436987, 0.6436987, 0.6436987, 0... ## $ Q97.5.neocortex &lt;dbl&gt; 0.6989923, 0.6989923, 0.6989923, 0.6989923, 0.6989923, 0.6989923, 0... ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0... ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.24415... To include the imputed neocortex values in the plot, we’ll extract the information from broom::tidy(). f_b14.3_mi &lt;- tidy(b14.3) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols(data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex)) ) # Here&#39;s what we did f_b14.3_mi %&gt;% head() ## term estimate std.error lower upper kcal neocortex logmass ## 1 Ymi_neocortex[2] 0.6327715 0.05080760 0.5522504 0.7188094 0.51 NA 0.7371641 ## 2 Ymi_neocortex[3] 0.6251508 0.05110562 0.5428096 0.7105406 0.46 NA 0.9202828 ## 3 Ymi_neocortex[4] 0.6223428 0.05213301 0.5399048 0.7102340 0.48 NA 0.4824261 ## 4 Ymi_neocortex[5] 0.6527304 0.04902977 0.5740584 0.7338760 0.60 NA 0.7839015 ## 5 Ymi_neocortex[9] 0.7014745 0.04943863 0.6226482 0.7822191 0.91 NA -0.3424903 ## 6 Ymi_neocortex[14] 0.6564597 0.04911862 0.5788738 0.7400665 0.71 NA -0.5108256 Data wrangling done–here’s our code for Figure 14.4.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] f_b14.3 %&gt;% ggplot(aes(x = neocortex, y = Estimate.kcal)) + geom_ribbon(aes(ymin = Q2.5.kcal, ymax = Q97.5.kcal), fill = color, alpha = 1/3) + geom_line(color = color) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.3_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.3_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) + labs(subtitle = &quot;Note: For the regression line in this plot, log(mass)\\nhas been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) Figure 14.4.b. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.3_mi, aes(x = logmass, y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(breaks = -2:4) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) + labs(x = &quot;log(mass)&quot;, y = &quot;neocortex proportion&quot;) 14.2.2 Improving the imputation model Like McElreath, we’ll update the imputation line of our statistical model to: \\[ \\begin{eqnarray} \\text{neocortex}_i &amp; \\sim &amp; \\text{Normal} (\\nu_i, \\sigma_\\text{neocortex}) \\\\ \\nu_i &amp; = &amp; \\alpha_\\text{neocortex} + \\gamma_1 \\text{logmass}_i \\\\ \\end{eqnarray} \\] which includes the updated priors \\[ \\begin{eqnarray} \\alpha_\\text{neocortex} &amp; \\sim &amp; \\text{Normal} (0.5, 1) \\\\ \\gamma_1 &amp; \\sim &amp; \\text{Normal} (0, 10) \\end{eqnarray} \\] As far as the brms code goes, adding logmass as a predictor to the neocortex submodel is pretty simple. # The model b_model &lt;- bf(kcal | mi() ~ 1 + mi(neocortex) + logmass) + bf(neocortex | mi() ~ 1 + logmass) + # Here&#39;s the big difference set_rescor(FALSE) # Fit the model b14.4 &lt;- brm(data = data_list, family = gaussian, b_model, prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2, # There were a couple divergent transitions with the default `adapt_delta = 0.8` control = list(adapt_delta = 0.9)) The parameter estimates: tidy(b14.4) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.86 0.49 -1.63 -0.05 ## 2 b_neocortex_Intercept 0.64 0.01 0.62 0.66 ## 3 b_kcal_logmass -0.09 0.02 -0.13 -0.05 ## 4 b_neocortex_logmass 0.02 0.01 0.01 0.03 ## 5 bsp_kcal_mineocortex 2.43 0.76 1.15 3.63 ## 6 sigma_kcal 0.13 0.02 0.10 0.17 ## 7 sigma_neocortex 0.04 0.01 0.03 0.06 ## 8 Ymi_neocortex[2] 0.63 0.03 0.57 0.69 ## 9 Ymi_neocortex[3] 0.63 0.04 0.57 0.69 ## 10 Ymi_neocortex[4] 0.62 0.04 0.56 0.68 ## 11 Ymi_neocortex[5] 0.65 0.03 0.59 0.70 ## 12 Ymi_neocortex[9] 0.66 0.04 0.60 0.72 ## 13 Ymi_neocortex[14] 0.63 0.04 0.57 0.68 ## 14 Ymi_neocortex[15] 0.68 0.03 0.62 0.74 ## 15 Ymi_neocortex[17] 0.70 0.03 0.64 0.75 ## 16 Ymi_neocortex[19] 0.71 0.03 0.66 0.77 ## 17 Ymi_neocortex[21] 0.66 0.04 0.60 0.72 ## 18 Ymi_neocortex[23] 0.68 0.03 0.62 0.74 ## 19 Ymi_neocortex[26] 0.74 0.04 0.68 0.80 ## 20 lp__ 48.77 4.15 41.17 54.65 Here’s our pre-Figure 14.5 data wrangling. f_b14.4 &lt;- fitted(b14.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.4_mi &lt;- tidy(b14.4) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols(data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex)) ) f_b14.4 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.2416885, 0.2710504, 0.3004123, 0.3297742, 0.3591361, 0.3884980, 0... ## $ Est.Error.kcal &lt;dbl&gt; 0.12999064, 0.12103147, 0.11210661, 0.10322496, 0.09439872, 0.08564... ## $ Q2.5.kcal &lt;dbl&gt; -0.005757169, 0.040048457, 0.085978561, 0.132036308, 0.178190782, 0... ## $ Q97.5.kcal &lt;dbl&gt; 0.5059238, 0.5173685, 0.5281662, 0.5390743, 0.5502195, 0.5618652, 0... ## $ Estimate.neocortex &lt;dbl&gt; 0.6671348, 0.6671348, 0.6671348, 0.6671348, 0.6671348, 0.6671348, 0... ## $ Est.Error.neocortex &lt;dbl&gt; 0.009529979, 0.009529979, 0.009529979, 0.009529979, 0.009529979, 0.... ## $ Q2.5.neocortex &lt;dbl&gt; 0.6480756, 0.6480756, 0.6480756, 0.6480756, 0.6480756, 0.6480756, 0... ## $ Q97.5.neocortex &lt;dbl&gt; 0.6859895, 0.6859895, 0.6859895, 0.6859895, 0.6859895, 0.6859895, 0... ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0... ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.24415... f_b14.4_mi %&gt;% glimpse() ## Observations: 12 ## Variables: 8 ## $ term &lt;chr&gt; &quot;Ymi_neocortex[2]&quot;, &quot;Ymi_neocortex[3]&quot;, &quot;Ymi_neocortex[4]&quot;, &quot;Ymi_neocortex[5]... ## $ estimate &lt;dbl&gt; 0.6315583, 0.6293565, 0.6198302, 0.6465987, 0.6631728, 0.6274336, 0.6799432, ... ## $ std.error &lt;dbl&gt; 0.03497748, 0.03521781, 0.03502643, 0.03362197, 0.03572891, 0.03537715, 0.034... ## $ lower &lt;dbl&gt; 0.5743664, 0.5715597, 0.5621254, 0.5921901, 0.6042473, 0.5698326, 0.6231185, ... ## $ upper &lt;dbl&gt; 0.6886370, 0.6876339, 0.6776145, 0.7012781, 0.7216406, 0.6846492, 0.7358647, ... ## $ kcal &lt;dbl&gt; 0.51, 0.46, 0.48, 0.60, 0.91, 0.71, 0.73, 0.72, 0.79, 0.48, 0.51, 0.53 ## $ neocortex &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ## $ logmass &lt;dbl&gt; 0.7371641, 0.9202828, 0.4824261, 0.7839015, -0.3424903, -0.5108256, 1.2441546... For our final plots, let’s play around with colors from viridis_pal(option = &quot;D&quot;). Figure 14.5.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] f_b14.4 %&gt;% ggplot(aes(x = neocortex, y = Estimate.kcal)) + geom_ribbon(aes(ymin = Q2.5.kcal, ymax = Q97.5.kcal), fill = color, alpha = 1/2) + geom_line(color = color) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.4_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.4_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) + labs(subtitle = &quot;Note: For the regression line in this plot, log(mass)\\nhas been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) Figure 14.5.b. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.4_mi, aes(x = logmass, y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(breaks = -2:4) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) + labs(x = &quot;log(mass)&quot;, y = &quot;neocortex proportion&quot;) If modern missing data methods are new to you, you might also check out van Burren’s great online text Flexible Imputation of Missing Data. Second Edition. I’m also a fan of Enders’ Applied Missing Data Analysis, for which you can find a free sample chapter here. I’ll also quickly mention that brms accommodates multiple imputation, too. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.4.5 viridis_0.5.1 viridisLite_0.3.0 bayesplot_1.6.0 brms_2.5.0 ## [6] Rcpp_0.12.18 rstan_2.17.3 StanHeaders_2.17.2 forcats_0.3.0 stringr_1.3.1 ## [11] dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 tibble_1.4.2 ## [16] ggplot2_3.0.0 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] labeling_0.3 mnormt_1.5-5 bridgesampling_0.4-0 ## [22] rprojroot_1.3-2 coda_0.19-1 xfun_0.3 ## [25] R6_2.2.2 markdown_0.8 HDInterval_0.2.0 ## [28] reshape_0.8.7 assertthat_0.2.0 promises_1.0.1 ## [31] scales_0.5.0 beeswarm_0.2.3 gtable_0.2.0 ## [34] rlang_0.2.1 extrafontdb_1.0 lazyeval_0.2.1 ## [37] inline_0.3.15 yaml_2.1.19 reshape2_1.4.3 ## [40] abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 ## [46] rsconnect_0.8.8 extrafont_0.17 tools_3.5.1 ## [49] bookdown_0.7 psych_1.8.4 RColorBrewer_1.1-2 ## [52] ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [55] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 ## [58] LaplacesDemon_16.1.1 haven_1.1.2 magrittr_1.5 ## [61] colourpicker_1.0 mvtnorm_1.0-8 matrixStats_0.54.0 ## [64] hms_0.4.2 shinyjs_1.0 mime_0.5 ## [67] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [70] shinystan_2.5.0 readxl_1.1.0 gridExtra_2.3 ## [73] rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [76] crayon_1.3.4 htmltools_0.3.6 later_0.7.3 ## [79] lubridate_1.7.4 MASS_7.3-50 Matrix_1.2-14 ## [82] cli_1.0.0 bindr_0.1.1 igraph_1.2.1 ## [85] pkgconfig_2.0.1 foreign_0.8-70 xml2_1.2.0 ## [88] svUnit_0.7-12 dygraphs_1.1.1.5 vipor_0.4.5 ## [91] rvest_0.3.2 digest_0.6.15 rmarkdown_1.10 ## [94] cellranger_1.1.0 shiny_1.1.0 gtools_3.8.1 ## [97] nlme_3.1-137 jsonlite_1.5 bindrcpp_0.2.2 ## [100] mapproj_1.2.6 pillar_1.2.3 lattice_0.20-35 ## [103] loo_2.0.0 httr_1.3.1 glue_1.2.0 ## [106] xts_0.10-2 shinythemes_1.1.1 pander_0.6.2 ## [109] stringi_1.2.3 "],
["horoscopes-insights.html", "15 Horoscopes Insights 15.1 Use R Notebooks 15.2 Save your model fits 15.3 Build your models slowly 15.4 Look at your data 15.5 Use the 0 + intercept syntax 15.6 Annotate your workflow 15.7 Annotate your code 15.8 Break up your workflow 15.9 Read Gelman’s blog 15.10 Check out other social media, too 15.11 Parting wisdom Reference Session info", " 15 Horoscopes Insights Statistical inference is indeed critically important. But only as much as every other part of research. Scientific discovery is not an additive process, in which sin in one part can be atoned by virtue in another. Everything interacts. So equally when science works as intended as when it does not, every part of the process deserves attention. (p. 441) In this final chapter, there are no models for us to fit and no figures for use to reimagine. McElreath took the opportunity to comment more broadly on the scientific process. He made a handful of great points, some of which I’ll quote in a bit. But for the bulk of this chapter, I’d like to take the opportunity to pass on a few of my own insights about workflow. I hope they’re of use. 15.1 Use R Notebooks OMG I first started using R in the winter of 2015/2016. Right from the start, I learned how to code from within the R Studio environment. But within R Studio I was using simple scripts. No longer. I now use R Notebooks for just about everything. Nathan Stephens wrote a nice blog on Why I love R Notebooks. I agree. This has fundamentally changed my workflow as a scientist. I only wish I’d learned about this before starting my dissertation project. So it goes… Do yourself a favor, adopt R Notebooks into your workflow. Do it today. If you prefer to learn with videos, here’s a nice intro by Kristine Yu and another one by JJ Allaire. Try it out for like one afternoon and you’ll be hooked. 15.2 Save your model fits It’s embarrassing how long it took for this to dawn on me. Unlike classical statistics, Bayesian models using MCMC take a while to compute. Most of the simple models in McElreath’s text take 30 seconds up to a couple minutes. If your data are small, well-behaved and of a simple structure, you might have a lot of wait times in that range in your future. It hasn’t been that way, for me. Most of my data have a complicated multilevel structure and often aren’t very well behaved. It’s normal for my models to take an hour or several to fit. Once you start measuring your model fit times in hours, you do not want to fit these things more than once. So, it’s not enough to document my code in a nice R Notebook file. I need to save my brm() fit objects in external files. Consider this model. It’s taken from Bürkner’s vignette, Estimating Multivariate Models with brms. It took about five minutes for my several-year-old laptop to fit. library(brms) data(&quot;BTdata&quot;, package = &quot;MCMCglmm&quot;) fit1 &lt;- brm(data = BTdata, family = gaussian, cbind(tarsus, back) ~ sex + hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2) Five minutes isn’t terribly long to wait, but still. I’d prefer to never have to wait for another five minutes, again. Sure, if I save my code in a document like this, I will always be able to fit the model again. But I can work smarter. Here I’ll save my fit1 object outside of R with the save() function. save(fit1, file = &quot;fit1.rda&quot;) Hopefully y’all are savvy Bayesian R users and find this insultingly remedial. But if it’s new to you like it was me, you can learn more about rda files here. Now fit1 is saved outside of R, I can safely remove it and then reload it. rm(fit1) load(&quot;fit1.rda&quot;) The file took a fraction of a second to reload. Once reloaded, I can perform typical operations, like examine summaries of the model parameters or refreshing my memory on what data I used. print(fit1) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Group-Level Effects: ## ~dam (Number of levels: 106) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(tarsus_Intercept) 0.48 0.05 0.39 0.58 700 1.01 ## sd(back_Intercept) 0.25 0.08 0.10 0.40 274 1.00 ## cor(tarsus_Intercept,back_Intercept) -0.51 0.22 -0.93 -0.06 588 1.00 ## ## ~fosternest (Number of levels: 104) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(tarsus_Intercept) 0.27 0.06 0.16 0.39 529 1.00 ## sd(back_Intercept) 0.35 0.06 0.23 0.47 519 1.00 ## cor(tarsus_Intercept,back_Intercept) 0.68 0.21 0.20 0.98 263 1.01 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## tarsus_Intercept -0.41 0.07 -0.55 -0.27 1091 1.00 ## back_Intercept -0.01 0.06 -0.14 0.11 2000 1.00 ## tarsus_sexMale 0.77 0.06 0.66 0.88 2000 1.00 ## tarsus_sexUNK 0.23 0.13 -0.02 0.48 2000 1.00 ## tarsus_hatchdate -0.04 0.06 -0.15 0.08 992 1.00 ## back_sexMale 0.01 0.07 -0.12 0.13 2000 1.00 ## back_sexUNK 0.14 0.14 -0.13 0.42 2000 1.00 ## back_hatchdate -0.09 0.05 -0.19 0.02 1766 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma_tarsus 0.76 0.02 0.72 0.80 2000 1.00 ## sigma_back 0.90 0.02 0.85 0.95 2000 1.00 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## rescor(tarsus,back) -0.05 0.04 -0.13 0.02 2000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). library(tidyverse) fit1$data %&gt;% head() ## tarsus sex hatchdate dam fosternest back ## 1 -1.89229718 Fem -0.6874021 R187557 F2102 1.1464212 ## 2 1.13610981 Male -0.6874021 R187559 F1902 -0.7596521 ## 3 0.98468946 Male -0.4279814 R187568 A602 0.1449373 ## 4 0.37900806 Male -1.4656641 R187518 A1302 0.2555847 ## 5 -0.07525299 Fem -1.4656641 R187528 A2602 -0.3006992 ## 6 -1.13519543 Fem 0.3502805 R187945 C2302 1.5577219 I believe Bürkner recently added an argument in brms:brm() that will help you do this, too. I haven’t played with it, yet. But if you’re curious, you can find out more here. 15.3 Build your models slowly The model from Bürkner’s vignette, fit1, was no joke. If you wanted to be verbose about it, it was a multilevel, multivariate, multivariable model. It had a cross-classified multilevel structure, two predictors (for each criterion), and two criteria. Not only is that a lot to keep track of, there’s a whole lot of places for things to go wrong. Even if that was the final model I was interested in as a scientist, I still wouldn’t start with it. I’d build up incrementally, just to make sure nothing looked fishy. One place to start would be a simple intercepts-only model. fit0 &lt;- brm(cbind(tarsus, back) ~ 1, data = BTdata, chains = 2, cores = 2) plot(fit0) print(fit0) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ 1 ## back ~ 1 ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## tarsus_Intercept -0.00 0.04 -0.07 0.07 2000 1.00 ## back_Intercept 0.00 0.03 -0.07 0.07 2000 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma_tarsus 1.00 0.02 0.95 1.05 2000 1.00 ## sigma_back 1.00 0.03 0.96 1.05 2000 1.00 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## rescor(tarsus,back) -0.03 0.04 -0.10 0.04 2000 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If the chains look good and the summary statistics look like what I’d expect, I’m on good footing to keep building up to the model I really care about. The results from this model, for example, suggest that both criteria were standardized (i.e., intercepts at 0 and \\(\\sigma\\)s at 1). If that wasn’t what I intended, I’d rather catch it here than spend five minutes fitting the more complicated fit1 model, the parameters for which are sufficiently complicated that I may have had trouble telling what scale the data were on. Note, this is not the same as \\(p\\)-hacking or wandering aimlessly down the garden of forking paths. We are not chasing the flashiest model to put in a paper. Rather, this is just good pragmatic data science. If you start off with a theoretically-justified but complicated model and run into computation problems or produce odd-looking estimates, it won’t be clear where things went awry. When you build up, step by step, it’s easier to catch mistakes like data cleaning failures, coding goofs, and the like. So, when I’m working on a project, I fit one or a few simplified models before fitting my complicated model of theoretical interest. This is especially the case when I’m working with model types that are new to me or that I haven’t worked with in a while. I document each step in my R Notebook files and I save the fit objects for each in external files. I have caught surprises, this way. Hopefully this will help you catch your mistakes, too. 15.4 Look at your data Relatedly, and perhaps even a precursor, you should always plot your data before fitting a model. There were plenty examples of this in the text, but it’s worth of making explicit. Simple summary statistics are great, but they’re not enough. For an entetrtaining exposition, check out Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. Though it might make for a great cocktail party story, I’d hate to pollute the literature with a linear model based on a set of dinosaur-shaped data. 15.5 Use the 0 + intercept syntax We covered this a little in the last couple chapters, but it’s easy to miss. If your real-world model has predictors (i.e., isn’t an intercept-only model), it’s important to keep track of how you have centered your predictors. When you specify a prior for a brms Intercept (i.e., an intercept resulting from the y ~ x or y ~ 1 + x style of syntax), that prior is applied under the presumption all the predictors are mean centered. In the Population-level (‘fixed’) effects subsection of the set_prior section of the brms reference manual (version 2.4.0), we read: Note that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means. To treat the intercept as an ordinary population-level effect and avoid the centering parameterization, use 0 + intercept on the right-hand side of the model formula. (p. 142) We get a little more information from the Parameterization of the population-level intercept subsection of the brmsformula section: This behavior can be avoided by using the reserved (and internally generated) variable intercept. Instead of y ~ x, you may write y ~ 0 + intercept + x. This way, priors can be defined on the real intercept, directly. In addition, the intercept is just treated as an ordinary population-level effect and thus priors defined on b will also apply to it. Note that this parameterization may be less efficient than the default parameterization discussed above. (p. 30) We didn’t bother with this for most of the project because our priors on the Intercept were often vague and the predictors were often on small enough scales (e.g., the mean of a dummy variable is close to 0) that it just didn’t matter. But this will not always be the case. Set your Intercept priors with care. There’s also the flip side of the issue. If there’s no strong reason not to, consider mean-centering or even standardizing your predictors. Not only will that solve the Intercept prior issue, but it often results in more meaningful parameter estimates. 15.6 Annotate your workflow In a typical model-fitting file, I’ll load my data, perhaps transform the data a bit, fit several models, and examine the output of each with trace plots, model summaries, information criteria, and the like. In my early days, I just figured each of these steps were self-explanatory. Nope. “In every project you have at least one other collaborator; future-you. You don’t want future-you to curse past-you.” My experience was that even a couple weeks between taking a break from a project and restarting it was enough time to make my earlier files confusing. And they were my files. I now start each R Notebook document with an introductory paragraph or two explaining exactly what the purpose of the file is. I separate my major sections by headers and subheaders. My working R Notebook files are peppered with bullets, sentences, and full on paragraphs between code blocks. 15.7 Annotate your code This idea is implicit in McElreath’s text. But it’s easy to miss the message. I know I did, at first. I find this is especially important for data wrangling. I’m a tidyverse guy and, for me, the big-money verbs like mutate(), gather(), select(), filter(), group_by(), and summarise() take care of the bulk of my data wrangling. But every once and a while I need to do something less common, like with str_extract() or case_when(). And when I end up using a new or less familiar function, I typically annotate right in the code and even sometimes leave a hyperlink to some R-bloggers post or stackoverflow question that explained how to use it. 15.8 Break up your workflow I’ve also learned to break up my projects into multiple R Notebook files. If you have a small project for which you just want a quick and dirty plot, fine, do it all in one file. My typical project has: A primary data cleaning file A file with basic descriptive statistics and the like At least one primary analysis file Possible secondary and tertiary analysis files A file or two for my major figures A file explaining and depicting my priors, often accompanied by my posteriors, for comparison Putting all that information in one R Notebook file would be overwhelming. Your workflow might well look different, but hopefully you get the idea. You don’t want working files with thousands of lines of code. And mainly to keep Jenny Bryan from setting my computer on fire, I’m also getting into the habit of organizing all these interconnected files with help from R Studio Projects, which you can learn even more about from this chapter in R4DS. 15.9 Read Gelman’s blog Yes, that Gelman. Actually, I started reading Gelman’s blog around the same time I dove into McElreath’s text. But if this isn’t the case for you, it’s time to correct that evil. My graduate mentor often recalled how transformative his first academic conference was. He was an undergrad at the time and it was his first experience meeting and talking with the people whose names he’d seen in his text books. He learned that science was an ongoing conversation among living scientists and–at that time–the best place to take part in that conversation was at conferences. Times keep changing. Nowadays, the living conversation of science occurs online on social media and in blogs. One of the hottest places to find scientists conversing about Bayesian statistics and related methods is Gelman’s blog. The posts are great. But a lot of the action is in the comments sections, too. 15.10 Check out other social media, too If you’re not on it, consider joining academic twitter. The word on the street is correct. Twitter can be rage-fueled dumpster fire. But if you’re selective about who you follow, it’s a great place to lean from and connect with your academic heroes. If you’re a fan of this project, here’s a list of some of the people you might want to follow: Richard McElreath Paul Bürkner Aki Vehtari Dan Simpson Michael Bentacourt Hadley Wickham Yihui Xie Jenny Bryan Roger Peng Mara Averick Matthew Kay Matti Vuorre I’m on twitter, too. If you’re on facebook and in the social sciences, you might check out the Bayesian Inference in Psychology group. It hasn’t been terribly active, as of late. But there are a lot of great folks to connect with, there. I’ve already mentioned Gelman’s blog. McElreath has one, too. He posts infrequently, but it’s usually pretty good when he does. Also, do check out the Stan Forums. They have a special brms tag there, under which you can find all kinds of hot brms talk. But if you’re new to the world of asking for help with your code online, you might acquaint yourself with the notion of a minimally reproducible example. In short, a good minimally reproducible example helps others help you. If you fail to do this, prepare to receive some skark. 15.11 Parting wisdom Okay, that’s enough from me. Let’s start wrapping this project up with some McElreath. There is an aspect of science that you do personally control: openness. Pre-plan your research together with the statistical analysis. Doing so will improve both the research design and the statistics. Document it in the form of a mock analysis that you would not be ashamed to share with a colleague. Register it publicly, perhaps in a simple repository, like Github or any other. But your webpage will do just fine, as well. Then collect the data. Then analyze the data as planned. If you must change the plan, that’s fine. But document the changes and justify them. Provide all of the data and scripts necessary to repeat your analysis. Do not provide scripts and data “on request,” but rather put them online so reviewers of your paper can access them without your interaction. There are of course cases in which full data cannot be released, due to privacy concerns. But the bulk of science is not of that sort. The data and its analysis are the scientific product. The paper is just an advertisement. If you do your honest best to design, conduct, and document your research, so that others can build directly upon it, you can make a difference. (p. 443) Toward that end, also check out the OSF and their YouTube channel, here. Katie Corker gets the last words: “Open science is stronger because we’re doing this together.” Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.3.0 stringr_1.3.1 dplyr_0.7.6 purrr_0.2.5 readr_1.1.1 tidyr_0.8.1 ## [7] tibble_1.4.2 tidyverse_1.2.1 brms_2.5.0 ggplot2_3.0.0 Rcpp_0.12.18 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.4.6 utf8_1.1.4 ggstance_0.3 ## [4] tidyselect_0.2.4 htmlwidgets_1.2 munsell_0.5.0 ## [7] codetools_0.2-15 nleqslv_3.3.2 DT_0.4 ## [10] miniUI_0.1.1.1 withr_2.1.2 Brobdingnag_1.2-5 ## [13] colorspace_1.3-2 highr_0.7 knitr_1.20 ## [16] rstudioapi_0.7 stats4_3.5.1 Rttf2pt1_1.3.7 ## [19] bayesplot_1.6.0 labeling_0.3 rstan_2.17.3 ## [22] mnormt_1.5-5 bridgesampling_0.4-0 rprojroot_1.3-2 ## [25] coda_0.19-1 xfun_0.3 R6_2.2.2 ## [28] markdown_0.8 HDInterval_0.2.0 reshape_0.8.7 ## [31] assertthat_0.2.0 promises_1.0.1 scales_0.5.0 ## [34] beeswarm_0.2.3 gtable_0.2.0 rlang_0.2.1 ## [37] extrafontdb_1.0 lazyeval_0.2.1 broom_0.4.5 ## [40] inline_0.3.15 yaml_2.1.19 reshape2_1.4.3 ## [43] abind_1.4-5 modelr_0.1.2 threejs_0.3.1 ## [46] crosstalk_1.0.0 backports_1.1.2 httpuv_1.4.4.2 ## [49] rsconnect_0.8.8 extrafont_0.17 tools_3.5.1 ## [52] bookdown_0.7 psych_1.8.4 RColorBrewer_1.1-2 ## [55] ggridges_0.5.0 plyr_1.8.4 base64enc_0.1-3 ## [58] progress_1.2.0 prettyunits_1.0.2 zoo_1.8-2 ## [61] LaplacesDemon_16.1.1 haven_1.1.2 magrittr_1.5 ## [64] colourpicker_1.0 mvtnorm_1.0-8 matrixStats_0.54.0 ## [67] hms_0.4.2 shinyjs_1.0 mime_0.5 ## [70] evaluate_0.10.1 arrayhelpers_1.0-20160527 xtable_1.8-2 ## [73] shinystan_2.5.0 readxl_1.1.0 gridExtra_2.3 ## [76] rstantools_1.5.0 compiler_3.5.1 maps_3.3.0 ## [79] crayon_1.3.4 StanHeaders_2.17.2 htmltools_0.3.6 ## [82] later_0.7.3 lubridate_1.7.4 MASS_7.3-50 ## [85] Matrix_1.2-14 cli_1.0.0 bindr_0.1.1 ## [88] igraph_1.2.1 pkgconfig_2.0.1 foreign_0.8-70 ## [91] xml2_1.2.0 svUnit_0.7-12 dygraphs_1.1.1.5 ## [94] vipor_0.4.5 rvest_0.3.2 digest_0.6.15 ## [97] rmarkdown_1.10 cellranger_1.1.0 shiny_1.1.0 ## [100] gtools_3.8.1 nlme_3.1-137 jsonlite_1.5 ## [103] bindrcpp_0.2.2 mapproj_1.2.6 pillar_1.2.3 ## [106] lattice_0.20-35 loo_2.0.0 httr_1.3.1 ## [109] glue_1.2.0 xts_0.10-2 shinythemes_1.1.1 ## [112] pander_0.6.2 stringi_1.2.3 "]
]
