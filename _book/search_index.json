[
["index.html", "Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.1.0 This is a love letter Why this? My assumptions about you How to use and understand this project You can do this, too We have updates Thank-you’s are in order", " Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.1.0 A Solomon Kurz 2020-03-01 This is a love letter I love McElreath’s Statistical Rethinking text. It’s the entry-level textbook for applied researchers I spent years looking for. McElreath’s freely-available lectures on the book are really great, too. However, I prefer using Bürkner’s brms package when doing Bayesian regression in R. It’s just spectacular. I also prefer plotting with Wickham’s ggplot2, and coding with functions and principles from the tidyverse, which you might learn about here or here. So, this project is an attempt to reexpress the code in McElreath’s textbook. His models are re-fit with brms, the figures are reproduced or reimagined with ggplot2, and the general data wrangling code now predominantly follows the tidyverse style. Why this? I’m not a statistician and I have no formal background in computer science. Though I benefited from a suite of statistics courses in grad school, a large portion of my training has been outside of the classroom, working with messy real-world data, and searching online for help. One of the great resources I happened on was idre, the UCLA Institute for Digital Education, which offers an online portfolio of richly annotated textbook examples. Their online tutorials are among the earliest inspirations for this project. We need more resources like them. With that in mind, one of the strengths of McElreath’s text is its thorough integration with the rethinking package. The rethinking package is a part of the R ecosystem, which is great because R is free and open source. And McElreath has made the source code for rethinking publicly available, too. Since he completed his text, many other packages have been developed to help users of the R ecosystem interface with Stan. Of those alternative packages, I think Bürkner’s brms is the best for general-purpose Bayesian data analysis. Its flexible, uses reasonably-approachable syntax, has sensible defaults, and offers a vast array of post-processing convenience functions. And brms has only gotten better over time. To my knowledge, there are no textbooks on the market that highlight the brms package, which seems like an evil worth correcting. In addition, McElreath’s data wrangling code is based in the base R style and he made most of his figures with base R plots. Though there are benefits to sticking close to base R functions (e.g., less dependencies leading to a lower likelihood that your code will break in the future), there are downsides. For beginners, base R functions can be difficult both to learn and to read. Happily, in recent years Hadley Wickham and others have been developing a group of packages collectively called the tidyverse. These tidyverse packages (e.g., dplyr, tidyr, purrr) were developed according to an underlying philosophy and they are designed to work together coherently and seamlessly. Though not all within the R community share this opinion, I am among those who think the tydyverse style of coding is generally easier to learn and sufficiently powerful that these packages can accommodate the bulk of your data needs. I also find tydyverse-style syntax easier to read. And of course, the widely-used ggplot2 package is part of the tidyverse, too. To be clear, students can get a great education in both Bayesian statistics and programming in R with McElreath’s text just the way it is. Just go slow, work through all the examples, and read the text closely. It’s a pedagogical boon. I could not have done better or even closely so. But what I can offer is a parallel introduction on how to fit the statistical models with the ever-improving and already-quite-impressive brms package. I can throw in examples of how to perform other operations according to the ethic of the tidyverse. And I can also offer glimpses of some of the other great packages in the R + Stan ecosystem, such as loo, bayesplot, and tidybayes. My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic or a researcher of some sort, which suggests you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out the free text books by Legler and Roback or Navarro before diving into Statistical Rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s R Programming for Data Science. For an introduction to the tidyvese-style of data analysis, the best source I’ve found is Grolemund and Wickham’s R for Data Science, which I extensively link to throughout this project. That said, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to McElreath’s Statistical Rethinking text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, some of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS, Healy’s Data Visualization: A practical introduction, Wilke’s Fundamentals of Data Visualization or Wickham’s ggplot2: Elegant Graphics for Data Analysis. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide, and R Markdown: The Definitive Guide. R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::mode_hdi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. You can do this, too This project is powered by Yihui Xie’s bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s Happy Git and GitHub for the useR. I’ve even blogged about what it was like putting together the first version of this project. The source code of the project is available here. We have updates For a brief rundown of the version history, we have: Version 0.9.0. I released the initial 0.9.0 version of this project in September 26, 2018. It was a full first draft and set the stage for all others. Version 1.0.0. In April 19, 2019 came the 1.0.0 version. Some of the major changes were: All models were refit with brms, 2.8.0. Adopting the seed argument within the brm() function made the model results more reproducible. The loo package was updated. As a consequence, our workflow for the WAIC and LOO changed, too. I improved the brms alternative to McElreath’s coeftab() function. I made better use of the tidyverse, especially some of the purrr functions. Particularly in the later chapters, there’s a greater emphasis on functions from the tidybayes package. Chapter 11.3.1 contains the updated brms 2.8.0 workflow for making custom distributions, using the beta-binomial model as the example. Chapter 12 received a new bonus section contrasting different methods for working with multilevel posteriors. Chapter 14 received a new bonus section introducing Bayesian meta-analysis and linking it to multilevel and measurement-error models. With the help of others within the community, I corrected many typos and streamlined some of the code (e.g., dropped an unnecessary use of the mi() function in section 14.2.1) And in some cases, I corrected sections that were just plain wrong (e.g., some of my initial attempts in section 3.3 were incorrect). Version 1.0.1. In May 5, 2019 came the 1.0.1 version, which finally added a PDF version of the book. Other noteworthy changes included: Major revisions to the LaTeX syntax underlying many of the in-text equations (e.g., dropping the “eqnarray” environment for &quot;align*&quot;) Adjusting some of the image syntax Updating the reference for the Bayesian \\(R^2\\) (Gelman, Goodrich, Gabry, &amp; Vehtari, 2018) Version 1.1.0. Welcome to version 1.1.0! Noteworthy changes include: substantial expansions to sections 10.3.1 (multinomial regression) and 11.3.2 (negative binomial regression), the addition of a new section in Chapter 15 (15.9) encouraging others to code in public, refitting all models with the current official version of brms, version 2.12.0, discussions (8.3.2) on the new Bulk_ESS and Tail_ESS summaries of HMC effective sample size (Vehtari et al., 2019), saving all fits as external files in the new GitHub fits folder, primarily with the file argument, extensive use of the patchwork package for combining ggplots, improving/updating some of the tidyverse code (e.g., using tidyr::crossing()), updates to the brms::custom_family()-related code in 11.3.1 to match brms 2.11.0 updates, replacing the depreciated brms::marginal_effects() with brms::conditional_effects() (see issue #735), replacing the depreciated brms::stanplot() with brms::mcmc_plot(), increased the plot resolution with fig.retina = 2.5, refreshed hyperlinks, and various typo corrections. We have room for improvement. There are still two models that need work. The current solution for model 10.16 is out of step with the text, which I try to make clear in the prose. It also appears that the Gaussian process model from section 13.4 is off. Both models are beyond my current skill set and friendly suggestions are welcome. In addition to modeling concerns, typos may yet be looming and I’m sure there are places where the code could be made more streamlined, more elegant, or just more in-line with the tidyverse style. Which is all to say, I hope to release better and more useful updates in the future. Thank-you’s are in order Before we move on, I’d like to thank the following for their helpful contributions: Paul-Christian Bürkner (@paul-buerkner), Joseph V. Casillas (@jvcasillas), Andrew Collier (@datawookie), Marco Colombo (@mcol), Jeff Hammerbacher (@hammer), Matthew Kay (@mjskay), Katrin Leinweber (@katrinleinweber), TJ Mahr (@tjmahr), Federico Marini (@federicomarini), Stijn Masschelein (@stijnmasschelein), JungSu Oh (@js-oh), Colin Quirk (@colinquirk), Rishi Sadhir (@RishiSadhir), Jon Spring (@jonspring), Richard Torkar (@torkar), Aki Vehtari (@avehtari), and Matti Vuorre (@mvuorre). "],
["the-golem-of-prague.html", "1 The Golem of Prague 1 Statistical golems Reference Session info", " 1 The Golem of Prague As he opened the chapter, McElreath told us that ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. 1 Statistical golems Scientists also make golems. Our golems rarely have physical form, but they too are often made of clay, living in silicon as computer code. These golems are scientific model. But these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with truth enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations. (p. 1, emphasis in the original) There are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pour over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro. Anyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by watching McElreath present it? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.2 magrittr_1.5 tools_3.6.2 htmltools_0.4.0 ## [5] yaml_2.2.1 Rcpp_1.0.3 stringi_1.4.5 rmarkdown_2.0 ## [9] highr_0.8 knitr_1.26 stringr_1.4.0 xfun_0.12 ## [13] digest_0.6.23 rlang_0.4.4 evaluate_0.14 "],
["small-worlds-and-large-worlds.html", "2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Reference Session info", " 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken wrote a great paper of a similar name and topic. The titles from this section and Gelman and Loken’s paper have their origins in the short story by Borges (1941), The Garden of Forking Paths. You can find copies of the original short story here or here. Here’s a snip: In all fictional works, each time a man is confronted with several alternatives, he chooses one and eliminates the others; in the fiction of Ts’ui Pên, he chooses–simultaneously–all of them. He creates, in this way, diverse futures, diverse times which themselves also proliferate and fork. The choices we make in our data analyses proliferate and fork in this way, too. 2.1.1 Counting possibilities. Throughout this project, we’ll use the tidyverse for data wrangling. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. Other than the pipe, the other big thing to be aware of is tibbles. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of data frames. So whenever we talk about data frames, we’re also talking about tibbles. For more on the topic, check out R4SD, Chapter 10. So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) head(d) ## # A tibble: 4 x 5 ## p_1 p_2 p_3 p_4 p_5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% gather() %&gt;% mutate(x = rep(1:4, times = 5), possibility = rep(1:5, each = 4)) %&gt;% ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data Wrangling Part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and dplyr::gather(). Here’s the basic structure of the possibilities per marble draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # … with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3) / (4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # … with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # … with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # … with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x) { rowSums(x == &quot;b&quot;) } n_white &lt;- function(x) { rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 If you are new to making custom functions in R, you might check out Chapter 19 in R4DS or Chapter 14 in R Programming for Data Science. We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # … with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index) { ifelse(index == &quot;a&quot;, position, ifelse(index == &quot;b&quot;, position + 4, position + 4 * 2) ) } Now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # … with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # … with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(# `pie_index == &quot;a&quot;` rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # `pie_index == &quot;b&quot;` rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # `pie_index == &quot;c&quot;` rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4! d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 2.1.2 Using prior information. We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) Here’s the table in the middle of page 25. t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 We might update to reproduce the table a the top of page 26, like this. t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post Data Wrangling Part 1: Basic to Advanced Ways to Select Columns. 2.1.3 From counts to probability. The opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as: plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of In other words: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} \\propto \\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p \\] But since we have to standardize the results to get them into a probability metric, the full equation is: \\[ \\text{plausibility of } p \\text{ after } D_\\text{new} = \\frac{\\text{ ways } p \\text{ can produce } D_\\text{new} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}} \\] You might make the table in the middle of page 27 like this. t %&gt;% select(p_1:p_4) %&gt;% mutate(p = seq(from = 0, to = 1, by = .25), `ways to produce data` = c(0, 3, 8, 9, 0)) %&gt;% mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 p `ways to produce data` plausibility ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 0.25 3 0.15 ## 3 b b w w 0.5 8 0.4 ## 4 b b b w 0.75 9 0.45 ## 5 b b b b 1 0 0 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == &quot;w&quot;), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials)) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, # putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% # plot! ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here. 2.2.3 Evaluate. The Bayesian model learns in a way that is demonstrably optimal, provided that the real, large world is accurately described by the model. This is to say that your Bayesian machine guarantees perfect inference, within the small world. No other way of using the available information, and beginning with the same state of information, could do better. Don’t get too excited about this logical virtue, however. The calculations may malfunction, so results always have to be checked. And if there are important differences between the model and reality, then there is no logical guarantee of large world performance. (p. 31) 2.2.3.1 Rethinking: Deflationary statistics. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) 2.3 Components of the model a likelihood function: “the number of ways each conjecture could produce an observation” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data” a prior: “the initial plausibility of each conjectured cause of the data” 2.3.1 Likelihood. If you let the count of water be \\(w\\) and the number of tosses be \\(n\\), then the binomial likelihood may be expressed as: \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Given a probability of .5, the binomial likelihood of 6 out of 9 tosses coming out water is: dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Let’s do so over the parameter space. tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2 Parameters. “For most likelihood functions, there are adjustable inputs” (p. 34). These are the parameters. 2.3.2.1 Rethinking: Datum or parameter? It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34) For more in this topic, check out his lecture Understanding Bayesian Statistics without Frequentist Language. 2.3.3 Prior. So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35) To learn more about “regularizing or weakly informative priors,” check out the Prior Choice Recommendations wiki from the Stan team. 2.3.3.1 Overthinking: Prior as a probability distribution McElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\). tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% mutate(prob = 1 / (b - a)) ## # A tibble: 5 x 3 ## a b prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 0 1.5 0.667 ## 3 0 2 0.5 ## 4 0 3 0.333 ## 5 0 9 0.111 I like to verify things with plots. tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% expand(nesting(a, b), parameter_space = seq(from = 0, to = 9, length.out = 500)) %&gt;% mutate(prob = dunif(parameter_space, a, b), b = str_c(&quot;b = &quot;, b)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + scale_x_continuous(breaks = c(0, 1:3, 9)) + scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1), labels = c(&quot;0&quot;, &quot;1/9&quot;, &quot;1/3&quot;, &quot;1/2&quot;, &quot;2/3&quot;, &quot;1&quot;)) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + facet_wrap(~b, ncol = 5) And as we’ll learn much later in the project, the \\(\\text{Uniform} (0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g., tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(prob = dbeta(parameter_space, 1, 1)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + ylim(0, 2) + theme(panel.grid = element_blank()) 2.3.4 Posterior. If we continue to focus on the globe tossing example, the posterior probability a toss will be water may be expressed as: \\[\\text{Pr} (p|w) = \\frac{\\text{Pr} (w|p) \\text{Pr} (p)}{\\text{Pr} (w)}\\] More generically and in words, this is: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\] 2.4 Making the model go Here’s the data wrangling for Figure 2.6. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2), exp(-abs(probability - .5) / .25) / ( 2 * .25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% gather(key, value, -probability, -row) %&gt;% ungroup() %&gt;% mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them. Whereas in earlier editions of this project I relied on the multiplot() function and the grid.arrange() function from the gridExtra package to combine ggplots, we will put them aside for the elegant and powerful syntax from Thomas Lin Pedersen’s patchwork package. p1 &lt;- d %&gt;% filter(key == &quot;prior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;prior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p2 &lt;- d %&gt;% filter(key == &quot;likelihood&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;likelihood&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p3 &lt;- d %&gt;% filter(key == &quot;posterior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) library(patchwork) p1 | p2 | p3 I’m not sure if it’s the same McElreath used in the text, but the formula I used for the triangle-shaped prior is the Laplace distribution with a location of .5 and a dispersion of .25. Also, to learn all about dplyr::filter(), check out Baert’s Data Wrangling Part 3: Basic and more advanced ways to filter rows. 2.4.1 Grid approximation. We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the code for the right panel of Figure 2.7. p1 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Now here’s the code for the left hand panel of Figure 2.7. p2 &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here we combine them and plot! p1 + p2 + plot_annotation(title = &quot;More grid points make for smoother approximations&quot;) 2.4.2 Quadratic approximation. Apply the quadratic approximation to the globe tossing data with rethinking::map(). library(rethinking) globe_qa &lt;- rethinking::map( alist( w ~ dbinom(9, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(w = 6)) # display summary of quadratic approximation precis(globe_qa) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.16 0.42 0.92 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- rethinking::map( alist( w ~ dbinom(9 * 2, p), p ~ dunif(0, 1) ), data = list(w = 6 *2)) globe_qa_36 &lt;- rethinking::map( alist( w ~ dbinom(9 * 4, p), p ~ dunif(0, 1) ), data = list(w = 6 * 4)) precis(globe_qa_18) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.11 0.49 0.84 precis(globe_qa_36) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.08 0.54 0.79 Now make Figure 2.8. n_grid &lt;- 100 # wrangle tibble(w = c(6, 12, 24), n = c(9, 18, 36), s = c(.16, .11, .08)) %&gt;% expand(nesting(w, n, s), p_grid = seq(from = 0, to = 1, length.out = n_grid)) %&gt;% mutate(prior = 1, m = .67) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% # plot ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.3 Markov chain Monte Carlo. Since the main goal of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) Here re-fit the last model from above, the one for which \\(w = 24\\) and \\(n = 36\\). b2.1 &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior(beta(1, 1), class = Intercept), iter = 4000, warmup = 1000, control = list(adapt_delta = .95), seed = 2, file = &quot;fits/b02.01&quot;) The model output from brms looks like so. print(b2.1) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.66 0.07 0.50 0.79 1.00 3965 4326 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\). Let’s plot the results of our model and compare them with those from rethinking::map(), above. posterior_samples(b2.1) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion water&quot;, limits = c(0, 1)) + theme(panel.grid = element_blank()) + facet_wrap(~n) If you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models in brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 10. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.3 rethinking_1.59 rstan_2.19.2 StanHeaders_2.19.0 ## [6] patchwork_1.0.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [11] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.3.1 rstudioapi_0.10 ## [9] farver_2.0.3 DT_0.11 fansi_0.4.1 mvtnorm_1.0-12 ## [13] lubridate_1.7.4 xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [17] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.3 ## [21] dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [29] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [41] vctrs_0.2.2 nlme_3.1-142 crosstalk_1.0.0 xfun_0.12 ## [45] ps_1.3.0 rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 zoo_1.8-7 ## [53] scales_1.1.0 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [57] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 loo_2.2.0 stringi_1.4.5 highr_0.8 ## [65] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.4 pkgconfig_2.0.3 ## [69] matrixStats_0.55.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [77] plyr_1.8.5 magrittr_1.5 R6_2.4.1 generics_0.0.2 ## [81] DBI_1.1.0 pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [85] xts_0.12-0 abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## [89] utf8_1.1.4 rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [93] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 digest_0.6.23 ## [97] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 ## [101] shinyjs_1.1 "],
["sampling-the-imaginary.html", "3 Sampling the Imaginary 3.1 Sampling from a grid-like approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice in brms Reference Session info", " 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}.\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time using the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 The posterior distribution is a probability distribution. And like all probability distributions, we can imagine drawing samples form it. The sampled events in this case are parameter values. Most parameters have no exact empirical realization. The Bayesian formalism treats parameter distributions as relative plausibility, not as any physical random process. In any event, randomness is always a property of information, never of the real world. But inside the computer, parameters are just as empirical as the outcome of a coin flip or a die toss or an agricultural experiment. The posterior defines the expected frequency that different parameter values will appear, once we start plucking parameters out of it. (p. 50, emphasis in the original) 3.1 Sampling from a grid-like approximate posterior Once again, here we use grid approximation to generate samples. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows Now we’ll use the dplyr::sample_n() function to sample rows from d, saving them as sample. # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) glimpse(samples) ## Observations: 10,000 ## Variables: 4 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… We’ll plot the zigzagging left panel of Figure 3.1 with geom_line(). But before we do, we’ll need to add a variable numbering the samples. samples %&gt;% mutate(sample_number = 1:n()) %&gt;% ggplot(aes(x = sample_number, y = p_grid)) + geom_line(size = 1/10) + scale_y_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) + xlab(&quot;sample number&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) That was based on 1e4 samples. On page 53, McElreath said the density would converge on the idealized shape if we keep increasing the number of samples. Here’s what it looks like with 1e6. set.seed(3) d %&gt;% sample_n(size = 1e6, weight = posterior, replace = T) %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) Yep, that’s more ideal. 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you might first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.171 To learn more about dplyr::summarise() and related functions, check out Baert’s Data Wrangling Part 4: Summarizing and slicing your data and Chapter 5.6 of R4DS. If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.162 A more explicit approach for the same computation is to follow up count() with mutate(). samples %&gt;% count(p_grid &lt; .5) %&gt;% mutate(probability = n / sum(n)) ## # A tibble: 2 x 3 ## `p_grid &lt; 0.5` n probability ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 8377 0.838 ## 2 TRUE 1623 0.162 And an even trickier approach for the same is to insert the logical statement p_grid &lt; .5 within the mean() function. samples %&gt;% summarise(sum = mean(p_grid &lt; .5)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.162 Much like McElreath discussed in the Overthinking: Counting with sum box, this works “because R internally converts a logical expression, like samples &lt; 0/5, to a vector of TRUE and FALSE results, one for each element of samples, saying whether or not each element matches the criterion” (p. 54). When we inserted that vector of TRUE and FALSE values within the mean() function, they were then internally converted to a vector of 1s and 0s, the mean of which was the probability. Tricky! To determine the posterior probability between 0.5 and 0.75, you can use &amp; within filter(). samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.602 Just multiply that value by 100 to get a percent. samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples, percent = 100 * n() / n_samples) ## # A tibble: 1 x 2 ## sum percent ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.602 60.2 And, of course, you can do that with our mean() trick, too. samples %&gt;% summarise(percent = 100 * mean(p_grid &gt; .5 &amp; p_grid &lt; .75)) ## # A tibble: 1 x 1 ## percent ## &lt;dbl&gt; ## 1 60.2 3.2.2 Intervals of defined mass. We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), some careful filtering, and a little patchwork syntax. # upper left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + # note this next line is the only difference in code from the last plot geom_ribbon(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) library(patchwork) p1 + p2 We’ll come back for the lower two panels in a bit. Since we saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$p_grid, prob = .8)) ## 80% ## 0.763 That value will come in handy for the lower left panel of Figure 3.2. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(). samples %&gt;% pull(p_grid) %&gt;% quantile(prob = .8) ## 80% ## 0.763 And we might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(p_grid, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.763 Here’s the summarise() approach with two probabilities: samples %&gt;% summarise(`10th percentile` = quantile(p_grid, p = .1), `90th percentile` = quantile(p_grid, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.452 0.814 The tydiverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(.1, .9))) ## 10% 90% ## 0.4520 0.8141 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) p1 + p2 We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_trials &lt;- 3 # update `d` d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) # make the next part reproducible set.seed(3) # here&#39;s our new samples tibble ( samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) ) ## # A tibble: 10,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.716 1 0.367 0.367 ## 2 0.651 1 0.276 0.276 ## 3 0.547 1 0.164 0.164 ## 4 0.999 1 0.997 0.997 ## 5 0.99 1 0.970 0.970 ## 6 0.787 1 0.487 0.487 ## 7 0.94 1 0.831 0.831 ## 8 0.817 1 0.545 0.545 ## 9 0.955 1 0.871 0.871 ## 10 0.449 1 0.0905 0.0905 ## # … with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$p_grid, prob = c(.25, .75)) ## 25% 75% ## 0.709 0.935 rethinking::PI(samples$p_grid, prob = .5) ## 25% 75% ## 0.709 0.935 Now’s a good time to introduce Matthew Kay’s tidybayes package, which offers an array of convenience functions for summarizing Bayesian models of the type we’ll be working with in this project. library(tidybayes) median_qi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.843 0.709 0.935 0.5 median qi The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.843 0.709000 0.935 0.50 median qi ## 2 0.843 0.570000 0.975 0.80 median qi ## 3 0.843 0.260985 0.999 0.99 median qi The .width column in the output indexed which line presented which interval. The value in the y column remained constant across rows. That’s because that column listed the measure of central tendency, the median in this case. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$p_grid, prob = .5) ## |0.5 0.5| ## 0.842 0.999 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9562951 0.842 0.999 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions. qi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.709 0.935 hdi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.842 0.999 These are nice in that they return simple numeric vectors, making them particularly useful to use as references within ggplot2. Now we have that skill, we can use it to make Figure 3.3. # lower left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + # check out our sweet `qi()` indexing geom_ribbon(data = d %&gt;% filter(p_grid &gt; qi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; qi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = . %&gt;% filter(p_grid &gt; hdi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; hdi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # combine! p1 | p2 In the geom_ribbon() line for the HPDI plot, did you notice how we replaced d with .? When using the pipe (i.e., %&gt;%), you can use the . as a placeholder for the original data object. It’s an odd and handy trick to have. Go here to learn more. The HPDI has some advantages over the PI. But in most cases, these two types of interval are very similar. They only look so different in this case because the posterior distribution is highly skewed. If we instead used samples from the posterior distribution for six waters in nine tosses, these intervals would be nearly identical. Try it for yourself, using different probability masses, such as prob=0.8 and prob=0.95. When the posterior is bell shaped, it hardly matters when type of interval you use. (p. 57) Let’s try it out. First we’ll update the simulation for six waters in nine tosses. n_success &lt;- 6 n_trials &lt;- 9 new_d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) set.seed(3) new_samples &lt;- new_d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) Here are the intervals by .width and type of .interval. bind_rows(mean_hdi(new_samples$p_grid, .width = c(.8, .95)), mean_qi(new_samples$p_grid, .width = c(.8, .95))) %&gt;% select(.width, .interval, ymin:ymax) %&gt;% arrange(.width) %&gt;% mutate_if(is.double, round, digits = 2) ## .width .interval ymin ymax ## 1 0.80 hdi 0.48 0.84 ## 2 0.80 qi 0.45 0.81 ## 3 0.95 hdi 0.37 0.90 ## 4 0.95 qi 0.35 0.88 We didn’t need that last mutate_if() line. It just made it easier to compare the ymin and ymax values. Anyway, McElreath was right. This time the differences between the HPDIs and QIs were trivial. Here’s a look at the posterior. new_d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;Six waters in nine tosses made\\nfor a more symmetrical posterior&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 ## 2 0.999 1 0.997 0.997 ## 3 0.998 1 0.994 0.994 ## 4 0.997 1 0.991 0.991 ## 5 0.996 1 0.988 0.988 ## 6 0.995 1 0.985 0.985 ## 7 0.994 1 0.982 0.982 ## 8 0.993 1 0.979 0.979 ## 9 0.992 1 0.976 0.976 ## 10 0.991 1 0.973 0.973 ## # … with 991 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 Or we could use the handy dplyr::top_n() function. d %&gt;% select(posterior) %&gt;% top_n(n = 1) ## # A tibble: 1 x 1 ## posterior ## &lt;dbl&gt; ## 1 1 We can get the mode with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.477 1 0.95 mode hdi samples %&gt;% mode_qi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.401 0.994 0.95 mode qi Those returned a lot of output in addition to the mode. If all you want is the mode itself, you can just use tidybayes::Mode(). Mode(samples$p_grid) ## [1] 0.9562951 Medians and means are typical measures of central tendency, too. samples %&gt;% summarise(mean = mean(p_grid), median = median(p_grid)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- bind_rows(samples %&gt;% mean_qi(p_grid), samples %&gt;% median_qi(p_grid), samples %&gt;% mode_qi(p_grid)) %&gt;% select(p_grid, .point) %&gt;% # these last two columns will help us annotate mutate(x = p_grid + c(-.03, .03, -.03), y = c(.1, .25, .4)) ) ## # A tibble: 3 x 4 ## p_grid .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 mean 0.773 0.1 ## 2 0.843 median 0.873 0.25 ## 3 0.956 mode 0.926 0.4 The plot: d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be: d %&gt;% mutate(loss = posterior * abs(0.5 - p_grid)) %&gt;% summarise(`expected loss` = sum(loss)) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 78.4 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package, which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d) { d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) ) ## # A tibble: 1,001 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 201. ## 2 0.001 200. ## 3 0.002 200. ## 4 0.003 200. ## 5 0.004 199. ## 6 0.005 199. ## 7 0.006 199. ## 8 0.007 199. ## 9 0.008 198. ## 10 0.009 198. ## # … with 991 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.841. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(p_grid)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # ammend our loss function make_loss &lt;- function(our_d) { d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.803 Usually, research scientists don’t think about loss functions. And so any point estimate like the mean or MAP that they may report isn’t intended to support any particular decision, but rather to describe the shape of the posterior. You might argue that the decision to make is whether or not to accept an hypothesis. But the challenge then is to say what the relevant costs and benefits would be, in terms of the knowledge gained or lost. Usually it’s better to communicate as much as you can about the posterior distribution, as well as the data and the model itself, so that others can build upon your work. Premature decisions to accept or reject hypotheses can cost lives. (p. 61) In the endnote (58) linked to the end of that quote in the text, McElreath wrote: “See Hauer (2004) for three tales from transportation safety in which testing resulted in premature incorrect decisions and a demonstrable and continuing loss of human life” (p. 448). 3.3 Sampling to simulate prediction McElreath’s four good reasons for posterior simulation were model checking, software validation, research design, and forecasting. 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the denisties are: tibble(n = 2, probability = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = probability)) ## # A tibble: 3 x 4 ## n probability w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(3) rbinom(1, size = 2, prob = .7) ## [1] 2 Here are ten reproducible draws. set.seed(3) rbinom(10, size = 2, prob = .7) ## [1] 2 1 2 2 1 1 2 2 1 1 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% count(draws) %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9000 0.09 ## 2 1 42051 0.421 ## 3 2 48949 0.489 As McElreath mused in the text (p. 63), those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up. Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability) { set.seed(3) rbinom(n_draws, size = n, prob = probability) } d &lt;- crossing(n = c(3, 6, 9), probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest(draws) head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 0 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 0 ## 5 n = 3 p = 0.3 1 ## 6 n = 3 p = 0.3 1 Let’s plot the simulation results. d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). If you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari, Gelman, Simpson, Carpenter, and Bürkner Rank-normalization, folding, and localization: An improved \\(\\hat R\\) for assessing convergence of MCMC. 3.3.2.2 Is the model adequate? The implied predictions of the model are uncertain in two ways, and it’s important to be aware of both. First, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)). Second, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes. We’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (p. 56, emphasis in the original) All this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows We can make our version of the top of Figure 3.6 with a little tricky filtering. d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), color = &quot;grey67&quot;, fill = &quot;grey67&quot;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(xend = p_grid, y = 0, yend = posterior, size = posterior), color = &quot;grey33&quot;, show.legend = F) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(y = posterior)) + annotate(geom = &quot;text&quot;, x = .08, y = .0025, label = &quot;Posterior probability&quot;) + scale_size_continuous(range = c(0, 1)) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Note how we weighted the widths of the vertical lines by the posterior density. We’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6. n_draws &lt;- 1e5 simulate_binom &lt;- function(probability) { set.seed(3) rbinom(n_draws, size = 9, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) head(d_small) ## # A tibble: 6 x 3 ## probability draws label ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.1 0 p = 0.1 ## 2 0.1 2 p = 0.1 ## 3 0.1 0 p = 0.1 ## 4 0.1 0 p = 0.1 ## 5 0.1 1 p = 0.1 ## 6 0.1 1 p = 0.1 Now we’re ready to plot. d_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Sampling distributions&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) To make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text). # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) glimpse(samples) ## Observations: 10,000 ## Variables: 5 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… ## $ w &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, … Here’s our histogram. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9, ylim = 0:3000) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longst sequence of the sampe values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 7 5 6 8 7 5 6 3 3 4 Notice this collapsed (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 0 1 1 1 0 0 0 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot. # make it reproducible set.seed(3) samples &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %&gt;% unnest(draws) glimpse(samples) ## Observations: 90,000 ## Variables: 7 ## $ p_grid &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0… ## $ posterior &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, … ## $ w &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ draws &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, … The main action is in the draws column. Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”. rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 6), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 3.4 Summary Let’s practice in brms Open brms. library(brms) With brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of the project for Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 1, # this is a flat prior prior(beta(1, 1), class = Intercept), control = list(adapt_delta = .999), seed = 3, file = &quot;fits/b03.01&quot;) We’ll learn more about the beta distribution in Chapter 11. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”. posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.64 0.14 0.36 0.88 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with the brms::fitted() function. But we will have to specify scale = &quot;linear&quot; in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. f &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(&quot;p&quot;) glimpse(f) ## Observations: 4,000 ## Variables: 1 ## $ p &lt;dbl&gt; 0.6920484, 0.5559454, 0.6096088, 0.5305334, 0.4819733, 0.6724561, 0.6402367, 0.8356569,… By default, we have a generically-named vector V1 of 4,000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &quot;grey50&quot;, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .08, y = 2.5, label = &quot;Posterior probability&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6. # the simulation set.seed(3) f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) # the plot f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0s and 1s, too. And we could even use those disaggregated 0s and 1s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.3 tidybayes_2.0.1.9000 patchwork_1.0.0 ## [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ggplot2_3.2.1 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.2 rsconnect_0.8.16 ## [4] markdown_1.1 base64enc_0.1-3 rethinking_1.59 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] rstan_2.19.2 DT_0.11 svUnit_0.7-12 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.3 dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 backports_1.1.5 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [34] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 crosstalk_1.0.0 ## [46] xfun_0.12 ps_1.3.0 rvest_0.3.5 ## [49] miniUI_0.1.1.1 mime_0.8 lifecycle_0.1.0 ## [52] gtools_3.8.1 MASS_7.3-51.4 zoo_1.8-7 ## [55] scales_1.1.0 colourpicker_1.0 hms_0.5.3 ## [58] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.2 ## [61] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [64] gridExtra_2.3 loo_2.2.0 StanHeaders_2.19.0 ## [67] stringi_1.4.5 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [70] rlang_0.4.4 pkgconfig_2.0.3 matrixStats_0.55.0 ## [73] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 ## [76] rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [79] processx_3.4.1 tidyselect_1.0.0 plyr_1.8.5 ## [82] magrittr_1.5 R6_2.4.1 generics_0.0.2 ## [85] DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [88] withr_2.1.2 xts_0.12-0 abind_1.4-5 ## [91] modelr_0.1.5 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [94] utf8_1.1.4 rmarkdown_2.0 grid_3.6.2 ## [97] readxl_1.3.1 callr_3.4.1 threejs_0.3.3 ## [100] reprex_0.3.0 digest_0.6.23 xtable_1.8-4 ## [103] httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 ## [106] viridisLite_0.3.0 shinyjs_1.1 "],
["linear-models.html", "4 Linear Models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 A Gaussian model of height 4.4 Adding a predictor 4.5 Polynomial regression Reference Session info", " 4 Linear Models Linear regression is the geocentric model of applied statistics. By “linear regression”, we will mean a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Like geocentrism, linear regression can usefully describe a very large variety of natural phenomena. Like geocentrism, linear is a descriptive model that corresponds to many different process models. If we read its structure too literally, we’re likely to make mistakes. But used wisely, these little linear golems continue to be useful. (p. 71) 4.1 Why normal distributions are normal After laying out his soccer field coin toss shuffle premise, McElreath wrote: It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72) 4.1.1 Normal by addition. Here’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2. library(tidyverse) # we set the seed to make the results of `runif()` reproducible. set.seed(4) pos &lt;- # make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person) crossing(person = 1:100, step = 0:16) %&gt;% # for all steps above `step == 0` simulate a `deviation` mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %&gt;% # after grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()` group_by(person) %&gt;% mutate(position = cumsum(deviation)) %&gt;% ungroup() That map_dbl() code within the first mutate() line might look odd. Go here to learn more about iterating with purrr::map_dbl(). We might glimpse() at the data. glimpse(pos) ## Observations: 1,700 ## Variables: 4 ## $ person &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ step &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, … ## $ deviation &lt;dbl&gt; 0.00000000, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47914446, 0.4… ## $ position &lt;dbl&gt; 0.0000000, -0.9821084, -1.3946292, -1.8398793, -1.2127308, -1.6918753, -1.24306… Here’s the actual plot code. ggplot(data = pos, aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&quot;step number&quot;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &quot;none&quot;) Now here’s the code for the bottom three plots of Figure 4.2. # Figure 4.2.a. p1 &lt;- pos %&gt;% filter(step == 4) %&gt;% ggplot(aes(x = position)) + geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;4 steps&quot;) # Figure 4.2.b. p2 &lt;- pos %&gt;% filter(step == 8) %&gt;% ggplot(aes(x = position)) + geom_density(color = &quot;dodgerblue2&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;8 steps&quot;) # this is an intermediary step to get an SD value pos %&gt;% filter(step == 16) %&gt;% summarise(sd = sd(position)) ## # A tibble: 1 x 1 ## sd ## &lt;dbl&gt; ## 1 2.36 # Figure 4.2.c. p3 &lt;- pos %&gt;% filter(step == 16) %&gt;% ggplot(aes(x = position)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2.180408), linetype = 2) + # 2.180408 came from the previous code block geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + coord_cartesian(xlim = -6:6) + labs(title = &quot;16 steps&quot;, y = &quot;density&quot;) library(patchwork) # combine the ggplots p1 | p2 | p3 While we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions, respectively. Any process that ads together random values from the same distribution converges to a normal. But it’s not easy to grasp why addition should result in a bell curve of sums. Here’s a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from the average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. The more terms in the sum, the more chances for each fluctuation to be canceled by another, or by a series of smaller ones in the opposite direction. So eventually the most likely sum, in the sense that there are the most ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum of zero (relative to the mean). (pp. 73–74) 4.1.2 Normal by multiplication. Here’s McElreath’s simple random growth rate. set.seed(4) prod(1 + runif(12, 0, 0.1)) ## [1] 1.774719 In the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values and then computed their product. Consider a more explicit variant of the code. set.seed(4) tibble(a = 1, b = runif(12, 0, 0.1)) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c)) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 1.77 Same result. Rather than using base R replicate() to do this many times, let’s practice with purrr::map_dbl() like before. set.seed(4) growth &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.1)))) ggplot(data = growth, aes(x = growth)) + geom_density() “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. # simulate set.seed(4) samples &lt;- tibble(big = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.5))), small = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.01)))) %&gt;% # wrangle gather(distribution, samples) # plot samples %&gt;% ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;, color = &quot;transparent&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) Yep, the small samples were more Gaussian. 4.1.3 Normal by log-multiplication. Instead of saving our tibble, we’ll just feed it directly into our plot. set.seed(4) tibble(samples = map_dbl(1:1e4, ~ log(prod(1 + runif(12, 0, 0.5))))) %&gt;% ggplot(aes(x = samples)) + geom_density(color = &quot;transparent&quot;, fill = &quot;gray33&quot;) What we did was really compact. Walking it out a bit, here’s what we all did within the second argument within map_dbl() (i.e., everything within log()). tibble(a = runif(12, 0, 0.5), b = 1) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c) %&gt;% log()) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 2.82 And based on the first argument within map_dbl(), we did that 10,000 times, after which we converted the results to a tibble and then fed those data into ggplot2. Anyway, “we get the Gaussian distribution back, because adding logs is equivalent to multiplying the original numbers. So even multiplicative interactions of large deviations can produce Gaussian distributions, once we measure the outcomes on the log scale” (p. 75). 4.1.4 Using Gaussian distributions. I really like the justifications in the following subsections. 4.1.4.1 Ontological justification. The Gaussian is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p. 75) But they can still be useful. 4.1.4.2 Epistemological justification. Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75–76) 4.1.4.3 Overthinking: Gaussian distribution. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is \\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\Bigg (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\Bigg).\\] McElreath’s right. “This looks monstrous” (p. 76). Why not demystify that monster with a little R code? For simplicity, we’ll look at \\(p(y)\\) over a series of \\(y\\) values ranging from -4 to 4, holding \\(\\mu = 0\\) and \\(\\sigma = 1\\). Then we’ll plot. # define our input values tibble(y = seq(from = -4, to = 4, by = .1), mu = 0, sigma = 1) %&gt;% # compute p(y) using a hand-made gaussian likelihood mutate(p_y = (1 / sqrt(2 * pi * sigma^2)) * exp(-(y - mu)^2 / (2 * sigma^2))) %&gt;% # plot! ggplot(aes(x = y, y = p_y)) + geom_line() + ylab(expression(italic(p)(italic(&quot;y|&quot;)*mu==0*&quot;,&quot;~sigma==1))) You get the same results is you switch out that mutate line with mutate(p_y = dnorm(y)) %&gt;%. To learn more, execute ?dnorm. 4.2 A language for describing models Our mathy ways of summarizing models will be something like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta \\times \\text{predictor}_i \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy}(0, 1). \\end{align*}\\] And as McElreath then followed up with, “If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome applied statistics! 4.2.1 Re-describing the globe tossing model. For the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was \\[\\begin{align*} w &amp; \\sim \\text{Binomial}(n, p) \\\\ p &amp; \\sim \\text{Uniform}(0, 1). \\end{align*}\\] We can break McElreath’s R code 4.6 down a little bit with a tibble like so. # how many `p_grid` points would you like? n_points &lt;- 100 d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n_points), w = 6, n = 9) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(w, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 6 ## p_grid w n prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6 9 1 0. 0. ## 2 0.0101 6 9 1 8.65e-11 8.74e-12 ## 3 0.0202 6 9 1 5.37e- 9 5.43e-10 ## 4 0.0303 6 9 1 5.93e- 8 5.99e- 9 ## 5 0.0404 6 9 1 3.23e- 7 3.26e- 8 ## 6 0.0505 6 9 1 1.19e- 6 1.21e- 7 In case you were curious, here’s what they look like. d %&gt;% select(-w, -n) %&gt;% gather(key, value, -p_grid) %&gt;% # this line allows us to dictate the order the panels will appear in mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) + geom_ribbon() + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, scales = &quot;free&quot;) The posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see that we almost never use flat priors in practice. 4.3 A Gaussian model of height There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. (p. 79) 4.3.1 The data. Let’s get the Howell (2000, 2010) data from McElreath’s rethinking package. library(rethinking) data(Howell1) d &lt;- Howell1 Here we open our main statistical package, Bürkner’s brms. But before we do, we’ll want to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, it is a good idea to make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Go ahead and investigate the data with str(), the tidyverse analogue for which is glimpse(). d %&gt;% str() ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... Here are the height values. d %&gt;% select(height) %&gt;% head() ## height ## 1 151.765 ## 2 139.700 ## 3 136.525 ## 4 156.845 ## 5 145.415 ## 6 163.830 We can use filter() to make an adults-only data frame. d2 &lt;- d %&gt;% filter(age &gt;= 18) There are a lot of ways we can make sure our d2 has 352 rows. Here’s one. d2 %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 352 4.3.1.1 Overthinking: Data frames. This probably reflects my training history, but the structure of a data frame seems natural and inherently appealing, to me. So I can’t relate to the “annoying” comment. But if you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan. 4.3.1.2 Overthinking: Index magic. For more on indexing, check out Chapter 9 of Roger Peng’s R Programming for Data Science or even the Subsetting subsection from R4DS. 4.3.2 The model. The likelihood for our model is \\[h_i \\sim \\operatorname{Normal}(\\mu, \\sigma),\\] our \\(\\mu\\) prior will be \\[\\mu \\sim \\operatorname{Normal}(178, 20),\\] and our prior for \\(\\sigma\\) will be \\[\\sigma \\sim \\operatorname{Uniform}(0, 50).\\] Here’s the shape of the prior for \\(\\mu\\) in \\(N(178, 20)\\). ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + ylab(&quot;density&quot;) And here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the y axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous(). tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) We can simulate from both priors at once to get a prior probability distribution of heights. n &lt;- 1e4 set.seed(4) tibble(sample_mu = rnorm(n, mean = 178, sd = 20), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% ggplot(aes(x = x)) + geom_density(fill = &quot;black&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(Prior~predictive~distribution~&quot;for&quot;~italic(h[i])), x = NULL) + theme(panel.grid = element_blank()) As McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p. 83). 4.3.3 Grid approximation of the posterior distribution. As McElreath explained, you’ll never use this for practical data analysis. But I found this helped me better understanding what exactly we’re doing with Bayesian estimation. So let’s play along. This is our version of the first three lines in McElreath’s R code 4.14. n &lt;- 200 d_grid &lt;- # we&#39;ll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()` crossing(mu = seq(from = 140, to = 160, length.out = n), sigma = seq(from = 4, to = 9, length.out = n)) glimpse(d_grid) ## Observations: 40,000 ## Variables: 2 ## $ mu &lt;dbl&gt; 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140… ## $ sigma &lt;dbl&gt; 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.150754, 4.175879, 4.2… d_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computations by making a custom function which we’ll plug into purrr::map2(). grid_function &lt;- function(mu, sigma) { dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’re ready to complete the tibble. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest(log_likelihood) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) head(d_grid) ## # A tibble: 6 x 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 -3813. -5.72 -3.91 -3822. 0 ## 2 140 4.03 -3778. -5.72 -3.91 -3787. 0 ## 3 140 4.05 -3743. -5.72 -3.91 -3753. 0 ## 4 140 4.08 -3709. -5.72 -3.91 -3719. 0 ## 5 140 4.10 -3676. -5.72 -3.91 -3686. 0 ## 6 140 4.13 -3644. -5.72 -3.91 -3653. 0 In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + coord_cartesian(xlim = range(d_grid$mu), ylim = range(d_grid$sigma)) + theme(panel.grid = element_blank()) We’ll make our heat map with geom_raster(aes(fill = probability)). d_grid %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_raster(aes(fill = probability), interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior. We can use dplyr::sample_n() to sample rows, with replacement, from d_grid. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) We can use gather() and then facet_warp() to plot the densities for both mu and sigma at once. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) We’ll use the tidybayes package to compute their posterior modes and 95% HDIs. library(tidybayes) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.95 mode hdi ## 2 sigma 7.82 7.14 8.30 0.95 mode hdi Let’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5). 4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior. Since we’ll be fitting models with brms almost exclusively from here on out, this section is largely moot. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3. set.seed(4) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 144.7800 154.9400 ## [11] 151.1300 147.9550 149.8600 162.5600 161.9250 164.4650 160.9852 151.7650 163.8300 149.8600 For our first step using d3, we’ll redefine d_grid. n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- crossing(mu = seq(from = 150, to = 170, length.out = n), sigma = seq(from = 4, to = 20, length.out = n)) Second, we’ll redefine our custom grid_function() function to operate over the height values of d3. grid_function &lt;- function(mu, sigma) { dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’ll use the amended grid_function() to make the posterior. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) Did you catch our use of purrr::map2_dbl(), there, in place of purrr::map2()? It turns out that purrr::map() and purrr::map2() always return a list (see here and here). But as Phil Straforelli kindly pointed out, we can add the _dbl suffix to those functions, which will instruct the purrr package to return a double vector (i.e., a common kind of numeric vector). The advantage of that approach is we no longer need to follow our map() or map2() lines with unnest(). To learn more about the ins and outs of the map() family, check out this section from R4DS or Jenny Bryan’s purrr tutorial. Next we’ll sample_n() and plot. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) Behold the updated densities. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;, labeller = label_parsed) That labeller = label_parsed bit in the facet_wrap() function is what converted our subplot strip labels into Greek. Anyway, \\(\\sigma\\) is not so Gaussian with that small \\(n\\). This is the point in the project where we hop off the grid-approximation train. On the one hand, I think this is a great idea. Most of y’all reading this will never use grid approximation in a real-world applied data analysis. On the other hand, there is some pedagogical utility in practicing with it. It can help you grasp what it is we’re dong when we apply Bayes’ theorem. If you’d like more practice, check out the first several chapters in Kruschke’s (2015) textbook and the corresponding chapters in my project translating it into brms and tidyverse. 4.3.5 Fitting the model with map brm(). We won’t actually use rethinking::map()–which you should not conflate with purrr::map()–, but will jumpt straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Plus, once in a blue moon we will actually use the rethinking package to fit a model in order to contrast it to one fit with brms. On those occasions, we will index them using the m prefix. Here’s the first model with brm(). b4.1 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.01&quot;) McElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in Chapter 8, Hamiltonian Monte Carlo (HMC) tends to work better when you default to a half Cauchy for \\(\\sigma\\). We can do that like this. b4.1_hc &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), # the magic lives here prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.01_hc&quot;) This leads to an important point. After running model with Hamiltonian Monte Carlo (HMC), it’s a good idea to inspect the chains. As we’ll see, McElreath coverd this in Chapter 8. Here’s a typical way to do so in brms. plot(b4.1_hc) If you want detailed diagnostics for the HMC chains, execute launch_shinystan(b4.1). It’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object. print(b4.1_hc) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.62 0.42 153.81 155.43 1.00 2750 2667 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 7.75 0.29 7.21 8.36 1.00 3347 2448 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summary() function works in a similar way. You can also get a Stan-like summary like this. b4.1_hc$fit ## Inference for Stan model: 1df58279fc244ef48da72e5429624e43. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 154.62 0.01 0.42 153.81 154.34 154.62 154.91 155.43 2725 1 ## sigma 7.75 0.01 0.29 7.21 7.56 7.75 7.94 8.36 3317 1 ## lp__ -1227.52 0.03 1.01 -1230.29 -1227.90 -1227.21 -1226.80 -1226.54 1317 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Feb 25 19:32:13 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Whereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print(). summary(b4.1_hc, prob = .89) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.62 0.42 153.96 155.28 1.00 2750 2667 ## ## Family Specific Parameters: ## Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS ## sigma 7.75 0.29 7.30 8.24 1.00 3347 2448 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Anyways, here’s how to fit the model with the shockingly-narrow prior on \\(\\mu\\). b4.2 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 0.1), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 3000, warmup = 2000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.02&quot;) Check the chains. plot(b4.2) I had to increase the warmup due to convergence issues. After doing so, everything looks to be on the up and up. The chains look great. And again, we will learn more about these technical details in Chapter 8. Here’s the model summary(). summary(b4.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 3000; warmup = 2000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 177.86 0.10 177.67 178.06 1.00 3465 2405 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 24.59 0.92 22.82 26.47 1.00 1748 829 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Subsetting the summary() output with $fixed provides a convenient way to compare the Intercept summaries between b4.1_hc and b4.2. summary(b4.1_hc)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.6231 0.4163572 153.8149 155.4335 1.000125 2750 2667 summary(b4.2)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 177.8632 0.09958398 177.6723 178.0604 0.9996049 3465 2405 4.3.6 Sampling from a map brm() fit. brms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. vcov(b4.1_hc) ## Intercept ## Intercept 0.1733533 This only returned the first element in the matrix it did for rethinking. That is, it appears the brms::vcov() function only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters (i.e., those used to model \\(\\mu\\)). However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. We do that with the posterior_samples(), which we’ll be using a lot of as we go along. post &lt;- posterior_samples(b4.1_hc) head(post) ## b_Intercept sigma lp__ ## 1 155.0785 7.335334 -1228.205 ## 2 154.0975 7.652924 -1227.324 ## 3 154.8050 7.499036 -1226.946 ## 4 155.2052 7.574270 -1227.748 ## 5 155.2581 7.565656 -1227.970 ## 6 154.0118 7.612666 -1227.663 Now select() the columns containing the draws from the desired parameters and feed them into cov(). select(post, b_Intercept:sigma) %&gt;% cov() ## b_Intercept sigma ## b_Intercept 0.173353306 -0.003242316 ## sigma -0.003242316 0.084394323 That was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix. # variances select(post, b_Intercept:sigma) %&gt;% cov() %&gt;% diag() ## b_Intercept sigma ## 0.17335331 0.08439432 # correlation post %&gt;% select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.00000000 -0.02680604 ## sigma -0.02680604 1.00000000 With our post &lt;- posterior_samples(b4.1_hc) code from a few lines above, we’ve already done the brms version of what McElreath did with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package, in brms we just extracted the iterations of the HMC chains and put them in a data frame. str(post) ## &#39;data.frame&#39;: 4000 obs. of 3 variables: ## $ b_Intercept: num 155 154 155 155 155 ... ## $ sigma : num 7.34 7.65 7.5 7.57 7.57 ... ## $ lp__ : num -1228 -1227 -1227 -1228 -1228 ... Notice how our data frame, post, includes a third vector named lp__. That’s the log posterior. See the brms reference manual or the “The Log-Posterior (function and gradient)” section of the Stan Development Team’s RStan: the R interface to Stan for details. The log posterior will largely be outside of our focus in this project. The summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. E.g., summary(post[, 1:2]) ## b_Intercept sigma ## Min. :152.9 Min. :6.785 ## 1st Qu.:154.3 1st Qu.:7.561 ## Median :154.6 Median :7.748 ## Mean :154.6 Mean :7.754 ## 3rd Qu.:154.9 3rd Qu.:7.938 ## Max. :156.1 Max. :8.902 Here’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here. t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75))) ## 50% 2.5% 75% ## b_Intercept 154.619016 153.814860 154.909843 ## sigma 7.748051 7.210529 7.938047 The base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code. post %&gt;% select(sigma:b_Intercept) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% summarise(mean = mean(value), SD = sd(value), `2.5_percentile` = quantile(value, probs = .025), `97.5_percentile` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## # A tibble: 2 x 5 ## parameter mean SD `2.5_percentile` `97.5_percentile` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.42 154. 155. ## 2 sigma 7.75 0.290 7.21 8.36 You can always get pretty similar information by just putting the brm() fit object into posterior_summary(). posterior_summary(b4.1_hc) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.623104 0.4163572 153.814860 155.433461 ## sigma 7.754454 0.2905070 7.210529 8.361077 ## lp__ -1227.519041 1.0103147 -1230.287040 -1226.540850 And if you’re willing to drop the posterior \\(SD\\)s, you can use tidybayes::mean_qi(), too. post %&gt;% select(sigma:b_Intercept) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% mean_qi(value) ## # A tibble: 2 x 7 ## parameter value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 155. 154. 155. 0.95 mean qi ## 2 sigma 7.75 7.21 8.36 0.95 mean qi 4.3.6.1 Overthinking: Under the hood with multivariate sampling. Again, brms::posterior_samples() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the iterations from the HMC chains. McElreath coverd all of this in Chapter 8 and we will too. You might also look at the brms reference manual or GitHub page for details. To get documentation in a hurry, you could also just execute ?posterior_samples. 4.3.6.2 Overthinking: Getting \\(\\sigma\\) right. There’s no need to fret about this when using brms. With HMC, we are not constraining the posteriors to the multivariate normal distribution. Here’s our posterior density for \\(\\sigma\\). ggplot(data = post, aes(x = sigma)) + geom_density(size = 1/10, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma)) + theme(panel.grid = element_blank()) See? HMC handled the mild skew just fine. But sometimes you want to actually model \\(\\sigma\\), such as in the case where your variances are systematically heterogeneous. Bürkner calls these kinds of models distributional models, which you can learn more about in his vignette Estimating Distributional Models with brms. As he explained in the vignette, you actually model \\(\\log (\\sigma)\\) in those instances. If you’re curious, we’ll practice with a model like this in Chapter 9. Kruschke also covered several models of this kind in his (2015) text, Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, which I’ve translated into brms and tidyverse code. Check out Section 16.3 for an example of modeling \\(\\log \\sigma\\) with a grouping variable. 4.4 Adding a predictor What we’ve done above is a Gaussian model of height in a population of adults. But it doesn’t really have the usual feel of “regression” to it. Typically, we are interested in modeling how an outcome is related to some predictor variable. And by including a predictor variable in a particular way, we’ll have linear regression. (p. 92) Here’s our scatter plot of our predictor weight and our criterion height. ggplot(data = d2, aes(x = weight, y = height)) + geom_point(shape = 1, size = 2) + theme_bw() + theme(panel.grid = element_blank()) There’s obviously a relationship: Knowing a person’s weight helps you predict height. To make this vague observation into a more precise quantitative model that relates values of weight to plausible values of height, we need some more technology. How do we take our Gaussian model from the previous section and incorporate predictor variables? (p. 92) 4.4.1 The linear model strategy. The strategy is to make the parameter for the mean of a Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model. The linear model strategy instructs the golem to assume that the predictor variable has a perfect constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship. (p. 92, emphasis in the original) Our new univariable model will follow the formula \\[\\begin{align*} h_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta x_i \\\\ \\alpha &amp; \\sim \\text{Normal}(178, 100) \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 50). \\end{align*}\\] 4.4.1.1 Likelihood. The likelihood for our model is \\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\). 4.4.1.2 Linear model. Our linear model is \\(\\mu_i = \\alpha + \\beta x_i\\). The thing we’re modeling is \\(\\mu_i\\), the conditional mean of our variable \\(h_i\\), and we’re modeling it with two parameters: \\(\\alpha\\) (i.e., the intercept) and \\(\\beta\\) (i.e., the slope). 4.4.1.3 Priors. Our univariable model has three priors: \\[\\begin{align*} \\alpha &amp; \\sim \\text{Normal}(178, 100), \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10), \\; \\text{and} \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 50). \\end{align*}\\] McElreath recommended we plot them. If you recall, we’ve already plotted \\(\\text{Normal}(178, 100)\\) and \\(\\text{Uniform}(0, 50)\\). Here’s what the prior for our new \\(\\beta\\) parameter looks like. tibble(beta = -40:40) %&gt;% mutate(density = dnorm(beta, mean = 0, sd = 10)) %&gt;% ggplot(aes(x = beta, ymin = 0, ymax = density)) + geom_ribbon(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(beta)) + theme(panel.grid = element_blank()) 4.4.2 Fitting the model. If you look closely at the statistical model and corresponding rethinking code at the bottom of page 95, you’ll see they contradict each other on the prior for \\(\\alpha\\). Though we didn’t note it at the time, there was similar contradiction in the middle of page 87 for m4.1. McElreath acknowledged this in his Errata, where he indicated the intended prior was \\(\\alpha \\sim \\text{Normal}(178, 100)\\). We will use that prior here, too. Unlike with the rethinking package, our brms::brm() syntax won’t perfectly mirror the formal statistical notation. But here are the analogues to the exposition at the bottom of page 95 (with the corrected \\(\\alpha\\) prior). \\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\): family = gaussian \\(\\mu_i = \\alpha + \\beta x_i\\): height ~ 1 + weight \\(\\alpha \\sim \\text{Normal}(178, 100)\\): prior(normal(178, 100), class = Intercept \\(\\beta \\sim \\text{Normal}(0, 10)\\): prior(normal(0, 10), class = b) \\(\\sigma \\sim \\text{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma) Thus, to add a predictor you just the + operator in the model formula. b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 41000, warmup = 40000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03&quot;) This was another example of how using a uniform prior for \\(\\sigma\\) required we use an unusually large number of warmup iterations before the HMC chains converged on the posterior. Change the prior to cauchy(0, 1) and the chains converge with no problem, resulting in much better effective samples, too. Here are the trace plots. plot(b4.3) 4.4.2.1 Overthinking: Embedding linear models. I’m not aware that you can embed the linear model within the likelihood function within brms::brm() the way McElreath did in R code 4.39. However, it can be pedagogically useful to write out the statistical model that way: \\[\\begin{align*} h_i &amp; \\sim \\text{Normal}(\\alpha + \\beta x_i, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal}(178, 100) \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 50) \\end{align*}\\] Whoah. What? Where did \\(\\mu_i\\) go? It’s still there. We just expressed it as \\(\\alpha + \\beta x_i\\). 4.4.3 Interpreting the model fit. One trouble with statistical models is that they are hard to understand. Once you’ve fit the model, it can only report posterior probabilities. These are the right answer to the question that is the combination of model and data. But it’s your responsibility to process the answer and make sense of it. There are two broad categories of processing: (1) reading tables and (2) plotting. (p. 97). 4.4.3.1 Tables of estimates. With a little [] subsetting we can exclude the log posterior from the posterior_summary() so we can fucus on the parameters. posterior_summary(b4.3)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 113.8471053 1.94964685 109.8593917 117.647380 ## b_weight 0.9058459 0.04295012 0.8218504 0.994324 ## sigma 5.1034876 0.19408621 4.7418032 5.517190 Again, brms doesn’t have a convenient corr = TRUE argument for plot() or summary(). But you can get that information after putting the chains in a data frame. posterior_samples(b4.3) %&gt;% select(b_Intercept:sigma) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight sigma ## b_Intercept 1.00 -0.99 0.03 ## b_weight -0.99 1.00 -0.03 ## sigma 0.03 -0.03 1.00 Much like the results from McElreath’s rethinking package, two of the parameters from our model fit with brm() are highly correlated, too. With centering, we can reduce that correlation. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) Fit the weight_c model, b4.4. b4.4 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 46000, warmup = 45000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.04&quot;) plot(b4.4) posterior_summary(b4.4)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.5969319 0.27786017 154.0644287 155.1561237 ## b_weight_c 0.9027448 0.04266622 0.8218413 0.9888198 ## sigma 5.1041375 0.19260589 4.7519768 5.5018477 Like before, the uniform prior required extensive warmup iterations to produce a good posterior. This is easily fixed using a half Cauchy prior, instead. Anyways, the effective samples improved. Here’s the parameter correlation info. posterior_samples(b4.4) %&gt;% select(b_Intercept:sigma) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight_c sigma ## b_Intercept 1.00 -0.02 0.02 ## b_weight_c -0.02 1.00 0.00 ## sigma 0.02 0.00 1.00 See? Now all the correlations are quite low. If you prefer a visual approach, try executing pairs(b4.4). Amidst all this talk about tables, I haven’t actually shown how to put these parameter summaries in a table. Here’s an example of how you might convert the posterior_summary() output into a summary table roughly following APA style. posterior_summary(b4.4)[1:3, ] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;) %&gt;% mutate_if(is.double, round, digits = 2) %&gt;% rename(mean = Estimate, sd = Est.Error) %&gt;% mutate(`95% CI` = str_c(&quot;[&quot;, Q2.5, &quot;, &quot;, Q97.5, &quot;]&quot;)) %&gt;% select(-starts_with(&quot;Q&quot;)) %&gt;% knitr::kable() parameter mean sd 95% CI b_Intercept 154.6 0.28 [154.06, 155.16] b_weight_c 0.9 0.04 [0.82, 0.99] sigma 5.1 0.19 [4.75, 5.5] This, of course, is just one way to present the summary information. Hopefully it’s a useful start. 4.4.3.2 Plotting posterior inference against the data. In truth, tables of estimates are usually insufficient for understanding the information contained in the posterior distribution. It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, bit it also provides an informal check on model assumptions. (p. 100) Here is the code for Figure 4.4. Note our use of the fixef() function. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + theme_bw() + theme(panel.grid = element_blank()) In the brms reference manual, Bürkner described the job of thefixef() function as “extract[ing] the population-level (‘fixed’) effects from a brmsfit object”. If you’re new to multilevel models, it might not be clear what he meant by “population-level” or “fixed” effects. Don’t worry. That’ll all become clear starting around Chapter 12. In the meantime, just think of them as the typical regression parameters, minus \\(\\sigma\\). 4.4.3.3 Adding uncertainty around the mean. Be default, we extract all the posterior iterations with posterior_samples(). Because we had 4,000 posterior draws, our output will contain 4,000 rows. post &lt;- posterior_samples(b4.3) post %&gt;% glimpse() ## Observations: 4,000 ## Variables: 4 ## $ b_Intercept &lt;dbl&gt; 114.6966, 114.1741, 113.2510, 114.2457, 115.5680, 115.6146, 113.6150, 113.530… ## $ b_weight &lt;dbl&gt; 0.8799084, 0.8969781, 0.9233995, 0.8921552, 0.8721226, 0.8737335, 0.9049578, … ## $ sigma &lt;dbl&gt; 4.982787, 5.315007, 4.621600, 5.610188, 5.597205, 5.287356, 5.234490, 4.80125… ## $ lp__ &lt;dbl&gt; -1083.088, -1082.824, -1085.849, -1085.621, -1085.679, -1083.558, -1082.850, … Each row is a correlated random sample from the point posterior of all three parameters, using the covariances provided by [cov(posterior_samples(b4.4)]. The paired values of [b_Intercept] and [b_weight] on each row define the line. The average of very many of these lines is the MAP line. (p. 101) Here are the four models leading up to McElreath’s Figure 4.5. To reduce my computation time, I used a cauchy(0, 1) prior on \\(\\sigma\\). If you are willing to wait for the warmups, switching that out for McElreath’s uniform prior should work fine as well. With the exception of that \\(\\sigma\\) prior, these models are all variations on b4.3 from above, which we’ll reflect in our naming convention. n &lt;- 10 b4.3_010 &lt;- brm(data = d2 %&gt;% slice(1:n), # note our tricky use of `n` and `slice()` family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_010&quot;) n &lt;- 50 b4.3_050 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_050&quot;) n &lt;- 150 b4.3_150 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_150&quot;) n &lt;- 352 b4.3_352 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_352&quot;) I’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information. plot(b4.3_010) print(b4.3_010) plot(b4.3_050) print(b4.3_050) plot(b4.3_150) print(b4.3_150) plot(b4.3_352) print(b4.3_352) We’ll need to put the chains of each model into data frames. post010 &lt;- posterior_samples(b4.3_010) post050 &lt;- posterior_samples(b4.3_050) post150 &lt;- posterior_samples(b4.3_150) post352 &lt;- posterior_samples(b4.3_352) Here is the code for the four individual plots. p1 &lt;- ggplot(data = d2[1:10 , ], aes(x = weight, y = height)) + geom_abline(intercept = post010[1:20, 1], slope = post010[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 10&quot;) p2 &lt;- ggplot(data = d2[1:50 , ], aes(x = weight, y = height)) + geom_abline(intercept = post050[1:20, 1], slope = post050[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 50&quot;) p3 &lt;- ggplot(data = d2[1:150 , ], aes(x = weight, y = height)) + geom_abline(intercept = post150[1:20, 1], slope = post150[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 150&quot;) p4 &lt;- ggplot(data = d2[1:352 , ], aes(x = weight, y = height)) + geom_abline(intercept = post352[1:20, 1], slope = post352[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 352&quot;) Note how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %&gt;% slice(1:10) instead. Now we can combine the ggplots with patchwork syntax to make the full version of Figure 4.5. (p1 + p2 + p3 + p4) &amp; theme_bw() &amp; theme(panel.grid = element_blank()) 4.4.3.4 Plotting regression intervals and contours. Remember, if you want to plot McElreath’s mu_at_50 with ggplot2, you’ll need to save it as a data frame or a tibble. mu_at_50 &lt;- post %&gt;% transmute(mu_at_50 = b_Intercept + b_weight * 50) head(mu_at_50) ## mu_at_50 ## 1 158.6920 ## 2 159.0230 ## 3 159.4210 ## 4 158.8535 ## 5 159.1741 ## 6 159.3013 And here is a version McElreath’s Figure 4.6 density plot. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() We’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean. mean_hdi(mu_at_50[,1], .width = c(.89, .95)) ## y ymin ymax .width .point .interval ## 1 159.1394 158.6182 159.7260 0.89 mean hdi ## 2 159.1394 158.4807 159.8337 0.95 mean hdi If you wanted to express those sweet 95% HPDIs on your density plot, you might use the tidybayes::geom_halfeyeh() function. Since geom_halfeyeh() also returns a point estimate, we’ll just throw in the mode. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = .95, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() In brms, you would use fitted() to do what McElreath accomplished with link() in R code 4.53. mu &lt;- fitted(b4.3, summary = F) str(mu) ## num [1:4000, 1:352] 157 157 157 157 157 ... When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your data. Because we had 4,000 post-warmup iterations and \\(n\\) = 352, fitted() returned a matrix of 4,000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead. Much like rethinking’s link(), fitted() can accommodate custom predictor values with its newdata argument. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) mu &lt;- fitted(b4.3, summary = F, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # here we name the columns after the `weight` values from which they were computed set_names(25:70) %&gt;% mutate(iter = 1:n()) str(mu) Anticipating ggplot2, we went ahead and converted the output to a tibble. But we might do a little more data processing with the aid of tidyr::gather(). With the gather() function, we’ll convert the data from the wide format to the long format. If you’re new to the distinction between wide and long data, you can learn more here or here. mu &lt;- mu %&gt;% gather(weight, height, -iter) %&gt;% # we might reformat `weight` to numerals mutate(weight = as.numeric(weight)) head(mu) ## # A tibble: 6 x 3 ## iter weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25 137. ## 2 2 25 137. ## 3 3 25 136. ## 4 4 25 137. ## 5 5 25 137. ## 6 6 25 137. That’s enough data processing. Here we reproduce McElreath’s Figure 4.7.a. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), alpha = .1) # or prettied up a bit d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), color = &quot;navyblue&quot;, alpha = .05) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) With fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = F argument. mu_summary &lt;- fitted(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # let&#39;s tack on the `weight` values from `weight_seq` bind_cols(weight_seq) head(mu_summary) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 136. 0.899 135. 138. 25 ## 2 137. 0.858 136. 139. 26 ## 3 138. 0.817 137. 140. 27 ## 4 139. 0.777 138. 141. 28 ## 5 140. 0.737 139. 142. 29 ## 6 141. 0.697 140. 142. 30 Here it is, our analogue to Figure 4.7.b. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) If you wanted to use intervals other than the default 95% ones, you’d enter a probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(.25, .75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile. 4.4.3.4.1 Overthinking: How link fitted() works. Similar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. For some quick documentation, execute ?fitted.brmsfit. 4.4.3.5 Prediction intervals. Even though our full statistical model (omitting priors for the sake of simplicity) is \\[h_i \\sim \\text{Normal}(\\mu_i = \\alpha + \\beta x_i, \\sigma),\\] we’ve only been plotting the \\(\\mu_i\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to the predict() function. Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim(). We can reuse our weight_seq data from before. But in case you forgot, here’s that code again. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) The predict() code looks a lot like what we used for fitted(). pred_height &lt;- predict(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_height %&gt;% slice(1:6) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 136. 5.12 126. 146. 25 ## 2 138. 5.15 128. 147. 26 ## 3 138. 5.10 128. 148. 27 ## 4 139. 5.08 129. 149. 28 ## 5 140. 5.14 130. 150. 29 ## 6 141. 5.14 131. 151. 30 This time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of \\(\\mu\\) AND \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Here’s our plot for Figure 4.8. d2 %&gt;% ggplot(aes(x = weight)) + geom_ribbon(data = pred_height, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + ylab(&quot;height&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5 Polynomial regression Remember d? d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 14… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0… The quadratic is probably the most commonly-used polynomial regression model. It follows the form \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2.\\] McElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so. d &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight)) / sd(weight)) Here’s the quadratic model in brms. b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.05&quot;) plot(b4.5) print(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + I(weight_s^2) ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 146.66 0.38 145.91 147.40 1.00 3694 2958 ## weight_s 21.40 0.29 20.83 21.97 1.00 3404 3152 ## Iweight_sE2 -8.41 0.29 -9.00 -7.87 1.00 3426 3096 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.77 0.18 5.43 6.12 1.00 4112 2964 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our quadratic plot requires new fitted()- and predict()-oriented wrangling. weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) f &lt;- fitted(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p &lt;- predict(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) Behold the code for our version of Figure 4.9.a. You’ll notice how little the code changed from that for Figure 4.8, above. ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) From a formula perspective, the cubic model is a simple extenstion of the quadratic: \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3.\\] Fit it like so. b4.6 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.06&quot;) And now we’ll fit the good old linear model. b4.7 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.07&quot;) Here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.c, the cubic model. f &lt;- fitted(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p &lt;- predict(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.a, the linear model. f &lt;- fitted(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p &lt;- predict(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5.0.0.1 Overthinking: Converting back to natural scale. You can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the linear model. at &lt;- c(-2, -1, 0, 1, 2) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + # here it is! scale_x_continuous(&quot;standardized weight converted back&quot;, breaks = at, labels = round(at * sd(d$weight) + mean(d$weight), 1)) Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ## [5] StanHeaders_2.19.0 patchwork_1.0.0 forcats_0.4.0 stringr_1.4.0 ## [9] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.3 ## [22] dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 assertthat_0.2.1 ## [28] Matrix_1.2-18 fastmap_1.0.1 lazyeval_0.2.2 ## [31] cli_2.0.1 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.3.1 ## [40] reshape2_1.4.3 cellranger_1.1.0 vctrs_0.2.2 ## [43] nlme_3.1-142 crosstalk_1.0.0 xfun_0.12 ## [46] ps_1.3.0 rvest_0.3.5 mime_0.8 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 gtools_3.8.1 ## [52] MASS_7.3-51.4 zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [64] stringi_1.4.5 highr_0.8 dygraphs_1.1.1.6 ## [67] pkgbuild_1.0.6 rlang_0.4.4 pkgconfig_2.0.3 ## [70] matrixStats_0.55.0 HDInterval_0.2.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [79] plyr_1.8.5 magrittr_1.5 R6_2.4.1 ## [82] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 ## [85] haven_2.2.0 withr_2.1.2 xts_0.12-0 ## [88] abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_2.0 ## [94] grid_3.6.2 readxl_1.3.1 callr_3.4.1 ## [97] threejs_0.3.3 reprex_0.3.0 digest_0.6.23 ## [100] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.2 ## [103] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.1 "],
["multivariate-linear-models.html", "5 Multivariate Linear Models 5.1 Spurious associations 5.2 Masked relationship 5.3 Multicollinearity 5.4 Categorical variables 5.5 Ordinary least squares and lm() Reference Session info", " 5 Multivariate Linear Models McElreath’s listed reasons for multivariable regression include statistical control for confounds, multiple causation, and interactions. We’ll approach the first two in this chapter. Interactions are reserved for Chapter 7. 5.0.0.1 Rethinking: Causal inference. “Despite its central importance, there is no unified approach to causal inference yet in the sciences or in statistics” (p. 120). McElreath didn’t cover this much in this edition of the text. However, it appears his second edition will cover it more extensively. He announced as much in this blog post and experimented in mixing it in with the material in during his Statistical Rethinking Winter 2019 lecture series. To dip in, you might check out the recent blog post by Finn Lattimore and David Rohde, Causal Inference with Bayes Rule. 5.1 Spurious associations Load the Waffle House data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce Unload rethinking and load brms and, while we’re at it, the tidyverse. rm(WaffleDivorce) detach(package:rethinking, unload = T) library(brms) library(tidyverse) I’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g., head(d) glimpse(d) Now we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package. But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of &quot;firebrick&quot;. # install.packages(&quot;ggrepel&quot;, depencencies = T) library(ggrepel) d %&gt;% ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, size = 1/2, color = &quot;firebrick4&quot;, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + geom_text_repel(data = d %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;OK&quot;, &quot;AR&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;NJ&quot;)), aes(label = Loc), size = 3, seed = 1042) + # this makes it reproducible scale_x_continuous(&quot;Waffle Houses per million&quot;, limits = c(0, 55)) + ylab(&quot;Divorce rate&quot;) + coord_cartesian(xlim = 0:50, ylim = 5:15) + theme_bw() + theme(panel.grid = element_blank()) Since these are geographically-based data, we might plot our three major variables in a map format. The urbnmapr package provides latitude and longitude data for the 50 states and the geom_sf() function for plotting them. We’ll use the left_join() function to combine those data with our primary data d. # devtools::install_github(&quot;UrbanInstitute/urbnmapr&quot;) library(urbnmapr) left_join( # get the map data get_urbn_map(map = &quot;states&quot;, sf = TRUE), # add the primary data d %&gt;% # tandardize the three variables to put them all on the same scale mutate(Divorce_z = (Divorce - mean(Divorce)) / sd(Divorce), MedianAgeMarriage_z = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage), Marriage_z = (Marriage - mean(Marriage)) / sd(Marriage), # convert the state names to characters to match the map data state_name = Location %&gt;% as.character()) %&gt;% select(Divorce_z:Marriage_z:state_name), by = &quot;state_name&quot; ) %&gt;% # convert to the long format for faceting pivot_longer(ends_with(&quot;_z&quot;)) %&gt;% # plot! ggplot() + geom_sf(aes(fill = value, geometry = geometry), size = 0) + scale_fill_gradient(low = &quot;#f8eaea&quot;, high = &quot;firebrick4&quot;) + theme_void() + theme(legend.position = &quot;none&quot;, strip.text = element_text(margin = margin(0, 0, .5, 0))) + facet_wrap(~name) One of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d %&gt;% distinct(Location) to see for yourself and click here to find out why it’s missing. Those missing data should motivate the skills we’ll cover in Chapter 14. But let’s get back on track. Here we’ll officially standardize the predictor, MedianAgeMarriage. d &lt;- d %&gt;% mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage)) Now we’re ready to fit the first univariable model. b5.1 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.01&quot;) Check the summary. print(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.69 0.21 9.27 10.10 1.00 4693 4019 ## MedianAgeMarriage_s -1.04 0.21 -1.45 -0.61 1.00 5462 4347 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.52 0.16 1.24 1.88 1.00 5362 4133 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll employ fitted() to make Figure 5.2.b. In preparation for fitted() we’ll make a new tibble, nd, composed of a handful of densely-packed values for our predictor, MedianAgeMarriage_s. With the newdata argument, we’ll use those values to return model-implied expected values for Divorce. # define the range of `MedianAgeMarriage_s` values we&#39;d like to feed into `fitted()` nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories f &lt;- fitted(b5.1, newdata = nd) %&gt;% as_tibble() %&gt;% # tack the `nd` data onto the `fitted()` results bind_cols(nd) # plot ggplot(data = f, aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;Divorce&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = range(d$Divorce)) + theme_bw() + theme(panel.grid = element_blank()) Before fitting the next model, we’ll standardize Marriage. d &lt;- d %&gt;% mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage)) We’re ready to fit our second univariable model, this time with Marriage_s as the predictor. b5.2 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.02&quot;) print(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.69 0.25 9.20 10.18 1.00 5779 4511 ## Marriage_s 0.64 0.25 0.16 1.14 1.00 5562 4279 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.75 0.18 1.44 2.14 1.00 5724 4593 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ll wangle and plot our version of Figure 5.2.a. nd &lt;- tibble(Marriage_s = seq(from = -2.5, to = 3.5, length.out = 30)) f &lt;- fitted(b5.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ggplot(data = f, aes(x = Marriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;Divorce&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = range(d$Divorce)) + theme_bw() + theme(panel.grid = element_blank()) But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other. So we’ll build a multivariate model with the goal of measuring the partial value of each predictor. The question we want answered is: What is the predictive value of a variable, once I already know all of the other predictor variables? (p. 123, emphasis in the original) 5.1.1 Multivariate notation. Now we’ll get both predictors in there with our very first multivariable model. We can write the statistical model as \\[\\begin{align*} \\text{Divorce}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{Marriage_s}_i + \\beta_2 \\text{MedianAgeMarriage_s}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(10, 10) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\operatorname{Uniform}(0, 10). \\end{align*}\\] It might help to read the \\(+\\) symbols as “or” and then say: A State’s divorce rate can be a function of its marriage rate or its median age at marriage. The “or” indicates independent associations, which may be purely statistical or rather causal. (p. 124, emphasis in the original) 5.1.2 Fitting the model. Much like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on. b5.3 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.03&quot;) Our multivariable summary will have multiple rows below the ‘Intercept’ row. print(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.69 0.22 9.26 10.12 1.00 6035 4188 ## Marriage_s -0.12 0.29 -0.68 0.45 1.00 4378 3861 ## MedianAgeMarriage_s -1.12 0.29 -1.68 -0.52 1.00 4446 3988 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.52 0.17 1.24 1.90 1.00 5384 4121 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The mcmc_plot() function is an easy way to get a default coefficient plot. You just put the brmsfit object into the function. mcmc_plot(b5.3) There are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package. A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the posterior_samples() instead of the brmsfit object. # install.packages(&quot;bayesplot&quot;, dependencies = T) library(bayesplot) post &lt;- posterior_samples(b5.3) color_scheme_set(&quot;red&quot;) mcmc_intervals(post[, 1:4], prob = .5, point_est = &quot;median&quot;) + ggtitle(&quot;My fancy bayesplot-based coefficient plot&quot;) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Because bayesplot produces a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out this vignette. The tidybaes::stat_pointintervalh() function offers a third way, this time with a more ground-up ggplot2 workflow. library(tidybayes) post %&gt;% select(b_Intercept:sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, size = 3/4, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = NULL, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank(), panel.grid.major.y = element_line(color = alpha(&quot;firebrick4&quot;, 1/4), linetype = 3)) The substantive interpretation of all those coefficient plots is: “Once we know median age at marriage for a State, there is little or no additive predictive power in also knowing the rate of marriage in that State” (p. 126, emphasis in the original). 5.1.3 Plotting multivariate posteriors. McElreath’s prose is delightfully deflationary. “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (p. 126). Now you’re inspired, let’s learn three: Predictor residual plots, Counterfactual plots, and Posterior prediction plots. 5.1.3.1 Predictor residual plots. To get ready to make our residual plots, we’ll predict Marriage_s with MedianAgeMarriage_s. b5.4 &lt;- brm(data = d, family = gaussian, Marriage_s ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.04&quot;) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Marriage_s ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.10 -0.20 0.20 1.00 5747 4226 ## MedianAgeMarriage_s -0.71 0.10 -0.92 -0.51 1.00 6011 4459 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.72 0.08 0.59 0.89 1.00 4927 4208 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With fitted(), we compute the expected values for each state (with the exception of Nevada). Since the MedianAgeMarriage_s values for each state are in the date we used to fit the model, we’ll omit the newdata argument. f &lt;- fitted(b5.4) %&gt;% as_tibble() %&gt;% bind_cols(d) head(f) ## # A tibble: 6 x 19 ## Estimate Est.Error Q2.5 Q97.5 Location Loc Population MedianAgeMarria… Marriage Marriage.SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.432 0.117 2.07e-1 0.663 Alabama AL 4.78 25.3 20.2 1.27 ## 2 0.489 0.121 2.54e-1 0.729 Alaska AK 0.71 25.2 26 2.93 ## 3 0.146 0.102 -5.68e-2 0.347 Arizona AZ 6.33 25.8 20.3 0.98 ## 4 1.00 0.175 6.62e-1 1.34 Arkansas AR 2.92 24.3 26.4 1.7 ## 5 -0.427 0.120 -6.52e-1 -0.190 Califor… CA 37.2 26.8 19.1 0.39 ## 6 0.203 0.104 -3.81e-4 0.407 Colorado CO 5.03 25.7 23.5 1.24 ## # … with 9 more variables: Divorce &lt;dbl&gt;, Divorce.SE &lt;dbl&gt;, WaffleHouses &lt;int&gt;, South &lt;int&gt;, ## # Slaves1860 &lt;int&gt;, Population1860 &lt;int&gt;, PropSlaves1860 &lt;dbl&gt;, MedianAgeMarriage_s &lt;dbl&gt;, ## # Marriage_s &lt;dbl&gt; After a little data processing, we can make Figure 5.3. f %&gt;% ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + coord_cartesian(ylim = range(d$Marriage_s)) + theme_bw() + theme(panel.grid = element_blank()) We get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual for details. With our residuals in hand, we just need a little more data processing to make Figure 5.4.a. r &lt;- residuals(b5.4) %&gt;% # to use this in ggplot2, we need to make it a tibble or data frame as_tibble() %&gt;% bind_cols(d) # for the annotation at the top text &lt;- tibble(Estimate = c(- 0.5, 0.5), Divorce = 14.1, label = c(&quot;slower&quot;, &quot;faster&quot;)) # plot r %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Marriage rate residuals&quot;, limits = c(-2, 2)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) To get the MedianAgeMarriage_s residuals, we have to fit the corresponding model first. b5.4b &lt;- brm(data = d, family = gaussian, MedianAgeMarriage_s ~ 1 + Marriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.04b&quot;) And now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to Figure 5.4.b. # redefine the annotation data text &lt;- tibble(Estimate = c(- 0.7, 0.5), Divorce = 14.1, label = c(&quot;younger&quot;, &quot;older&quot;)) # extract the residuals residuals(b5.4b) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% # plot ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Age of marriage residuals&quot;, limits = c(-2, 3)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) So what’s the point of all this? There’s direct value in seeing the model-based predictions displayed against the outcome, after subtracting out the influence of other predictors. The plots in Figure 5.4 do this. But this procedure also brings home the message that regression models answer with the remaining association of each predictor with the outcome, after already knowing the other predictors. In computing the predictor residual plots, you had to perform those calculations yourself. In the unified [multivariable] model, it all happens automatically. (p. 129) 5.1.3.2 Counterfactual plots. A second sort of inferential plot displays the implied predictions of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variable you like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 129, emphasis in the original) Making Figure 5.5.a requires a little more data wrangling than before. # we need new `nd` data nd &lt;- tibble(Marriage_s = seq(from = -3, to = 3, length.out = 30), MedianAgeMarriage_s = mean(d$MedianAgeMarriage_s)) fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since `fitted()` and `predict()` name their intervals the same way, # we&#39;ll need to `rename()` them to keep them straight rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% # note how we&#39;re just nesting the `predict()` code right inside `bind_cols()` bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since we only need the intervals, we&#39;ll use `transmute()` rather than `mutate()` transmute(p_ll = Q2.5, p_ul = Q97.5), # now tack on the `nd` data nd) %&gt;% # we&#39;re finally ready to plot ggplot(aes(x = Marriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(subtitle = &quot;Counterfactual plot for which\\nMedianAgeMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = c(6, 14)) + theme_bw() + theme(panel.grid = element_blank()) We follow the same process for Figure 5.5.b. # new data nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30), Marriage_s = mean(d$Marriage_s)) # `fitted()` + `predict()` fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), nd ) %&gt;% # plot ggplot(aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(subtitle = &quot;Counterfactual plot for which\\nMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = c(6, 14)) + theme_bw() + theme(panel.grid = element_blank()) A tension with such plots, however, lies in their counterfactual nature. In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not… …If our goal is to intervene in the world, there may not be any realistic way to manipulate each predictor without also manipulating the others. This is a serious obstacle to applied science, whether you are an ecologist, an economist, or an epidemiologist [or a psychologist] (p. 131) 5.1.3.3 Posterior prediction plots. “In addition to understanding the estimates, it’s important to check the model fit against the observed data” (p. 131). For more on the topic, check out Gabry and colleagues’ Visualization in Bayesian workflow or Simpson’s related blog post, Touch me, I want to feel your data. In this version of Figure 5.6.a, the thin lines are the 95% intervals and the thicker lines are \\(\\pm\\) the posterior \\(SD\\), both of which are returned when you use fitted(). p1 &lt;- fitted(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Divorce, y = Estimate)) + geom_abline(linetype = 2, color = &quot;grey50&quot;, size = .5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 3/4) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), size = 1/4, color = &quot;firebrick4&quot;) + geom_linerange(aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), size = 1/2, color = &quot;firebrick4&quot;) + # Note our use of the dot placeholder, here: https://magrittr.tidyverse.org/reference/pipe.html geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)), aes(label = Loc), hjust = 0, nudge_x = - 0.65) + labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) p1 In order to make Figure 5.6.b, we need to clarify the relationships among fitted(), predict(), and residuals(). Here’s my attempt in a table. tibble(`brms function` = c(&quot;fitted&quot;, &quot;predict&quot;, &quot;residual&quot;), mean = c(&quot;same as the data&quot;, &quot;same as the data&quot;, &quot;in a deviance-score metric&quot;), scale = c(&quot;excludes sigma&quot;, &quot;includes sigma&quot;, &quot;excludes sigma&quot;)) %&gt;% knitr::kable() brms function mean scale fitted same as the data excludes sigma predict same as the data includes sigma residual in a deviance-score metric excludes sigma Hopefully that clarified that if we want to incorporate the prediction interval in a deviance metric, we’ll need to first use predict() and then subtract the intervals from their corresponding Divorce values in the data. p2 &lt;- residuals(b5.3) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), d ) %&gt;% # here we put our `predict()` intervals into a deviance metric mutate(p_ll = Divorce - p_ll, p_ul = Divorce - p_ul) %&gt;% # now plot! ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) + geom_hline(yintercept = 0, size = 1/2, color = &quot;firebrick4&quot;, alpha = 1/10) + geom_pointrange(aes(ymin = f_ll, ymax = f_ul), size = 2/5, shape = 20, color = &quot;firebrick4&quot;) + geom_segment(aes(y = Estimate - Est.Error, yend = Estimate + Est.Error, x = Loc, xend = Loc), size = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(y = p_ll, yend = p_ul, x = Loc, xend = Loc), size = 3, color = &quot;firebrick4&quot;, alpha = 1/10) + labs(x = NULL, y = NULL) + coord_flip(ylim = c(-6, 5)) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) p2 Compared to the last couple plots, Figure 5.6.c is pretty simple. p3 &lt;- residuals(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(wpc = WaffleHouses / Population) %&gt;% ggplot(aes(x = wpc, y = Estimate)) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, size = 1/2, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;AR&quot;, &quot;MS&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;ID&quot;)), aes(label = Loc), seed = 5.6) + scale_x_continuous(&quot;Waffles per capita&quot;, limits = c(0, 45)) + ylab(&quot;Divorce error&quot;) + coord_cartesian(xlim = range(0, 40)) + theme_bw() + theme(panel.grid = element_blank()) p3 For the sake of good practice, let’s use patchwork syntax to combine those three subplots like they appear in the text. library(patchwork) ((p1 / p3) | p2) + plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) More McElreath inspiration: “No matter how many predictors you’ve already included in a regression, it’s still possible to find spurious correlations with the remaining variation” (p. 134). 5.1.3.3.1 Rethinking: Stats, huh, yeah what is it good for? To keep our deflation train going, it’s worthwhile repeating this message: Often people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is real or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of another predictor, we might never notice this. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 134) 5.1.3.3.2 Overthinking: Simulating spurious association. Simulate the spurious predictor data. n &lt;- 100 # number of cases set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_real = rnorm(n), # x_real as Gaussian with mean 0 and SD 1 (i.e., the defaults) x_spur = rnorm(n, x_real), # x_spur as Gaussian with mean = x_real y = rnorm(n, x_real)) # y as Gaussian with mean = x_real Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) We may as well fit a model. b5.0_spur &lt;- brm(data = d, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.00_spur&quot;) fixef(b5.0_spur) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.00 0.10 -0.20 0.20 ## x_real 0.98 0.15 0.69 1.28 ## x_spur 0.06 0.10 -0.13 0.25 5.2 Masked relationship A second reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This kind of problem tends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it. (p. 134) Let’s load the Hindle and Milligan (2011) milk data. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) You might inspect the data like this. d %&gt;% select(kcal.per.g, mass, neocortex.perc) %&gt;% pairs(col = &quot;firebrick4&quot;) By just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math. McElreath has us start of with a simple univariable milk model. b5.5 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex.perc, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.05&quot;) The uniform prior was difficult on Stan. After playing around a bit, I just switched to a unit-scale half Cauchy. Similar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 14 where we’ll learn how to do better. To compliment how McElreath removed cases with missing values on our variables of interest with base R complete.cases(), here we’ll do so with tidyr::drop_na() and a little help with ends_with(). dcc &lt;- d %&gt;% drop_na(ends_with(&quot;_s&quot;)) But anyway, let’s inspect the parameter summary. print(b5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc ## Data: d (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.344 0.551 -0.728 1.419 1.001 4674 3808 ## neocortex.perc 0.005 0.008 -0.011 0.021 1.001 4702 3749 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.192 0.038 0.135 0.284 1.001 2988 3315 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice now we set digits = 3 within print() much the way McElreath set digits=3 within precis()? To get the brms answer to what McElreath did with coef(), we can use the fixef() function. fixef(b5.5)[2] * (76 - 55) ## [1] 0.0976635 Yes, indeed, “that’s less than 0.1 kilocalories” (p. 137). Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.7, top left. nd &lt;- tibble(neocortex.perc = 54:80) fitted(b5.5, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + theme_bw() + theme(panel.grid = element_blank()) Do note the probs argument in the fitted() code, above. Let’s make the log_mass variable. dcc &lt;- dcc %&gt;% mutate(log_mass = log(mass)) Now we use log_mass as the new sole predictor. b5.6 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 5, file = &quot;fits/b05.06&quot;) print(b5.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.706 0.058 0.592 0.818 1.000 4982 3620 ## log_mass -0.032 0.024 -0.079 0.017 1.000 4836 3524 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.182 0.037 0.128 0.269 1.000 4670 3449 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, top right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30)) fitted(b5.6, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + theme_bw() + theme(panel.grid = element_blank()) Finally, we’re ready to fit with both predictors included in the “joint model.” Here’s the statistical formula: \\[\\begin{align*} \\text{kcal.per.g}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex.perc}_i + \\beta_2 \\log (\\text{mass}_i) \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 100) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\operatorname{Uniform}(0, 1). \\end{align*}\\] Note, the HMC chains required a longer warmup period and a higher adapt_delta setting for the model to converge properly. Life will be much better once we ditch the uniform prior for good. b5.7 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + neocortex.perc + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), seed = 5, file = &quot;fits/b05.07&quot;) print(b5.7, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.089 0.577 -2.220 0.085 1.002 3299 4186 ## neocortex.perc 0.028 0.009 0.010 0.046 1.002 3217 4104 ## log_mass -0.096 0.027 -0.150 -0.041 1.001 3305 4155 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.139 0.029 0.096 0.209 1.002 3356 4010 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, bottom left. nd &lt;- tibble(neocortex.perc = 54:80 %&gt;% as.double(), log_mass = mean(dcc$log_mass)) p1 &lt;- b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) Now make Figure 5.7, bottom right, and combine. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30), neocortex.perc = mean(dcc$neocortex.perc)) p2 &lt;- b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) (p1 | p2) &amp; theme_bw() &amp; theme(panel.grid = element_blank()) What [this regression model did was] ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model [asked] if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we statistically account for both. (pp. 140–141, emphasis in the original) 5.2.0.1 Overthinking: Simulating a masking relationship. Simulate the data. n &lt;- 100 # number of cases rho &lt;- .7 # correlation between x_pos and x_neg set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_pos = rnorm(n), # x_pos as a standard Gaussian x_neg = rnorm(n, rho * x_pos, sqrt(1 - rho^2)), # x_neg correlated with x_pos y = rnorm(n, x_pos - x_neg)) # y equally associated with x_pos and x_neg Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) Here we fit the models with a little help from the update() function. b5.0_both &lt;- brm(data = d, family = gaussian, y ~ 1 + x_pos + x_neg, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), seed = 5, file = &quot;fits/b05.00_both&quot;) b5.0_pos &lt;- update(b5.0_both, formula = y ~ 1 + x_pos, seed = 5, file = &quot;fits/b05.00_pos&quot;) b5.0_neg &lt;- update(b5.0_both, formula = y ~ 1 + x_neg, seed = 5, file = &quot;fits/b05.00_neg&quot;) Compare the coefficients. fixef(b5.0_pos) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.12 -0.25 0.22 ## x_pos 0.26 0.13 0.01 0.51 fixef(b5.0_neg) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.01 0.12 -0.23 0.23 ## x_neg -0.29 0.11 -0.51 -0.06 fixef(b5.0_both) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.10 -0.20 0.19 ## x_pos 0.96 0.15 0.67 1.26 ## x_neg -0.90 0.13 -1.15 -0.63 If you move the value of rho closer to zero, this masking phenomenon will diminish. I you make rho closer to 1 or -1, it will magnify. But if rho gets very close to 1 or -1, then the two predictors contain exactly the same information, and there’s no hope for any statistical model to tease out the true underlying association used in the simulation. (p. 141) 5.3 Multicollinearity Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will say that a very large range of parameter values are plausible, from tiny associations to massive ones, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how statistical control works. So once you understand multicollinearity, you will better understand [multivariable] models in general. (pp. 141–142) 5.3.1 Multicollinear legs. Let’s simulate some leg data. n &lt;- 100 set.seed(5) d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) leg_left and leg_right are highly correlated. d %&gt;% select(leg_left:leg_right) %&gt;% cor() %&gt;% round(digits = 4) ## leg_left leg_right ## leg_left 1.0000 0.9996 ## leg_right 0.9996 1.0000 Have you ever even seen a \\(\\rho = .9996\\) correlation, before? Here it is in a plot. d %&gt;% ggplot(aes(x = leg_left, y = leg_right)) + geom_point(alpha = 1/2, color = &quot;firebrick4&quot;) + theme_bw() + theme(panel.grid = element_blank()) Here’s our attempt to predict height with both legs in the model. b5.8 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.08&quot;) Let’s inspect the damage. print(b5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left + leg_right ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.79 0.29 1.23 2.38 1.00 5754 4367 ## leg_left 0.69 2.16 -3.58 4.86 1.00 1762 2374 ## leg_right 1.15 2.17 -3.05 5.40 1.00 1764 2407 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.61 0.04 0.53 0.70 1.00 3422 3383 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior [estimates] and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 143). Here’s our coefficient plot using brms::mcmc_plot(). mcmc_plot(b5.8, type = &quot;intervals&quot;, prob = .5, prob_outer = .95, point_est = &quot;median&quot;) + labs(title = &quot;The coefficient plot for the two-leg model&quot;, subtitle = &quot;Holy smokes; look at the widths of those betas!&quot;) + theme_bw() + theme(text = element_text(size = 14), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) This is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients, Figure 6.2.a. pairs(b5.8, pars = parnames(b5.8)[2:3]) If you’d like a nicer and more focused attempt, you might have to revert to the posterior_samples() function and a little ggplot2 code. post &lt;- posterior_samples(b5.8) post %&gt;% ggplot(aes(x = b_leg_left, y = b_leg_right)) + geom_point(color = &quot;firebrick&quot;, alpha = 1/10, size = 1/3) + theme_bw() + theme(panel.grid = element_blank()) While we’re at it, you can make a similar plot with the mcmc_scatter() function. post %&gt;% mcmc_scatter(pars = c(&quot;b_leg_left&quot;, &quot;b_leg_right&quot;), size = 1/3, alpha = 1/10) + theme_bw() + theme(panel.grid = element_blank()) But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign. post %&gt;% select(b_leg_left:b_leg_right) %&gt;% cor() ## b_leg_left b_leg_right ## b_leg_left 1.0000000 -0.9995672 ## b_leg_right -0.9995672 1.0000000 On page 165, McElreath clarified that “from the computer’s perspective, this model is simply:” \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_2) x_i. \\end{align*}\\] Accordingly, here’s the posterior of the sum of the two regression coefficients, Figure 6.2.b. We’ll use tidybayes::geom_halfeyeh() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base. library(tidybayes) post %&gt;% ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = &quot;firebrick&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Sum the multicollinear coefficients&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;) + theme_bw() + theme(panel.grid = element_blank()) Now we fit the model after ditching one of the leg lengths. b5.9 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.09&quot;) print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.80 0.29 1.23 2.36 1.00 5377 4527 ## leg_left 1.84 0.06 1.72 1.96 1.00 5328 4379 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.60 0.04 0.53 0.70 1.00 6459 4737 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That posterior \\(SD\\) looks much better. Compare this density to the one in Figure 6.1.b. posterior_samples(b5.9) %&gt;% ggplot(aes(x = b_leg_left, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = &quot;firebrick&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Just one coefficient needed&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;, x = &quot;only b_leg_left, this time&quot;) + theme_bw() + theme(panel.grid = element_blank()) We might also use tidybayes::stat_pointintervalh() to compare the posterior of b_leg_left from b5.9 to the joint posterior b_leg_left + b_leg_right from b5.8. For kicks, we’ll depict the posteriors with tidybayes::stat_gradientintervalh() and use three levels of posterior intervals. bind_cols(post %&gt;% transmute(`b_leg_left + b_leg_right` = b_leg_left + b_leg_right), posterior_samples(b5.9) %&gt;% transmute(`only b_leg_left` = b_leg_left)) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = key)) + stat_gradientintervalh(.width = c(.5, .8, .95), fill = &quot;firebrick&quot;) + labs(x = NULL, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) The results are within simulation variance of one another. When two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such a case. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. (p. 145, emphasis in the original) 5.3.2 Multicollinear milk. Multicollinearity arises in real data, too. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = TRUE) library(brms) We’ll follow the text and fit the two univariable models, first. Note our use of the update() function. # `kcal.per.g` regressed on `perc.fat` b5.10 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + perc.fat, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.10&quot;) # `kcal.per.g` regressed on `perc.lactose` b5.11 &lt;- update(b5.10, newdata = d, formula = kcal.per.g ~ 1 + perc.lactose, iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.11&quot;) Compare the coefficient summaries. posterior_summary(b5.10)[1:3, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.301 0.039 0.224 0.377 ## b_perc.fat 0.010 0.001 0.008 0.012 ## sigma 0.080 0.012 0.061 0.106 posterior_summary(b5.11)[1:3, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.167 0.046 1.074 1.260 ## b_perc.lactose -0.011 0.001 -0.012 -0.009 ## sigma 0.067 0.010 0.051 0.089 If you’d like to get just the 95% intervals similar to the way McElreath reported them in the prose on page 146, you might use the handy posterior_interval() function. posterior_interval(b5.10)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## 0.008 0.012 posterior_interval(b5.11)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## -0.012 -0.009 Now “watch what happens when we place both predictor variables in the same regression model” (p. 146). b5.12 &lt;- update(b5.11, newdata = d, formula = kcal.per.g ~ 1 + perc.fat + perc.lactose, iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.12&quot;) The posteriors for coefficients, especially for perc.fat, shrank to zero. posterior_summary(b5.12)[1:4, ] %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.009 0.226 0.573 1.444 ## b_perc.fat 0.002 0.003 -0.003 0.007 ## b_perc.lactose -0.009 0.003 -0.014 -0.003 ## sigma 0.068 0.010 0.052 0.090 You can make also pairs plots with GGally, which will also compute the point estimates for the bivariate correlations. Here’s a default plot. #install.packages(&quot;GGally&quot;, dependencies = T) library(GGally) ggpairs(data = d, columns = c(3:4, 6)) + theme(panel.grid = element_blank()) But you can customize these, too. E.g., my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;firebrick4&quot;, size = 0) } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick4&quot;, size = 1/3, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(color = &quot;firebrick&quot;, alpha = .8, size = 1/4) } # then plug those custom functions into `ggpairs()` ggpairs(data = d, columns = c(3:4, 6), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank(), strip.background = element_rect(fill = &quot;white&quot;, color = &quot;white&quot;)) Our two predictor “variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps much once you already know the other” (p. 148, emphasis in the original). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also showed the Pearson’s correlation coefficients, se we don’t need to use the cor() function like McElreath did in the text. In the next section, we’ll run the simulation necessary for our version of Figure 5.10. 5.3.2.1 Overthinking: Simulating collinearity. First we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block. sim_coll &lt;- function(seed, rho) { set.seed(seed) d &lt;- d %&gt;% mutate(x = rnorm(n(), mean = perc.fat * rho, sd = sqrt((1 - rho^2) * var(perc.fat)))) m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d) sqrt(diag(vcov(m)))[2] # parameter SD } # how many simulations per `rho`-value would you like? n_seed &lt;- 100 # how many `rho`-values from 0 to .99 would you like to evaluate the process over? n_rho &lt;- 30 d &lt;- tibble(seed = 1:n_seed) %&gt;% expand(seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) %&gt;% group_by(rho) %&gt;% # we&#39;ll `summarise()` our output by the mean and 95% intervals summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) We’ve added 95% interval bands to our version of Figure 5.10. d %&gt;% ggplot(aes(x = rho, y = mean)) + geom_smooth(aes(ymin = ll, ymax = ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + labs(x = expression(rho), y = &quot;parameter SD&quot;) + coord_cartesian(ylim = c(0, .0072)) + theme_bw() + theme(panel.grid = element_blank()) Did you notice we used the base R lm() function to fit the models? As McElreath rightly pointed out, lm() presumes flat priors. Proper Bayesian modeling could improve on that. But then we’d have to wait for a whole lot of HMC chains to run and until our personal computers or the algorithms we use to fit our Bayesian models become orders of magnitude faster, we just don’t have time for that. 5.3.3 Post-treatment bias. It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph: seed and sprout plants measure heights apply different antifungal soil treatments (i.e., the experimental manipulation) measure (a) the heights and (b) the presence of fungus Based on the design, let’s simulate our data. n &lt;- 100 set.seed(5) d &lt;- tibble(treatment = rep(0:1, each = n / 2), fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4), h0 = rnorm(n, mean = 10, sd = 2), h1 = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) We’ll use head() to peek at the data. d %&gt;% head() ## # A tibble: 6 x 4 ## treatment fungus h0 h1 ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 12.9 18.9 ## 2 0 1 10.4 13.1 ## 3 0 1 12.0 13.5 ## 4 0 0 8.82 14.6 ## 5 0 0 9.78 14.2 ## 6 0 1 8.15 11.4 These data + the model were rough on Stan, at first, which spat out warnings about divergent transitions. The model ran fine after setting warmup = 1000 and adapt_delta = 0.99. b5.13 &lt;- brm(data = d, family = gaussian, h1 ~ 1 + h0 + treatment + fungus, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.99), seed = 5, file = &quot;fits/b05.13&quot;) print(b5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ 1 + h0 + treatment + fungus ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.12 0.55 3.06 5.21 1.00 3184 2442 ## h0 1.09 0.05 0.99 1.20 1.00 3379 2517 ## treatment 0.00 0.22 -0.42 0.42 1.00 1101 1255 ## fungus -2.93 0.23 -3.39 -2.47 1.00 2529 2329 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.00 0.07 0.87 1.16 1.00 4022 2715 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now fit the model after excluding fungus, our post-treatment variable. b5.14 &lt;- update(b5.13, formula = h1 ~ 1 + h0 + treatment, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.14&quot;) print(b5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 + treatment ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.64 0.89 1.94 5.44 1.00 3341 2562 ## h0 1.00 0.09 0.83 1.16 1.00 3415 2870 ## treatment 0.87 0.34 0.20 1.52 1.00 3891 2346 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.67 0.13 1.45 1.94 1.00 3734 2876 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “Now the impact of treatment is strong and positive, as it should be” (p. 152). In this case, there were really two outcomes. The first was the one we modeled, the height at the end of the experiment (i.e., h1). The second outcome, which was clearly related to h1, was the presence of fungus, captured by our binomial variable fungus. If you wanted to model that, you’d fit a logistic regression model, which we’ll learn about in Chapter 10. 5.4 Categorical variables Many readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are included in a model… Knowing how the machine works removes a lot of this difficulty. (p. 153, emphasis in the original) 5.4.1 Binary categories. Reload the Howell1 data. library(rethinking) data(Howell1) d &lt;- Howell1 Unload rethinking and load brms. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Just in case you forgot what these data were like: d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 14… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0… Let’s fit the first height model with the male dummy. Note. The uniform prior McElreath used in the text in conjunction with the brms::brm() function seemed to cause problems for the HMC chains, here. After experimenting with start values, increasing warmup, and increasing adapt_delta, switching out the uniform prior did the trick. Anticipating Chapter 8, I recommend you use a weakly-regularizing half Cauchy for \\(\\sigma\\). b5.15 &lt;- brm(data = d, family = gaussian, height ~ 1 + male, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.15&quot;) “To interpret these estimates, you have to note that the parameter \\(\\alpha\\) ([Intercept]) is now the average height among females” (p. 154). print(b5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + male ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 134.82 1.59 131.65 137.90 1.00 6178 4612 ## male 7.29 2.28 2.78 11.73 1.00 6190 3872 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 27.39 0.82 25.85 29.06 1.00 6445 4737 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our samples from the posterior are already in the HMC iterations. All we need to do is put them in a data frame and we’ll be well-positioned to compute the intervals for the average height among men. post &lt;- posterior_samples(b5.15) post %&gt;% transmute(male_height = b_Intercept + b_male) %&gt;% mean_qi(.width = .89) ## male_height .lower .upper .width .point .interval ## 1 142.1093 139.445 144.7136 0.89 mean qi You can also do this with fitted(). nd &lt;- tibble(male = 1) fitted(b5.15, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 142.1093 1.657389 138.8337 145.2833 And you could even plot. fitted(b5.15, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = &quot;firebrick4&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied male heights&quot;, x = expression(alpha + beta[&quot;male&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.1.1 Overthinking: Re-parameterizing the model. The reparameterized model follows the form \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_\\text{female} (1 - \\text{male}_i) + \\alpha_\\text{male} \\text{male}_i. \\end{align*}\\] So then a female dummy would satisfy the condition \\(\\text{female}_i = (1 - \\text{male}_i)\\). Let’s make that dummy. d &lt;- d %&gt;% mutate(female = 1 - male) Everyone has their own idiosyncratic way of coding. One of my quirks is I always explicitly specify a model’s intercept following the form y ~ 1 + x, where y is the criterion, x stands for the predictors, and 1 is the intercept. You don’t have to do this, of course. You could just code y ~ x to get the same results. The brm() function assumes you want that intercept. One of the reasons I like the verbose version is it reminds me to think about the intercept and to include it in my priors. Another nice feature is that is helps me make sense of the code for this model: height ~ 0 + male + female. When we replace … ~ 1 + … with … ~ 0 + …, we tell brm() to remove the intercept. Removing the intercept allows us to include ALL levels of a given categorical variable in our model. In this case, we’ve expressed sex as two dummies, female and male. Taking out the intercept lets us put both dummies into the formula. b5.15b &lt;- brm(data = d, family = gaussian, height ~ 0 + male + female, prior = c(prior(normal(178, 100), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.15b&quot;) print(b5.15b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + male + female ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## male 142.34 1.72 138.98 145.73 1.00 5580 4236 ## female 134.66 1.58 131.53 137.70 1.00 5559 4243 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 27.36 0.82 25.80 29.02 1.00 6094 4382 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If we wanted the formal difference score from such a model, we’d subtract. posterior_samples(b5.15b) %&gt;% transmute(dif = b_male - b_female) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = &quot;firebrick4&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied difference score&quot;, x = expression(alpha[&quot;male&quot;] - alpha[&quot;female&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.2 Many categories. When there are more than two categories, you’ll need more than one dummy variable. Here’s the general rule: To include \\(k\\) categories in a linear model, you require \\(k - 1\\) dummy variables. Each dummy variable indicates, with the value 1, a unique category. The category with no dummy variable assigned to it ends up again as the “intercept” category. (p. 155) We’ll practice with milk. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) With the tidyverse, we can peek at clade with distinct() in the place of base R unique(). d %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape As clade has 4 categories, let’s use if_else() to convert these to 4 dummy variables. d &lt;- d %&gt;% mutate(clade_nwm = if_else(clade == &quot;New World Monkey&quot;, 1, 0), clade_owm = if_else(clade == &quot;Old World Monkey&quot;, 1, 0), clade_s = if_else(clade == &quot;Strepsirrhine&quot;, 1, 0), clade_ape = if_else(clade == &quot;Ape&quot;, 1, 0)) Now we’ll fit the model with three of the four dummies. In this model, clade_ape is the reference category captured by the intercept. b5.16 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.16&quot;) print(b5.16) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.55 0.04 0.46 0.63 1.00 4414 3632 ## clade_nwm 0.17 0.06 0.05 0.29 1.00 4757 3896 ## clade_owm 0.24 0.07 0.10 0.38 1.00 4753 3955 ## clade_s -0.04 0.07 -0.18 0.11 1.00 4764 4021 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.13 0.02 0.10 0.18 1.00 4663 3993 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we grab the chains, our draws from the posterior. post &lt;- b5.16 %&gt;% posterior_samples() head(post) ## b_Intercept b_clade_nwm b_clade_owm b_clade_s sigma lp__ ## 1 0.5211312 0.1581891 0.3042534 0.001079368 0.10293987 9.518550 ## 2 0.5954901 0.1087041 0.1737535 -0.117439967 0.14470641 9.371049 ## 3 0.5799949 0.1671201 0.1565664 -0.109732643 0.16373731 7.801610 ## 4 0.5640016 0.1055704 0.2408978 -0.068618153 0.09104343 7.772699 ## 5 0.4726887 0.3495940 0.3452817 -0.008980261 0.19920467 3.223453 ## 6 0.5644917 0.1069963 0.1653900 -0.010638759 0.09163243 6.300873 You might compute averages for each category and summarizing the results with the transpose of base R’s apply() function, rounding to two digits of precision. post$mu_ape &lt;- post$b_Intercept post$mu_nwm &lt;- post$b_Intercept + post$b_clade_nwm post$mu_owm &lt;- post$b_Intercept + post$b_clade_owm post$mu_s &lt;- post$b_Intercept + post$b_clade_s round(t(apply(post[ ,7:10], 2, quantile, c(.5, .025, .975))), digits = 2) ## 50% 2.5% 97.5% ## mu_ape 0.55 0.46 0.63 ## mu_nwm 0.71 0.63 0.80 ## mu_owm 0.79 0.68 0.89 ## mu_s 0.51 0.39 0.62 Here’s a more tidyverse sort of way to get the same thing, but this time with means and HPDIs via the tidybayes::mean_hdi() function. post %&gt;% transmute(mu_ape = b_Intercept, mu_nwm = b_Intercept + b_clade_nwm, mu_owm = b_Intercept + b_clade_owm, mu_s = b_Intercept + b_clade_s) %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu_ape 0.55 0.45 0.63 0.95 mean hdi ## 2 mu_nwm 0.71 0.63 0.79 0.95 mean hdi ## 3 mu_owm 0.79 0.69 0.9 0.95 mean hdi ## 4 mu_s 0.51 0.39 0.62 0.95 mean hdi You could also summarize with fitted(). nd &lt;- tibble(clade_nwm = c(1, 0, 0, 0), clade_owm = c(0, 1, 0, 0), clade_s = c(0, 0, 1, 0), primate = c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;)) fitted(b5.16, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(primate = rep(c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;), each = n() / 4)) %&gt;% ggplot(aes(x = value, y = reorder(primate, value))) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + labs(x = &quot;kcal.per.g&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) And there are multiple ways to compute summary statistics for the difference between NWM and OWM, too. # base R quantile(post$mu_nwm - post$mu_owm, probs = c(.5, .025, .975)) ## 50% 2.5% 97.5% ## -0.07592752 -0.20780640 0.06244937 # tidyverse + tidybayes post %&gt;% transmute(dif = mu_nwm - mu_owm) %&gt;% median_qi() ## dif .lower .upper .width .point .interval ## 1 -0.07592752 -0.2078064 0.06244937 0.95 median qi 5.4.3 Adding regular predictor variables. If we wanted to fit the model including perc.fat as an additional predictor, the basic statistical formula would be \\[ \\mu_i = \\alpha + \\beta_\\text{clade_nwm} \\text{clade_nwm}_i + \\beta_\\text{clade_owm} \\text{clade_owm}_i + \\beta_\\text{clade_s} \\text{clade_s}_i + \\beta_\\text{perc.fat} \\text{perc.fat}_i. \\] The corresponding formula argument within brm() would be kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s + perc.fat. 5.4.4 Another approach: Unique intercepts. “Another way to conceptualize categorical variables is to construct a vector of intercept parameters, one parameter for each category” (p. 158). Using the code below, there’s no need to transform d$clade into d$clade_id. The advantage of this approach is the indices in the model summary are more descriptive than a[1] through a[4]. b5.16_alt &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 0 + clade, prior = c(prior(normal(.6, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.16_alt&quot;) print(b5.16_alt) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 0 + clade ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cladeApe 0.55 0.04 0.46 0.63 1.00 7564 3900 ## cladeNewWorldMonkey 0.71 0.04 0.63 0.80 1.00 7076 4475 ## cladeOldWorldMonkey 0.79 0.05 0.68 0.90 1.00 7060 4241 ## cladeStrepsirrhine 0.51 0.06 0.39 0.62 1.00 7373 3682 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.13 0.02 0.10 0.18 1.00 5448 4520 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See? This is much easier than trying to remember which one was which in an arbitrary numeric index. 5.5 Ordinary least squares and lm() Since this section centers on the frequentist lm() function, I’m going to largely ignore it. A couple things, though. You’ll note how the brms package uses the lm()-like design formula syntax. Although not as pedagogical as the more formal rethinking syntax, it has the advantage of cohering with the popular lme4 syntax for multilevel models. Also, on page 161 McElreath clarified that one cannot use the I() syntax with his rethinking package. Not so with brms. The I() syntax works just fine with brms::brm(). We’ve already made use of it in the “Polynomial regression” section of Chapter 4. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GGally_1.4.0 patchwork_1.0.0 tidybayes_2.0.1.9000 bayesplot_1.7.1 ## [5] urbnmapr_0.0.0.9002 ggrepel_0.8.1 forcats_0.4.0 stringr_1.4.0 ## [9] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [13] tibble_2.1.3 tidyverse_1.3.0 brms_2.12.0 Rcpp_1.0.3 ## [17] rstan_2.19.2 ggplot2_3.2.1 StanHeaders_2.19.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 class_7.3-15 ## [4] ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [7] base64enc_0.1-3 fs_1.3.1 rstudioapi_0.10 ## [10] farver_2.0.3 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.3 ## [22] dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 assertthat_0.2.1 ## [28] Matrix_1.2-18 fastmap_1.0.1 lazyeval_0.2.2 ## [31] cli_2.0.1 later_1.0.0 htmltools_0.4.0 ## [34] prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.3.1 ## [40] reshape2_1.4.3 cellranger_1.1.0 vctrs_0.2.2 ## [43] nlme_3.1-142 crosstalk_1.0.0 xfun_0.12 ## [46] ps_1.3.0 rvest_0.3.5 mime_0.8 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 gtools_3.8.1 ## [52] MASS_7.3-51.4 zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 inline_0.3.15 RColorBrewer_1.1-2 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] loo_2.2.0 reshape_0.8.8 stringi_1.4.5 ## [67] highr_0.8 dygraphs_1.1.1.6 e1071_1.7-3 ## [70] pkgbuild_1.0.6 rlang_0.4.4 pkgconfig_2.0.3 ## [73] matrixStats_0.55.0 HDInterval_0.2.0 evaluate_0.14 ## [76] lattice_0.20-38 sf_0.8-1 rstantools_2.0.0 ## [79] htmlwidgets_1.5.1 labeling_0.3 processx_3.4.1 ## [82] tidyselect_1.0.0 plyr_1.8.5 magrittr_1.5 ## [85] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [88] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [91] units_0.6-5 xts_0.12-0 abind_1.4-5 ## [94] modelr_0.1.5 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [97] KernSmooth_2.23-16 utf8_1.1.4 rmarkdown_2.0 ## [100] grid_3.6.2 readxl_1.3.1 callr_3.4.1 ## [103] threejs_0.3.3 classInt_0.4-2 reprex_0.3.0 ## [106] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [109] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 "],
["overfitting-regularization-and-information-criteria.html", "6 Overfitting, Regularization, and Information Criteria 6.1 The problem with parameters 6.2 Information theory and model performance 6.3 Regularization 6.4 Information criteria 6.5 Using information criteria 6.6 Summary Bonus: \\(R^2\\) talk Reference Session info", " 6 Overfitting, Regularization, and Information Criteria In this chapter we contend with two contrasting kinds of statistical error: overfitting, “which leads to poor prediction by learning too much from the data” underfitting, “which leads to poor prediction by learning too little from the data” (p. 166, emphasis added) Toward that end, we will employ two common families of approaches. The first approach is to use a regularizing prior to tell the model not to get too excited by the data. This is the same device that non-Bayesian methods refer to as “penalized likelihood.” The second approach is to use some scoring device, like information criteria, to model the prediction risk and estimate predictive accuracy for some purpose. Both families of approaches are routinely used in the natural and social sciences Furthermore, they can be–maybe should be–used in combination. (p. 167, emphasis in the original) 6.0.0.1 Rethinking: Stargazing. The most common form of model selection among practicing scientists is a search for a model in which every coefficient is statistically significant. Statisticians sometimes call this stargazing, as it is embodied by scanning for asterisks (**) trailing after estimates… …Whatever you thing about null hypothesis significance testing in general, using it to select among structurally different models is a mistake–\\(p\\)-values are not designed to help you navigate between undercutting and overfitting. (p. 167, emphasis in the original) McElreath spent little time discussing \\(p\\)-values and null hypothesis testing in the text. If you’d like to learn more from a Bayesian perspective, you might check out the first several chapters (particularly 10–13) in Kruschke’s (2015) text and my project translating it to brms and the tidyverse. The great Frank Harrell has complied A Litany of Problems With p-values, which you might also find of use. 6.1 The problem with parameters The \\(R^2\\) is a popular way to measure how well you can retrodict the data. It traditionally follows the form \\[R^2 = \\frac{\\text{var(outcome)} - \\text{var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}.\\] By \\(\\text{var()}\\), of course, we meant variance (i.e., the var() function in R). McElreath is not a fan of the \\(R^2\\). But it’s important in my field, so instead of a summary at the end of the chapter, we will cover the Bayesian version of \\(R^2\\) and how to use it in brms. 6.1.1 More parameters always improve fit. Overfitting occurs when a model learns too much from the sample. What this means is that there are both regular and irregular features in every sample. The regular features are the targets of our learning, because they generalize well or answer a question of interest. Regular features are useful, given an objective of our choice. The irregular features are instead aspects of the data that do not generalize and so may mislead us. (p. 169, emphasis in the original) There’s a lot to like about this section, but my experience as a clinical psychologist inclines me to approach this topic differently. In practice, therapy is often a one-on-one process where the outcome is for a specific person in a specific time and set of circumstances in their life. Therapy research is often the aggregate of many such individual cases. As such, both the regular and irregular features of every therapy trial are of great interest. So yes, I agree with McElreath that overfitting of the kind we’re about to entertain is silly and to be avoided. And yet the goal of my line of research is to develop treatments (and to examine those treatments with statistical models) that are robust enough to handle irregularities as well. It’s my job to deal with the outliers. For more on what you might think of as robust models (i.e., models that can handle unusual observations), check out my blog on Student-\\(t\\) regression. Though not covered in this edition of the text, McElreath plans on covering Student-\\(t\\) regression in his upcoming second edition. In other words, the topics McElreath grappled with in this chapter are difficult and ones I hope you return to again and again during your data analysis career. Back to the text! We’ll start off by making the data with brain size and body size for seven species (see McHenry &amp; Coffing, 2000). library(tidyverse) ( d &lt;- tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) ) ## # A tibble: 7 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 africanus 452 35.5 ## 3 habilis 612 34.5 ## 4 boisei 521 41.5 ## 5 rudolfensis 752 55.5 ## 6 ergaster 871 61 ## 7 sapiens 1350 53.5 Let’s get ready for Figure 6.2. The plots in this chapter will be characterized by theme_classic() + theme(text = element_text(family = &quot;Courier&quot;)). Our color palette will come from the rcartocolor package, which provides color schemes designed by ‘CARTO’. # install.packages(&quot;rcartocolor&quot;, dependencies = T) library(rcartocolor) The specific palette we’ll be using is “BurgYl.” In addition to palettes, the rcartocolor package offers a few convenience functions which make it easier to use their palettes. The carto_pal() function will return the HEX numbers associated with a given palette’s colors and the display_carto_pal() function will display the actual colors. carto_pal(7, &quot;BurgYl&quot;) ## [1] &quot;#fbe6c5&quot; &quot;#f5ba98&quot; &quot;#ee8a82&quot; &quot;#dc7176&quot; &quot;#c8586c&quot; &quot;#9c3f5d&quot; &quot;#70284a&quot; display_carto_pal(7, &quot;BurgYl&quot;) We’ll be using a diluted version of the third color for the panel background (i.e., theme(panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)))) and the darker purples for other plot elements. Here’s the plot. library(ggrepel) d %&gt;% ggplot(aes(x = mass, y = brain, label = species)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_text_repel(size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[7], family = &quot;Courier&quot;, seed = 438) + labs(subtitle = &quot;Average brain volume by body\\nmass for six hominin species&quot;, x = &quot;body mass (kg)&quot;, y = &quot;brain volume (cc)&quot;) + coord_cartesian(xlim = 30:65) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) The first six models, which we’ll be fitting using OLS regression via the lm() function, range in complexity from the simple univariable model \\[\\begin{align*} \\text{brain}_i &amp; \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 \\text{mass}_i, \\end{align*}\\] to the dizzying sixth-degree polynomial model \\[\\begin{align*} \\text{brain}_i &amp; \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta_0 + \\beta_1 \\text{mass}_i + \\beta_2 \\text{mass}_i^2 + \\beta_3 \\text{mass}_i^3 + \\beta_4 \\text{mass}_i^4 + \\beta_5 \\text{mass}_i^5 + \\beta_6 \\text{mass}_i^6. \\end{align*}\\] Let’s fit them in bulk. First we’ll make a custom function, fit_lm(), into which we’ll feed the desired names and formulas of our models. We’ll make a tibble initially composed of those names (i.e., model) and formulas (i.e., formula). Via purrr::map2() within mutate(), we’ll then fit the models and save the model objects within the tibble. The broom package provides an array of convenience functions to convert statistical analysis summaries into tidy data objects. We’ll employ broom::tidy() and broom::glance() to extract information from the model fits. library(broom) fit_lm &lt;- function(model, formula) { model &lt;- lm(data = d, formula = formula) } fits &lt;- tibble(model = str_c(&quot;b6.&quot;, 1:6), formula = c(&quot;brain ~ mass&quot;, &quot;brain ~ mass + I(mass^2)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)&quot;, &quot;brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)&quot;)) %&gt;% mutate(fit = map2(model, formula, fit_lm)) %&gt;% mutate(tidy = map(fit, tidy), glance = map(fit, glance)) # what did we just do? print(fits) ## # A tibble: 6 x 5 ## model formula fit tidy glance ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 b6.1 brain ~ mass &lt;lm&gt; &lt;tibble [2 ×… &lt;tibble [1 × … ## 2 b6.2 brain ~ mass + I(mass^2) &lt;lm&gt; &lt;tibble [3 ×… &lt;tibble [1 × … ## 3 b6.3 brain ~ mass + I(mass^2) + I(mass^3) &lt;lm&gt; &lt;tibble [4 ×… &lt;tibble [1 × … ## 4 b6.4 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) &lt;lm&gt; &lt;tibble [5 ×… &lt;tibble [1 × … ## 5 b6.5 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(ma… &lt;lm&gt; &lt;tibble [6 ×… &lt;tibble [1 × … ## 6 b6.6 brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(ma… &lt;lm&gt; &lt;tibble [7 ×… &lt;tibble [1 × … Our fits object is a nested tibble. To learn more about this bulk approach to fitting models, check out Hadley Wickham’s talk, Managing many models with R. As you might learn in the talk, we can extract the \\(R^2\\) from each model with map_dbl(&quot;r.squared&quot;), which we’ll then display in a plot. fits &lt;- fits %&gt;% mutate(r2 = glance %&gt;% map_dbl(&quot;r.squared&quot;)) %&gt;% mutate(r2_text = round(r2, digits = 2) %&gt;% as.character() %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) fits %&gt;% ggplot(aes(x = r2, y = formula, label = r2_text)) + geom_text(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 3.5) + scale_x_continuous(expression(italic(R)^2), limits = 0:1, breaks = 0:1) + ylab(NULL) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If we wanted to look at the model coefficients, we could unnest(tidy) and wrangle a bit. fits %&gt;% unnest(tidy) %&gt;% select(model, term:estimate) %&gt;% mutate_if(is.double, round, digits = 1) %&gt;% complete(model, term) %&gt;% spread(key = term, value = estimate) %&gt;% select(model, `(Intercept)`, mass, everything()) %&gt;% knitr::kable() model (Intercept) mass I(mass^2) I(mass^3) I(mass^4) I(mass^5) I(mass^6) b6.1 -227.6 20.7 NA NA NA NA NA b6.2 -2618.1 127.3 -1.1 NA NA NA NA b6.3 21990.4 -1473.8 32.8 -0.2 NA NA NA b6.4 322887.2 -27945.5 892.2 -12.4 0.1 NA NA b6.5 -1535342.4 180049.0 -8324.7 189.6 -2.1 0.0 NA b6.6 10849890.9 -1473227.5 82777.0 -2462.6 40.9 -0.4 0 For Figure 6.3, we’ll make each plot individually and them glue them together with the patchwork package. Since they all share a common stucture, we’ll start by specifying a base plot which we’ll save as p. p &lt;- d %&gt;% ggplot(aes(x = mass, y = brain)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_x_continuous(&quot;body mass (kg)&quot;, limits = c(33, 62), expand = c(0, 0)) + ylab(&quot;brain volume (cc)&quot;) + coord_cartesian(ylim = c(300, 1500)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) Now for each subplot, we’ll tack the subplot-specific components onto p. The main action is in stat_smooth(). For each subplot, the first three lines in stat_smooth() are identical, with only the bottom formula line differing. Like McElreath did in the text, we also adjust the y-axis range for the last two plots. # linear p1 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, # note our rare use of 89% intervals color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ x) + ggtitle(NULL, subtitle = expression(italic(R)^2==&quot;.49&quot;)) # quadratic p2 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 2)) + ggtitle(NULL, subtitle = expression(italic(R)^2==&quot;.54&quot;)) # cubic p3 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 3)) + ggtitle(NULL, subtitle = expression(italic(R)^2==&quot;.68&quot;)) # fourth-order polynomial p4 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 4)) + ggtitle(NULL, subtitle = expression(italic(R)^2==&quot;.81&quot;)) # fifth-order polynomial p5 &lt;- p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 5)) + # we&#39;re adjusting the y-axis range for this plot (and the next) coord_cartesian(ylim = c(150, 1900)) + ggtitle(NULL, subtitle = expression(italic(R)^2==&quot;.99&quot;)) # sixth-order polynomial p6 &lt;- p + # mark off 0 on the y-axis geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ poly(x, 6)) + coord_cartesian(ylim = c(-300, 1500)) + ggtitle(NULL, subtitle = expression(italic(R)^2==1)) Okay, now we’re ready to combine the six subplots and produce our version of Figure 6.3. library(patchwork) (p1 + p2) / (p3 + p4) / (p5 + p6) “If you adopt a model family with enough parameters, you can fit the data exactly. But such a model will make rather absurd predictions for yet-to-be observed cases” (p. 172) 6.1.2 Too few parameters hurts, too. The overfit polynomial models manage to fit the data extremely well, bu they suffer for this within-sample accuracy by making nonsensical out-of-sample predictions. In contract, underfitting produces models that are inaccurate both within and out of sample. They have learned too little, failing to recover regular features of the sample. (p. 172, emphasis in the original) Fit the underfit intercept-only model, b6.7. b6.7 &lt;- lm(data = d, brain ~ 1) summary(b6.7) ## ## Call: ## lm(formula = brain ~ 1, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -275.71 -227.21 -101.71 97.79 636.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 713.7 121.8 5.86 0.00109 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 322.2 on 6 degrees of freedom With the intercept-only model, we didn’t even get an \\(R^2\\) value in the summary. The broom::glance() function offers a quick way to get one. glance(b6.7) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 0 322. NA NA 1 -49.8 104. 104. 623061. 6 Zero. Our intercept-only b6.7 explained exactly zero variance in brain. All it did was tell us what the unconditional mean and variance (i.e., ‘Residual standard error’) were. I hope that makes sense. They were the only things in the model: \\(\\text{brain}_i \\sim \\text{Normal}(\\mu = \\alpha, \\sigma)\\). To get the intercept-only model for Figure 6.4, we plug formula = y ~ 1 into the stat_smooth() function. p + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, level = .89, color = carto_pal(7, &quot;BurgYl&quot;)[6], fill = carto_pal(7, &quot;BurgYl&quot;)[6], size = 1/2, alpha = 1/3, formula = y ~ 1) + ggtitle(NULL, subtitle = expression(italic(R)^2==0)) Our underfit model b6.7 didn’t learn anything about the relation between mass and brain. “Such a model not only fails to describe the sample. It would also do a poor job for new data” (p. 173). 6.1.2.1 Overthinking: Dropping rows. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of d like this. d %&gt;% filter(row_number() != 2) ## # A tibble: 6 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 habilis 612 34.5 ## 3 boisei 521 41.5 ## 4 rudolfensis 752 55.5 ## 5 ergaster 871 61 ## 6 sapiens 1350 53.5 We can then extend that logic into a custom function, make_lines(), that will drop a row from d, fit the simple model brain ~ mass, and then use base R predict() to return the model-implied trajectory over new data values. # because these lines are straight, we only need new data over two points of `mass` nd &lt;- tibble(mass = c(30, 70)) make_lines &lt;- function(row) { # fit the model my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass) # compute fitted lines predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } Here we’ll make a tibble, lines, which will specify rows 1 through 7 in the row column. We’ll then feed those row numbers into our custom make_lines() function, which will return the predicted values and their corresponding mass values, per model. ( lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) ) ## # A tibble: 14 x 3 ## row brain mass ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 436. 30 ## 2 1 1201. 70 ## 3 2 421. 30 ## 4 2 1205. 70 ## 5 3 323. 30 ## 6 3 1264. 70 ## 7 4 423. 30 ## 8 4 1221. 70 ## 9 5 376. 30 ## 10 5 1335. 70 ## 11 6 332. 30 ## 12 6 1433. 70 ## 13 7 412. 30 ## 14 7 964. 70 Now we’re ready to plot the left panel of Figure 6.5. p + scale_x_continuous(expand = c(0, 0)) + geom_line(data = lines, aes(x = mass, y = brain, group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) To make the right panel for Figure 6.5, we’ll need to increase the number of mass points in our nd data and redefine the make_lines() function to fit the sixth-order-polynomial model. # because these lines will be very curvy, we&#39;ll need new data over many points of `mass` nd &lt;- tibble(mass = seq(from = 30, to = 65, length.out = 200)) # redifine the function make_lines &lt;- function(row) { my_fit &lt;- d %&gt;% filter(row_number() != row) %&gt;% lm(formula = brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)) predict(my_fit, nd) %&gt;% as_tibble() %&gt;% rename(brain = value) %&gt;% bind_cols(nd) } # make our new tibble lines &lt;- tibble(row = 1:7) %&gt;% mutate(p = map(row, make_lines)) %&gt;% unnest(p) # plot! p + geom_line(data = lines, aes(group = row), color = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2, size = 1/2) + coord_cartesian(ylim = -300:2000) 6.2 Information theory and model performance Whether you end up using regularization or information criteria or both, the first thing you must do is pick a criterion of model performance. What do you want the model to do well at? We’ll call this criterion the target, and in this section you’ll see how information theory provides a common and useful target, the out-of-sample deviance. (p. 174, emphasis in the original) 6.2.1 Firing the weatherperson. If you let rain = 1 and sun = 0, here’s a way to make a plot of the first table of page 175, the weatherperson’s predictions. weatherperson &lt;- tibble(day = 1:10, prediction = rep(c(1, 0.6), times = c(3, 7)), observed = rep(c(1, 0), times = c(3, 7))) weatherperson %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) Here’s how the newcomer fared. newcomer &lt;- tibble(day = 1:10, prediction = 0, observed = rep(c(1, 0), times = c(3, 7))) newcomer %&gt;% gather(key, value, -day) %&gt;% ggplot(aes(x = day, y = key, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) If we do the math entailed in the tibbles, we’ll see why the newcomer could boast “I’m the best person for the job” (p. 175). weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n() / 2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person) %&gt;% summarise(n_days = n(), n_hits = sum(hit), hit_rate = mean(hit)) ## # A tibble: 2 x 4 ## person n_days n_hits hit_rate ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 newcomer 10 7 0.7 ## 2 weatherperson 10 5.8 0.58 6.2.1.1 Costs and benefits. Our new points variable doesn’t fit into the nice color-based geom_tile() plots from above. But we can still do the math. weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), points = ifelse(observed == 1 &amp; prediction != 1, -5, ifelse(observed == 1 &amp; prediction == 1, -1, -1 * prediction))) %&gt;% group_by(person) %&gt;% summarise(happiness = sum(points)) ## # A tibble: 2 x 2 ## person happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer -15 ## 2 weatherperson -7.2 6.2.1.2 Measuring accuracy. Consider for example computing the probability of predicting the exact sequence of days. This means computing the probability of a correct prediction for each day. Then multiply all of these probabilities together to get the joint probability of correctly predicting the observed sequence. This is the same thing as the joint likelihood, which you’ve been using up to this point to fit models with Bayes’ theorem. In this light, the newcomer looks even worse. (p. 176) weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n() / 2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person, hit) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(power_character = str_c(hit, &quot;^&quot;, n), # this line is just for pedagogy power = hit ^ n, term = rep(letters[1:2], times = 2)) %&gt;% select(person, term, power) %&gt;% spread(key = term, value = power) %&gt;% mutate(probability_correct_sequence = a * b) ## # A tibble: 2 x 4 ## person a b probability_correct_sequence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 newcomer 0 1 0 ## 2 weatherperson 0.00164 1 0.00164 6.2.2 Information and uncertainty. Within the context of information theory, information is “the reduction of uncertainty derived from learning an outcome” (p. 177). Information entropy is a way of measuring that uncertainty in a way that is (a) continuous, (b) increases as the number of possible events increases, and (c) is additive. The formula for information entropy is: \\[H(p) = - \\text E \\log (p_i) = - \\sum_{i = 1}^n p_i \\log (p_i).\\] McElreath put it in words as “the uncertainty contained in a probability distribution is the average log-probability of the event” (p. 178). We’ll compute the information entropy for weather at the first unnamed location, which we’ll call McElreath's house, and Abu Dhabi at once. tibble(place = c(&quot;McElreath&#39;s house&quot;, &quot;Abu Dhabi&quot;), p_rain = c(.3, .01)) %&gt;% mutate(p_shine = 1 - p_rain) %&gt;% group_by(place) %&gt;% mutate(H_p = (p_rain * log(p_rain) + p_shine * log(p_shine)) %&gt;% mean() * -1) ## # A tibble: 2 x 4 ## # Groups: place [2] ## place p_rain p_shine H_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 McElreath&#39;s house 0.3 0.7 0.611 ## 2 Abu Dhabi 0.01 0.99 0.0560 Did you catch how we used the equation \\(H(p) = - \\sum_{i = 1}^n p_i \\log (p_i)\\) in our mutate() code, there? Our computation indicated the uncertainty is less in Abu Dhabi because it rarely rains, there. If you have sun, rain and snow, the entropy for weather is: p &lt;- c(.7, .15, .15) -sum(p * log(p)) ## [1] 0.8188085 “These entropy values by themselves don’t mean much to us, though. Instead we can use them to build a measure of accuracy. That comes next” (p. 178). 6.2.3 From entropy to accuracy. How can we use information entropy to say how far a model is from the target? The key lies in divergence: Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. This is often known as Kullback-Leibler divergence or simply K-L divergence. (p. 179, emphasis in the original; see Kullback &amp; Leibler, 1951) The formula for K-L divergence is \\[D_\\text{KL} (p, q) = \\sum_i p_i \\big ( \\log (p_i) - \\log (q_i) \\big ) = \\sum_i p_i \\log \\bigg ( \\frac{p_i}{q_i} \\bigg ),\\] which, in plainer language, is what McElreath described as “the average difference in log probability between the target (\\(p\\)) and model (\\(q\\))” (p. 179). In McElreath’s example \\(p_1 = .3\\), \\(p_2 = .7\\), \\(q_1 = .25\\), and \\(q_2 = .75\\). With those values, we can compute \\(D_\\text{KL} (p, q)\\) within a tibble like so. tibble(p_1 = .3, p_2 = .7, q_1 = .25, q_2 = .75) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.25 0.75 0.00640 Did you notice how we used the formula \\(D_\\text{KL} (p, q) = \\sum_i p_i \\log \\Big ( \\frac{p_i}{q_i} \\Big )\\) within mutate()? Our systems in this section are binary (e.g., \\(q = \\lbrace q_i, q_2 \\rbrace\\)). Thus if you know \\(q_1 = .3\\) you know of a necessity \\(q_2 = 1 - q_1\\). Therefore we can code the tibble for the next example of when \\(p = q\\) like this. tibble(p_1 = .3) %&gt;% mutate(p_2 = 1 - p_1, q_1 = p_1) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.3 0.7 0 Building off of that, you can make the data required for Figure 6.6 like this. t &lt;- tibble(p_1 = .3, p_2 = .7, q_1 = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) head(t) ## # A tibble: 6 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.01 0.99 0.778 ## 2 0.3 0.7 0.02 0.98 0.577 ## 3 0.3 0.7 0.03 0.97 0.462 ## 4 0.3 0.7 0.04 0.96 0.383 ## 5 0.3 0.7 0.05 0.95 0.324 ## 6 0.3 0.7 0.06 0.94 0.276 Now we have the data, plotting Figure 6.6 is little more than geom_line() with stylistic flourishes. t %&gt;% ggplot(aes(x = q_1, y = d_kl)) + geom_vline(xintercept = .3, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1.5) + annotate(geom = &quot;text&quot;, x = .4, y = 1.5, label = &quot;q = p&quot;, color = carto_pal(7, &quot;BurgYl&quot;)[5], family = &quot;Courier&quot;, size = 3.5) + labs(x = expression(italic(q)[1]), y = &quot;Divergence of q from p&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) What divergence can do for us now is help us contrast different approximations to \\(p\\). As an approximating function \\(q\\) becomes more accurate, \\(D_\\text{KL} (p, q)\\) will shrink. So if we have a pair of candidate distributions, then the candidate that minimizes the divergence will be closest to the target. Since predictive models specify probabilities of events (observations), we can use divergence to compare the accuracy of models. (p. 180) 6.2.3.1 Rethinking: Divergence depends upon direction. Here we see \\(H(p, q) \\neq H(q, p)\\). That is, direction matters. tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 2 x 6 ## direction p_1 q_1 p_2 q_2 d_kl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 ## 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 The \\(D_\\text{KL}\\) was double when applying Martian estimates to Terran estimates. 6.2.4 From divergence to deviance. The point of all the preceding material about information theory and divergence is to establish both: How to measure the distance of a model from our target. Information theory gives us the distance measure we need, the K-L divergence. How to estimate the divergence. Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks. (p. 181) Now we’ll start working on item #2. Within the context of science, say we’ve labeled the true model for our topic of interest as \\(p\\). We don’t actually know what \\(p\\) is–we wouldn’t need the scientific method if we did. But say what we do have are two candidate models \\(q\\) and \\(r\\). We would at least like to know which is closer to \\(p\\). It turns out we don’t even need to know the absolute value of \\(p\\) to achieve this. Just the relative values of \\(q\\) and \\(r\\) will suffice. We express model \\(q\\)’s average log-probability as \\(\\text E \\log (q_i)\\). Extrapolating, the difference \\(\\text E \\log (q_i) - \\text E \\log (r_i)\\) gives us a sense about the divergence of both \\(q\\) and \\(r\\) from the target \\(p\\). That is, we can compare the average log-probability from each model to get an estimate of the relative distance of each model from the target…, [which] delivers us to a very common measure of relative model fit, one that also turns out to be an approximation of K-L divergence. To approximate the relative value of we can use a model’s deviance, which is defined as: \\[D(q) = -2 \\sum_i \\log (q_i)\\] where \\(i\\) indexes each observation (case), and each \\(q_i\\) is just the likelihood of case \\(i\\). (p. 182, emphasis in the original) Here’s the deviance from model b6.1. lm(data = d, brain ~ mass) %&gt;% logLik() * -2 ## &#39;log Lik.&#39; 94.92499 (df=3) Next we learn how do this by hand. 6.2.4.1 Overthinking: Computing deviance. To follow along with the text, we’ll need to standardize mass before we fit our model. d &lt;- d %&gt;% mutate(mass_s = (mass - mean(mass)) / sd(mass)) Open brms. library(brms) Now we’ll specify the initial values and fit the model. # define the starting values inits &lt;- list(Intercept = mean(d$brain), mass_s = 0, sigma = sd(d$brain)) inits_list &lt;- list(inits, inits, inits, inits) # The model b6.8 &lt;- brm(data = d, family = gaussian, brain ~ 1 + mass_s, prior = c(prior(normal(0, 1000), class = Intercept), prior(normal(0, 1000), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, # here we insert our start values seed = 6, file = &quot;fits/b06.08&quot;) print(b6.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: brain ~ 1 + mass_s ## Data: d (Number of observations: 7) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 703.11 112.44 461.42 919.59 1.00 2536 1750 ## mass_s 224.95 115.99 -16.64 456.37 1.00 2710 2131 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 267.85 97.39 152.06 512.22 1.00 1611 1860 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Details about inits: You don’t have to specify your inits lists outside of the brm() function the way we did, here. This is just how I currently prefer. When you specify start values for the parameters in your Stan models, you need to do so with a list of lists. You need as many lists as HMC chains–four in this example. And then you put your–in this case–four lists inside a list. You need lists within lists. Also, we were lazy and specified the same start values across all our chains. You can mix them up across chains if you want. Anyway, the brms log_lik() function returns a matrix. Each occasion gets a column and each HMC chain iteration gets a row. To make it easier to understand the output, we’ll name the columns by species using the .name_repair argument within the as_tibble() function. ll &lt;- b6.8 %&gt;% log_lik() %&gt;% as_tibble(.name_repair = ~ d$species) ll %&gt;% glimpse() ## Observations: 4,000 ## Variables: 7 ## $ afarensis &lt;dbl&gt; -6.271889, -6.629663, -6.316051, -6.322241, -6.427522, -6.535583, -6.382093, … ## $ africanus &lt;dbl&gt; -6.086936, -6.631468, -6.167139, -6.226852, -6.349123, -6.503257, -6.301547, … ## $ habilis &lt;dbl&gt; -6.089175, -6.845953, -6.337591, -6.456844, -6.568457, -6.553667, -6.484237, … ## $ boisei &lt;dbl&gt; -6.204363, -6.628546, -6.454196, -6.413987, -6.558804, -6.490261, -6.444100, … ## $ rudolfensis &lt;dbl&gt; -6.150062, -6.628820, -7.331396, -7.032504, -7.512183, -6.495369, -6.858456, … ## $ ergaster &lt;dbl&gt; -6.021019, -6.637155, -7.540802, -7.175751, -7.864640, -6.590322, -6.928784, … ## $ sapiens &lt;dbl&gt; -12.345567, -8.857263, -8.250506, -8.217079, -7.304940, -9.862717, -8.208726,… Deviance, recall, is the sum of the occasion-level LLs multiplied by -2: \\(D(q) = -2 \\sum_i \\log (q_i)\\). Why by -2? “The -2 in front doesn’t do anything important. It’s there for historical reasons” (p. 182). If you follow footnote 93 at the end of that sentence in the text, you’ll learn “under somewhat general conditions, for many common model types, a difference between two deviances has a chi-squared distribution. The factor of 2 is there to scale it that way” (p. 451). ll &lt;- ll %&gt;% mutate(sums = rowSums(.), deviance = -2 * sums) glimpse(ll) ## Observations: 4,000 ## Variables: 9 ## $ afarensis &lt;dbl&gt; -6.271889, -6.629663, -6.316051, -6.322241, -6.427522, -6.535583, -6.382093, … ## $ africanus &lt;dbl&gt; -6.086936, -6.631468, -6.167139, -6.226852, -6.349123, -6.503257, -6.301547, … ## $ habilis &lt;dbl&gt; -6.089175, -6.845953, -6.337591, -6.456844, -6.568457, -6.553667, -6.484237, … ## $ boisei &lt;dbl&gt; -6.204363, -6.628546, -6.454196, -6.413987, -6.558804, -6.490261, -6.444100, … ## $ rudolfensis &lt;dbl&gt; -6.150062, -6.628820, -7.331396, -7.032504, -7.512183, -6.495369, -6.858456, … ## $ ergaster &lt;dbl&gt; -6.021019, -6.637155, -7.540802, -7.175751, -7.864640, -6.590322, -6.928784, … ## $ sapiens &lt;dbl&gt; -12.345567, -8.857263, -8.250506, -8.217079, -7.304940, -9.862717, -8.208726,… ## $ sums &lt;dbl&gt; -49.16901, -48.85887, -48.39768, -47.84526, -48.58567, -49.03118, -47.60794, … ## $ deviance &lt;dbl&gt; 98.33802, 97.71773, 96.79536, 95.69052, 97.17134, 98.06235, 95.21589, 94.9593… Because we used HMC, deviance is a distribution rather than a single number. That is, we have a deviance \\(D(q)\\) value for each row, for each HMC iteration. library(tidybayes) ll %&gt;% ggplot(aes(x = deviance, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_x_continuous(breaks = quantile(ll$deviance, c(.025, .5, .975)), labels = quantile(ll$deviance, c(.025, .5, .975)) %&gt;% round(1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;The deviance distribution&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) But notice our deviance distribution was centered right around the sole value McElreath reported in the text. 6.2.5 From deviance to out-of-sample. Deviance is a principled way to measure distance from the target. But deviance as computed in the previous section has the same flaw as \\(R^2\\): It always improves as the model gets more complex, at least for the types of models we have considered so far. Just like \\(R^2\\), deviance in-sample is a measure of retrodictive accuracy, not predictive accuracy. In the next subsection, we’ll see this in a simulation which will produce the data necessary to make Figure 6.7. 6.2.5.1 Overthinking: Simulated training and testing. I find the rethinking::sim.train.test() function opaque. If you’re curious, you can find McElreath’s code here. Let’s simulate and see what happens. library(rethinking) n &lt;- 20 kseq &lt;- 1:5 # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 n_cores &lt;- 4 # here&#39;s our dev object based on `N &lt;- 20` dev_20 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) } ) # here&#39;s our dev object based on N &lt;- 100 n &lt;- 100 dev_100 &lt;- sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) } ) If you didn’t quite catch it, the simulation yields dev_20 and dev_100. We’ll want to convert them to tibbles, bind them together, and wrangle extensively before we’re ready to plot. dev_tibble &lt;- dev_20 %&gt;% as_tibble() %&gt;% bind_rows( dev_100 %&gt;% as_tibble() ) %&gt;% mutate(n = rep(c(&quot;n = 20&quot;, &quot;n = 100&quot;), each = 4), statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 2)) %&gt;% gather(n_par, value, -n, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .075, n_par + .075)) head(dev_tibble) ## # A tibble: 6 x 5 ## n sample n_par mean sd ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 n = 100 in 0.925 282. 13.5 ## 2 n = 100 in 1.92 280. 13.4 ## 3 n = 100 in 2.92 263. 11.6 ## 4 n = 100 in 3.92 262. 11.3 ## 5 n = 100 in 4.92 262. 10.9 ## 6 n = 100 out 1.08 285. 14.4 Now we’re ready to make Figure 6.7. # this intermediary tibble will make `geom_text()` easier dev_text &lt;- dev_tibble %&gt;% filter(n_par &gt; 1.5, n_par &lt; 2.5) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .2, n_par + .28)) # the plot dev_tibble %&gt;% ggplot(aes(x = n_par, y = mean, ymin = mean - sd, ymax = mean + sd, group = sample, color = sample, fill = sample)) + geom_pointrange(shape = 21) + geom_text(data = dev_text, aes(label = sample)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[5], carto_pal(7, &quot;BurgYl&quot;)[7])) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;)) + facet_wrap(~n, scale = &quot;free_y&quot;) Even with a substantially smaller \\(N\\), our simulation results matched up well with those in the text. Deviance is an assessment of predictive accuracy, not of truth. The true model, in terms of which predictors are included, is not guaranteed to produce the best predictions. Likewise a false model, in terms of which predictors are included, is not guaranteed to produce poor predictions. The point of this thought experiment is to demonstrate how deviance behaves, in theory. While deviance on training data always improves with additional predictor variables, deviance on future data may or may not, depending upon both the true data-generating processes and how much data is available to precisely estimate the parameters. These facts form the basis for understanding both regularizing priors and information criteria. (p. 185) 6.3 Regularization The root of overfitting is a model’s tendency to get overexcited by the training sample… One way to prevent a model from getting too excited by the training sample is to give it a skeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample. (p. 186) In case you were curious, here’s how you might do Figure 6.8 with ggplot2. All the action is in the geom_ribbon() portions. tibble(x = seq(from = - 3.5, to = 3.5, by = .01)) %&gt;% ggplot(aes(x = x, ymin = 0)) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 0.2)), fill = carto_pal(7, &quot;BurgYl&quot;)[7], alpha = 1/2) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 0.5)), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/2) + geom_ribbon(aes(ymax = dnorm(x, mean = 0, sd = 1)), fill = carto_pal(7, &quot;BurgYl&quot;)[5], alpha = 1/2) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;parameter value&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) In our version of the plot, darker purple = more regularizing. But to prepare for Figure 6.9, let’s simulate. This time we’ll wrap the basic simulation code we used before into a function we’ll call make_sim(). Our make_sim() function has two parameters, N and b_sigma, both of which come from McElreath’s simulation code. So you’ll note that instead of hard coding the values for N and b_sigma within the simulation, we’re leaving them adjustable (i.e., sim.train.test(N = n, k = k, b_sigma = b_sigma)). Also notice that instead of saving the simulation results as objects, like before, we’re just converting them to tibbles with the as_tibble() function at the bottom. Our goal is to use make_sim() within a purrr::map2() statement. The result will be a nested tibble into which we’ve saved the results of 6 simulations based off of two sample sizes (i.e., n = c(20, 100)) and three values of \\(\\sigma\\) for our Gaussian \\(\\beta\\) prior (i.e., b_sigma = c(1, .5, .2)). library(rethinking) # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 make_sim &lt;- function(n, b_sigma) { sapply(kseq, function(k) { print(k); # this is an augmented line of code r &lt;- mcreplicate(n_sim, sim.train.test(N = n, k = k, b_sigma = b_sigma), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) }) %&gt;% # this is a new line of code as_tibble() } s &lt;- tibble(n = rep(c(20, 100), each = 3), b_sigma = rep(c(1, .5, .2), times = 2)) %&gt;% mutate(sim = map2(n, b_sigma, make_sim)) %&gt;% unnest() We’ll follow the same principles for wrangling these data as we did those from the previous simulation, dev_tibble. And after wrangling, we’ll feed the data directly into the code for our version of Figure 6.9. # wrangle the simulation data s %&gt;% mutate(statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 3 * 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 3 * 2)) %&gt;% gather(n_par, value, -n, -b_sigma, -statistic, -sample) %&gt;% spread(key = statistic, value = value) %&gt;% mutate(n = str_c(&quot;n = &quot;, n) %&gt;% factor(., levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_remove(n_par, &quot;V&quot;) %&gt;% as.double()) %&gt;% # now plot ggplot(aes(x = n_par, y = mean, group = interaction(sample, b_sigma))) + geom_line(aes(color = sample, size = b_sigma %&gt;% as.character())) + # this function contains the data from the previous simulation geom_point(data = dev_tibble, aes(group = sample, fill = sample), color = &quot;black&quot;, shape = 21, size = 2.5, stroke = .1) + scale_fill_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + scale_size_manual(values = c(1, .5, .2)) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;)) + facet_wrap(~n, scale = &quot;free_y&quot;) Our results don’t perfectly align with those in the text. I suspect his is because we used 1e3 iterations, rather than the 1e4 of the text. If you’d like to wait all night long for the simulation to yield more stable results, be my guest. Regularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. So to use them effectively, you need some way to tune them. Tuning them isn’t always easy. (p. 187) For more on this how to choose your priors, consider Gelman, Simpson, and Betancourt’s The prior can generally only be understood in the context of the likelihood, a paper that will probably make more sense after Chapter 9. And if you’re feeling feisty, also check out Simpson’s related blog post (It’s never a) Total Eclipse of the Prior. 6.3.0.1 Rethinking: Multilevel models as adaptive regularization. When you encounter multilevel models in Chapter 12, you’ll see that their central device is to learn the strength of the prior form the data itself. So you can think of multilevel models as adaptive regularization, where the model itself tries to learn how skeptical it should be. (p. 188) I found this connection difficult to grasp for a long time. Practice now and hopefully it’ll sink in for you faster than it did me. 6.3.0.2 Rethinking: Ridge regression. Within the brms framework, you can do something like this with the horseshoe prior via the horseshoe() function. You can learn all about it from the horseshoe section of the brms reference manual (version 2.12.0). Here’s an extract from the section: The horseshoe prior is a special shrinkage prior initially proposed by Carvalho et al. (2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is non-zero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(&quot;horseshoe(1)&quot;). (p. 93) And to dive even deeper into the horseshoe prior, check out Michael Betancourt’s tutorial, Bayes Sparse Regression. 6.4 Information criteria The data from our initial simulation isn’t formatted well to plot Figure 6.10. We’ll have to wrangle a little. ( dev_tibble &lt;- dev_tibble %&gt;% select(-sd) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par + .075, n_par - .075)) %&gt;% spread(key = sample, value = mean) %&gt;% mutate(height = (out - `in`) %&gt;% round(digits = 1) %&gt;% as.character(), dash = `in` + 2 * n_par) ) ## # A tibble: 10 x 6 ## n n_par `in` out height dash ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 n = 20 1 55.8 57.8 2 57.8 ## 2 n = 20 2 54.6 58.4 3.9 58.6 ## 3 n = 20 3 50.7 56.5 5.7 56.7 ## 4 n = 20 4 49.9 57.0 7.2 57.9 ## 5 n = 20 5 49.2 58.9 9.7 59.2 ## 6 n = 100 1 282. 285. 3.1 284. ## 7 n = 100 2 280. 284. 3.6 284. ## 8 n = 100 3 263. 268. 5.2 269. ## 9 n = 100 4 262. 268. 6.3 270. ## 10 n = 100 5 262. 270. 8.2 272. Now we’re ready to plot. dev_tibble %&gt;% ggplot(aes(x = n_par)) + geom_line(aes(y = dash), linetype = 2, color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_point(aes(y = `in`), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 2) + geom_point(aes(y = out), color = carto_pal(7, &quot;BurgYl&quot;)[5], size = 2) + geom_errorbar(aes(x = n_par + .15, ymin = `in`, ymax = out), width = .1, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_text(aes(x = n_par + .4, y = (out + `in`) / 2, label = height), family = &quot;Courier&quot;, size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[6]) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;)) + facet_wrap(~n, scale = &quot;free_y&quot;) Again, our numbers aren’t the exact same as McElreath’s because a) this is a simulation and b) our number of simulations was an order of magnitude smaller than his. But the overall pattern is the same. More to the point, the distances between the in- and out-of-sample points are nearly the same, for each model, at both \\(N = 20\\) (left) and \\(N = 100\\) (right). Each distance is nearly twice the number of parameters, as labeled on the horizontal axis. The dashed lines show exactly the [dark purple] points plus twice the number of parameters, tracing closely along the average out-of-sample deviance for each model. This is the phenomenon behind information criteria. (p. 189) Information criteria estimate out-of-sample deviance. The frequentist AIC is the oldest and most restrictive. In the text, McElreach focused on the DIC and WAIC. As you’ll see, the LOO has increased in popularity since he published the text. Going forward, we’ll juggle the WAIC and the LOO in this project. But we will respect the text and work in a little DIC talk. 6.4.1 DIC. The DIC has been widely used for some time, now. For a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. If we define \\(D\\) as the deviance’s posterior distribution, \\(\\bar D\\) as its mean and \\(\\hat D\\) as the deviance when computed at the posterior mean, then we define the DIC as \\[\\text{DIC} = \\bar D + (\\bar D - \\hat D) = \\bar D + p_D\\] And \\(p_D\\) is the number of effective parameters in the model, which is also sometimes referred to as the penalty term. As you’ll see, you can get the \\(p_D\\) for brms::brm() models. However, I’m not aware of a way to that brms or the loo package–to be introduced shortly–offer convenience functions that yield the DIC. I’m also not aware of an easy to compute the DIC by hand the way we’ve been doing, so far. I’ll walk it out with our model b6.8. Remember how we had an entire posterior distribution for the deviance? ll %&gt;% select(deviance) %&gt;% glimpse() ## Observations: 4,000 ## Variables: 1 ## $ deviance &lt;dbl&gt; 98.33802, 97.71773, 96.79536, 95.69052, 97.17134, 98.06235, 95.21589, 94.95937, … If we call that deviance distribution \\(D\\), we might refer to it’s mean as \\(\\bar D\\). Here’s \\(\\bar D\\) for b6.8. ll %&gt;% summarise(d_bar = mean(deviance)) ## # A tibble: 1 x 1 ## d_bar ## &lt;dbl&gt; ## 1 98.3 No problem. The tricky part is \\(\\hat D\\), which “is the deviance calculated at the posterior mean. This means we compute the average of each parameter in the posterior distribution. Then we plug those averages into the deviance formula to get \\(\\hat D\\) out” (p. 190). It’s no problem to get the posterior means for each of the parameters in a model. For example, here they are for b6.8. posterior_summary(b6.8)[1:3, 1] ## b_Intercept b_mass_s sigma ## 703.1127 224.9541 267.8506 This is just standard brms summary stuff. The problem is plugging those into the deviance formula, \\(D(q) = -2 \\sum_i \\log (q_i)\\), where \\(i\\) in this case is just our single vector of means (i.e., the ones we just computed). The challenge is with \\(q\\), which is the joint likelihood for those values. I am not aware of a way to compute the likelihood of a given vector of parameter values for a brms model. Without that capability, we have no good way to compute \\(\\hat D\\) and without \\(\\hat D\\) we can’t compute the DIC. We’re no better off using the second version of the formula, \\(\\text{DIC} = \\bar D + p_D\\), because I have no reliable way to compute \\(p_D\\). This largely doesn’t matter. As we’ll see in just a moment, brms offers the WAIC and LOO, which are better estimates of out-of-sample deviance. If you take a look on the Stan Forums, you’ll see the members of the Stan team are adamant on this topic (see here or here or here). But if you ever find yourself emerging triumphant from a deep computational rabbit hole with a robust method for computing \\(\\hat D\\) or \\(p_D\\) from a brms model, please share your code. It’d be fun to flesh out this section of the book. 6.4.2 WAIC. It’s okay that the brms and loo packages don’t yield the DIC because even better than the DIC is the Widely Applicable Information Criterion (WAIC[; Wanatabe, 2010])… Define \\(\\text{Pr} (y_i)\\) as the average likelihood of observation \\(i\\) in the training sample. This means we compute the likelihood of \\(y_i\\) for each set of parameters sampled from the posterior distribution. Then we average the likelihoods for each observation \\(i\\) and finally sum over all observations. This produces the first part of WAIC, the log-pointwise-predictive-density, lppd: \\[\\text{lppd} = \\sum_{i = 1}^N \\text{log Pr} (y_i)\\] You might say this out loud as: The log-pointwise-predictive-density is the total across observations of the logarithm of the average likelihood of each observation. … The second piece of WAIC is the effect number of parameters \\(p_\\text{WAIC}\\). Define \\(V(y_i)\\) as the variance in log-likelihood for observation \\(i\\) in the training sample. This means we compute the log-likelihood for observation \\(y_i\\) for each sample from the posterior distribution. Then we take the variance of those values. This is \\(V(y_i)\\). Now \\(p_\\text{WAIC}\\) is defined as: \\[p_\\text{WAIC} = \\sum_{i=1}^N V (y_i)\\] Now WAIC is defined as: \\[\\text{WAIC} = -2 (\\text{lppd} - p_\\text{WAIC})\\] And this value is yet another estimate of out-of-sample deviance. (pp. 191–192, emphasis in the original) You’ll see how to compute the WAIC in brms in just a bit. 6.4.2.1 Overthinking: WAIC calculation. Here is how to fit the pre-WAIC model in brms. data(cars) b6.0_waic &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 30), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.00_waic&quot;) Here’s the summary. print(b6.0_waic) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: dist ~ 1 + speed ## Data: cars (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -17.96 6.86 -31.26 -4.50 1.00 1803 2130 ## speed 3.95 0.42 3.14 4.77 1.01 1764 1920 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 15.78 1.68 12.98 19.62 1.00 2388 1969 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To refresh, you return the log-likelihood values for a brms model with log_lik(). ll &lt;- b6.0_waic %&gt;% log_lik() %&gt;% as_tibble() Now we compute the “Bayesian deviance” (i.e., \\(\\text{lppd}\\)), which follows the form \\[\\text{lppd} = \\sum_{i = 1}^N \\log \\text{Pr} (y_i).\\] We’ll do it in two steps, saving the \\(\\log \\text{Pr} (y_i)\\) and the \\(\\text{lppd}\\) as two objects. log_p_y &lt;- ll %&gt;% gather(case, loglikelihood) %&gt;% mutate(likelihood = exp(loglikelihood)) %&gt;% group_by(case) %&gt;% summarise(log_mean_likelihood = mean(likelihood) %&gt;% log()) ( lppd &lt;- log_p_y %&gt;% pull(log_mean_likelihood) %&gt;% # sum the logarighms of the casewise mean likelihoods sum() ) ## [1] -206.698 We’ll follow a similar process for computing the \\(p_\\text{WAIC}\\). We’ll save the casewise variance in log-likelihood values, \\(V (y_i)\\), separately from their sum, the \\(p_\\text{WAIC}\\). v_i &lt;- ll %&gt;% gather(case, loglikelihood) %&gt;% group_by(case) %&gt;% summarise(var_loglikelihood = var(loglikelihood)) pwaic &lt;- v_i %&gt;% pull(var_loglikelihood) %&gt;% sum() pwaic ## [1] 3.184257 Now we can finally plug our hand-made lppd and pwaic values into the formula \\(-2 (\\text{lppd} - p_\\text{WAIC})\\) to compute the WAIC. Compare it to the value returned by the brms waic() function. -2 * (lppd - pwaic) ## [1] 419.7646 waic(b6.0_waic) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -209.9 6.3 ## p_waic 3.2 1.1 ## waic 419.8 12.6 ## ## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Before we move on, did you notice the elpd_waic row in the tibble returned by thewaic() function? That value is the lppd minus the pwaic, but without multiplying the result by -2. E.g., lppd - pwaic ## [1] -209.8823 That tidbit will come in handy a little bit later. But for now, here’s how we compute the WAIC standard error. bind_cols(log_p_y %&gt;% select(log_mean_likelihood), v_i %&gt;% select(var_loglikelihood)) %&gt;% mutate(waic_vec = -2 * (log_mean_likelihood - var_loglikelihood)) %&gt;% summarise(waic_se = (var(waic_vec) * nrow(log_p_y)) %&gt;% sqrt()) ## # A tibble: 1 x 1 ## waic_se ## &lt;dbl&gt; ## 1 12.6 6.4.3 DIC and WAIC as estimates of deviance. Once again, we’ll wrap McElreath’s sim.train.test()-based simulation code within a custom function, make_sim(). This time we’ve adjusted make_sim() to take one argument, b_sigma. We will then feed that value into the same-named argument within sim.train.test(). Also notice that within sim.train.test(), we’ve specified TRUE for the information criteria and deviance arguments. Be warned: it takes extra time to compute the WAIC. Because we do that for every model, this simulation takes longer than the previous ones. To get a taste, try running it with something like n_sim &lt;- 5 first. n_sim &lt;- 1e3 make_sim &lt;- function(b_sigma) { sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim.train.test(N = 20, k = k, b_sigma = b_sigma, DIC = T, WAIC = T, devbar = T, devbarout = T), mc.cores = n_cores); c(dev_in = mean(r[1, ]), dev_out = mean(r[2, ]), DIC = mean(r[3, ]), WAIC = mean(r[4, ]), devbar = mean(r[5, ]), devbarout = mean(r[6, ])) } ) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% rename(statistic = rowname) } s &lt;- tibble(b_sigma = c(100, .5)) %&gt;% mutate(sim = purrr::map(b_sigma, make_sim)) %&gt;% unnest() Here we wrangle and plot. s %&gt;% gather(n_par, value, -b_sigma, -statistic) %&gt;% mutate(n_par = str_remove(n_par, &quot;X&quot;) %&gt;% as.double()) %&gt;% filter(statistic != &quot;devbar&quot; &amp; statistic != &quot;devbarout&quot;) %&gt;% spread(key = statistic, value = value) %&gt;% gather(ic, value, -b_sigma, -n_par, -dev_in, -dev_out) %&gt;% gather(sample, deviance, -b_sigma, -n_par, -ic, -value) %&gt;% filter(sample == &quot;dev_out&quot;) %&gt;% mutate(b_sigma = b_sigma %&gt;% as.character()) %&gt;% ggplot(aes(x = n_par)) + geom_point(aes(y = deviance, color = b_sigma), size = 2.5) + geom_line(aes(y = value, group = b_sigma, color = b_sigma)) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[7], carto_pal(7, &quot;BurgYl&quot;)[5])) + labs(subtitle = &quot;n = 20&quot;, x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), legend.position = &quot;none&quot;, panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;white&quot;)) + facet_wrap(~ic, ncol = 1) And again, our results don’t perfectly match those in the text because a) we’re simulating and b) we used fewer iterations than McElreath did. But the overall pattern remains. “Both [DIC and WAIC] are accurate on average, being within 1 point of deviance of the actual average in most cases. Both DIC and WAIC are useful estimates of the deviance, but WAIC is more accurate in this context” (p. 194). 6.4.3.1 Rethinking: Diverse prediction frameworks. AIC orders models in a way that approximates some forms of cross-validation, and WAIC is explicitly derived as an approximate Bayesian cross-validation… Perhaps a larger concern is that our train-test thought experiment pulls the test sample from exactly the same process as the training sample. This is a kind of uniformitarian assumption, in which future data are expected to come from the same process as past data and have the same rough range of values. (pp. 194–195, emphasis in the original) This is one of those big-ticket issues that precisely-computed and neatly-presented statistics isn’t so well suited to solve. If you’re ready to ponder this further, Navarro’s Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection is a fine place to start. 6.5 Using information criteria In contrast to model selection, “this section provides a brief example of model comparison and averaging” (p. 195, emphasis in the original). 6.5.1 Model comparison. Load the milk data from earlier in the text. library(rethinking) data(milk) d &lt;- milk %&gt;% drop_na(ends_with(&quot;_s&quot;)) rm(milk) d &lt;- d %&gt;% mutate(neocortex = neocortex.perc / 100) The dimensions of d are: dim(d) ## [1] 17 9 Load brms. detach(package:rethinking, unload = T) library(brms) We’re ready to fit the competing kcal.per.g models. Note our use of update() in the last two models. inits &lt;- list(Intercept = mean(d$kcal.per.g), sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.11 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, seed = 6, file = &quot;fits/b06.11&quot;) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.12 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex, prior = c(prior(uniform(-1000, 1000), class = Intercept), prior(uniform(-1000, 1000), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = inits_list, seed = 6, file = &quot;fits/b06.12&quot;) inits &lt;- list(Intercept = mean(d$kcal.per.g), `log(mass)` = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.13 &lt;- update(b6.12, newdata = d, formula = kcal.per.g ~ 1 + log(mass), inits = inits_list, chains = 4, cores = 4, file = &quot;fits/b06.13&quot;) inits &lt;- list(Intercept = mean(d$kcal.per.g), neocortex = 0, `log(mass)` = 0, sigma = sd(d$kcal.per.g)) inits_list &lt;-list(inits, inits, inits, inits) b6.14 &lt;- update(b6.13, newdata = d, formula = kcal.per.g ~ 1 + neocortex + log(mass), inits = inits_list, chains = 4, cores = 4, file = &quot;fits/b06.14&quot;) The priors are all flat above, which is clearly not the best idea. But his will let you get a sense of what the sample alone says, in the absence of regularization, and how WAIC measures overfitting. Then in the problems at the end of the chapter, you’ll explore regularization. (p. 197) 6.5.1.1 Comparing WAIC values. In brms, you can get a model’s WAIC value with the waic() function. waic(b6.14) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 8.4 2.6 ## p_waic 3.1 0.8 ## waic -16.8 5.2 ## ## 2 (11.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. Note the warning message. Statisticians have made notable advances in Bayesian information criteria since McElreath published Statistical Rethinking. I won’t go into detail here, but the “We recommend trying loo instead” part of the message is designed to prompt us to use a different information criteria, the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; aka, the LOO). In brms this is available with the loo() function, which you can learn more about in this vignette from the makers of the loo package. For now, back to the WAIC. There are a few ways to approach information criteria within the brms framework. If all you want are the quick results for a model, just plug the name of your brm() fit object into the waic() function. waic(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 4.4 1.8 ## p_waic 1.3 0.3 ## waic -8.8 3.7 The WAIC estimate and its standard error are on the bottom row. The \\(p_\\text{WAIC}\\) and its SE are stacked atop that. And look there on the top row. Remember how we pointed out, above, that we get the WAIC by multiplying (lppd - pwaic) by -2? Well, if you just do the subtraction without multiplying the result by -2, you get the elpd_waic. File that away. It’ll become important in a bit. Following the version 2.8.0 update, part of the suggested workflow for using information criteria with brms (i.e., execute ?loo.brmsfit) is to add the estimates to the brm() fit object itself. You do that with the add_criterion() function. Here’s how we’d do so with b6.11. b6.11 &lt;- add_criterion(b6.11, &quot;waic&quot;) ## Automatically saving the model object in &#39;fits/b06.11.rds&#39; With that in place, here’s how you’d extract the WAIC information from the fit object. b6.11$criteria$waic ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_waic 4.4 1.8 ## p_waic 1.3 0.3 ## waic -8.8 3.7 Why would I go through all that trouble?, you might ask. Well, two reasons. First, now your WAIC information is saved with all the rest of your fit output, which can be convenient. But second, it sets you up to use the loo_compare() function to compare models by their information criteria. To get a sense of that workflow, here we use add_criterion() for the next three models. Then we’ll use loo_compare(). # compute and save the WAIC information for the next three models b6.12 &lt;- add_criterion(b6.12, &quot;waic&quot;) b6.13 &lt;- add_criterion(b6.13, &quot;waic&quot;) b6.14 &lt;- add_criterion(b6.14, &quot;waic&quot;) # compare the WAIC estimates w &lt;- loo_compare(b6.11, b6.12, b6.13, b6.14, criterion = &quot;waic&quot;) print(w) ## elpd_diff se_diff ## b6.14 0.0 0.0 ## b6.13 -3.9 1.7 ## b6.11 -4.0 2.5 ## b6.12 -4.8 2.5 You don’t have to save those results as an object like we just did with w. But that’ll serve some pedagogical purposes in just a bit. With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences. The models have been rank ordered from the lowest (i.e., b6.14) to the highest (i.e., b6.12). The scores listed are the differences of b6.14 minus the comparison model. Since b6.14 is the comparison model in the top row, the values are naturally 0 (i.e., \\(x - x = 0\\)). But now here’s another critical thing to understand: Since the brms version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(-2 * x\\) metric. Remember how we keep rehearsing that multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist chi-square test? We’ll, the makers of the loo package aren’t fans and they no longer support the conversion. So here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the same metric, we’ll need to multiply the se_diff column by 2. You wouldn’t multiply by -2 because that would return a negative standard error, which would be silly. Here’s a quick way to do those conversions. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b6.14 0.000000 0.000000 ## b6.13 7.753212 3.493850 ## b6.11 8.009060 4.928090 ## b6.12 9.661431 5.076174 One more thing. On page 198, and on many other pages to follow in the text, McElreath used the rethinking::compare() function to return a rich table of information about the WAIC information for several models. If we’re tricky, we can do something similar with loo_compare. To learn how, let’s peer further into the structure of our w object. str(w) ## &#39;compare.loo&#39; num [1:4, 1:8] 0 -3.88 -4 -4.83 0 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:4] &quot;b6.14&quot; &quot;b6.13&quot; &quot;b6.11&quot; &quot;b6.12&quot; ## ..$ : chr [1:8] &quot;elpd_diff&quot; &quot;se_diff&quot; &quot;elpd_waic&quot; &quot;se_elpd_waic&quot; ... When we used print(w), a few code blocks earlier, it only returned two columns. It appears we actually have eight. We can see the full output with the simplify = F argument. print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b6.14 0.0 0.0 8.4 2.6 3.1 0.8 -16.8 5.2 ## b6.13 -3.9 1.7 4.5 2.1 2.0 0.4 -9.0 4.1 ## b6.11 -4.0 2.5 4.4 1.8 1.3 0.3 -8.8 3.7 ## b6.12 -4.8 2.5 3.6 1.6 1.9 0.3 -7.1 3.2 The results are quite analogous to those from rethinking::compare(). Again, the difference estimates are in the metric of the \\(\\text{elpd}\\). But the interpretation is the same and we can convert them to the traditional information criteria metric with simple multiplication. As we’ll see later, this basic workflow applies to the LOO, too. If you want to get those WAIC weights, you can use the brms::model_weights() function like so: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b6.11 b6.12 b6.13 b6.14 ## 0.02 0.01 0.02 0.96 That last round() line was just to limit the decimal-place precision. If you really wanted to go through the trouble, you could make yourself a little table like this: model_weights(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;) %&gt;% as_tibble() %&gt;% rename(weight = value) %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), weight = weight %&gt;% round(digits = 2)) %&gt;% select(model, weight) %&gt;% arrange(desc(weight)) %&gt;% knitr::kable() model weight b6.14 0.96 b6.11 0.02 b6.13 0.02 b6.12 0.01 With a little [] subsetting and light wrangling, we can convert the contents of our w object to a format suitable for plotting the WAIC estimates. w[, 7:8] %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model_name&quot;) %&gt;% ggplot(aes(x = model_name, y = waic, ymin = waic - se_waic, ymax = waic + se_waic)) + geom_pointrange(color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5], shape = 21) + labs(title = &quot;My custom WAIC plot&quot;, x = NULL, y = NULL) + coord_flip() + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) We briefly discussed the alternative information criteria, the LOO, above. Here’s how to use it in brms. loo(b6.11) ## ## Computed from 4000 by 17 log-likelihood matrix ## ## Estimate SE ## elpd_loo 4.4 1.9 ## p_loo 1.3 0.3 ## looic -8.7 3.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. The Pareto \\(k\\) values are a useful model fit diagnostic tool, which we’ll discuss later. But for now, realize that brms uses functions from the loo package to compute its WAIC and LOO values. In addition to the vignette, above, this vignette demonstrates the LOO with these very same examples from McElreath’s text. And if you’d like to dive a little deeper, check out Aki Vehtari’s GPSS2017 workshop or his talk from November 2018, Model assessment, selection and averaging. Let’s get back on track with the text. To put all this model comparison in perspective, in this analysis, the best model has more than 90% of the model weight. That’s pretty good. But with only [17] cases, the error on the WAIC estimate is substantial, and of course that uncertainty should propagate to the Akaike weights. So don’t get too excited. If we take the standard error of the difference from the [loo_compare()] table literally, you can think of the difference as a Gaussian distribution centered (for the difference between models [b6.14 and b6.11]) on [9.66] with a standard deviation of [5.08]. (p. 200) Did you notice the “[17]” part of the quote? In his Errata, McElreath pointed out he accidentally referred to the number of cases as 12, though it was indeed 17 (execute dim(d)). The quotation here reflects that correction. Anyway, here are those two values in the \\(\\text{elpd}\\) metric. w[4, 1:2] ## elpd_diff se_diff ## -4.830716 2.538087 And here we convert them to the WAIC metric. round(w[4, 1] * -2, 2) ## [1] 9.66 round(w[4, 2] * 2, 2) ## [1] 5.08 If it’s easier to see, here’s the same information in a tibble. tibble(value = c(&quot;difference&quot;, &quot;se&quot;), elpd = w[4, 1:2], conversion_factor = c(-2, 2)) %&gt;% mutate(waic = elpd * conversion_factor) ## # A tibble: 2 x 4 ## value elpd conversion_factor waic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 difference -4.83 -2 9.66 ## 2 se 2.54 2 5.08 Before we forget, McElreath gave some perspective difference between the models with the highest and lowest WAIC values (p. 200). But to the point, we can extract the two numerals and plug them into rnorm(). # how many draws would you like? n &lt;- 1e5 set.seed(6) # simulate diff &lt;- tibble(diff = rnorm(n, mean = w[4, 1] * -2, sd = w[4, 2] * 2)) diff %&gt;% summarise(the_probability_a_difference_is_negative = sum(diff &lt; 0) / n) ## # A tibble: 1 x 1 ## the_probability_a_difference_is_negative ## &lt;dbl&gt; ## 1 0.0286 In case you’re curious, this is a graphic version of what we just did. tibble(diff = -20:30) %&gt;% ggplot(aes(x = diff, ymin = 0)) + geom_ribbon(aes(ymax = dnorm(diff, w[4, 1] * -2, w[4, 2] * 2)), fill = carto_pal(7, &quot;BurgYl&quot;)[7]) + geom_ribbon(data = tibble(diff = -20:0), aes(ymax = dnorm(diff, w[4, 1] * -2, w[4, 2] * 2)), fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_vline(xintercept = 0, linetype = 3, color = carto_pal(7, &quot;BurgYl&quot;)[3]) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;WAIC difference distribution&quot;) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.5.1.2 Comparing estimates. The brms package doesn’t have anything like rethinking’s coeftab() function. However, one can get that information with a little ingenuity. Here we’ll employ the broom::tidy() function, which will save the summary statistics for our model parameters. For example, this is what it will produce for the full model, b6.14. tidy(b6.14) ## term estimate std.error lower upper ## 1 b_Intercept -1.07522093 0.58623296 -2.02818394 -0.14769761 ## 2 b_neocortex 2.77707110 0.90799759 1.33294393 4.24939244 ## 3 b_logmass -0.09602456 0.02786611 -0.14052089 -0.05174133 ## 4 sigma 0.13938968 0.03077474 0.09922028 0.19306080 ## 5 lp__ -19.17986597 1.65649656 -22.37652411 -17.25880850 Note, tidy() also grabs the log posterior (i.e., lp__), which we’ll exclude for our purposes. With a little purrr::map() code, you can save the brm() fits and their tidy() summaries into a nested tibble, and then unnest() the tibble for coeftab()-like use. my_coef_tab &lt;- tibble(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) head(my_coef_tab) ## # A tibble: 6 x 7 ## model fit term estimate std.error lower upper ## &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b6.11 &lt;brmsfit&gt; b_Intercept 0.659 0.0456 0.584 0.734 ## 2 b6.11 &lt;brmsfit&gt; sigma 0.188 0.0392 0.138 0.259 ## 3 b6.12 &lt;brmsfit&gt; b_Intercept 0.341 0.552 -0.554 1.26 ## 4 b6.12 &lt;brmsfit&gt; b_neocortex 0.471 0.815 -0.874 1.80 ## 5 b6.12 &lt;brmsfit&gt; sigma 0.194 0.0402 0.140 0.269 ## 6 b6.13 &lt;brmsfit&gt; b_Intercept 0.706 0.0579 0.612 0.801 Just a little more work and we’ll have a table analogous to the one McElreath produced with his coef_tab() function. my_coef_tab %&gt;% # learn more about `dplyr::complete()` here: https://rdrr.io/cran/tidyr/man/expand.html complete(model, term) %&gt;% select(model, term, estimate) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) %&gt;% knitr::kable() term b6.11 b6.12 b6.13 b6.14 b_Intercept 0.66 0.34 0.71 -1.08 b_logmass NA NA -0.03 -0.10 b_neocortex NA 0.47 NA 2.78 sigma 0.19 0.19 0.18 0.14 I’m also not aware of an efficient way in brms to reproduce Figure 6.12 for which McElreath nested his coeftab() argument in a plot() argument. However, one can build something similar by hand with a little data wrangling. # data wrangling wrangled_my_coef_tab &lt;- my_coef_tab %&gt;% complete(model, term) %&gt;% bind_rows( tibble( model = NA, term = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;), fit = NA, estimate = NA, std.error = NA, lower = NA, upper = NA)) %&gt;% mutate(axis = ifelse(is.na(model), term, model), model = factor(model, levels = str_c(&quot;b6.1&quot;, 1:4)), term = factor(term, levels = c(&quot;b_logmass&quot;, &quot;b_neocortex&quot;, &quot;sigma&quot;, &quot;b_Intercept&quot;, NA))) %&gt;% arrange(term, model) %&gt;% mutate(axis_order = letters[1:20], axis = ifelse(str_detect(axis, &quot;b6.&quot;), str_c(&quot; &quot;, axis), axis)) # plot ggplot(data = wrangled_my_coef_tab, aes(x = axis_order, y = estimate, ymin = lower, ymax = upper)) + theme_classic() + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + scale_x_discrete(NULL, labels = wrangled_my_coef_tab$axis) + ggtitle(&quot;My coeftab() plot&quot;) + coord_flip() + theme(text = element_text(family = &quot;Courier&quot;), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), panel.grid = element_blank()) However, if you’re willing to deviate just a bit from the format of McElreath’s coeftab() plot, here’s a more elegant way to work with our my_coef_tab tibble. my_coef_tab %&gt;% ggplot(aes(x = model, y = estimate, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2]) + geom_pointrange(shape = 21, color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5]) + labs(x = NULL, y = NULL) + coord_flip() + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4)), panel.grid = element_blank(), strip.background = element_rect(color = &quot;transparent&quot;)) + facet_wrap(~term, ncol = 1) 6.5.1.3 Rethinking: Barplots suck. Man, I agree. “The only problem with barplots is that they have bars” (p. 203). You can find alternatives here, here, here, here, and a whole bunch here. 6.5.2 Model averaging. Within the current brms framework, you can do model-averaged predictions with the pp_average() function. The default weighting scheme is with the LOO. Here we’ll use the weights = &quot;waic&quot; argument to match McElreath’s method in the text. Because pp_average() yields a matrix, we’ll want to convert it to a tibble before feeding it into ggplot2. # we need new data for both the `fitted()` and `pp_average()` functions nd &lt;- tibble(neocortex = seq(from = .5, to = .8, length.out = 30), mass = 4.5) # we&#39;ll get the `b6.14`-implied trajectory with `fitted()` f &lt;- fitted(b6.14, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # the model-average trajectory comes from `pp_average()` pp_average(b6.11, b6.12, b6.13, b6.14, weights = &quot;waic&quot;, method = &quot;fitted&quot;, # for new data predictions, use `method = &quot;predict&quot;` newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% # plot Figure 6.13 ggplot(aes(x = neocortex, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 1/4) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[6]) + geom_ribbon(data = f, aes(ymin = Q2.5, ymax = Q97.5), color = carto_pal(7, &quot;BurgYl&quot;)[5], fill = &quot;transparent&quot;, linetype = 2) + geom_line(data = f, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_point(data = d, aes(y = kcal.per.g), size = 2, color = carto_pal(7, &quot;BurgYl&quot;)[7]) + ylab(&quot;kcal.per.g&quot;) + coord_cartesian(xlim = range(d$neocortex), ylim = range(d$kcal.per.g)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) 6.6 Summary Bonus: \\(R^2\\) talk At the beginning of the chapter (pp. 167–168), McElreath briefly introduced \\(R^2\\) as a popular way to assess the variance explained in a model. He pooh-poohed it because of its tendency to overfit. It’s also limited in that it doesn’t generalize well outside of the single-level Gaussian framework, which will be a big deal for us starting in Chapter 10. However, if you should find yourself in a situation where \\(R^2\\) suits your purposes, the brms bayes_R2() function might be of use. Simply feeding a model brm() fit object into bayes_R2() will return the posterior mean, \\(SD\\), and 95% intervals. For example: bayes_R2(b6.14) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.497 0.129 0.179 0.664 With just a little data processing, you can get a tibble table of each of models’ \\(R^2\\) ‘Estimate’. rbind(bayes_R2(b6.11), bayes_R2(b6.12), bayes_R2(b6.13), bayes_R2(b6.14)) %&gt;% as_tibble() %&gt;% mutate(model = c(&quot;b6.11&quot;, &quot;b6.12&quot;, &quot;b6.13&quot;, &quot;b6.14&quot;), r_square_posterior_mean = round(Estimate, digits = 2)) %&gt;% select(model, r_square_posterior_mean) ## # A tibble: 4 x 2 ## model r_square_posterior_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 b6.11 0 ## 2 b6.12 0.07 ## 3 b6.13 0.15 ## 4 b6.14 0.5 If you want the full distribution of the \\(R^2\\), you’ll need to add a summary = F argument. Note how this returns a numeric vector. r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) r2_b6.13 %&gt;% glimpse() ## num [1:4000, 1] 0.3935 0.0376 0.0936 0.2198 0.0722 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; If you want to use these in ggplot2, you’ll need to put them in tibbles or data frames. Here we do so for two of our model fits. # model `b6.13` r2_b6.13 &lt;- bayes_R2(b6.13, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_13 = R2) # model `b6.14` r2_b6.14 &lt;- bayes_R2(b6.14, summary = F) %&gt;% as_tibble() %&gt;% rename(r2_14 = R2) # let&#39;s put them in the same data object r2_combined &lt;- bind_cols(r2_b6.13, r2_b6.14) %&gt;% mutate(dif = r2_14 - r2_13) # plot their densities r2_combined %&gt;% ggplot() + geom_density(aes(x = r2_13), fill = carto_pal(7, &quot;BurgYl&quot;)[4], alpha = 3/4, size = 0, ) + geom_density(aes(x = r2_14), fill = carto_pal(7, &quot;BurgYl&quot;)[6], alpha = 3/4, size = 0, ) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + labs(title = expression(italic(R)^2~distributions), subtitle = &quot;Going from left to right, these are\\nfor models b6.13 and b6.14.&quot;, x = NULL) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If you do your work in a field where folks use \\(R^2\\) change, you might do that with a simple difference score, which we computed above with mutate(dif = R2.14 - R2.13). Here’s the \\(\\Delta R^2\\) (i.e., dif) plot: r2_combined %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7], point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(Delta*italic(R)^2)) + theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) If you multiply those values by 100, they make a distribution for the greater percentage in kcal.per.g variance b6.14 explained compared to b6.13. The brms package did not get these \\(R^2\\) values by traditional method used in, say, ordinary least squares estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out the paper by Gelman, Goodrich, Gabry, and Vehtari. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rstan_2.19.2 StanHeaders_2.19.0 tidybayes_2.0.1.9000 brms_2.12.0 ## [5] Rcpp_1.0.3 patchwork_1.0.0 broom_0.5.3 ggrepel_0.8.1 ## [9] rcartocolor_2.0.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [13] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [17] ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 dbplyr_1.4.2 ## [22] shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 prettyunits_1.1.1 htmltools_0.4.0 ## [34] tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [43] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [46] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 ## [52] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 loo_2.2.0 stringi_1.4.5 ## [64] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [67] rlang_0.4.4 pkgconfig_2.0.3 matrixStats_0.55.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 tidyselect_1.0.0 ## [76] processx_3.4.1 plyr_1.8.5 magrittr_1.5 ## [79] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [82] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [85] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [94] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [97] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [100] stats4_3.6.2 munsell_0.5.0 viridisLite_0.3.0 ## [103] shinyjs_1.1 "],
["interactions.html", "7 Interactions 7.1 Building an interaction. 7.2 Symmetry of the linear interaction. 7.3 Continuous interactions 7.4 Interactions in design formulas 7.5 Summary Bonus: marginal_effects()/conditional_effects() Reference Session info", " 7 Interactions Every model so far in [McElreath’s text] has assumed that each predictor has an independent association with the mean of the outcome. What if we want to allow the association to be conditional?… To model deeper conditionality—where the importance of one predictor depends upon another predictor—we need interaction. Interaction is a kind of conditioning, a way of allowing parameters (really their posterior distributions) to be conditional on further aspects of the data. (p. 210) 7.1 Building an interaction. “Africa is special” (p. 211). Let’s load the rugged data (Nunn &amp; Puga, 2011) to see one of the reasons why. library(rethinking) data(rugged) d &lt;- rugged And here we switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) rm(rugged) We’ll continue to use tidyverse-style syntax to wrangle the data. library(tidyverse) # make the log version of criterion d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) # extract countries with GDP data dd &lt;- d %&gt;% filter(complete.cases(rgdppc_2000)) # split the data into countries in Africa and not in Africa d.A1 &lt;- dd %&gt;% filter(cont_africa == 1) d.A0 &lt;- dd %&gt;% filter(cont_africa == 0) The first two models predicting log_gdp are univariable. b7.1 &lt;- brm(data = d.A1, family = gaussian, log_gdp ~ 1 + rugged, prior = c(prior(normal(8, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.01&quot;) b7.2 &lt;- update(b7.1, newdata = d.A0, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.02&quot;) In the text, McElreath more or less dared us to figure out how to make Figure 7.2. Here’s the brms-relevant data wrangling. nd &lt;- tibble(rugged = seq(from = 0, to = 6.3, length.out = 30)) f &lt;- # bind the two `fitted()` summaries together rbind(fitted(b7.1, newdata = nd), fitted(b7.2, newdata = nd)) %&gt;% as_tibble() %&gt;% # add the `nd` data, one copy stacked atop another bind_cols( bind_rows(nd, nd) ) %&gt;% mutate(cont_africa = rep(c(&quot;Africa&quot;, &quot;not Africa&quot;), each = 30)) For this chapter, we’ll take our plot theme from the ggthemes package. # install.packages(&quot;ggthemes&quot;, dependencies = T) library(ggthemes) Here’s the plot code for our version of Figure 7.2. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged, color = cont_africa, fill = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) It’s generally not a good idea to split up your data and run separate analyses, like this. McElreath listed four reasons why: “There are usually some parameters, such as \\(\\sigma\\), that the model says do not depend in any way upon an African identity for each nation. By splitting the data table, you are hurting the accuracy f the estimates for these parameters” (p. 213). “In order to acquire probability statements about the variable you used to split the data, cont_africa, in this case, you need to include it in the model” (p. 213). “We many want to use information criteria or another method to compare models” (p. 214). “Once you begin using multilevel models (Chapter 12), you’ll see that there are advantages to borrowing information across categories like ‘Africa’ and ‘not Africa’” (p. 214). 7.1.1 Adding a dummy variable doesn’t work. Here’s our model with all the countries, but without the cont_africa dummy. b7.3 &lt;- update(b7.1, newdata = dd, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.03&quot;) Now we’ll add the dummy. b7.4 &lt;- update(b7.3, newdata = dd, formula = log_gdp ~ 1 + rugged + cont_africa, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.04&quot;) Using the skills from Chapter 6, let’s compute the information criteria for the two models. Note how with the add_criterion() function, you can compute both the LOO and the WAIC at once. b7.3 &lt;- add_criterion(b7.3, c(&quot;loo&quot;, &quot;waic&quot;)) b7.4 &lt;- add_criterion(b7.4, c(&quot;loo&quot;, &quot;waic&quot;)) Here we’ll compare the models with the loo_compare() function, first by the WAIC and then by the LOO. loo_compare(b7.3, b7.4, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## b7.4 0.0 0.0 ## b7.3 -31.7 7.3 loo_compare(b7.3, b7.4, criterion = &quot;loo&quot;) ## elpd_diff se_diff ## b7.4 0.0 0.0 ## b7.3 -31.7 7.3 Happily, the WAIC and the LOO are in agreement. The model with the dummy, b7.4, fit the data much better. Here are the WAIC model weights. model_weights(b7.3, b7.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 ## 0 1 As in the text, almost all the weight went to the multivariable model, b7.4. Before we can plot that model, we need to wrangle a bit. nd &lt;- crossing(cont_africa = 0:1, rugged = seq(from = 0, to = 6.3, length.out = 30)) f &lt;- fitted(b7.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) Behold our Figure 7.3. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged, fill = cont_africa, color = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.direction = &quot;horizontal&quot;, legend.position = c(.69, .94), legend.title = element_blank()) 7.1.2 Adding a linear interaction does work. Yes, it sure does. But before we fit, here’s the equation: \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = \\beta_1 + \\beta_3 \\text{cont_africa}_i \\\\ \\alpha &amp; \\sim \\text{Normal}(8, 100) \\\\ \\beta_1, \\beta_2, \\text{ and } \\beta_3 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 10). \\end{align*}\\] Because \\(\\gamma_i\\) is just a placeholder for a second linear model, we can just substitute that second linear model in for \\(\\gamma_i\\). If we did, here’s what the composite linear model would look like: \\[\\mu_i = \\alpha + (\\beta_1 + \\beta_3 \\text{cont_africa}_i) \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i\\] Fit the model. b7.5 &lt;- update(b7.4, formula = log_gdp ~ 1 + rugged*cont_africa, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.05&quot;) For kicks, we’ll just use the LOO to compare the last three models. b7.5 &lt;- add_criterion(b7.5, c(&quot;loo&quot;, &quot;waic&quot;)) l &lt;- loo_compare(b7.3, b7.4, b7.5, criterion = &quot;loo&quot;) print(l, simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b7.4 0.0 0.0 -237.9 7.4 4.0 0.8 475.9 14.7 ## b7.5 0.0 0.0 -237.9 7.4 4.0 0.8 475.9 14.7 ## b7.3 -31.7 7.3 -269.7 6.5 2.5 0.3 539.3 12.9 And recall, if we want those LOO difference scores in the traditional metric like McElreath displayed in the text, we can do a quick conversion with algebra and cbind(). cbind(loo_diff = l[, 1] * -2, se = l[, 2] * 2) ## loo_diff se ## b7.4 0.00000 0.00000 ## b7.5 0.00000 0.00000 ## b7.3 63.44501 14.54413 And we can weight the models based on the LOO rather than the WAIC, too. model_weights(b7.3, b7.4, b7.5, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b7.3 b7.4 b7.5 ## 0.0 0.5 0.5 7.1.2.1 Overthinking: Conventional form of interaction. The conventional equation for the interaction model might look like: \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i. \\end{align*}\\] Instead of the y ~ 1 + x1*x2 approach, which will work fine with brm(), you can use this more explicit syntax. b7.5b &lt;- update(b7.5, formula = log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.05b&quot;) From here on, I will default to this style of syntax for interactions. Since this is the same model, it yields the same information criteria estimates. Here we’ll confirm that with the LOO. b7.5b &lt;- add_criterion(b7.5b, c(&quot;loo&quot;, &quot;waic&quot;)) b7.5$criteria$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -237.9 7.4 ## p_loo 4.0 0.8 ## looic 475.9 14.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. b7.5b$criteria$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -237.9 7.4 ## p_loo 4.0 0.8 ## looic 475.9 14.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. When compared, they have the exact same LOO weights, too. model_weights(b7.5, b7.5b, weights = &quot;loo&quot;) ## b7.5 b7.5b ## 0.5 0.5 7.1.3 Plotting the interaction. Here’s our prep work for the figure. f &lt;- fitted(b7.5, newdata = nd) %&gt;% # we can use the same `nd` data from last time as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) And here’s the code for our version of Figure 7.4. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged, fill = cont_africa, color = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), size = 2/3) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Terrain Ruggedness Index&quot;, expand = c(0, 0)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) + facet_wrap(~cont_africa) 7.1.4 Interpreting an interaction estimate. Interpreting interaction estimates is tricky. It’s trickier than interpreting ordinary estimates. And for this reason, I usually advise against trying to understand an interaction from tables of numbers alone. Plotting implied predictions does far more for both our own understanding and for our audience’s. (p. 219) 7.1.4.1 Parameters change meaning. In a simple linear regression with no interactions, each coefficient says how much the average outcome, \\(\\mu\\), changes when the predictor changes by one unit. And since all of the parameters have independent influences on the outcome, there’s no trouble in interpreting each parameter separately. Each slope parameter gives us a direct measure of each predictor variable’s influence. Interaction models ruin this paradise. (p. 220) Return the parameter estimates. posterior_summary(b7.5) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 9.1859322 0.14021753 8.9145645 9.46384641 ## b_rugged -0.1839381 0.07629199 -0.3324139 -0.03105652 ## b_cont_africa -1.8459602 0.22240216 -2.2864535 -1.42064224 ## b_rugged:cont_africa 0.3478548 0.12783067 0.0966784 0.60024964 ## sigma 0.9517599 0.05345764 0.8553603 1.06328631 ## lp__ -244.4199901 1.55496137 -248.1708028 -242.31659934 “Since \\(\\gamma\\) (gamma) doesn’t appear in this table–it wasn’t estimated–we have to compute it ourselves” (p. 221). Like in the text, we’ll do so first by working with the point estimates. # slope relating `rugged` to `log_gdp` within Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 1 ## [1] 0.1639167 # slope relating `rugged` to `log_gdp` outside of Africa fixef(b7.5)[2, 1] + fixef(b7.5)[4, 1] * 0 ## [1] -0.1839381 7.1.4.2 Incorporating uncertainty. To get some idea of the uncertainty around those \\(\\gamma\\) values, we’ll need to use the whole posterior. Since \\(\\gamma\\) depends upon parameters, and those parameters have a posterior distribution, \\(\\gamma\\) must also have a posterior distribution. Read the previous sentence again a few times. It’s one of the most important concepts in processing Bayesian model fits. Anything calculated using parameters has a distribution. (p. 212, emphasis added) Like McElreath, we’ll avoid integral calcus in favor of working with the posterior_samples(). post &lt;- posterior_samples(b7.5) post %&gt;% transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa` * 1, gamma_notAfrica = b_rugged + `b_rugged:cont_africa` * 0) %&gt;% gather(key, value) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) ## # A tibble: 2 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 gamma_Africa 0.164 ## 2 gamma_notAfrica -0.184 And here is our version of Figure 7.5. post %&gt;% # this `transmute()` code returns the same thing as the one in the block above transmute(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% gather(key, value) %&gt;% ggplot(aes(x = value, group = key, color = key, fill = key)) + geom_density(alpha = 1/4) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(expression(gamma), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Terraine Ruggedness slopes&quot;, subtitle = &quot;Blue = African nations, Green = others&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) What proportion of these differences is below zero? post %&gt;% mutate(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% mutate(diff = gamma_Africa -gamma_notAfrica) %&gt;% summarise(Proportion_of_the_difference_below_0 = sum(diff &lt; 0) / length(diff)) ## Proportion_of_the_difference_below_0 ## 1 0.00275 The distributions in the figure are marginal, like silhouettes of each distribution, ignoring all of the other dimensions in the posterior. The calculation above is the distribution of the difference between the two. The distribution of their difference is not the same as the visual overlap of their marginal distributions. This is also the reason we can’t use overlap in confidence intervals of different parameters as an informal test of “significance” of the difference. If you care about the difference, you must compute the distribution of the difference directly. (p. 222, emphasis in the original) Why stop with computation when you can plot? post %&gt;% mutate(gamma_Africa = b_rugged + `b_rugged:cont_africa`, gamma_notAfrica = b_rugged) %&gt;% mutate(diff = gamma_Africa -gamma_notAfrica) %&gt;% ggplot(aes(x = diff)) + geom_density(alpha = 1/4, color = palette_pander(n = 5)[5], fill = palette_pander(n = 5)[5]) + scale_x_continuous(expression(gamma[1]-gamma[0]), expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Distribution of the difference&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) 7.2 Symmetry of the linear interaction. Consider for example the GDP and terrain ruggedness problem. The interaction there has two equally valid phrasings. How much does the influence of ruggedness (on GDP) depend upon whether the nation is in Africa? How much does the influence of being in Africa (on GDP) depend upon ruggedness? While these two possibilities sound different to most humans, your golem thinks they are identical. (p. 223) 7.2.1 Buridan’s interaction. Recall the original equation, \\[\\begin{align*} \\text{log_gdp}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\gamma_i \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ \\gamma_i &amp; = \\beta_1 + \\beta_3 \\text{cont_africa}_i. \\end{align*}\\] Next McElreath replaced \\(\\gamma_i\\) with the expression for \\(\\mu_i\\): \\[\\begin{align*} \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_3 \\text{cont_africa}_i) \\times \\text{rugged}_i + \\beta_2 \\text{cont_africa}_i \\\\ &amp; = \\alpha + \\beta_1 \\text{rugged}_i + \\beta_3 \\text{rugged}_i \\times \\text{cont_africa}_i + \\beta_2 \\text{cont_africa}_i. \\end{align*}\\] And now we’ll factor together the terms containing \\(\\text{cont_africa}_i\\): \\[ \\mu_i = \\alpha + \\beta_1 \\text{rugged}_i + \\underbrace{(\\beta_2 + \\beta_3 \\text{rugged}_i)}_G \\times \\text{cont_africa}_i. \\] And just as in the text, our \\(G\\) term looks a lot like the original \\(\\gamma_i\\) term. 7.2.2 Africa depends upon ruggedness. Here is our version of McElreath’s Figure 7.6. # new predictor data for `fitted()` nd &lt;- crossing(cont_africa = 0:1, rugged = range(dd$rugged)) # `fitted()` f &lt;- fitted(b7.5, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(ox = rep(c(-0.05, 0.05), times = 2)) # augment the `dd` data a bit dd %&gt;% mutate(ox = ifelse(rugged &gt; median(rugged), 0.05, -0.05), cont_africa = cont_africa + ox) %&gt;% mutate(ox = factor(ox)) %&gt;% select(cont_africa, everything()) %&gt;% # plot ggplot(aes(x = cont_africa, color = ox, fill = ox)) + geom_smooth(data = f %&gt;% mutate(ox = factor(ox)), aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, linetype = ox), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp), alpha = 1/2, shape = 1) + scale_colour_pander() + scale_fill_pander() + scale_x_continuous(&quot;Continent&quot;, breaks = 0:1, labels = c(&quot;other&quot;, &quot;Africa&quot;)) + coord_cartesian(xlim = c(-.2, 1.2)) + ylab(&quot;log GDP from year 2000&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), legend.position = &quot;none&quot;) 7.3 Continuous interactions Though continuous interactions can be more challenging to interpret, they’re just as easy to fit as interactions including dummies. 7.3.1 The data. Look at the tulips (adapted from Grafen &amp; Hails, 2000). library(rethinking) data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 7.3.2 The un-centered models. The equations for the next two models are \\[\\begin{align*} \\text{blooms}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water}_i + \\beta_2 \\text{shade}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim \\text{Uniform} (0, 100) \\end{align*}\\] and \\[\\begin{align*} \\text{blooms}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water} + \\beta_2 \\text{shade}_i + \\beta_3 \\text{water}_i \\times \\text{shade}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_3 &amp; \\sim \\text{Normal} (0, 100) \\\\ \\sigma &amp; \\sim \\text{Uniform} (0, 100). \\end{align*}\\] Load brms. detach(package:rethinking, unload = T) library(brms) rm(tulips) Here we continue with McElreath’s very-flat priors for the multivariable and interaction models. b7.6 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water + shade, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(uniform(0, 100), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 7, file = &quot;fits/b07.06&quot;) b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade, iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 7, file = &quot;fits/b07.07&quot;) Much like in the text, these models yielded divergent transitions. Here, we’ll try to combat them by following Stan’s advice and “[increase] adapt_delta above 0.8.” While we’re at it, we’ll put better priors on \\(\\sigma\\). b7.6 &lt;- update(b7.6, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), control = list(adapt_delta = 0.9), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 7, file = &quot;fits/b07.06&quot;) b7.7 &lt;- update(b7.6, formula = blooms ~ 1 + water + shade + water:shade, iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 7, file = &quot;fits/b07.07&quot;) Increasing adapt_delta did the trick. Instead of coeftab(), we can also use the posterior_summary() function, which gets us most of the way there. posterior_summary(b7.6) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 60.91 43.23 -25.67 145.54 ## b_water 73.85 14.41 44.54 101.82 ## b_shade -40.78 14.63 -68.78 -11.44 ## sigma 61.41 9.18 46.52 81.32 ## lp__ -169.75 1.53 -173.65 -167.87 posterior_summary(b7.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -106.23 62.81 -224.92 20.94 ## b_water 159.27 28.94 100.33 215.54 ## b_shade 43.36 29.18 -16.22 97.65 ## b_water:shade -42.86 13.43 -68.93 -15.67 ## sigma 50.08 7.76 37.52 68.04 ## lp__ -170.68 1.74 -174.97 -168.39 This is an example where HMC yielded point estimates notably different from MAP. However, look at the size of those posterior standard deviations (i.e., ‘Est.Error’ column)! The MAP estimates are well within a fraction of those \\(SD\\)s. Let’s look at WAIC. b7.6 &lt;- add_criterion(b7.6, &quot;waic&quot;) b7.7 &lt;- add_criterion(b7.7, &quot;waic&quot;) w &lt;- loo_compare(b7.6, b7.7, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b7.7 0.0 0.0 -146.8 3.9 4.6 1.2 293.6 7.9 ## b7.6 -5.2 2.7 -152.0 3.8 4.0 1.0 304.0 7.7 Here we use our cbind() trick to convert the difference from the \\(\\text{elpd}\\) metric to the more traditional WAIC metric. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b7.7 0.00000 0.000000 ## b7.6 10.40028 5.330121 Why not compute the WAIC weights? model_weights(b7.6, b7.7, weights = &quot;waic&quot;) ## b7.6 b7.7 ## 0.005485528 0.994514472 As in the text, almost all the weight went to the interaction model, b7.7. 7.3.2.1 Rethinking: Fighting with your robot. The trouble-shooting in the preceding section is annoying, but it’s realistic. These kinds of issues routinely arise in model fitting. With linear models like these, there are ways to compute the posterior distribution that avoid many of these complications. But with non-linear models to come, there is really no way to dodge the issue. In general, how you fit the model is part of the model. (pp. 229–230, emphasis added) 7.3.3 Center and re-estimate. To center a variable means to create a new variable that contains the same information as the original, but has a new mean of zero. For example, to make centered versions of shade and water, just subtract the mean of the original from each value. (p. 230, emphasis in the original) Here’s a tidyverse way to center the predictors. d &lt;- d %&gt;% mutate(shade_c = shade - mean(shade), water_c = water - mean(water)) Now refit the models with our shiny new centered predictors. b7.8 &lt;- brm(data = d, family = gaussian, blooms ~ 1 + water_c + shade_c, prior = c(prior(normal(130, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 7, file = &quot;fits/b07.08&quot;) b7.9 &lt;- update(b7.8, formula = blooms ~ 1 + water_c + shade_c + water_c:shade_c, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.09&quot;) Check out the results. posterior_summary(b7.8) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 129.14 11.67 106.23 152.11 ## b_water_c 74.03 14.88 45.50 103.39 ## b_shade_c -40.76 14.78 -70.25 -11.38 ## sigma 61.76 9.19 46.97 83.38 ## lp__ -168.96 1.57 -172.93 -167.02 posterior_summary(b7.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 129.14 9.70 109.34 148.22 ## b_water_c 74.60 11.93 50.63 98.56 ## b_shade_c -40.95 11.62 -64.73 -17.87 ## b_water_c:shade_c -51.53 14.25 -79.19 -23.27 ## sigma 49.71 7.54 37.24 67.08 ## lp__ -168.59 1.73 -172.79 -166.27 And okay fine, if you really want a coeftab()-like summary, here’s a way to do it. tibble(model = str_c(&quot;b7.&quot;, 8:9)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, broom::tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% select(term, estimate, model) %&gt;% spread(key = model, value = estimate) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 5 x 3 ## term b7.8 b7.9 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 129. 129. ## 2 b_shade_c -40.8 -41.0 ## 3 b_water_c 74.0 74.6 ## 4 b_water_c:shade_c NA -51.5 ## 5 sigma 61.8 49.7 Anyway, centering helped a lot. Now, not only do the results in the text match up better than those from Stan, but the ‘Est.Error’ values are uniformly smaller. 7.3.3.1 Estimation worked better. Nothing to add, here. 7.3.3.2 Estimates changed less across models. On page 231, we read: The interaction parameter always factors into generating a prediction. Consider for example a tulip at the average moisture and shade levels, 2 in each case. The expected blooms for such a tulip is: \\[\\mu_i | \\text{shade}_{i = 2}, \\text{water}_{i = 2} = \\alpha + \\beta_\\text{water} (2) + \\beta_\\text{shade} (2) + \\beta_{\\text{water} \\times \\text{shade}} (2 \\times 2)\\] So to figure out the effect of increasing water by 1 unit, you have to use all of the \\(\\beta\\) parameters. Plugging in the [HMC] values for the un-centered interaction model, [b7.7], we get: \\[\\mu_i | \\text{shade}_{i = 2}, \\text{water}_{i = 2} = -106.2 + 159.3 (2) + 43.4 (2) -42.9 (2 \\times 2)\\] With our brms workflow, we use fixef() to compute the predictions. k &lt;- fixef(b7.7) k[1] + k[2] * 2 + k[3] * 2 + k[4] * 2 * 2 ## [1] 127.6031 Even though or HMC parameters differ a bit from the MAP estimates McElreath reported in the text, the value they predicted matches quite closely with the one in the text. Same thing for the next example. k &lt;- fixef(b7.9) k[1] + k[2] * 0 + k[3] * 0 + k[4] * 0 * 0 ## [1] 129.1408 Here are the coefficient summaries for the centered model. print(b7.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: blooms ~ water_c + shade_c + water_c:shade_c ## Data: d (Number of observations: 27) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 129.14 9.70 109.34 148.22 1.00 4705 2616 ## water_c 74.60 11.93 50.63 98.56 1.00 5449 3174 ## shade_c -40.95 11.62 -64.73 -17.87 1.00 4853 2518 ## water_c:shade_c -51.53 14.25 -79.19 -23.27 1.00 4586 2950 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 49.71 7.54 37.24 67.08 1.00 4089 2727 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 7.3.4 Plotting implied predictions. Now we’re ready for the bottom row of Figure 7.7. Here’s our variation on McElreath’s tryptych loop code, adjusted for brms and ggplot2. # loop over values of `water_c` and plot predictions shade_seq &lt;- -1:1 for(w in -1:1) { # define the subset of the original data dt &lt;- d[d$water_c == w, ] # defining our new data nd &lt;- tibble(water_c = w, shade_c = shade_seq) # use our sampling skills, like before f &lt;- fitted(b7.9, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # specify our custom plot fig &lt;- ggplot() + geom_smooth(data = f, aes(x = shade_c, y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#CC79A7&quot;, color = &quot;#CC79A7&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dt, aes(x = shade_c, y = blooms), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + labs(&quot;Blooms&quot;, title = paste(&quot;Water (centered) =&quot;, w)) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;)) # plot that joint plot(fig) } But we don’t necessarily need a loop. We can achieve all of McElreath’s Figure 7.7 with fitted(), some data wrangling, and a little help from ggplot2::facet_grid(). # `fitted()` for model b7.8 fitted(b7.8) %&gt;% as_tibble() %&gt;% # add `fitted()` for model b7.9 bind_rows( fitted(b7.9) %&gt;% as_tibble() ) %&gt;% # we&#39;ll want to index the models mutate(fit = rep(c(&quot;b7.8&quot;, &quot;b7.9&quot;), each = 27)) %&gt;% # here we add the data, `d` bind_cols(bind_rows(d, d)) %&gt;% # these will come in handy for `ggplot2::facet_grid()` mutate(x_grid = str_c(&quot;water_c = &quot;, water_c), y_grid = str_c(&quot;model: &quot;, fit)) %&gt;% # plot! ggplot(aes(x = shade_c)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#CC79A7&quot;, color = &quot;#CC79A7&quot;, alpha = 1/5, size = 1/2) + geom_point(aes(y = blooms, group = x_grid), shape = 1, color = &quot;#CC79A7&quot;) + coord_cartesian(xlim = range(d$shade_c), ylim = range(d$blooms)) + scale_x_continuous(&quot;Shade (centered)&quot;, breaks = c(-1, 0, 1)) + ylab(&quot;Blooms&quot;) + theme_pander() + theme(text = element_text(family = &quot;Times&quot;), panel.background = element_rect(color = &quot;black&quot;)) + facet_grid(y_grid ~ x_grid) 7.4 Interactions in design formulas The brms syntax generally follows the design formulas typical of lm(). Hopefully this is all old hat. 7.5 Summary Bonus: marginal_effects()/conditional_effects() The brms package includes the conditional_effects() function as a convenient way to look at simple effects and two-way interactions. To clarify, it was previously known as marginal_effects() until brms version 2.10.3 (see here). Recall the simple univariable model, b7.3: b7.3$formula ## log_gdp ~ 1 + rugged We can look at the regression line and its percentile-based intervals like so: conditional_effects(b7.3) If we feed the conditional_effects() output into the plot() function with a points = T argument, we can add the original data to the figure. conditional_effects(b7.3) %&gt;% plot(points = T) We can further customize the plot. For example, we can replace the intervals with a spaghetti plot. While we’re at it, we can use point_args to adjust the geom_jitter() parameters. conditional_effects(b7.3, spaghetti = T, nsamples = 200) %&gt;% plot(points = T, point_args = c(alpha = 1/2, size = 1)) With multiple predictors, things get more complicated. Consider our multivariable, non-interaction model, b7.4. b7.4$formula ## log_gdp ~ rugged + cont_africa conditional_effects(b7.4) We got one plot for each predictor, controlling the other predictor at zero. Note how the plot for cont_africa treated it as a continuous variable. This is because the variable was saved as an integer in the original data set: b7.4$data %&gt;% glimpse() ## Observations: 170 ## Variables: 3 ## $ log_gdp &lt;dbl&gt; 7.492609, 8.216929, 9.933263, 9.407032, 7.792343, 9.212541, 10.143191, 10.274… ## $ rugged &lt;dbl&gt; 0.858, 3.427, 0.769, 0.775, 2.688, 0.006, 0.143, 3.513, 1.672, 1.780, 0.388, … ## $ cont_africa &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,… One way to fix that is to adjust the data set and refit the model. d_factor &lt;- b7.4$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.4_factor &lt;- update(b7.4, newdata = d_factor, file = &quot;fits/b07.04_factor&quot;) Using the update() syntax often speeds up the re-fitting process. conditional_effects(b7.4_factor) Now our second marginal plot more clearly expresses the cont_africa predictor as categorical. Things get more complicated with the interaction model, b7.5. b7.5$formula ## log_gdp ~ rugged + cont_africa + rugged:cont_africa conditional_effects(b7.5) The conditional_effects() function defaults to expressing interactions such that the first variable in the term–in this case, rugged–is on the x axis and the second variable in the term–cont_africa, treated as an integer–is depicted in three lines corresponding its mean and its mean \\(\\pm\\) one standard deviation. This is great for continuous variables, but incoherent for categorical ones. The fix is, you guessed it, to refit the model after adjusting the data. d_factor &lt;- b7.5$data %&gt;% mutate(cont_africa = factor(cont_africa)) b7.5_factor &lt;- update(b7.5, newdata = d_factor, file = &quot;fits/b07.05_factor&quot;) Just for kicks, we’ll use probs = c(.25, .75) to return 50% intervals, rather than the conventional 95%. conditional_effects(b7.5_factor, probs = c(.25, .75)) With the effects argument, we can just return the interaction effect, which is where all the action’s at. While we’re at it, we’ll use plot() to change some of the settings. conditional_effects(b7.5_factor, effects = &quot;rugged:cont_africa&quot;, spaghetti = T, nsamples = 150) %&gt;% plot(points = T, point_args = c(alpha = 2/3, size = 1), mean = F) Note, the ordering of the variables matters for the interaction term. Consider our interaction model for the tulips data. b7.9$formula ## blooms ~ water_c + shade_c + water_c:shade_c The plot tells a slightly different story, depending on whether you specify effects = &quot;shade_c:water_c&quot; or effects = &quot;water_c:shade_c&quot;. conditional_effects(b7.9, effects = &quot;shade_c:water_c&quot;) %&gt;% plot(points = T) conditional_effects(b7.9, effects = &quot;water_c:shade_c&quot;) %&gt;% plot(points = T) One might want to evaluate the effects of the second term in the interaction–water_c, in this case–at values other than the mean and the mean \\(\\pm\\) one standard deviation. When we reproduced the bottom row of Figure 7.7, we expressed the interaction based on values -1, 0, and 1 for water_c. We can do that, here, by using the int_conditions argument. It expects a list, so we’ll put our desired water_c values in just that. ic &lt;- list(water_c = c(-1, 0, 1)) conditional_effects(b7.9, effects = &quot;shade_c:water_c&quot;, int_conditions = ic) %&gt;% plot(points = T) Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggthemes_4.2.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [6] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 tidyverse_1.3.0 brms_2.12.0 ## [11] Rcpp_1.0.3 rstan_2.19.2 ggplot2_3.2.1 StanHeaders_2.19.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.3.1 rstudioapi_0.10 ## [9] farver_2.0.3 DT_0.11 fansi_0.4.1 mvtnorm_1.0-12 ## [13] lubridate_1.7.4 xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [17] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 broom_0.5.3 ## [21] dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [29] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 htmltools_0.4.0 ## [33] prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [41] vctrs_0.2.2 nlme_3.1-142 crosstalk_1.0.0 xfun_0.12 ## [45] ps_1.3.0 rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 zoo_1.8-7 ## [53] scales_1.1.0 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [57] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 pander_0.6.3 loo_2.2.0 stringi_1.4.5 ## [65] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.4 pkgconfig_2.0.3 ## [69] matrixStats_0.55.0 evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [77] plyr_1.8.5 magrittr_1.5 R6_2.4.1 generics_0.0.2 ## [81] DBI_1.1.0 pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [85] xts_0.12-0 abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## [89] utf8_1.1.4 rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [93] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 digest_0.6.23 ## [97] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 ## [101] shinyjs_1.1 "],
["markov-chain-monte-carlo.html", "8 Markov Chain Monte Carlo 8.1 Good King Markov and His island kingdom 8.2 Markov chain Monte Carlo 8.3 Easy HMC: map2stan brm() 8.4 Care and feeding of your Markov chain. Reference Session info", " 8 Markov Chain Monte Carlo “This chapter introduces one of the more marvelous examples of how Fortuna and Minerva cooperate: the estimation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC) estimation” (p. 241). Though we’ve been using MCMC via the brms package for chapters, now, this chapter should clarify some of the questions you might have about the details. 8.1 Good King Markov and His island kingdom Here we simulate King Markov’s journey. In this version of the code, we’ve added set.seed(), which helps make the exact results reproducible. set.seed(8) num_weeks &lt;- 1e5 positions &lt;- rep(0, num_weeks) current &lt;- 10 for (i in 1:num_weeks) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around the archipelago if (proposal &lt; 1) proposal &lt;- 10 if (proposal &gt; 10) proposal &lt;- 1 # move? prob_move &lt;- proposal / current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } In this chapter, we’ll borrow a theme, theme_ipsum(), from the hrbrthemes package. # install.packages(&quot;hrbrthemes&quot;, dependencies = T) library(hrbrthemes) Figure 8.2.a. library(tidyverse) tibble(week = 1:1e5, island = positions) %&gt;% ggplot(aes(x = week, y = island)) + geom_point(shape = 1) + scale_x_continuous(breaks = seq(from = 0, to = 100, by = 20)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + coord_cartesian(xlim = 0:100) + labs(title = &quot;Behold: The Metropolis algorithm in action!&quot;, subtitle = &quot;The dots show the king&#39;s path over the first 100 weeks.&quot;) + theme_ipsum() Figure 8.2.b. tibble(week = 1:1e5, island = positions) %&gt;% mutate(island = factor(island)) %&gt;% ggplot(aes(x = island)) + geom_bar() + labs(title = &quot;Old Metropolis shines in the long run.&quot;, subtitle = &quot;Sure enough, the time the king spent on each island was\\nproportional to its population size.&quot;) + theme_ipsum() 8.2 Markov chain Monte Carlo “The metropolis algorithm is the grandparent of several different strategies for getting samples from unknown posterior distributions” (p. 245). If you’re interested, Robert and Casells (2011) wrote a good historical overview of MCMC. 8.2.1 Gibbs sampling. The Gibbs sampler (Geman &amp; Geman, 1984; Casella &amp; George, 1992) uses conjugate pairs (i.e., pairs of priors and likelihoods that have analytic solutions for the posterior of an individual parameter) to efficiently sample from the posterior. Gibbs was the workhorse algorithm during the rise of Bayesian computation in the 1990s. However, it’s limited in that (a) you might not want to use conjugate priors and (b) it can be quite inefficient with complex hierarchical models, which we’ll be fitting soon. We will not be using the Gibbs sampler in this project. It’s available for use in R. For an extensive applied introduction, check out Kruschke’s (2015) text. 8.2.2 Hamiltonian Monte Carlo. Hamiltonian Monte Carlo (HMC) is more computationally costly and more efficient than Gibbs at sampling from the posterior. It needs fewer samples, especially when fitting models with many parameters. To learn more about how HMC works, check out McElreath’s lecture on the topic from January 2019 or one of these lectures (here, here, or here) by Michael Betancourt. 8.3 Easy HMC: map2stan brm() Much like McElreath’s rethinking package, brms provides a convenient interface to HMC via Stan. Other packages providing Stan interfaces include rstanarm and blavaan. I’m not aware of any up-to-date comparisons across the packages. If you’re ever inclined to make one, let the rest of us know! Here we load the rugged data. library(rethinking) data(rugged) d &lt;- rugged Switch from rethinking to brms. detach(package:rethinking) library(brms) rm(rugged) It takes just a sec to do a little data manipulation. d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) dd &lt;- d %&gt;% drop_na(rgdppc_2000) In the context of this chapter, it doesn’t make sense to translate McElreath’s m8.1 map() code to brm() code. Below, we’ll just go directly to the brm() variant of his m8.1stan. 8.3.1 Preparation. When working with brms, you don’t need to do the data processing McElreath did on pages 248 and 249. If you wanted to, however, here’s how you might do it within the tidyverse. dd.trim &lt;- dd %&gt;% select(log_gdp, rugged, cont_africa) str(dd.trim) 8.3.2 Estimation. Finally, we get to work that sweet HMC via brms::brm(). b8.1 &lt;- brm(data = dd, family = gaussian, log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), seed = 8, file = &quot;fits/b08.01&quot;) Now we have officially ditched the uniform distribution for \\(\\sigma\\). Never again! Here’s the posterior: print(b8.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.22 0.14 8.94 9.49 1.00 2632 2549 ## rugged -0.20 0.08 -0.35 -0.05 1.00 2512 2526 ## cont_africa -1.95 0.23 -2.40 -1.50 1.00 2137 2731 ## rugged:cont_africa 0.40 0.13 0.13 0.65 1.00 2154 2463 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.95 0.05 0.85 1.06 1.00 3801 2537 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do note a couple things: If you look closely at the summary information at the top, you’ll see that the brm() function defaults to chains = 4. If you check the manual, you’ll see it also defaults to cores = 1, as well as iter = 2000 and warmup = 1000. Also of note, McElreath’s rethinking::precis() returns highest posterior density intervals (HPDIs) when summarizing map2stan() models. Not so with brms. If you want HPDIs, you’ll have to use the convenience functions from the tidybayes package. Here’s an example. library(tidybayes) post &lt;- posterior_samples(b8.1) post %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) # note our rare use of 89% intervals ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_cont_africa -1.95 -2.32 -1.59 0.89 mean hdi ## 2 b_Intercept 9.22 9.00 9.46 0.89 mean hdi ## 3 b_rugged -0.203 -0.328 -0.0831 0.89 mean hdi ## 4 b_rugged:cont_africa 0.396 0.184 0.604 0.89 mean hdi ## 5 lp__ -249. -251. -246. 0.89 mean hdi ## 6 sigma 0.949 0.858 1.03 0.89 mean hdi There’s one more important difference in our brms summary output compared to McElreath’s rethinking::precis() output. In the text we learn precis() returns n_eff values for each parameter. Earlier versions of brms used to have a direct analogue named Eff.Sample. Both were estimates of the effective number of samples (a.k.a. the effective sample size) for each parameter. As with typical sample size, the more the merrier. Starting with version 2.10.0, brms now returns two columns: Bulk_ESS and Tail_ESS. These originate from a (2019) paper by Stan-team all-stars Vehtari, Gelman, Simpson, Carpenter, and Bürkner. From their paper, we read: If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original) For more technical details, see the paper. In short, Bulk_ESS in the output from brms 2.10.0+ is what was previously referred to as Eff.Sample in earlier versions. It’s also what corresponds to what McElreath calls n_eff. This indexed the number of effective samples in ‘the center of the’ posterior distribution (i.e., the posterior mean or median). But since we also care about uncertainty in our parameters, we care about stability in the 95% intervals and such. The new Tail_ESS in brms output allows us to gauge the effective sample size for those intervals. 8.3.3 Sampling again, in parallel. Here we sample in parallel by adding cores = 4. b8.1b &lt;- update(b8.1, cores = 4, seed = 8, file = &quot;fits/b08.01b&quot;) This model sampled so fast that it really didn’t matter if we sampled in parallel or not. It will for others. print(b8.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.22 0.14 8.94 9.49 1.00 2632 2549 ## rugged -0.20 0.08 -0.35 -0.05 1.00 2512 2526 ## cont_africa -1.95 0.23 -2.40 -1.50 1.00 2137 2731 ## rugged:cont_africa 0.40 0.13 0.13 0.65 1.00 2154 2463 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.95 0.05 0.85 1.06 1.00 3801 2537 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 8.3.4 Visualization. Unlike the way rethinking’s extract.samples() yields a list, brms’s posterior_samples() returns a data frame. post &lt;- posterior_samples(b8.1) str(post) ## &#39;data.frame&#39;: 4000 obs. of 6 variables: ## $ b_Intercept : num 9.61 8.92 8.91 9.06 9.06 ... ## $ b_rugged : num -0.3236 -0.0398 -0.0492 -0.1258 -0.1269 ... ## $ b_cont_africa : num -2.56 -1.75 -1.84 -1.82 -1.88 ... ## $ b_rugged:cont_africa: num 0.575 0.0664 0.1911 0.2774 0.2858 ... ## $ sigma : num 0.919 0.981 0.95 1.037 1.121 ... ## $ lp__ : num -252 -252 -250 -248 -252 ... As with McElreath’s rethinking, brms allows users to put the post data frame or the brmsfit object directly in pairs(). pairs(b8.1, off_diag_args = list(size = 1/5, alpha = 1/5)) Another nice way to customize your pairs plot is with the GGally package. library(GGally) post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs() Since GGally::ggpairs() returns a ggplot2 object, you can customize it as you please. my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;grey50&quot;) } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(shape = 1, size = 1/2, alpha = 1/6) } post %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs(diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + labs(subtitle = &quot;My custom pairs plot&quot;) + theme_ipsum() + theme(strip.text = element_text(size = 8)) For more ideas on customizing a ggpairs() plot, go here. 8.3.5 Using the samples. Older versions of brms allowed users to include information criteria as a part of the model summary by adding loo = T and/or waic = T in the summary() function (e.g., summary(b8.1, loo = T, waic = T). However, this is no longer the case. E.g., summary(b8.1, loo = T, waic = T) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 9.22 0.14 8.94 9.49 1.00 2632 2549 ## rugged -0.20 0.08 -0.35 -0.05 1.00 2512 2526 ## cont_africa -1.95 0.23 -2.40 -1.50 1.00 2137 2731 ## rugged:cont_africa 0.40 0.13 0.13 0.65 1.00 2154 2463 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.95 0.05 0.85 1.06 1.00 3801 2537 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Although R didn’t bark at us for adding loo = T, waic = T, they didn’t do anything. Nowadays, if you want that information, you’ll have to use the waic() and/or loo() functions. waic(b8.1) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_waic -234.8 7.5 ## p_waic 5.2 0.9 ## waic 469.5 14.9 ## ## 2 (1.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. (l_b8.1 &lt;- loo(b8.1)) ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.8 7.5 ## p_loo 5.3 0.9 ## looic 469.7 14.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 169 99.4% 1242 ## (0.5, 0.7] (ok) 1 0.6% 1127 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. And the recommended workflow since brms version 2.8.0 is to save the information criteria information with your brm() fit objects with the add_criterion() function. b8.1 &lt;- add_criterion(b8.1, c(&quot;waic&quot;, &quot;loo&quot;)) ## Automatically saving the model object in &#39;fits/b08.01.rds&#39; You retrieve that information by subsetting the fit object. b8.1$criteria$waic ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_waic -234.8 7.5 ## p_waic 5.2 0.9 ## waic 469.5 14.9 ## ## 2 (1.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. b8.1$criteria$loo ## ## Computed from 4000 by 170 log-likelihood matrix ## ## Estimate SE ## elpd_loo -234.8 7.5 ## p_loo 5.3 0.9 ## looic 469.7 14.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 169 99.4% 1242 ## (0.5, 0.7] (ok) 1 0.6% 1127 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. In response to the brms version 2.8.0 update, which itself accommodated updates to the loo package and both of which occurred years after McElreath published the first edition of his text, we’ve been bantering on about the \\(\\text{elpd}\\) and its relation to the WAIC and the LOO since Chapter 6. This is a fine place to go into some detail. The elpd values returned by loo() and waic() are the expected log pointwise predictive density for new data. It follows the formula \\[\\text{elpd} = \\sum_{i = 1}^n \\int p_t (\\tilde y_i) \\log p (\\tilde y_i | y) d \\tilde y_i,\\] where \\(p_t (\\tilde y_i)\\) is the distribution representing the true data-generating process for \\(\\tilde y_i\\). The \\(p_t (\\tilde y_i)\\)’s are unknown, and we will use cross-validation or WAIC to approximate. In a regression, these distributions are also implicitly conditioned on any predictors in the model. (Vehtari, Gelman, &amp; Gabry, 2016, p. 2). Later in the paper, we learn the elpd_loo (i.e., the Bayesian LOO estimate of out-of-sample predictive fit) is defined as \\[\\text{elpd}_\\text{loo} = \\sum_{i = 1}^n \\log p (y_i | y - _i),\\] where \\[p (y_i | y - _i) = \\int p (y_i | \\theta) p (\\theta | y - _i) d \\theta\\] “is the leave-one-out predictive density given the data without the \\(i\\)th data point” (p. 3). And recall, you can convert the \\(\\text{elpd}\\) to the conventional information criteria metric by multiplying it by -2. To learn more about the \\(\\text{elpd}\\), read the rest of the paper and the other works referenced by the loo package team. And if you prefer watching video lectures to reading technical papers, check out Vehtari’s Model assessment, selection and averaging. 8.3.6 Checking the chain. Using plot() for a brm() fit returns both density and trace lots for the parameters. plot(b8.1) The bayesplot package allows a little more control. Here, we use bayesplot’s mcmc_trace() to show only trace plots with our custom theme. Note that mcmc_trace() works with data frames, not brmfit objects. There’s a further complication. Recall how we made post (i.e., post &lt;- posterior_samples(b8.1)). Our post data frame carries no information on chains. To retain that information, we’ll need to add an add_chain = T argument to our posterior_samples() function. library(bayesplot) post &lt;- posterior_samples(b8.1, add_chain = T) mcmc_trace(post[, c(1:5, 7)], # we need to include column 7 because it contains the chain info facet_args = list(ncol = 3), size = .15) + scale_color_ipsum() + labs(title = &quot;My custom trace plots&quot;) + theme_ipsum() + theme(legend.position = c(.95, .2)) The bayesplot package offers a variety of diagnostic plots. Here we make autocorrelation plots for all model parameters, one for each HMC chain. mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;b_rugged&quot;, &quot;b_cont_africa&quot;, &quot;b_rugged:cont_africa&quot;, &quot;sigma&quot;), lags = 5) + scale_color_ipsum() + theme_ipsum() That’s just what we like to see–nice L-shaped autocorrelation plots. Those are the kinds of shapes you’d expect when you have reasonably large effective samples. Before we move on, there’s an important difference between the trace plots McElreath showed in the text and the ones we just made. McElreath’s trace plots include the warmup iterations. Ours did not. That’s why his x-axis values ranged from 1 to 2,000 and ours only ranged from 1 to 1,000. To my knowledge, neither the brms::plot() nor the bayesplot::mcmc_trace() functions support including warmups in their trace plots. One quick way to get them is with the ggmcmc package. # install.packages(&quot;ggmcmc&quot;, dependencies = T) library(ggmcmc) The ggmcmc package has a variety of convenience functions for working with MCMC chains. The ggs() function extracts the posterior draws, including warmup, and arranges them in a tidy tibble. ggs(b8.1) %&gt;% str() ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 40000 obs. of 4 variables: ## $ Iteration: int 1 2 3 4 5 6 7 8 9 10 ... ## $ Chain : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Parameter: Factor w/ 5 levels &quot;b_cont_africa&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ value : num 0.815 0.815 0.815 0.815 0.871 ... ## - attr(*, &quot;nChains&quot;)= int 4 ## - attr(*, &quot;nParameters&quot;)= int 5 ## - attr(*, &quot;nIterations&quot;)= int 2000 ## - attr(*, &quot;nBurnin&quot;)= num 1000 ## - attr(*, &quot;nThin&quot;)= num 1 ## - attr(*, &quot;description&quot;)= chr &quot;7800258486a6dfd09b501505ab8f8544&quot; With this in hand, we can now include those warmup draws in our trace plots. Here’s how to do so without convenience functions like bayesplot::mcmc_trace(). ggs(b8.1) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + # this marks off the warmups annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = &quot;grey80&quot;, alpha = 1/2, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_ipsum() + labs(title = &quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme_ipsum() + theme(legend.position = c(.95, .2)) + facet_wrap(~Parameter, scales = &quot;free_y&quot;) Following brms defaults, we won’t include warmup iterations in the trace plots for other models in this book. A nice thing about plots that do contain them, though, is they reveal how quickly our HMC chains transition away from their start values into the posterior. To get a better sense of this, let’s make those trace plots once more, but this time zooming in on the first 100 iterations. ggs(b8.1) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + # this marks off the warmups annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = &quot;grey80&quot;, alpha = 1/2, size = 0) + geom_line(aes(color = chain), size = .5) + scale_color_ipsum() + labs(title = &quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + coord_cartesian(xlim = 1:100) + theme_ipsum() + theme(legend.position = c(.95, .2)) + facet_wrap(~Parameter, scales = &quot;free_y&quot;) For each parameter, the all four chains had moved away from their starting values to converge on the marginal posteriors by the 50th iteration or so. 8.3.6.1 Overthinking: Raw Stan model code. The stancode() function works in brms much like it does in rethinking. brms::stancode(b8.1) ## // generated with brms 2.12.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## int Kc = K - 1; ## matrix[N, Kc] Xc; // centered version of X without an intercept ## vector[Kc] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // population-level effects ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0&gt; sigma; // residual SD ## } ## transformed parameters { ## } ## model { ## // priors including all constants ## target += normal_lpdf(b | 0, 10); ## target += normal_lpdf(Intercept | 0, 100); ## target += cauchy_lpdf(sigma | 0, 2) ## - 1 * cauchy_lccdf(0 | 0, 2); ## // likelihood including all constants ## if (!prior_only) { ## target += normal_id_glm_lpdf(Y | Xc, Intercept, b, sigma); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept - dot_product(means_X, b); ## } You can also get that information with b8.1$model or b8.1$fit@stanmodel. 8.4 Care and feeding of your Markov chain. Markov chain Monte Carlo is a highly technical and usually automated procedure. Most people who use it don’t really understand what it is doing. That’s okay, up to a point. Science requires division of labor, and if every one of us had to write our own Markov chains from scratch, a lot less research would get done in the aggregate. (p. 255) But if you do want to learn more about HMC, McElreath has some nice introductory lectures on the topic (see here and here). To dive even deeper, Michael Betancourt from the Stan team has given many lectures on the topic (e.g., here and here). 8.4.1 How many samples do you need? The brms defaults for iter and warmup match those of McElreath’s rethinking. If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. (p. 255) And remember, with changes from brms version 2.10.0, we now have both Bulk_ESS and Tail_ESS to consult when thinking about the effective sample size. 8.4.2 How many chains do you need? “Using 3 or 4 chains is conventional, and quite often more than enough to reassure us that the sampling is working properly” (p. 257). 8.4.2.1 Convergence diagnostics. Times have changed. In the text, we read: The default diagnostic output from Stan includes two metrics, n_eff and Rhat. The first is a measure of the effective number of samples. The second is the Gelman-Rubin convergence diagnostic, \\(\\hat R\\). When n_eff is much lower than the actual number of iterations (minus warmup) of your chains, it means the chains are inefficient, but possibly still okay. When Rhat is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples. If you draw more iterations, it could be fine, or it could never converge. See the Stan user manual for more details. It’s important however not to rely too much on these diagnostics. Like all heuristics, there are cases in which they provide poor advice. (p. 257) We’ve already covered how brms has expanded the traditional notion of effective samples (i.e., n_eff) to Bulk_ESS and Tail_ESS. Times are changing for the \\(\\hat R\\), too. As it turns out, the Stan team has found some deficiencies with the \\(\\hat R\\), for which they’ve made recommendations that will be implemented in the Stan ecosystem sometime soon (see here for a related thread on the Stan Forums). In the meantime, you can read all about it in their preprint and in one of Dan Simpson’s blogs. If you learn best by sassy twitter banter, click through this interchange among some of our Stan team all-stars. For more on these topics, you might also check out Gabry and Modrák’s vignette, Visual MCMC diagnostics using the bayesplot package. 8.4.3 Taming a wild chain. As with rethinking, brms can take data in the form of a list. Recall however, that in order to specify starting values, you need to specify a list of lists with an inits argument rather than with start. b8.2 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(uniform(-1e10, 1e10), class = Intercept), prior(uniform(0, 1e10), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8, file = &quot;fits/b08.02&quot;) Those were some silly flat priors. Check the damage. post &lt;- posterior_samples(b8.2, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.a.&quot;, subtitle = &quot;These trace plots do not look like the fuzzy caterpillars we usually hope for.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Let’s peek at the summary. print(b8.2) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing ## the results! We recommend running more iterations and/or setting stronger priors. ## Warning: There were 122 divergent transitions after warmup. Increasing adapt_delta above 0.8 may ## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 12353449.01 32582754.90 -8996475.21 111009346.70 1.23 11 23 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 65329608.35 339797572.81 2516.98 488637001.70 1.25 7 18 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Holy smokes, those parameters are a mess! Plus we got nasty warning messages, too. Watch our reasonable priors save the day. b8.3 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), inits = list(list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8, file = &quot;fits/b08.03&quot;) print(b8.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.06 1.82 -3.60 4.03 1.00 1045 822 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 2.13 2.04 0.62 7.61 1.00 1166 1233 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As in the text, no more warning signs and no more silly estimates. The trace plots look great, too. post &lt;- posterior_samples(b8.3, add_chain = T) mcmc_trace(post[, c(1:2, 4)], size = .25) + labs(title = &quot;My version of Figure 8.5.b&quot;, subtitle = &quot;Oh man. This looks so much better.&quot;) + scale_color_ipsum() + theme_ipsum() + theme(legend.position = c(.85, 1.5), legend.direction = &quot;horizontal&quot;) Now behold our version of Figure 8.6. # left p1 &lt;- post %&gt;% select(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = min(post$b_Intercept), to = max(post$b_Intercept), length.out = 50)), aes(x = x, y = dnorm(x = x, mean = 0, sd = 10)), color = ipsum_pal()(1), linetype = 2) + theme_ipsum() # right p2 &lt;- post %&gt;% select(sigma) %&gt;% ggplot(aes(x = sigma)) + stat_density(geom = &quot;line&quot;) + geom_line(data = data.frame(x = seq(from = 0, to = max(post$sigma), length.out = 50)), aes(x = x, y = dcauchy(x = x, location = 0, scale = 1)*2), color = ipsum_pal()(2)[2], linetype = 2) + coord_cartesian(xlim = c(0, 10)) + ylab(NULL) + theme_ipsum() # combine the two library(patchwork) p1 + p2 + plot_annotation(title = &quot;Prior (dashed) and posterior (solid) distributions for the\\nmodel with weakly-informative priors, b8.3&quot;, theme = theme_ipsum()) 8.4.3.1 Overthinking: Cauchy distribution. Behold the beautiful Cauchy probability density: \\[p(x|x_0, \\gamma) = \\Bigg ( \\pi \\gamma \\Bigg [ 1 + \\Big ( \\frac{x - x_0}{\\gamma} \\Big ) ^2 \\Bigg ] \\Bigg ) ^{-1}\\] The Cauchy has no mean and variance, but \\(x_0\\) is the location and \\(\\gamma\\) is the scale. Here’s our version of the simulation. Note our use of the cummean() function. n &lt;- 1e4 set.seed(8) tibble(y = rcauchy(n, location = 0, scale = 5), mu = cummean(y), index = 1:n) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line() + theme_ipsum() The whole thing is quite remarkable. Just for kicks, here we do it again with nine simulations. n &lt;- 1e4 set.seed(8) tibble(a = rcauchy(n, location = 0, scale = 5), b = rcauchy(n, location = 0, scale = 5), c = rcauchy(n, location = 0, scale = 5), d = rcauchy(n, location = 0, scale = 5), e = rcauchy(n, location = 0, scale = 5), f = rcauchy(n, location = 0, scale = 5), g = rcauchy(n, location = 0, scale = 5), h = rcauchy(n, location = 0, scale = 5), i = rcauchy(n, location = 0, scale = 5)) %&gt;% gather() %&gt;% group_by(key) %&gt;% mutate(mu = cummean(value)) %&gt;% ungroup() %&gt;% mutate(index = rep(1:n, times = 9)) %&gt;% ggplot(aes(x = index, y = mu)) + geom_line(aes(color = key)) + scale_color_manual(values = ipsum_pal()(9)) + scale_x_continuous(breaks = c(0, 5000, 10000)) + theme_ipsum() + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, ncol = 3, scales = &quot;free&quot;) 8.4.4 Non-identifiable parameters. It appears that the only way to get a brms version of McElreath’s m8.4 and m8.5 is to augment the data. In addition to the Gaussian y vector, we’ll add two constants to the data, intercept_1 = 1 and intercept_2 = 1. set.seed(8) y &lt;- rnorm(100, mean = 0, sd = 1) b8.4 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(uniform(-1e10, 1e10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8, file = &quot;fits/b08.04&quot;) Our model results don’t perfectly mirror McElreath’s, but they’re identical in spirit. print(b8.4) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing ## the results! We recommend running more iterations and/or setting stronger priors. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## intercept_1 -307.39 1584.08 -2458.98 3110.09 2.12 3 11 ## intercept_2 307.29 1584.08 -3110.19 2458.81 2.12 3 11 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.08 0.07 0.95 1.23 1.07 54 183 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note the frightening warning message. Those results are a mess! Let’s try again. b8.5 &lt;- brm(data = list(y = y, intercept_1 = 1, intercept_2 = 1), family = gaussian, y ~ 0 + intercept_1 + intercept_2, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), inits = list(list(intercept_1 = 0, intercept_2 = 0, sigma = 1), list(intercept_1 = 0, intercept_2 = 0, sigma = 1)), iter = 4000, warmup = 1000, chains = 2, seed = 8, file = &quot;fits/b08.05&quot;) print(b8.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + intercept_1 + intercept_2 ## Data: list(y = y, intercept_1 = 1, intercept_2 = 1) (Number of observations: 100) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## intercept_1 -0.14 7.20 -14.39 13.82 1.00 1185 1294 ## intercept_2 0.05 7.20 -13.95 14.34 1.00 1184 1296 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.09 0.08 0.95 1.25 1.00 2162 2252 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Much better. For our version of Figure 8.7, we’ll make the trace plots for the two models saparately and combine them with patchwork. post &lt;- posterior_samples(b8.4, add_chain = T) p1 &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + labs(subtitle = &quot;flat priors&quot;) + theme_ipsum() + theme(legend.position = &quot;none&quot;, strip.text = element_text(size = 10)) post &lt;- posterior_samples(b8.5, add_chain = T) p2 &lt;- mcmc_trace(post[, c(1:3, 5)], size = .25, facet_args = c(ncol = 1)) + scale_color_ipsum() + labs(subtitle = &quot;weakly-informative priors&quot;) + theme_ipsum() + theme(legend.position = &quot;none&quot;, strip.text = element_text(size = 10)) p1 + p2 + plot_annotation(title = &quot;Prior strength matters&quot;, theme = theme_ipsum()) The central message in the text, default to weakly-regularizing priors, holds for brms just as it does in rethinking. For more on the topic, see the recommendations from the Stan team. If you want to dive deeper, check out Simpson’s post on Gelman’s blog and their corresponding paper with Betancourt. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 ggmcmc_1.3 bayesplot_1.7.1 GGally_1.4.0 ## [5] tidybayes_2.0.1.9000 brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ## [9] StanHeaders_2.19.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [13] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [17] ggplot2_3.2.1 tidyverse_1.3.0 extrafont_0.17 hrbrthemes_0.6.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] rethinking_1.59 fs_1.3.1 rstudioapi_0.10 ## [10] farver_2.0.3 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 jsonlite_1.6.1 broom_0.5.3 ## [22] Rttf2pt1_1.3.8 dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 backports_1.1.5 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [34] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 extrafontdb_1.0 ## [46] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [49] rvest_0.3.5 miniUI_0.1.1.1 mime_0.8 ## [52] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 ## [55] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [58] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [61] inline_0.3.15 RColorBrewer_1.1-2 shinystan_2.5.0 ## [64] yaml_2.2.1 gridExtra_2.3 gdtools_0.2.1 ## [67] loo_2.2.0 reshape_0.8.8 stringi_1.4.5 ## [70] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.4 ## [73] pkgconfig_2.0.3 systemfonts_0.1.1 matrixStats_0.55.0 ## [76] HDInterval_0.2.0 evaluate_0.14 lattice_0.20-38 ## [79] rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [82] processx_3.4.1 tidyselect_1.0.0 plyr_1.8.5 ## [85] magrittr_1.5 R6_2.4.1 generics_0.0.2 ## [88] DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [91] withr_2.1.2 xts_0.12-0 abind_1.4-5 ## [94] modelr_0.1.5 crayon_1.3.4 arrayhelpers_1.0-20160527 ## [97] utf8_1.1.4 rmarkdown_2.0 grid_3.6.2 ## [100] readxl_1.3.1 callr_3.4.1 threejs_0.3.3 ## [103] reprex_0.3.0 digest_0.6.23 xtable_1.8-4 ## [106] httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 ## [109] shinyjs_1.1 "],
["big-entropy-and-the-generalized-linear-model.html", "9 Big Entropy and the Generalized Linear Model 9.1 Maximum entropy 9.2 Generalized linear models Reference Session info", " 9 Big Entropy and the Generalized Linear Model Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention. To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice. When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. (p. 267) 9.1 Maximum entropy In Chapter 6, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution \\(p\\) with probabilities \\(p_i\\) for each possible event \\(i\\) turns out to be just the average log-probability: \\[H(p) = - \\sum_i p_i \\log p_i\\] This function is known as information entropy. The principle of maximum entropy applies this measure of uncertainty to the problem of choosing among probability distributions. Perhaps the simplest way to sate the maximum entropy principle is: The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints. There’s nothing intuitive about this idea, so if it seems weird, you are normal. (pp. 268–269, emphasis in the original) Let’s execute the code for the pebbles-in-buckets example. library(tidyverse) d &lt;- tibble(a = c(0, 0, 10, 0, 0), b = c(0, 1, 8, 1, 0), c = c(0, 2, 6, 2, 0), d = c(1, 2, 4, 2, 1), e = 2) # this is our analogue to McElreath&#39;s `lapply()` code d %&gt;% mutate_all(~ . / sum(.)) %&gt;% # the next few lines constitute our analogue to his `sapply()` code gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) ## # A tibble: 5 x 2 ## key h ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0 ## 2 b 0.639 ## 3 c 0.950 ## 4 d 1.47 ## 5 e 1.61 For more on the formula syntax we used within mutate_all(), you might check out this or this. Anyway, we’re almost ready to plot, which brings us to color. For the plots in this chapter, we’ll be taking our color palettes from the ghibli package, which provides palettes based on scenes from anime films by the Studio Ghibli. # install.packages(&quot;ghibli&quot;, dependencies = T) library(ghibli) The main function is ghibli_palette() which you can use to both preview the palettes before using them and also index in order to use specific colors. For example, we’ll play with “MarnieMedium1”, first. ghibli_palette(&quot;MarnieMedium1&quot;) ghibli_palette(&quot;MarnieMedium1&quot;)[1:7] ## [1] &quot;#28231D&quot; &quot;#5E2D30&quot; &quot;#008E90&quot; &quot;#1C77A3&quot; &quot;#C5A387&quot; &quot;#67B8D6&quot; &quot;#E9D097&quot; Now we’re ready to plot five of the six panels of Figure 9.1. d %&gt;% mutate(bucket = 1:5) %&gt;% gather(letter, pebbles, - bucket) %&gt;% ggplot(aes(x = bucket, y = pebbles)) + geom_col(width = 1/5, fill = ghibli_palette(&quot;MarnieMedium1&quot;)[2]) + geom_text(aes(y = pebbles + 1, label = pebbles)) + geom_text(data = tibble( letter = letters[1:5], bucket = 5.5, pebbles = 10.5, label = str_c(c(1, 90, 1260, 37800, 113400), rep(c(&quot; way&quot;, &quot; ways&quot;), times = c(1, 4)))), aes(label = label), hjust = 1) + scale_y_continuous(breaks = c(0, 5, 10), limits = c(0, 12)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6]), strip.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[7])) + facet_wrap(~letter, ncol = 2) We might plot our version of the final panel like so. d %&gt;% # the next four lines are the same from above mutate_all(~ . / sum(.)) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) %&gt;% # here&#39;s the R code 9.4 stuff mutate(n_ways = c(1, 90, 1260, 37800, 113400)) %&gt;% group_by(key) %&gt;% mutate(log_ways = log(n_ways) / 10, text_y = ifelse(key &lt; &quot;c&quot;, h + .15, h - .15)) %&gt;% # plot ggplot(aes(x = log_ways, y = h)) + geom_abline(intercept = 0, slope = 1.37, color = &quot;white&quot;) + geom_point(size = 2.5, color = ghibli_palette(&quot;MarnieMedium1&quot;)[7]) + geom_text(aes(y = text_y, label = key)) + labs(x = &quot;log(ways) per pebble&quot;, y = &quot;entropy&quot;) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6])) “The distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution” (p. 271). Among the pebbles, the maximum entropy distribution was e (i.e., the uniform). 9.1.0.1 Rethinking: What good is intuition? “Like many aspects of information theory, maximum entropy is not very intuitive. But note that intuition is just a guide to developing methods. When a method works, it hardly matters whether our intuition agrees” (p. 271). 9.1.1 Gaussian. Behold the probability density for the generalized normal distribution: \\[\\text{Pr} (y | \\mu, \\alpha, \\beta) = \\frac{\\beta}{2 \\alpha \\Gamma \\bigg (\\frac{1}{\\beta} \\bigg )} e ^ {- \\bigg (\\frac{|y - \\mu|}{\\alpha} \\bigg ) ^ {\\beta}},\\] where \\(\\alpha =\\) the scale, \\(\\beta =\\) the shape, \\(\\mu =\\) the location, and \\(\\Gamma =\\) the gamma function. If you read closely in the text, you’ll discover that the densities in the right panel of Figure 9.2 were all created with the constraint \\(\\sigma^2 = 1\\). But \\(\\sigma^2 \\neq \\alpha\\) and there’s no \\(\\sigma\\) in the equations in the text. However, it appears the variance for the generalized normal distribution follows the form \\[\\sigma^2 = \\frac{\\alpha^2 \\Gamma (3/\\beta)}{\\Gamma (1/\\beta)}.\\] So if you do the algebra, you’ll see that you can compute \\(\\alpha\\) for a given \\(\\sigma^2\\) and \\(\\beta\\) with the equation \\[\\alpha = \\sqrt{ \\frac{\\sigma^2 \\Gamma (1/\\beta)}{\\Gamma (3/\\beta)} }.\\] I got the formula from Wikipedia.com. Don’t judge. We can wrap that formula in a custom function, alpha_per_beta(), use it to solve for the desired \\(\\beta\\) values, and plot. But one more thing: McElreath didn’t tell us exactly which \\(\\beta\\) values the left panel of Figure 9.2 was based on. So the plot below is my best guess. alpha_per_beta &lt;- function(beta, variance = 1) { sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } tibble(mu = 0, # I arrived at these values by trial and error beta = c(1, 1.5, 2, 4)) %&gt;% mutate(alpha = alpha_per_beta(beta)) %&gt;% expand(nesting(mu, beta, alpha), value = seq(from = -5, to = 5, by = .1)) %&gt;% # behold the formula for the generalized normal distribution in code! mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% # plot ggplot(aes(x = value, y = density, group = beta)) + geom_line(aes(color = beta == 2, size = beta == 2)) + scale_color_manual(values = c(ghibli_palette(&quot;MarnieMedium2&quot;)[c(2, 4)])) + scale_size_manual(values = c(1/4, 1.25)) + labs(subtitle = &quot;Guess which color denotes the Gaussian.&quot;) + coord_cartesian(xlim = -4:4) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) Here’s Figure 9.2’s right panel. tibble(mu = 0, # this time we need a more densely-packed sequence of `beta` values beta = seq(from = 1, to = 4, length.out = 100)) %&gt;% mutate(alpha = alpha_per_beta(beta)) %&gt;% expand(nesting(mu, beta, alpha), value = -8:8) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% # this is just an abbreviated version of the formula we used in our first code block summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_vline(xintercept = 2, color = &quot;white&quot;) + geom_line(size = 2, color = ghibli_palette(&quot;MarnieMedium2&quot;)[6]) + xlab(expression(beta*&quot; (i.e., shape)&quot;)) + coord_cartesian(ylim = c(1.34, 1.42)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7])) If you look closely, you’ll see our version doesn’t quite match up with McElreath’s. Over x-axis values of 2 to 4, they match up pretty well. But as you go from 2 to 1, you’ll see our line drops off more steeply than his did. [And no, coord_cartesian() isn’t the problem.] If you can figure out why our numbers diverged, please share the answer. But getting back on track: The take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian. (p. 274) 9.1.2 Binomial. The binomial likelihood entails counting the numbers of ways that a given observation could arise, according to assumptions… If only two things can happen (blue or white marble, for example), and there’s a constant chance \\(p\\) of each across \\(n\\) trials, then the probability of observing \\(y\\) events of type 1 and \\(n - y\\) events of type 2 is: \\[\\text{Pr} (y | n, p) = \\frac{n!}{y! (n - y)!} p^y (1 - p)^{n - y}\\] It may help to note that the fraction with the factorials is just saying how many different ordered sequences of \\(n\\) outcomes have a count of \\(y\\). (p. 275) For me, that last sentence made more sense when I walked it out in an example. To do so, let’s wrap that fraction of factorials into a function. count_ways &lt;- function(n, y) { # n = the total number of trials (i.e., the number of rows in your vector) # y = the total number of 1s (i.e., successes) in your vector (factorial(n) / (factorial(y) * factorial(n - y))) } Now consider three sequences: 0, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 0\\)) 1, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 1\\)) 1, 1, 0, 0 (i.e., \\(n = 4\\) and \\(y = 2\\)) We can organize that information in a little tibble and then demo our count_ways() function. tibble(sequence = 1:3, n = 4, y = c(0, 1, 2)) %&gt;% mutate(n_ways = count_ways(n = n, y = y)) ## # A tibble: 3 x 4 ## sequence n y n_ways ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 0 1 ## 2 2 4 1 4 ## 3 3 4 2 6 Here’s the pre-Figure 9.3 data McElreath presented at the bottom of page 275. ( d &lt;- tibble(distribution = letters[1:4], ww = c(1/4, 2/6, 1/6, 1/8), bw = c(1/4, 1/6, 2/6, 4/8), wb = c(1/4, 1/6, 2/6, 2/8), bb = c(1/4, 2/6, 1/6, 1/8)) ) ## # A tibble: 4 x 5 ## distribution ww bw wb bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.25 0.25 0.25 0.25 ## 2 b 0.333 0.167 0.167 0.333 ## 3 c 0.167 0.333 0.333 0.167 ## 4 d 0.125 0.5 0.25 0.125 Those data take just a tiny bit of wrangling before they’re ready to plot in our version of Figure 9.3. d %&gt;% gather(key, value, -distribution) %&gt;% mutate(key = factor(key, levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) %&gt;% ggplot(aes(x = key, y = value, group = 1)) + geom_point(size = 2, color = ghibli_palette(&quot;PonyoMedium&quot;)[4]) + geom_line(color = ghibli_palette(&quot;PonyoMedium&quot;)[5]) + labs(x = NULL, y = NULL) + coord_cartesian(ylim = 0:1) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[2]), strip.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[6])) + facet_wrap(~distribution) If we go step by step, we might count the expected value for each distribution like follows. d %&gt;% gather(sequence, probability, -distribution) %&gt;% # `str_count()` will count the number of times &quot;b&quot; occurs within a given row of `sequence` mutate(n_b = str_count(sequence, &quot;b&quot;)) %&gt;% mutate(product = probability * n_b) %&gt;% group_by(distribution) %&gt;% summarise(expected_value = sum(product)) ## # A tibble: 4 x 2 ## distribution expected_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 1 ## 3 c 1 ## 4 d 1 We can use the same gather() and group_by() strategies on the way to computing the entropies. d %&gt;% gather(sequence, probability, -distribution) %&gt;% group_by(distribution) %&gt;% summarise(entropy = -sum(probability * log(probability))) ## # A tibble: 4 x 2 ## distribution entropy ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.39 ## 2 b 1.33 ## 3 c 1.33 ## 4 d 1.21 Like in the text, distribution == &quot;a&quot; had the largest entropy of the four. In the next example, the \\(\\text{expected value} = 1.4\\) and \\(p = .7\\). p &lt;- 0.7 ( a &lt;- c((1 - p)^2, p * (1 - p), (1 - p) * p, p^2) ) ## [1] 0.09 0.21 0.21 0.49 Here’s the entropy for our distribution a. -sum(a * log(a)) ## [1] 1.221729 I’m going to alter McElreath’s simulation function from R code block 9.9 to take a seed argument. In addition, I altered the names of the objects within the function and changed the output to a tibble that will also include the conditions “ww”, “bw”, “wb”, and “bb”. sim_p &lt;- function(seed, g = 1.4) { set.seed(seed) x_123 &lt;- runif(3) x_4 &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g) z &lt;- sum(c(x_123, x_4)) p &lt;- c(x_123, x_4) / z tibble(h = -sum(p * log(p)), p = p, key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) } For a given seed and g value, our augmented sim_p() function returns a \\(4 \\times 3\\) tibble. sim_p(seed = 9.9, g = 1.4) ## # A tibble: 4 x 3 ## h p key ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1.02 0.197 ww ## 2 1.02 0.0216 bw ## 3 1.02 0.184 wb ## 4 1.02 0.597 bb So the next step is to determine how many replications we’d like, create a tibble with seed values ranging from 1 to that number, and then feed those seed values into sim_p() via purrr::map2(), which will return a nested tibble. We’ll then unnest() and take a peek. # how many replications would you like? n_rep &lt;- 1e5 d &lt;- tibble(seed = 1:n_rep) %&gt;% mutate(sim = map2(seed, 1.4, sim_p)) %&gt;% unnest(sim) Take a look. head(d) ## # A tibble: 6 x 4 ## seed h p key ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1.21 0.108 ww ## 2 1 1.21 0.151 bw ## 3 1 1.21 0.233 wb ## 4 1 1.21 0.508 bb ## 5 2 1.21 0.0674 ww ## 6 2 1.21 0.256 bw In order to intelligently choose which four replications we want to highlight in Figure 9.4, we’ll want to rank order them by entropy, h. ranked_d &lt;- d %&gt;% group_by(seed) %&gt;% arrange(desc(h)) %&gt;% ungroup() %&gt;% # here&#39;s the rank order step mutate(rank = rep(1:n_rep, each = 4)) head(ranked_d) ## # A tibble: 6 x 5 ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 ## 5 71132 1.22 0.0902 ww 2 ## 6 71132 1.22 0.210 bw 2 And we’ll also want a subset of the data to correspond to McElreath’s “A” through “D” distributions. subset_d &lt;- ranked_d %&gt;% # I arrived at these `rank` values by trial and error filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% # I arrived at the `height` values by trial and error, too mutate(height = rep(c(8, 2.25, .75, .5), each = 4), distribution = rep(letters[1:4], each = 4)) head(subset_d) ## # A tibble: 6 x 7 ## seed h p key rank height distribution ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 55665 1.22 0.0903 ww 1 8 a ## 2 55665 1.22 0.209 bw 1 8 a ## 3 55665 1.22 0.210 wb 1 8 a ## 4 55665 1.22 0.490 bb 1 8 a ## 5 50981 1.00 0.0459 ww 87373 2.25 b ## 6 50981 1.00 0.0459 bw 87373 2.25 b We’re finally ready to make our version of the left panel of Figure 9.4. p1 &lt;- d %&gt;% ggplot(aes(x = h)) + geom_density(size = 0, fill = ghibli_palette(&quot;LaputaMedium&quot;)[3], adjust = 1/4) + # note the data statements for the next two geoms geom_linerange(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(ymin = 0, ymax = height), color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_text(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(y = height + .5, label = distribution)) + scale_x_continuous(&quot;Entropy&quot;, breaks = seq(from = .7, to = 1.2, by = .1)) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7])) Did you notice how our adjust = 1/4 with geom_density() served a similar function to the adj=0.1 in McElreath’s dens() code? Anyways, here we make the right panel and combine the two with patchwork. p2 &lt;- ranked_d %&gt;% filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% mutate(distribution = rep(letters[1:4], each = 4)) %&gt;% ggplot(aes(x = key, y = p, group = 1)) + geom_line(color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_point(size = 2, color = ghibli_palette(&quot;LaputaMedium&quot;)[4]) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, .75)) + xlab(NULL) + theme(axis.ticks.x = element_blank(), panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7]), strip.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[6])) + facet_wrap(~distribution) # combine and plot library(patchwork) p1 | p2 Because we were simulating, our values won’t match up identically with those in the text. But we’re pretty close, eh? Since we saved our sim_p() output in a nested tibble, which we then unnested(), there’s no need to separate the entropy values from the distributional values the way McElreath did in R code 9.11. If we wanted to determine our highest entropy value–and the corresponding seed and p values, while we’re at it–, we might use max(h) within slice(). ranked_d %&gt;% group_by(key) %&gt;% slice(max(h)) ## # A tibble: 4 x 5 ## # Groups: key [4] ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 That maximum h value matched up nicely with the one in the text. If you look at the p column, you’ll see our values approximated McElreath’s distribution values, too. In both cases, they’re real close to the a values we computed, above. a ## [1] 0.09 0.21 0.21 0.49 “All four of these distributions really do have expected value 1.4. But among the infinite distributions that satisfy this constraint, it is only the most even distribution, the exact one nominated by the binomial distribution, that has greatest entropy” (p. 278). 9.2 Generalized linear models For an outcome variable that is continuous and far from any theoretical maximum or minimum, [a simple] Gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. (p. 280) I winged the values for our Figure 9.5. tibble(x = seq(from = -1, to = 3, by = .01)) %&gt;% mutate(probability = .35 + x * .5) %&gt;% ggplot(aes(x = x, y = probability)) + geom_rect(xmin = -1, xmax = 3, ymin = 0, ymax = 1, fill = ghibli_palette(&quot;MononokeMedium&quot;)[5]) + geom_hline(yintercept = 0:1, linetype = 2, color = ghibli_palette(&quot;MononokeMedium&quot;)[7]) + geom_line(aes(linetype = probability &gt; 1, color = probability &gt; 1), size = 1) + geom_segment(x = 1.3, xend = 3, y = 1, yend = 1, size = 2/3, color = ghibli_palette(&quot;MononokeMedium&quot;)[3]) + scale_color_manual(values = c(ghibli_palette(&quot;MononokeMedium&quot;)[c(3, 4)])) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(xlim = 0:2, ylim = c(0, 1.2)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;MononokeMedium&quot;)[1])) For a count outcome \\(y\\) for which each observation arises from \\(n\\) trials and with constant expected value \\(np\\), the binomial distribution has maximum entropy. So it’s the least informative distribution that satisfies our prior knowledge of the outcomes \\(y\\). (p. 281) The binomial model follows the basic form \\[\\begin{align*} y_i &amp; \\sim \\text{Binomial} (n, p_i) \\\\ f(p_i) &amp; = \\alpha + \\beta x_i, \\end{align*}\\] where the \\(f()\\) portion of the second line represents the link function. We need the link function because, though the shape of the Binomial distribution is determined by two parameters–\\(n\\) and \\(p\\)–, neither is equivalent to the Gaussian mean \\(\\mu\\). The mean outcome, rather, is \\(np\\)–a function of both. The link function also ensures the model doesn’t make probability predictions outside of the boundary \\([0, 1]\\). Let’s get more general. 9.2.1 Meet the family. The most common distributions used in statistical modeling are members of a family known as the exponential family. Every member of this family is a maximum entropy distribution, for some set of constraints. And conveniently, just about every other statistical modeling tradition employs the exact same distributions, even though they arrive at them via justifications other than maximum entropy. (p. 282, emphasis in the original) Here are the Gamma and Exponential panels for Figure 9.6. length_out &lt;- 100 tibble(x = seq(from = 0, to = 5, length.out = length_out)) %&gt;% mutate(Gamma = dgamma(x, 2, 2), Exponential = dexp(x)) %&gt;% gather(key, density, -x) %&gt;% mutate(label = rep(c(&quot;y %~% Gamma(lambda, kappa)&quot;, &quot;y %~% Exponential(lambda)&quot;), each = n()/2)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~label, scales = &quot;free_y&quot;, labeller = label_parsed) The Gaussian: tibble(x = seq(from = -5, to = 5, length.out = length_out)) %&gt;% mutate(density = dnorm(x), strip = &quot;y %~% Normal(mu, sigma)&quot;) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = -4:4) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Here is the Poisson. tibble(x = 0:20) %&gt;% mutate(density = dpois(x, lambda = 2.5), strip = &quot;y %~% Poisson(lambda)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) Finally, the Binomial: tibble(x = 0:10) %&gt;% mutate(density = dbinom(x, size = 10, prob = .85), strip = &quot;y %~% Binomial(n, p)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:10) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~strip, labeller = label_parsed) 9.2.1.1 Rethinking: A likelihood is a prior. In traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” However, likelihoods are themselves prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon context. (p. 284) For a little more in this, check out McElreath’s great lecture, Bayesian Statistics without Frequentist Language. This subsection also reminds me of the title of one of Gelman’s blog posts, “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”. The title, which itself is a quote, comes from one of his papers, which he linked to in the blog, along with several related papers. It’s taken some time for the weight of that quote to sink in with me, and indeed it’s still sinking. Perhaps you’ll benefit from it, too. 9.2.2 Linking linear models to distributions. To build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a link function to prevent mathematical accidents like negative distances or probability masses that exceed 1. (p. 284) These models generally follow the form \\[\\begin{align*} y_i &amp; \\sim \\text{Some distribution} (\\theta_i, \\phi) \\\\ f(\\theta_i) &amp; = \\alpha + \\beta x_i, \\end{align*}\\] where \\(\\theta_i\\) is a parameter of central interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not of primary substantive interest (e.g., \\(\\sigma\\) in work-a-day Gaussian models). And as stated earlier, \\(f()\\) is the link function. Speaking of, the logit link maps a parameter that is defined as a probability mass and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this: \\[\\begin{align*} y_i &amp; \\sim \\text{Binomial}(n, p_i)\\\\ \\text{logit}(p_i) &amp; = \\alpha+\\beta x_i \\end{align*}\\] And the logit function itself is defined as the log-odds: \\[\\text{logit} (p_i) = \\text{log} \\frac{p_i}{1 - p_i}\\] The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is: \\[\\text{log} \\frac{p_i}{1 - p_i} = \\alpha + \\beta x_i\\] If we do the final algebraic manipulation on page 285, we can solve for \\(p_i\\) in terms of the linear model \\[p_i = \\frac{\\exp (\\alpha + \\beta x_i)}{1 + \\exp (\\alpha + \\beta x_i)}.\\] As we’ll see later, we will make great use of this formula via the brms::inv_logit_scaled() when making sense of logistic regression models. Now we have that last formula in hand, we can make the data necessary for Figure 9.7. # first, we&#39;ll make data for the horizontal lines alpha &lt;- 0 beta &lt;- 4 lines &lt;- tibble(x = seq(from = -1, to = 1, by = .25)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we&#39;re ready to make the primary data beta &lt;- 2 d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-odds`)) + geom_hline(data = lines, aes(yintercept = `log-odds`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = probability)) + geom_hline(data = lines, aes(yintercept = probability), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory p1 | p2 The key lesson for now is just that no regression coefficient, such as \\(\\beta\\), from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 7) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change… The second very common link function is the log link. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation of \\(\\sigma\\) of a Gaussian distribution so it is a function of a predictor variable \\(x\\). The parameter \\(\\sigma\\) must be positive, because a standard deviation cannot be negative no can it be zero. The model might look like: \\[\\begin{align*} y_i &amp; \\sim \\text{Normal} (\\mu, \\sigma_i) \\\\ \\log (\\sigma_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] In this model, the mean \\(\\mu\\) is constant, but the standard deviation scales with the value \\(x_i\\). (p. 268) This kind of model is trivial in the brms framework, which you can learn more about in Bürkner’s vignette Estimating Distributional Models with brms. Before moving on with the text, let’s detour and see how we might fit such a model. First, we’ll simulate some continuous data y for which the \\(SD\\) is affected by a dummy variable x. set.seed(9) ( d &lt;- tibble(x = rep(0:1, each = 100)) %&gt;% mutate(y = rnorm(n = n(), mean = 100, sd = 10 + x * 10)) ) ## # A tibble: 200 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 0 92.3 ## 2 0 91.8 ## 3 0 98.6 ## 4 0 97.2 ## 5 0 104. ## 6 0 88.1 ## 7 0 112. ## 8 0 99.8 ## 9 0 97.5 ## 10 0 96.4 ## # … with 190 more rows We can view what data like these look like with aid from tidybayes::geom_halfeyeh(). library(tidybayes) d %&gt;% mutate(x = x %&gt;% as.character()) %&gt;% ggplot(aes(x = y, y = x, fill = x)) + geom_halfeyeh(point_interval = mean_qi, .width = .68, color = ghibli_palette(&quot;KikiMedium&quot;)[2]) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[c(4, 6)])) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) Even though the means of y are the same for both levels of the x dummy, the variance for x == 1 is substantially larger than that for x == 0. Let’s open brms. library(brms) For such a model, we have two formulas: one for \\(\\mu\\) and one for \\(\\sigma\\). We wrap both within the bf() function. b9.1 &lt;- brm(data = d, family = gaussian, bf(y ~ 1, sigma ~ 1 + x), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = Intercept, dpar = sigma), prior(normal(0, 10), class = b, dpar = sigma)), seed = 9, file = &quot;fits/b09.01&quot;) Do note our use of the dpar arguments in the prior statements. Here’s the summary. print(b9.1) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 1 ## sigma ~ 1 + x ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 99.04 0.86 97.37 100.77 1.00 4178 3060 ## sigma_Intercept 2.26 0.07 2.13 2.40 1.00 3799 3170 ## sigma_x 0.73 0.10 0.53 0.92 1.00 4273 3177 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we get an intercept for both \\(\\mu\\) and \\(\\sigma\\), with the intercept for sigma identified as sigma_Intercept. And note the coefficient for \\(\\sigma\\) was named sigma_x. Also notice the scale the sigma_i coefficients are on. These are not in the original metric, but rather based on a logarithmic transformation of \\(\\sigma\\). You can confirm that by the second line of the print() output: Links: mu = identity; sigma = log. So if you want to get a sense of the effects of x on the \\(\\sigma\\) for y, you have to exponentiate the formula. Here we’ll do so with the posterior_samples(). post &lt;- posterior_samples(b9.1) head(post) ## b_Intercept b_sigma_Intercept b_sigma_x lp__ ## 1 99.75865 2.281731 0.6406836 -818.1893 ## 2 99.83593 2.244206 0.7046929 -817.9380 ## 3 98.40659 2.282739 0.7512565 -817.9899 ## 4 98.59265 2.222575 0.6913024 -818.1270 ## 5 98.05085 2.244358 0.7482665 -818.1277 ## 6 99.71570 2.306853 0.6402182 -818.0676 With the samples in hand, we’ll use the model formula to compute the model-implied standard deviations of y based on the x dummy and then examine them in a plot. post %&gt;% transmute(`x == 0` = exp(b_sigma_Intercept + b_sigma_x * 0), `x == 1` = exp(b_sigma_Intercept + b_sigma_x * 1)) %&gt;% gather(key, sd) %&gt;% ggplot(aes(x = sd, y = key, fill = key)) + geom_halfeyeh(point_interval = median_qi, .width = .95, color = ghibli_palette(&quot;KikiMedium&quot;)[2]) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[c(4, 6)])) + labs(subtitle = &quot;Model-implied standard deviations by group&quot;, x = expression(sigma[x]), y = NULL) + coord_cartesian(ylim = c(1.5, 2)) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7])) And if we looked back at the data, those \\(SD\\) estimates are just what we’d expect. d %&gt;% group_by(x) %&gt;% summarise(sd = sd(y) %&gt;% round(digits = 1)) ## # A tibble: 2 x 2 ## x sd ## &lt;int&gt; &lt;dbl&gt; ## 1 0 9.6 ## 2 1 19.8 For more on models like this, check out Christakis’s 2014: What scientific idea is ready for retirement? or The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright. Kruschke covered modeling \\(\\sigma\\) a bit in his Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan, my translation for which lives here. Finally, this is foreshadowing a bit because it requires the multilevel model (see Chapters 12 and 13), but you might also check out the preprint by Williams, Liu, Martin, and Rast, Bayesian Multivariate Mixed-Effects Location Scale Modeling of Longitudinal Relations among Affective Traits, States, and Physical Activity. But getting back to the text, what the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving \\(\\log (\\sigma_i) = \\alpha + \\beta x_i\\) for \\(\\sigma_i\\) yields the inverse link: \\[\\sigma_i = \\exp (\\alpha + \\beta x_i)\\] The impact of this assumption can be seen in [our version of] Figure 9.8. (pp. 286—287) # first, we&#39;ll make data that&#39;ll be make the horizontal lines alpha &lt;- 0 beta &lt;- 2 lines &lt;- tibble(`log-measurement` = -3:3, `original measurement` = exp(-3:3)) # now we&#39;re ready to make the primary data d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-measurement` = alpha + x * beta, `original measurement` = exp(alpha + x * beta)) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-measurement`)) + geom_hline(data = lines, aes(yintercept = `log-measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5])) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = `original measurement`)) + geom_hline(data = lines, aes(yintercept = `original measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + scale_y_continuous(position = &quot;right&quot;, limits = c(0, 10)) + coord_cartesian(xlim = -1:1) + theme(panel.grid = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7])) # combine the ggplots p1 | p2 Using a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right). Another way to think of this relationship is to remember that logarithms are magnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the untransformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot of Figure 9.8. (p. 287, emphasis in the original) 9.2.3 Absolute and relative differences. Within the context of GLMs with non-identity link functions, parameter estimates do not by themselves tell you the importance of a predictor on the outcome. The reason is that each parameter represents a relative difference on the scale of the linear model, ignoring other parameters, while we are really interested in the absolute differences in the outcomes that must incorporate all parameters. (p. 288, emphasis in the original) This will make more sense after we start playing around with logistic regression, count regression, and so on. For now, just file it away. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.3 tidybayes_2.0.1.9000 patchwork_1.0.0 ## [5] ghibli_0.3.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [9] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [13] ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] rstan_2.19.2 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 ## [16] xml2_1.2.2 bridgesampling_0.8-1 knitr_1.26 ## [19] shinythemes_1.1.2 bayesplot_1.7.1 jsonlite_1.6.1 ## [22] broom_0.5.3 dbplyr_1.4.2 shiny_1.4.0 ## [25] compiler_3.6.2 httr_1.4.1 backports_1.1.5 ## [28] assertthat_0.2.1 Matrix_1.2-18 fastmap_1.0.1 ## [31] lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [34] prettyunits_1.1.1 htmltools_0.4.0 tools_3.6.2 ## [37] igraph_1.2.4.2 coda_0.19-3 gtable_0.3.0 ## [40] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [43] vctrs_0.2.2 nlme_3.1-142 crosstalk_1.0.0 ## [46] xfun_0.12 ps_1.3.0 rvest_0.3.5 ## [49] mime_0.8 miniUI_0.1.1.1 lifecycle_0.1.0 ## [52] gtools_3.8.1 zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 parallel_3.6.2 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 ## [64] StanHeaders_2.19.0 loo_2.2.0 stringi_1.4.5 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.4 ## [70] pkgconfig_2.0.3 matrixStats_0.55.0 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [79] plyr_1.8.5 magrittr_1.5 R6_2.4.1 ## [82] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 ## [85] haven_2.2.0 withr_2.1.2 xts_0.12-0 ## [88] abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_2.0 ## [94] grid_3.6.2 readxl_1.3.1 callr_3.4.1 ## [97] threejs_0.3.3 reprex_0.3.0 digest_0.6.23 ## [100] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.2 ## [103] munsell_0.5.0 shinyjs_1.1 "],
["counting-and-classification.html", "10 Counting and Classification 10.1 Binomial regression 10.2 Poisson regression 10.3 Other count regressions 10.4 Summary Reference Session info", " 10 Counting and Classification All over the world, every day, scientists throw away information. Sometimes this is through the removal of “outliers,” cases in the data that offend the model and are exiled. More routinely, counted things are converted to proportions before analysis. Why does analysis of proportions throw away information? Because 10/20 and ½ are the same proportion, one-half, but have very different sample sizes. Once converted to proportions, and treated as outcomes in a linear regression, the information about sample size has been destroyed. It’s easy to retain the information about sample size. All that is needed is to model what has actually been observed, the counts instead of the proportions. (p. 291) In this chapter, we focus on the two most common types of count models: the binomial and the Poisson. Side note: For a nice Bayesian way to accommodate outliers in your Gaussian models, check out my blog post, Robust Linear Regression with Student’s \\(t\\)-Distribution. 10.1 Binomial regression The basic binomial model follows the form \\[y \\sim \\text{Binomial} (n, p),\\] where \\(y\\) is some count variable, \\(n\\) is the number of trials, and \\(p\\) it the probability a given trial was a 1, which is sometimes termed a success. When \\(n = 1\\), then \\(y\\) is a vector of 0s and 1s. Presuming the logit link, which we just covered in Chapter 9, models of this type are commonly termed logistic regression. When \\(n &gt; 1\\), and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model. 10.1.1 Logistic regression: Prosocial chimpanzees. Load the Silk et al. (2005) chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(chimpanzees) We start with the simple intercept-only logistic regression model, which follows the statistical formula \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] In the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above. b10.1 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 10), class = Intercept), seed = 10, file = &quot;fits/b10.01&quot;) You might use fixef() to get a focused summary of the intercept. library(tidyverse) fixef(b10.1) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.32 0.09 0.13 0.5 The brms::inv_logit_scaled() function will be our alternative to the logistic() function in rethinking. Here we use it to convert the 89% interval estimates McElreath reported on page 294. c(.18, .46) %&gt;% inv_logit_scaled() ## [1] 0.5448789 0.6130142 Here we use it to convert our fixef() output (which contains 95% intervals). fixef(b10.1) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.5785745 0.5231442 0.5333637 0.6224224 With the next two chimp models, we add predictors in the usual way. b10.2 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), seed = 10, file = &quot;fits/b10.02&quot;) b10.3 &lt;- update(b10.2, newdata = d, formula = pulled_left | trials(1) ~ 1 + prosoc_left + condition:prosoc_left, seed = 10, file = &quot;fits/b10.03&quot;) Compute the WAIC for each model and save the results within the brmfit objects. b10.1 &lt;- add_criterion(b10.1, &quot;waic&quot;) b10.2 &lt;- add_criterion(b10.2, &quot;waic&quot;) b10.3 &lt;- add_criterion(b10.3, &quot;waic&quot;) Compare them with the loo_compare() and make sure to add the criterion = &quot;waic&quot; argument. w &lt;- loo_compare(b10.1, b10.2, b10.3, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b10.2 0.0 0.0 -340.2 4.7 2.0 0.0 680.5 9.3 ## b10.3 -1.0 0.4 -341.2 4.7 3.1 0.1 682.5 9.4 ## b10.1 -3.8 3.1 -344.0 3.5 1.1 0.0 688.1 7.0 Recall our cbind() trick to convert the differences from the \\(\\text{elpd}\\) metric to the WAIC metric. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) %&gt;% round(digits = 2) ## waic_diff se ## b10.2 0.00 0.00 ## b10.3 2.01 0.86 ## b10.1 7.60 6.16 For this chapter, we’ll take our color scheme from the wesanderson package’s Moonrise2 palette. # install.packages(&quot;wesanderson&quot;, dependencies = T) library(wesanderson) wes_palette(&quot;Moonrise2&quot;) wes_palette(&quot;Moonrise2&quot;)[1:4] ## [1] &quot;#798E87&quot; &quot;#C27D38&quot; &quot;#CCC591&quot; &quot;#29211F&quot; We’ll also take a few formatting cues from Edward Tufte, courtesy of the ggthemes package. The theme_tufte() function will change the default font and remove some chart junk. The theme_set() function, below, will make these adjustments the default for all subsequent ggplot2 plots. To undo this, just execute theme_set(theme_default()). library(ggthemes) theme_set(theme_default() + theme_tufte() + theme(plot.background = element_rect(fill = wes_palette(&quot;Moonrise2&quot;)[3], color = wes_palette(&quot;Moonrise2&quot;)[3]))) Finally, here’s our WAIC plot. w %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model&quot;) %&gt;% ggplot() + geom_pointrange(aes(x = reorder(model, -waic), y = waic, ymin = waic - se_waic, ymax = waic + se_waic, color = model), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1:2, 4)]) + coord_flip() + labs(title = &quot;WAIC&quot;, x = NULL, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) The full model, b10.3, did not have the lowest WAIC value. Though note how wide those standard error bars are relative to the point estimates. There’s a lot of model uncertainty there. Here are the WAIC weights. model_weights(b10.1, b10.2, b10.3, weights = &quot;waic&quot;) ## b10.1 b10.2 b10.3 ## 0.01611244 0.71991083 0.26397673 Let’s look at the parameter summaries for the theory-based model. print(b10.3) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ prosoc_left + prosoc_left:condition ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.05 0.13 -0.19 0.30 1.00 3350 2971 ## prosoc_left 0.62 0.23 0.17 1.07 1.00 2944 2858 ## prosoc_left:condition -0.11 0.27 -0.63 0.42 1.00 2817 2799 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the odds are multiplied by: fixef(b10.3)[2] %&gt;% exp() ## [1] 1.850765 Given an estimated value of 4, the probability of a pull, all else equal, would be close to 1. inv_logit_scaled(4) ## [1] 0.9820138 Adding the coefficient, fixef(b10.3)[2], would yield an even higher estimate. (4 + fixef(b10.3)[2]) %&gt;% inv_logit_scaled() ## [1] 0.9902007 For our variant of Figure 10.2, we use brms::pp_average() in place of rethinking::ensemble(). # the combined `fitted()` results of the three models weighted by their WAICs ppa &lt;- pp_average(b10.1, b10.2, b10.3, weights = &quot;waic&quot;, method = &quot;fitted&quot;) %&gt;% as_tibble() %&gt;% bind_cols(b10.3$data) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # the empirically-based summaries d_plot &lt;- d %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(pulled_left = mean(pulled_left)) %&gt;% mutate(x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # the plot ppa %&gt;% ggplot(aes(x = x_axis)) + geom_smooth(aes(y = pulled_left, ymin = Q2.5, ymax = Q97.5, group = 0), stat = &quot;identity&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[2], color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_line(data = d_plot, aes(y = pulled_left, group = actor), color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/3) + scale_x_discrete(&quot;prosoc_left/condition&quot;, expand = c(.03, .03)) + ylab(&quot;proportion pulled left&quot;) + coord_cartesian(ylim = 0:1) + theme(axis.ticks.x = element_blank()) McElreath didn’t show the actual pairs plot in the text. Here’s ours using mcmc_pairs(). library(bayesplot) # this helps us set our custom color scheme color_scheme_set(wes_palette(&quot;Moonrise2&quot;)[c(3, 1, 2, 2, 1, 1)]) # the actual plot mcmc_pairs(x = posterior_samples(b10.3), pars = c(&quot;b_Intercept&quot;, &quot;b_prosoc_left&quot;, &quot;b_prosoc_left:condition&quot;), off_diag_args = list(size = 1/10, alpha = 1/6), diag_fun = &quot;dens&quot;) As McElreath observed, the posterior looks multivariate Gaussian. In equations, the next model follows the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{actor}} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] Enclosing the actor variable within factor() will produce the indexing we need to get actor-specific intercepts. Also notice we’re using the 0 + factor(actor) part of the model formula to suppress the brms default intercept. As such, the priors for all parameters in the model will be of class = b. And since we’re using the same Gaussian prior for each, we only need one line for the prior argument. b10.4 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left , prior(normal(0, 10), class = b), iter = 2500, warmup = 500, chains = 2, cores = 2, control = list(adapt_delta = 0.9), seed = 10, file = &quot;fits/b10.04&quot;) Within the tidyverse, the distinct() function returns the information you’d otherwise get from unique(). d %&gt;% distinct(actor) ## actor ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 With a single-level model like this, we have no need to use something like depth=2 for our posterior summary. print(b10.4) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 0 + factor(actor) + prosoc_left + condition:prosoc_left ## Data: d (Number of observations: 504) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## factoractor1 -0.74 0.28 -1.29 -0.20 1.00 4754 2762 ## factoractor2 10.96 5.45 3.96 23.97 1.00 2496 1869 ## factoractor3 -1.05 0.28 -1.60 -0.50 1.00 3549 2195 ## factoractor4 -1.05 0.28 -1.61 -0.51 1.00 3637 2520 ## factoractor5 -0.74 0.27 -1.29 -0.23 1.00 4001 2808 ## factoractor6 0.22 0.26 -0.29 0.73 1.00 3809 2904 ## factoractor7 1.81 0.40 1.08 2.64 1.00 4931 2485 ## prosoc_left 0.83 0.26 0.32 1.35 1.00 2413 2827 ## prosoc_left:condition -0.13 0.30 -0.71 0.46 1.00 3017 2822 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Correspondingly, brms::posterior_samples() returns an object for b10.4 that doesn’t quite follow the same structure as from rethinking::extract.samples(). We just have a typical 2-dimensional data frame. post &lt;- posterior_samples(b10.4) post %&gt;% glimpse() ## Observations: 4,000 ## Variables: 10 ## $ b_factoractor1 &lt;dbl&gt; -0.86797739, -0.93520202, -0.63946975, -0.80774780, -0.66771046, -0.64356… ## $ b_factoractor2 &lt;dbl&gt; 7.373280, 24.273011, 14.656043, 10.191818, 6.128601, 12.478147, 4.292197,… ## $ b_factoractor3 &lt;dbl&gt; -1.0495867, -1.3019965, -1.0967206, -0.9397774, -1.2818615, -1.0100174, -… ## $ b_factoractor4 &lt;dbl&gt; -0.9992568, -1.5222670, -0.7605698, -1.3537894, -0.9106560, -0.9069872, -… ## $ b_factoractor5 &lt;dbl&gt; -0.6238342, -1.2122306, -0.5372679, -0.9066737, -0.5157994, -0.9395416, -… ## $ b_factoractor6 &lt;dbl&gt; 0.21549313, -0.26665606, 0.38965066, 0.05657997, 0.24754017, 0.06684226, … ## $ b_factoractor7 &lt;dbl&gt; 2.4172225, 1.0623785, 2.7637801, 0.9149522, 2.6228352, 0.9291420, 2.89685… ## $ b_prosoc_left &lt;dbl&gt; 1.4145261, 1.1451838, 1.1729203, 0.7954634, 0.9415575, 0.7836842, 0.45656… ## $ `b_prosoc_left:condition` &lt;dbl&gt; -1.0153484251, 0.1731559266, -0.8834968509, 0.3417360614, -0.3324282324, … ## $ lp__ &lt;dbl&gt; -292.2549, -293.9714, -293.4702, -291.0704, -289.2766, -290.5880, -291.79… Here’s our variant of Figure 10.3, the \\(\\alpha\\) density for actor == 2. post %&gt;% ggplot(aes(x = b_factoractor2)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Actor 2&#39;s large and uncertain intercept&quot;, subtitle = &quot;Once your log-odds are above, like, 4, it&#39;s all\\npretty much a probability of 1.&quot;, x = NULL) Figure 10.4. shows the idiographic trajectories for four of our chimps. # subset the `d_plot` data d_plot_4 &lt;- d_plot %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor)) # compute the model-implied estimates with `fitted()` and wrangle f &lt;- fitted(b10.4) %&gt;% as_tibble() %&gt;% bind_cols(b10.4$data) %&gt;% filter(actor %in% c(3, 5:7)) %&gt;% distinct(Estimate, Q2.5, Q97.5, condition, prosoc_left, actor) %&gt;% select(actor, everything()) %&gt;% mutate(actor = str_c(&quot;actor &quot;, actor), x_axis = str_c(prosoc_left, condition, sep = &quot;/&quot;)) %&gt;% mutate(x_axis = factor(x_axis, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% rename(pulled_left = Estimate) # plot f %&gt;% ggplot(aes(x = x_axis, y = pulled_left, group = actor)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[2], color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_line(data = d_plot_4, color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1.25) + scale_x_discrete(&quot;prosoc_left/condition&quot;, expand = c(.03, .03)) + scale_y_continuous(&quot;proportion pulled left&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(axis.ticks.x = element_blank(), # color came from: https://www.color-hex.com/color/ccc591 panel.background = element_rect(fill = &quot;#d1ca9c&quot;, color = &quot;transparent&quot;)) + facet_wrap(~actor) McElreath mused: “There are a number of loose ends with this analysis. Does model [b10.4], with its 6 additional parameters, still look good after estimating overfitting with WAIC?” (p. 302). We won’t be following along with the practice problems at the end of the chapter, but we may as well just check the WAIC real quick. b10.4 &lt;- add_criterion(b10.4, &quot;waic&quot;) loo_compare(b10.1, b10.2, b10.3, b10.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b10.4 0.0 0.0 -265.0 10.0 8.4 0.5 530.0 20.0 ## b10.2 -75.2 9.6 -340.2 4.7 2.0 0.0 680.5 9.3 ## b10.3 -76.2 9.6 -341.2 4.7 3.1 0.1 682.5 9.4 ## b10.1 -79.0 10.0 -344.0 3.5 1.1 0.0 688.1 7.0 Here are the updated WAIC weights. model_weights(b10.1, b10.2, b10.3, b10.4, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b10.1 b10.2 b10.3 b10.4 ## 0 0 0 1 Both the WAIC differences and the WAIC weights suggest our 9-parameter b10.4 was substantially better than the previous models, even after correcting for overfitting. Some times group averages aren’t good enough. When you have data with many occasions within cases, fitting models that allow for individual differences is generally the way to go. 10.1.1.1 Overthinking: Using the by group_by() function. Instead of focusing on base R, let’s work within the tidyverse. If you wanted to compute the proportion of trials pulled_left == 1 for each combination of prosoc_left, condition, and chimp actor, you’d put those last three variables within group_by() and then compute the mean() of pulled_left within summarise(). d %&gt;% group_by(prosoc_left, condition, actor) %&gt;% summarise(`proportion pulled_left` = mean(pulled_left)) ## # A tibble: 28 x 4 ## # Groups: prosoc_left, condition [4] ## prosoc_left condition actor `proportion pulled_left` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0 1 0.333 ## 2 0 0 2 1 ## 3 0 0 3 0.278 ## 4 0 0 4 0.333 ## 5 0 0 5 0.333 ## 6 0 0 6 0.778 ## 7 0 0 7 0.778 ## 8 0 1 1 0.278 ## 9 0 1 2 1 ## 10 0 1 3 0.167 ## # … with 18 more rows And since we’re working within the tidyverse, that operation returns a tibble rather than a list. 10.1.2 Aggregated binomial: Chimpanzees again, condensed. With the tidyverse, we use group_by() and summarise() to achieve what McElreath did with aggregate(). d_aggregated &lt;- d %&gt;% select(-recipient, -block, -trial, -chose_prosoc) %&gt;% group_by(actor, condition, prosoc_left) %&gt;% summarise(x = sum(pulled_left)) d_aggregated %&gt;% filter(actor %in% c(1, 2)) ## # A tibble: 8 x 4 ## # Groups: actor, condition [4] ## actor condition prosoc_left x ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 6 ## 2 1 0 1 9 ## 3 1 1 0 5 ## 4 1 1 1 10 ## 5 2 0 0 18 ## 6 2 0 1 18 ## 7 2 1 0 18 ## 8 2 1 1 18 To fit an aggregated binomial model in brms, we augment the &lt;criterion&gt; | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing \\(n\\). Either way, at least some of those trials will have an \\(n &gt; 1\\). Here we’ll use the hard-code method, just like McElreath did in the text. b10.5 &lt;- brm(data = d_aggregated, family = binomial, x | trials(18) ~ 1 + prosoc_left + condition:prosoc_left, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.05&quot;) We might compare b10.3 with b10.5 like this. fixef(b10.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.13 -0.19 0.30 ## prosoc_left 0.62 0.23 0.17 1.07 ## prosoc_left:condition -0.11 0.27 -0.63 0.42 fixef(b10.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.05 0.12 -0.19 0.30 ## prosoc_left 0.61 0.23 0.18 1.07 ## prosoc_left:condition -0.11 0.26 -0.63 0.39 A coefficient plot can offer a complimentary perspective. library(broom) # wrangle tibble(model = str_c(&quot;b10.&quot;, c(3, 5))) %&gt;% mutate(fit = map(model, get)) %&gt;% mutate(tidy = map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% # plot ggplot() + geom_pointrange(aes(x = model, y = estimate, ymin = lower, ymax = upper, color = term), shape = 16) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1:2, 4)]) + labs(x = NULL, y = NULL) + coord_flip() + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~term, ncol = 1) The two are close within simulation error. 10.1.3 Aggregated binomial: Graduate school admissions. Load the infamous UCBadmit data (see Bickel, Hammel, &amp; O’Connell, 1975). # detach(package:brms) library(rethinking) data(UCBadmit) d &lt;- UCBadmit Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(UCBadmit) d ## dept applicant.gender admit reject applications ## 1 A male 512 313 825 ## 2 A female 89 19 108 ## 3 B male 353 207 560 ## 4 B female 17 8 25 ## 5 C male 120 205 325 ## 6 C female 202 391 593 ## 7 D male 138 279 417 ## 8 D female 131 244 375 ## 9 E male 53 138 191 ## 10 E female 94 299 393 ## 11 F male 22 351 373 ## 12 F female 24 317 341 Now compute our newly-constructed dummy variable, male. d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0)) The univariable logistic model with male as the sole predictor of admit follows the form \\[\\begin{align*} n_{\\text{admit}_i} &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\beta \\text{male}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] The second model omits the male predictor. b10.6 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male , prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.06&quot;) b10.7 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 10), class = Intercept), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.07&quot;) Compute the information criteria for each model and save the results within the brmfit objects. b10.6 &lt;- add_criterion(b10.6, &quot;waic&quot;) b10.7 &lt;- add_criterion(b10.7, &quot;waic&quot;) Here’s the WAIC comparison. w &lt;- loo_compare(b10.6, b10.7, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b10.6 0.0 0.0 -498.4 164.7 115.8 41.8 996.9 329.4 ## b10.7 -26.5 81.8 -525.0 164.5 85.3 37.2 1050.0 329.1 If you prefer the difference in the WAIC metric, use our cbind() conversion method from above. Here are the WAIC weights. model_weights(b10.6, b10.7, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b10.6 b10.7 ## 1 0 Bonus: Information criteria digression. Let’s see what happens if we switch to the LOO. b10.6 &lt;- add_criterion(b10.6, &quot;loo&quot;) ## Warning: Found 7 observations with a pareto_k &gt; 0.7 in model &#39;b10.6&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 7 times to compute the ELPDs for the problematic observations directly. ## Automatically saving the model object in &#39;fits/b10.06.rds&#39; b10.7 &lt;- add_criterion(b10.7, &quot;loo&quot;) ## Warning: Found 4 observations with a pareto_k &gt; 0.7 in model &#39;b10.7&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 4 times to compute the ELPDs for the problematic observations directly. ## Automatically saving the model object in &#39;fits/b10.07.rds&#39; If you just ape the text and use the WAIC, everything appears fine. But holy smokes look at those nasty warning messages from the loo()! One of the frightening but ultimately handy things about working with the PSIS-LOO is that it requires we estimate a Pareto \\(k\\) parameter, which you can learn all about in the loo-package section of the loo reference manual. As it turns out, the Pareto \\(k\\) can be used as a diagnostic tool. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)s are low. The makers of the loo package get worried when those \\(k\\)s exceed 0.7 and as a result, loo() spits out a warning message when they do. First things first, if you explicitly open the loo package, you’ll have access to some handy diagnostic functions. library(loo) We’ll be leveraging those \\(k\\) values with the pareto_k_table() and pareto_k_ids() functions. Both functions take objects created by the loo() or psis() functions. So, before we can get busy, we’ll first make two objects with the loo(). l_b10.6 &lt;- loo(b10.6) ## Warning: Found 7 observations with a pareto_k &gt; 0.7 in model &#39;b10.6&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 7 times to compute the ELPDs for the problematic observations directly. l_b10.7 &lt;- loo(b10.7) ## Warning: Found 4 observations with a pareto_k &gt; 0.7 in model &#39;b10.7&#39;. It is recommended to set &#39;reloo = TRUE&#39; ## in order to calculate the ELPD without the assumption that these observations are negligible. This will refit ## the model 4 times to compute the ELPDs for the problematic observations directly. There are those warning messages, again. Using the loo-object for model b10.6, which we’ve named l_b10.6, let’s take a look at the pareto_k_table() function. pareto_k_table(l_b10.6) ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 3 25.0% 636 ## (0.5, 0.7] (ok) 2 16.7% 234 ## (0.7, 1] (bad) 3 25.0% 18 ## (1, Inf) (very bad) 4 33.3% 2 You may have noticed that this same table pops out when you just do something like loo(b10.6). Recall that this data set has 12 observations (i.e., execute count(d)). With pareto_k_table(), we see how the Pareto \\(k\\) values have been categorized into bins ranging from “good” to “very bad”. Clearly, we like low \\(k\\)s. In this example, our observations are all over the place, with 4 in the “bad” \\(k\\) range. We can take a closer look like this: plot(l_b10.6) So when you plot() a loo object, you get a nice diagnostic plot for those \\(k\\) values, ordered by observation number. Our plot indicates cases 1, 3, 11, and 12 had “very bad” \\(k\\) values for this model. Case 2 was just below the “very bad” threshold. If we wanted to further verify to ourselves which observations those were, we’d use the pareto_k_ids() function. pareto_k_ids(l_b10.6, threshold = 1) ## [1] 1 3 11 12 Note our use of the threshold argument. Play around with it to see how it works. If you want an explicit look at those \\(k\\) values, you execute something like this: l_b10.6$diagnostics ## $pareto_k ## [1] 2.58093333 0.97848826 1.82356562 0.05539252 0.42991124 0.56326823 0.89152947 0.38165089 0.52524629 ## [10] 0.76460442 2.26268833 1.97800574 ## ## $n_eff ## [1] 2.798072 17.901945 4.205057 1607.055703 728.522114 233.658967 47.491819 636.317683 ## [9] 360.663407 74.831073 4.011311 2.341799 The pareto_k values can be used to examine cases that are overly-influential on the model parameters, something like a Cook’s \\(D_{i}\\). See, for example this discussion on stackoverflow.com in which several members of the Stan team weighed in. The issue is also discussed in this paper and in this presentation by Aki Vehtari. Anyway, the implication of all this is these values suggest model b10.6 isn’t a great fit for these data. Part of the warning message for model b10.6 read: It is recommended to set ‘reloo = TRUE’ in order to calculate the ELPD without the assumption that these observations are negligible. This will refit the model 7 times to compute the ELPDs for the problematic observations directly. Let’s do that. l_b10.6_reloo &lt;- loo(b10.6, reloo = T) Check the results. l_b10.6_reloo ## ## Computed from 4000 by 12 log-likelihood matrix ## ## Estimate SE ## elpd_loo -510.8 167.7 ## p_loo 128.1 47.6 ## looic 1021.5 335.4 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 10 83.3% 2 ## (0.5, 0.7] (ok) 2 16.7% 234 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Now that looks better. We’ll do the same thing for model b10.7. l_b10.7_reloo &lt;- loo(b10.7, reloo = T) Okay, let’s compare models with formal \\(\\text{elpd}_\\text{loo}\\) differences before and after adjusting with reloo = T. loo_compare(l_b10.6, l_b10.7) ## elpd_diff se_diff ## b10.6 0.0 0.0 ## b10.7 -24.8 76.2 loo_compare(l_b10.6_reloo, l_b10.7_reloo) ## elpd_diff se_diff ## b10.6 0.0 0.0 ## b10.7 -19.2 81.5 In this case, the results are kinda similar. The standard errors for the differences are huge compared to the point estimates, suggesting large uncertainty. Watch out for this in your real-world data, though. But this has all been a tangent from the central thrust of this section. Back from our information criteria digression. Let’s get back on track with the text. Here’s a look at b10.6, the unavailable model: print(b10.6) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 1 + male ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.83 0.05 -0.93 -0.73 1.00 2136 2359 ## male 0.61 0.06 0.48 0.74 1.00 2526 2561 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the relative difference in admission odds. fixef(b10.6)[2] %&gt;% exp() %&gt;% round(digits = 2) ## [1] 1.85 And now we’ll compute difference in admission probabilities. post &lt;- posterior_samples(b10.6) post %&gt;% mutate(p_admit_male = inv_logit_scaled(b_Intercept + b_male), p_admit_female = inv_logit_scaled(b_Intercept), diff_admit = p_admit_male - p_admit_female) %&gt;% summarise(`2.5%` = quantile(diff_admit, probs = .025), `50%` = median(diff_admit), `97.5%` = quantile(diff_admit, probs = .975)) ## 2.5% 50% 97.5% ## 1 0.1126993 0.141872 0.1707217 Instead of the summarise() code, we could have also used tidybayes::median_qi(diff_admit). It’s good to have options. Here’s our version of Figure 10.5. d &lt;- d %&gt;% mutate(case = factor(1:12)) p &lt;- predict(b10.6) %&gt;% as_tibble() %&gt;% bind_cols(d) text &lt;- d %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) ggplot(data = d, aes(x = case, y = admit / applications)) + geom_pointrange(data = p, aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + labs(title = &quot;Posterior validation check&quot;, y = &quot;Proportion admitted&quot;) + coord_cartesian(ylim = 0:1) + theme(axis.ticks.x = element_blank()) As alluded to in all that LOO/pareto_k talk, above, this is not a great fit. So we’ll ditch the last model paradigm for one that answers the new question “What is the average difference in probability of admission between females and males within departments?” (p. 307). The statistical formula for the full model follows the form \\[\\begin{align*} n_{\\text{admit}_i} &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept}_i} + \\beta \\text{male}_i \\\\ \\alpha_{\\text{dept}} &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] We don’t need to coerce an index like McElreath did in the text. But here are the models. b10.8 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 0 + dept, prior(normal(0, 10), class = b), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.08&quot;) b10.9 &lt;- update(b10.8, newdata = d, formula = admit | trials(applications) ~ 0 + dept + male, seed = 10, file = &quot;fits/b10.09&quot;) Let’s make two more loo() objects using reloo = T. l_b10.8_reloo &lt;- loo(b10.8, reloo = T) l_b10.9_reloo &lt;- loo(b10.9, reloo = T) Now compare them. loo_compare(l_b10.6_reloo, l_b10.7_reloo, l_b10.8_reloo, l_b10.9_reloo) ## elpd_diff se_diff ## b10.8 0.0 0.0 ## b10.9 -4.8 2.6 ## b10.6 -451.4 164.8 ## b10.7 -470.6 163.0 Here are the LOO weights. model_weights(b10.6, b10.7, b10.8, b10.9, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b10.6 b10.7 b10.8 b10.9 ## 0.000 0.000 0.854 0.146 The parameters summaries for our multivariable model, b10.9, look like this: fixef(b10.9) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## deptA 0.68 0.10 0.49 0.87 ## deptB 0.64 0.12 0.40 0.87 ## deptC -0.58 0.07 -0.73 -0.44 ## deptD -0.62 0.09 -0.79 -0.45 ## deptE -1.06 0.10 -1.26 -0.87 ## deptF -2.64 0.16 -2.96 -2.34 ## male -0.10 0.08 -0.26 0.06 And on the proportional odds scale, the posterior mean for b_male is: fixef(b10.9)[7, 1] %&gt;% exp() ## [1] 0.9071708 Since we’ve been using brms, there’s no need to fit our version of McElreath’s m10.9stan. We already have that in our b10.9. But just for kicks and giggles, here’s another way to get the model summary. b10.9$fit ## Inference for Stan model: 84ca3eaf4031602c7ef89b411c155ae6. ## 2 chains, each with iter=2500; warmup=500; thin=1; ## post-warmup draws per chain=2000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_deptA 0.68 0.00 0.10 0.49 0.61 0.68 0.75 0.87 2084 1 ## b_deptB 0.64 0.00 0.12 0.40 0.56 0.63 0.72 0.87 2265 1 ## b_deptC -0.58 0.00 0.07 -0.73 -0.63 -0.58 -0.53 -0.44 3700 1 ## b_deptD -0.62 0.00 0.09 -0.79 -0.67 -0.62 -0.56 -0.45 2502 1 ## b_deptE -1.06 0.00 0.10 -1.26 -1.13 -1.06 -0.99 -0.87 3237 1 ## b_deptF -2.64 0.00 0.16 -2.96 -2.75 -2.63 -2.53 -2.34 2914 1 ## b_male -0.10 0.00 0.08 -0.26 -0.15 -0.10 -0.04 0.06 1532 1 ## lp__ -70.68 0.04 1.86 -75.27 -71.75 -70.34 -69.31 -68.00 1831 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Feb 27 08:51:42 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Here’s our version of Figure 10.6, the posterior validation check. predict(b10.9) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + labs(title = &quot;Posterior validation check&quot;, y = &quot;Proportion admitted&quot;) + coord_cartesian(ylim = 0:1) + theme(axis.ticks.x = element_blank()) The model precisions are imperfect, but way more valid than before. The posterior looks reasonably multivariate Gaussian. pairs(b10.9, off_diag_args = list(size = 1/10, alpha = 1/6)) 10.1.3.1 Rethinking: Simpson’s paradox is not a paradox. This empirical example is a famous one in statistical teaching. It is often used to illustrate a phenomenon known as Simpson’s paradox. Like most paradoxes, there is no violation of logic, just of intuition. And since different people have different intuition, Simpson’s paradox means different things to different people. The poor intuition being violated in this case is that a positive association in the entire population should also hold within each department. (p. 309, emphasis in the original) In my field of clinical psychology, Simpson’s paradox is an important, if under-appreciated, phenomenon. If you’re in the social sciences as well, I highly recommend spending more time thinking about it. To get you started, I blogged about it here and Kievit, Frankenhuis, Waldorp, and Borsboom (2013) wrote a great tutorial paper, Simpson’s paradox in psychological science: a practical guide. 10.1.3.2 Overthinking: WAIC and aggregated binomial models. McElreath wrote: The WAIC function in rethinking detects aggregated binomial models and automatically splits them apart into 0/1 Bernoulli trials, for the purpose of calculating WAIC. It does this, because WAIC is computed point by point (see Chapter 6). So what you define as a “point” affects WAIC’s value. In an aggregated binomial each “point” is a bunch of independent trials that happen to share the same predictor values. In order for the disaggregated and aggregated models to agree, it makes sense to use the disaggregated representation. (p. 309) To my knowledge, brms::waic() and brms::loo() do not do this, which might well be why some of our values didn’t match up with those in the text. If you have additional insight on this, please share with the rest of the class. 10.1.4 Fitting binomial regressions with glm(). We’re not here to learn frequentist code, so we’re going to skip most of this section. But model m.good is worth fitting. Here are the data. # outcome and predictor almost perfectly associated y &lt;- c(rep(0, 10), rep(1, 10)) x &lt;- c(rep(-1, 9), rep(1, 11)) We are going to depart from McElreath’s naming convention in the text and call this fit b10.good. It’ll make it easier to find in this project’s fits folder. b10.good &lt;- brm(data = list(y = y, x = x), family = binomial, y | trials(1) ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), seed = 10, file = &quot;fits/b10.good&quot;) Our model summary will differ a bit from the one in the text. It seems this is because of the MAP/HMC contrast and our choice of priors. print(b10.good) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 1 + x ## Data: list(y = y, x = x) (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -5.28 4.16 -15.24 0.39 1.01 522 623 ## x 8.05 4.14 2.34 18.13 1.01 538 655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You might experiment with different prior \\(SD\\)s to see how they influence the posterior \\(SD\\)s. Anyways, here’s the pairs() plot McElreath excluded from the text. pairs(b10.good, off_diag_args = list(size = 1/10, alpha = 1/6)) That posterior, my friends, is not multivariate Gaussian. The plot deserves an extensive quote from McElreath. Inspecting the pairs plot (not shown) demonstrates just how subtle even simple models can be, once we start working with GLMs. I don’t say this to scare the reader. But it’s true that even simple models can behave in complicated ways. How you fit the model is part of the model, and in principle no GLM is safe for MAP estimation. (p. 311, emphasis added) 10.2 Poisson regression When a binomial distribution has a very small probability of an event \\(p\\) and a very large number of trials \\(n\\), then it takes on a special shape. The expected value of a binomial distribution is just \\(np\\), and its variance is \\(np(1 - p)\\). But when \\(n\\) is very large and \\(p\\) is very small, then these are approximately the same. (p. 311) Data of this kind are often called count data. Here we simulate some. set.seed(10) # make the results reproducible tibble(y = rbinom(1e5, 1000, 1/1000)) %&gt;% summarise(y_mean = mean(y), y_variance = var(y)) ## # A tibble: 1 x 2 ## y_mean y_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.994 0.995 Yes, those statistics are virtually the same. When dealing with Poisson data, \\(\\mu = \\sigma^2\\). When you have a number of trials for which \\(n\\) is unknown or much larger than seen in the data, the Poisson likelihood is a useful tool. We define it as \\[y \\sim \\text{Poisson} (\\lambda).\\] As \\(\\lambda\\) expresses both mean and variance because, within this model, the variance scales right along with the mean. Since \\(\\lambda\\) is constrained to be positive, we typically use the log link. Thus the basic Poisson regression model is \\[\\begin{align*} y_i &amp; \\sim \\text{Poisson} (\\lambda_i) \\\\ \\log (\\lambda_i) &amp; = \\alpha + \\beta x_i, \\end{align*}\\] where all model parameters receive priors following the forms we’ve been practicing. We read further: The parameter \\(\\lambda\\) is the expected value, but it’s also commonly thought of as a rate. Both interpretations are correct, and realizing this allows to make Poisson models for which the exposure varies across cases \\(i\\)… Implicitly, \\(\\lambda\\) is equal to an expected number of events, \\(\\mu\\), per unit of time or distance, \\(\\tau\\). This implies that \\(\\lambda = \\mu/\\tau\\), which lets us redefine the link: \\[\\begin{align*} y_i &amp; \\sim \\text{Poisson} (\\lambda_i) \\\\ \\log \\lambda_i &amp; = \\log \\frac{\\mu_i}{\\tau_i} = \\alpha + \\beta x_i \\end{align*}\\] Since the logarithm of a ratio is the same as a difference of logarithms, we can also write: \\[\\log \\lambda_i = \\log \\mu_i - \\log \\tau_i = \\alpha + \\beta x_i\\] These \\(\\tau\\) values are the “exposures.” So if different observations \\(i\\) have different exposures, this implies that the expected value on row \\(i\\) is given by: \\[\\log \\mu_i = \\log \\tau_i + \\alpha + \\beta x_i\\] When \\(\\tau_i = 1\\), then \\(\\log \\tau_i = 0\\) and we’re back where we started. But when the exposure varies across cases, then \\(\\tau_i\\) does the important work of correctly scaling the expected number of events for each case \\(i\\). (pp. 312–313) 10.2.1 Example: Oceanic tool complexity. Load the Kline data (see Kline &amp; Boyd, 2010). library(rethinking) data(Kline) d &lt;- Kline Switch from rethinking to brms. detach(package:rethinking, unload = T) library(brms) rm(Kline) d ## culture population contact total_tools mean_TU ## 1 Malekula 1100 low 13 3.2 ## 2 Tikopia 1500 low 22 4.7 ## 3 Santa Cruz 3600 low 24 4.0 ## 4 Yap 4791 high 43 5.0 ## 5 Lau Fiji 7400 high 33 5.0 ## 6 Trobriand 8000 high 19 4.0 ## 7 Chuuk 9200 high 40 3.8 ## 8 Manus 13000 low 28 6.6 ## 9 Tonga 17500 high 55 5.4 ## 10 Hawaii 275000 low 71 6.6 Here are our new columns. d &lt;- d %&gt;% mutate(log_pop = log(population), contact_high = ifelse(contact == &quot;high&quot;, 1, 0)) Our statistical model will follow the form \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\text{Poisson} (\\lambda_i) \\\\ \\text{log} (\\lambda_i) &amp; = \\alpha + \\beta_1 \\text{log_pop}_i + \\beta_2 \\text{contact_high}_i + \\beta_3 \\text{contact_high}_i \\times \\text{log_pop}_i \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_3 &amp; \\sim \\text{Normal} (0, 1). \\end{align*}\\] The only new thing in our model code is family = poisson. brms defaults to the log() link. b10.10 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/b10.10&quot;) print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.95 0.36 0.23 1.66 1.00 3909 3968 ## log_pop 0.26 0.04 0.19 0.33 1.00 4104 3916 ## contact_high -0.10 0.84 -1.72 1.52 1.00 3014 3893 ## log_pop:contact_high 0.04 0.09 -0.14 0.22 1.00 3020 3804 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can use vcov() to extract the correlation matrix for the parameters. vcov(b10.10, cor = T) %&gt;% round(digits = 2) ## Intercept log_pop contact_high log_pop:contact_high ## Intercept 1.00 -0.98 -0.11 0.05 ## log_pop -0.98 1.00 0.11 -0.07 ## contact_high -0.11 0.11 1.00 -0.99 ## log_pop:contact_high 0.05 -0.07 -0.99 1.00 And here’s the coefficient plot via bayesplot::mcmc_intervals(). post &lt;- posterior_samples(b10.10) # we&#39;ll set a renewed color theme color_scheme_set(wes_palette(&quot;Moonrise2&quot;)[c(2, 1, 4, 2, 1, 1)]) post %&gt;% select(-lp__) %&gt;% rename(b_interaction = `b_log_pop:contact_high`) %&gt;% mcmc_intervals(prob = .5, prob_outer = .95) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) How plausible is it a high-contact island will have more tools than a low-contact island? post &lt;- post %&gt;% mutate(lambda_high = exp(b_Intercept + b_contact_high + (b_log_pop + `b_log_pop:contact_high`) * 8), lambda_low = exp(b_Intercept + b_log_pop * 8)) %&gt;% mutate(diff = lambda_high - lambda_low) post %&gt;% summarise(sum = sum(diff &gt; 0) / length(diff)) ## sum ## 1 0.955625 Quite, it turns out. Behold the corresponding Figure 10.8.a. post %&gt;% ggplot(aes(x = diff)) + geom_density(color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1]) + geom_vline(xintercept = 0, linetype = 2, color = wes_palette(&quot;Moonrise2&quot;)[2]) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;lambda_high - lambda_low&quot;) I’m not happy with how clunky this solution is, but one way to get those marginal dot and line plots for the axes is to make intermediary tibbles. Anyway, here’s a version of Figure 10.8.b. # intermediary tibbles for our the dot and line portoin of the plot point_tibble &lt;- tibble(x = c(median(post$b_contact_high), min(post$b_contact_high)), y = c(min(post$`b_log_pop:contact_high`), median(post$`b_log_pop:contact_high`))) line_tibble &lt;- tibble(parameter = rep(c(&quot;b_contact_high&quot;, &quot;b_log_pop:contact_high&quot;), each = 2), x = c(quantile(post$b_contact_high, probs = c(.025, .975)), rep(min(post$b_contact_high), times = 2)), y = c(rep(min(post$`b_log_pop:contact_high`), times = 2), quantile(post$`b_log_pop:contact_high`, probs = c(.025, .975)))) # the plot post %&gt;% ggplot(aes(x = b_contact_high, y = `b_log_pop:contact_high`)) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) + geom_point(data = point_tibble, aes(x = x, y = y)) + geom_line(data = line_tibble, aes(x = x, y = y, group = parameter)) The ggMarginal() function from the ggExtra package offers an interesting alternative. library(ggExtra) # the base plot p &lt;- post %&gt;% ggplot(aes(x = b_contact_high, y = `b_log_pop:contact_high`)) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) # add the marginal plots p %&gt;% ggMarginal(data = post, type = &#39;density&#39;, color = &quot;transparent&quot;, fill = wes_palette(&quot;Moonrise2&quot;)[1], size = 4) To get a feel for what’s possible with ggMarginal(), check out Dean Attali’s great shiny app. Here we deconstruct model b10.10, bit by bit. # no interaction b10.11 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop + contact_high, seed = 10, file = &quot;fits/b10.11&quot;) # no contact rate b10.12 &lt;- update(b10.10, formula = total_tools ~ 1 + log_pop, seed = 10, file = &quot;fits/b10.12&quot;) # no log-population b10.13 &lt;- update(b10.10, formula = total_tools ~ 1 + contact_high, seed = 10, file = &quot;fits/b10.13&quot;) # intercept only b10.14 &lt;- update(b10.10, formula = total_tools ~ 1, seed = 10, file = &quot;fits/b10.14&quot;) I know we got all excited with the LOO, above. Let’s just be lazy and go WAIC. [Though beware, the LOO opens up a similar can of worms, here.] b10.10 &lt;- add_criterion(b10.10, criterion = &quot;waic&quot;) b10.11 &lt;- add_criterion(b10.11, criterion = &quot;waic&quot;) b10.12 &lt;- add_criterion(b10.12, criterion = &quot;waic&quot;) b10.13 &lt;- add_criterion(b10.13, criterion = &quot;waic&quot;) b10.14 &lt;- add_criterion(b10.14, criterion = &quot;waic&quot;) Now compare them. w &lt;- loo_compare(b10.10, b10.11, b10.12, b10.13, b10.14, criterion = &quot;waic&quot;) cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) %&gt;% round(digits = 2) ## waic_diff se ## b10.11 0.00 0.00 ## b10.10 1.06 1.21 ## b10.12 5.50 8.34 ## b10.14 62.58 34.64 ## b10.13 71.64 47.17 Let’s get those WAIC weights, too. model_weights(b10.10, b10.11, b10.12, b10.13, b10.14, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b10.10 b10.11 b10.12 b10.13 b10.14 ## 0.36 0.60 0.04 0.00 0.00 Now wrangle w a little and make the WAIC plot. w %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;model&quot;) %&gt;% ggplot(aes(x = reorder(model, -waic), y = waic, ymin = waic - se_waic, ymax = waic + se_waic, color = model)) + geom_pointrange(shape = 16, show.legend = F) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1, 2, 1, 1, 1)]) + coord_flip() + labs(title = &quot;WAIC&quot;, x = NULL, y = NULL) + theme(axis.ticks.y = element_blank()) Here’s our version of Figure 10.9. Recall, to do an “ensemble” posterior prediction in brms, one uses the pp_average() function. I know we were just lazy and focused on the WAIC. But let’s play around, a bit. Here we’ll weight the models based on the LOO by adding a weights = &quot;loo&quot; argument to the pp_average() function. If you check the corresponding section of the brms reference manual, you’ll find several weighting schemes. nd &lt;- crossing(contact_high = 0:1, log_pop = seq(from = 6.5, to = 13, length.out = 50)) ppa &lt;- pp_average(b10.10, b10.11, b10.12, weights = &quot;loo&quot;, method = &quot;fitted&quot;, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ppa %&gt;% ggplot(aes(x = log_pop, group = contact_high, color = contact_high)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = contact_high), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_text(data = d, aes(y = total_tools, label = total_tools), size = 3.5) + labs(subtitle = &quot;Blue is the high contact rate; black is the low.&quot;, x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(7.1, 12.4), ylim = c(12, 70)) + theme(legend.position = &quot;none&quot;, panel.border = element_blank()) In case you were curious, here are those LOO weights. model_weights(b10.10, b10.11, b10.12, weights = &quot;loo&quot;) ## b10.10 b10.11 b10.12 ## 0.3235618 0.6445988 0.0318394 10.2.2 MCMC islands. We fit our analogue to m10.10stan, b10.10, some time ago. print(b10.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop + contact_high + contact_high:log_pop ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.95 0.36 0.23 1.66 1.00 3909 3968 ## log_pop 0.26 0.04 0.19 0.33 1.00 4104 3916 ## contact_high -0.10 0.84 -1.72 1.52 1.00 3014 3893 ## log_pop:contact_high 0.04 0.09 -0.14 0.22 1.00 3020 3804 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Center log_pop. d &lt;- d %&gt;% mutate(log_pop_c = log_pop - mean(log_pop)) Now fit the log_pop-centered model. b10.10_c &lt;- brm(data = d, family = poisson, total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 10, file = &quot;fits/b10.10_c&quot;) print(b10.10_c) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 + log_pop_c + contact_high + contact_high:log_pop_c ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.31 0.09 3.13 3.48 1.00 5579 5468 ## log_pop_c 0.26 0.04 0.19 0.33 1.00 5818 6354 ## contact_high 0.28 0.12 0.05 0.52 1.00 6329 5996 ## log_pop_c:contact_high 0.07 0.17 -0.26 0.40 1.00 6938 6183 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ve been using bayesplot::mcmc_pairs() a lot for our posterior pairs plots. Let’s get in a little practice with GGally::ggpairs() for Figure 10.10. In Chapters 5 and 8, we used custom functions to augment the default ggpairs() output. We’ll continue that trend, here. Here are the custom functions for the upper triangle, the diagonal, and the lower triangle. my_upper &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) r &lt;- unname(cor.test(x, y)$estimate) rt &lt;- format(r, digits = 2)[1] tt &lt;- as.character(rt) # plot the cor value ggally_text( label = tt, mapping = aes(), size = 4, color = wes_palette(&quot;Moonrise2&quot;)[4], alpha = 4/5, family = &quot;Times&quot;) + theme_void() } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = wes_palette(&quot;Moonrise2&quot;)[2], size = 0) + theme_void() } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) + theme_void() } To learn more about the nature of the code for the my_upper() function, check out Issue #139 in the GGally GitHub repository. Here are our plots for the left and right panels of Figure 10.10. library(GGally) # left panel for `b10.10` posterior_samples(b10.10) %&gt;% select(-lp__) %&gt;% set_names(c(&quot;alpha&quot;, &quot;beta[log_pop]&quot;, &quot;beta[contact_high]&quot;, &quot;beta[interaction]&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = &quot;label_parsed&quot;) + ggtitle(&quot;Model: b10.10&quot;) + theme(strip.text = element_text(size = 11)) # right panel for `b10.10_c` posterior_samples(b10.10_c) %&gt;% select(-lp__) %&gt;% set_names(c(&quot;alpha&quot;, &quot;beta[log_pop_c]&quot;, &quot;beta[contact_high]&quot;, &quot;beta[interaction]&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = &quot;label_parsed&quot;) + ggtitle(&quot;Model: b10.10_c&quot;) + theme(strip.text = element_text(size = 11)) In case you were wondering, it turns out you cannot combine GGally::ggpairs() plots with syntax from the patchwork package. Check out issue #52 on the patchwork GitHub repo to learn why. 10.2.3 Example: Exposure and the offset. For the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained on page 312 [of the text], is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset. (p. 321, emphasis in the original) Here we simulate our data. set.seed(10) num_days &lt;- 30 y &lt;- rpois(num_days, 1.5) num_weeks &lt;- 4 y_new &lt;- rpois(num_weeks, 0.5 * 7) Let’s make them tidy and add log_days. ( d &lt;- tibble(y = c(y, y_new), days = c(rep(1, num_days), rep(7, num_weeks)), monastery = c(rep(0, num_days), rep(1, num_weeks))) %&gt;% mutate(log_days = log(days)) ) ## # A tibble: 34 x 4 ## y days monastery log_days ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 0 ## 2 1 1 0 0 ## 3 1 1 0 0 ## 4 2 1 0 0 ## 5 0 1 0 0 ## 6 1 1 0 0 ## 7 1 1 0 0 ## 8 1 1 0 0 ## 9 2 1 0 0 ## 10 1 1 0 0 ## # … with 24 more rows With the brms package, you use the offset() syntax, in which you put a pre-processed variable like log_days or the log of a variable, such as log(days). b10.15 &lt;- brm(data = d, family = poisson, y ~ 1 + offset(log_days) + monastery, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.15&quot;) As we look at the model summary, keep in mind that the parameters are on the per-one-unit-of-time scale. Since we simulated the data based on summary information from two units of time–one day and seven days–, this means the parameters are in the scale of \\(\\log (\\lambda)\\) per one day. print(b10.15) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + offset(log_days) + monastery ## Data: d (Number of observations: 34) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.30 0.16 -0.02 0.59 1.00 2595 2501 ## monastery -1.09 0.31 -1.70 -0.51 1.00 2742 2475 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The model summary helps clarify that when you use offset(), brm() fixes the value. Thus there is no parameter estimate for the offset(). It’s a fixed part of the model not unlike the \\(\\nu\\) parameter of the Student-\\(t\\) distribution gets fixed to infinity when you use the Gaussian likelihood. To get the posterior distributions for average daily outputs for the old and new monasteries, respectively, we’ll use use the formulas \\[\\begin{align*} \\lambda_\\text{old} &amp; = \\exp (\\alpha) \\;\\;\\; \\text{and} \\\\ \\lambda_\\text{new} &amp; = \\exp (\\alpha + \\beta_\\text{monastery}). \\end{align*}\\] Following those transformations, we’ll summarize the \\(\\lambda\\) distributions with medians and 89% HDIs with help from the tidybayes::mean_hdi() function. library(tidybayes) posterior_samples(b10.15) %&gt;% transmute(lambda_old = exp(b_Intercept), lambda_new = exp(b_Intercept + b_monastery)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;lambda_old&quot;, &quot;lambda_new&quot;))) %&gt;% group_by(key) %&gt;% mean_hdi(value, .width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lambda_old 1.36 1.03 1.7 0.89 mean hdi ## 2 lambda_new 0.47 0.26 0.64 0.89 mean hdi As McElreath pointed out in the text, “Your estimates will be slightly different, because you got different randomly simulated data” (p. 322). However, if you look back up to our simulation code, those median values are pretty close to the 1.5 and 0.5 values we plugged into the rpois() functions. 10.3 Other count regressions The next two of the remaining four models are maximum entropy distributions for certain problem types. The last two are mixtures, of which we’ll see more in the next chapter. 10.3.1 Multinomial. When more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. [We] already met the multinomial, implicitly, in Chapter 9 when we tossed pebbles into buckets as an introduction to maximum entropy. The binomial is really a special case of this distribution. And so its distribution formula resembles the binomial, just extrapolated out to three or more types of events. If there are \\(K\\) types of events with probabilities \\(p_1, …, p_K\\), then the probability of observing \\(y_1, …, y_K\\) events of each type out of \\(n\\) trials is (p. 323): \\[\\text{Pr} (y_1, ..., y_K | n, p_1, ..., p_K) = \\frac{n!}{\\prod_i y_i!} \\prod_{i = 1}^K p_i^{y_i}\\] Compare that equation with the simpler version in section 2.3.1 (page 33 in the text). 10.3.1.1 Explicit multinomial models. “The conventional and natural link is this context is the multinomial logit. This link function takes a vector of scores, one for each \\(K\\) event types, and computed the probability of a particular type of event \\(K\\) as” (p. 323, emphasis in the original) \\[\\text{Pr} (k |s_1, s_2, ..., s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}\\] McElreath then went on to explain how multinomial logistic regression models are among the more difficult of the GLMs to master. He wasn’t kidding. To get a grasp on these, we’ll cover them in a little more detail than he did in the text. To begin, let’s simulate the data just like McElreath did in the R code 10.56 block. library(rethinking) # simulate career choices among 500 individuals n &lt;- 500 # number of individuals income &lt;- 1:3 # expected income of each career score &lt;- 0.5 * income # scores for each career, based on income # next line converts scores to probabilities p &lt;- softmax(score[1], score[2], score[3]) # now simulate choice # outcome career holds event type values, not counts career &lt;- rep(NA, n) # empty vector of choices for each individual set.seed(10) # sample chosen career for each individual for(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p) Here’s what the data look like. tibble(career = career) %&gt;% ggplot(aes(x = career)) + geom_bar(size = 0, fill = wes_palette(&quot;Moonrise2&quot;)[2]) Our career variable is composed of three categories, 1:3, with each category more likely than the one before. Here’s a breakdown of the counts, percentages, and probabilities of each category. tibble(career) %&gt;% count(career) %&gt;% mutate(percent = (100 * n / sum(n)), probability = n / sum(n)) ## # A tibble: 3 x 4 ## career n percent probability ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 93 18.6 0.186 ## 2 2 152 30.4 0.304 ## 3 3 255 51 0.51 To help build an appreciation for how we simulated data with these proportions and how the process links in with the formulas, above, we’ll retrace the first few simulation steps within a tidyverse-centric work flow. Recall how in those first few steps we defined values for income, score, and p. Here they are again in a tibble. tibble(income = 1:3) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 3 ## income score p ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.5 0.186 ## 2 2 1 0.307 ## 3 3 1.5 0.506 Notice how the values in the p column match up well with the probability values from the output from the block just above. Our simulation successfully produces data corresponding to the data-generating values. Woot! Also note how the code we just used to compute those p values, p = exp(score) / sum(exp(score)), corresponds nicely with the formula from above: \\[\\text{Pr} (k |s_1, s_2, ..., s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}.\\] What still might seem mysterious is what those \\(s\\) values in the equation are. In the simulation and in the prose, McElreath called them scores. Another way to think about them is as weights. The thing to get is that their exact values aren’t important so much as their difference one from another. You’ll note that score for income == 2 was 0.5 larger than that of income == 1. The same was true for income == 3 and income == 2. So if we add an arbitrary constant to each of those score values, like 104, we’ll get the same p values. tibble(score = 104 + c(0.5, 1, 1.5)) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 2 ## score p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 104. 0.186 ## 2 105 0.307 ## 3 106. 0.506 Now keeping that in mind, recall how McElreath said that though we have \\(K\\) categories, \\(K = 3\\) in this case, we only estimate \\(K - 1\\) linear models. “In a multinomial (or categorical) GLM, you need \\(K - 1\\) linear models for \\(K\\) types of events” (pp. 323–324). Right before he showed the code for m10.16, he further wrote: We also have to pick one of the event types to be the reference type. We’ll use the first one. Instead of getting a linear model, that type is assigned a constant value. Then the other types get linear models that contain parameters relative to the reference type. (p. 324) In his model code (R code 10.57), you’ll see he used zero as that constant value. As it turns out, it is common practice to set the score value for the reference category to zero. It’s also a common practice to use the first event type as the reference category. Importantly, in his Parameterization of Response Distributions in brms vignette, Bürkner clarified the brms default is to use the first response category as the reference and set it to a zero as well. Returning to our tibble, here are what the p values for each income level are if we set the score for income == 1 to 0 and have each following score value increase by 0.5. tibble(income = 1:3, score = c(0, 0.5, 1)) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 3 ## income score p ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0.186 ## 2 2 0.5 0.307 ## 3 3 1 0.506 Those p values are still the same as in the prior examples. If our model fitting is successful, our statistical model will return just those probability estimates. To get ready to fit our model, let’s switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Before we fit the model, we might take a quick look at the prior structure with brms::get_prior(). get_prior(data = list(career = career), family = categorical(link = logit), career ~ 1) ## prior class coef group resp dpar nlpar bound ## 1 Intercept ## 2 student_t(3, 3, 10) Intercept mu2 ## 3 student_t(3, 3, 10) Intercept mu3 In brms-parlance, this an Intercepts-only model. We have two “intercepts”, which are differentiated in the dpar column. We’ll talk more about what these are in just a bit; don’t worry. I show this here because as of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” The upshot is even if we want to use the same prior for both, we need to use the dpar argument for each. With that in mind, here’s our multinomial model in brms. Do note the specification family = categorical(link = logit). b10.16 &lt;- brm(data = list(career = career), family = categorical(link = logit), career ~ 1, prior = c(prior(normal(0, 5), class = Intercept, dpar = mu2), prior(normal(0, 5), class = Intercept, dpar = mu3)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.16&quot;) Check the results. print(b10.16) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 ## Data: list(career = career) (Number of observations: 500) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu2_Intercept 0.49 0.14 0.23 0.76 1.00 1220 1751 ## mu3_Intercept 1.01 0.12 0.77 1.25 1.00 1274 1876 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). brms::brm() referred to the \\(K\\) categories as mu1, mu2, and mu3. Since career == 1 is the reference category, the score for which was set to zero, there is no parameter for mu1_Intercept. That’s a zero. Now notice how mu2_Intercept is about 0.5 and mu3_Intercept is about 1. That’s just like those score values from our last tibble block! But these parameters are of scores or weights; they are not probabilities. This means just applying the inv_logit_scaled() function to them won’t work. fixef(b10.16) %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## mu2_Intercept 0.6210687 0.5337550 0.5575840 0.6816873 ## mu3_Intercept 0.7334680 0.5306414 0.6835413 0.7779927 Those Estimate values are not the probability values we’re looking for. Why? Because the weights are all relative to one another. The easiest way to get what we want, the probabilities for the three categories, is with fitted(). Since this model has no predictors, only intercepts, we won’t specify any newdata. In such a case, fitted() will return fitted values for each case in the data. Going slow, let’s take a look at the structure of the output. fitted(b10.16) %&gt;% str() ## num [1:500, 1:4, 1:3] 0.186 0.186 0.186 0.186 0.186 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:3] &quot;P(Y = 1)&quot; &quot;P(Y = 2)&quot; &quot;P(Y = 3)&quot; Just as expected, we have 500 rows–one for each case in the original data. We have four summary columns, the typical Estimate, Est.Error, Q2.5, and Q97.5. We also have third dimension composed of three levels, P(Y = 1), P(Y = 2), and P(Y = 3). Those index which of the three career categories each probability summary is for. Since the results are identical for each row, we’ll simplify the output by only keeping the first row. fitted(b10.16)[1, , ] %&gt;% round(digits = 2) ## P(Y = 1) P(Y = 2) P(Y = 3) ## Estimate 0.19 0.30 0.51 ## Est.Error 0.02 0.02 0.02 ## Q2.5 0.15 0.26 0.47 ## Q97.5 0.22 0.34 0.55 If we take the transpose of that, it will put the results in the order we’re more accustomed to. fitted(b10.16)[1, , ] %&gt;% t() %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## P(Y = 1) 0.19 0.02 0.15 0.22 ## P(Y = 2) 0.30 0.02 0.26 0.34 ## P(Y = 3) 0.51 0.02 0.47 0.55 Now compare those summaries with the empirically-derived percent and probability values we computed earlier. tibble(career) %&gt;% count(career) %&gt;% mutate(percent = (100 * n / sum(n)), probability = n / sum(n)) ## # A tibble: 3 x 4 ## career n percent probability ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 93 18.6 0.186 ## 2 2 152 30.4 0.304 ## 3 3 255 51 0.51 We did it! “Be aware that the estimates you get from these models are extraordinarily difficult to interpret. You absolutely must convert them to a vector of probabilities, to make much sense of them” (p. 325). Indeed. We spent a lot of time fussing about score values. But at no time did we really ever care about those. We wanted probabilities! And somewhat maddeningly, the parameters of our brm() model were in the metric of \\(K - 1\\) scores, not probabilities. Sigh. At least we can get an intuitive diagnostic summary with pp_check(). # this helps us set our custom color scheme color_scheme_set(wes_palette(&quot;Moonrise2&quot;)[c(1, 3, 2, 2, 2, 3)]) pp_check(b10.16, type = &quot;hist&quot;, binwidth = 1) + theme(legend.position = c(.91, .125), legend.key = element_rect(color = &quot;transparent&quot;)) Our posterior predictive check indicates the model produces synthetic data that resemble the original data. Before we move on, I have a confession to make. If you look closely at the code McElreath used fo fit his m10.16, you’ll see it yields a single b parameter. But our b10.16 model has two parameters. What gives? Whereas we estimated the score values for career == 2 and career == 3 separately, McElreath used an interesting nonlinear syntax to get them with one. I have not been able to reproduce his exact method with brms. Based on a comment from Bürkner, it appears McElreath’s model requires the custom_family() feature from brms. I’m not currenlty nimple enough with these models to impliment that. If you’ve got the chops, please share your code. Let’s move forward. Here is the second data simulation, this time based on McElreath’s R code 10.58. library(rethinking) n &lt;- 100 set.seed(10) # simulate family incomes for each individual family_income &lt;- runif(n) # assign a unique coefficient for each type of event b &lt;- (1:-1) career &lt;- rep(NA, n) # empty vector of choices for each individual for (i in 1:n) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } We might examine what the family_income distributions look like across the three levels of career. We’ll do it in two plots and combine them with the patchwork syntax. The first will be overlapping densities. For the second, we’ll display the proportions of career across a discretized version of family_income in a stacked area plot. p1 &lt;- tibble(career = as.factor(career), family_income) %&gt;% ggplot(aes(x = family_income, fill = career)) + geom_density(size = 0, alpha = 3/4) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + theme(legend.position = &quot;none&quot;) p2 &lt;- tibble(career = as.factor(career), family_income) %&gt;% mutate(fi = santoku::chop_width(family_income, width = .2, start = 0, labels = 1:5)) %&gt;% count(fi, career) %&gt;% group_by(fi) %&gt;% mutate(proportion = n / sum(n)) %&gt;% mutate(f = as.double(fi)) %&gt;% ggplot(aes(x = (f - 1) / 4, y = proportion, fill = career)) + geom_area() + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + xlab(&quot;family_income, descritized&quot;) library(patchwork) p1 + p2 If we were working with a larger \\(N\\), we could have gotten away with discretizing family_income into narrower bins. This is about as good as it gets with only 100 cases. It’s time to switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Here’s the brms version of McElreath’s m10.17. b10.17 &lt;- brm(data = list(career = career, # note how we used a list instead of a tibble family_income = family_income), family = categorical(link = logit), career ~ 1 + family_income, prior = c(prior(normal(0, 5), class = Intercept, dpar = mu2), prior(normal(0, 5), class = Intercept, dpar = mu3), prior(normal(0, 5), class = b, dpar = mu2), prior(normal(0, 5), class = b, dpar = mu3)), iter = 2500, warmup = 500, cores = 2, chains = 2, seed = 10, file = &quot;fits/b10.17&quot;) Happily, these results cohere with the rethinking model. print(b10.17) ## Family: categorical ## Links: mu2 = logit; mu3 = logit ## Formula: career ~ 1 + family_income ## Data: list(career = career, family_income = family_incom (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu2_Intercept 1.03 0.55 0.02 2.11 1.00 2774 2761 ## mu3_Intercept 1.04 0.54 0.02 2.13 1.00 2778 2602 ## mu2_family_income -1.77 1.00 -3.72 0.16 1.00 2942 2981 ## mu3_family_income -1.72 0.99 -3.76 0.13 1.00 2886 2563 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “Again, computing implied predictions is the safest way to interpret these models. They do a great job of classifying discrete, unordered events. But the parameters are on a scale that is very hard to interpret” (p. 325). Like before, we’ll do that with fitted(). Now we have a predictor, this time we will use the newdata argument. nd &lt;- tibble(family_income = seq(from = 0, to = 1, length.out = 60)) f &lt;- fitted(b10.17, newdata = nd) First we’ll plot the fitted probabilities for each career level across the full range of family_income values. # wrangle rbind(f[, , 1], f[, , 2], f[, , 3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% expand(career = 1:3, family_income)) %&gt;% mutate(career = str_c(&quot;career: &quot;, career)) %&gt;% # plot ggplot(aes(x = family_income, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_ribbon(aes(fill = career), alpha = 2/3) + geom_line(aes(color = career), size = 3/4) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + scale_x_continuous(breaks = 0:2 / 2) + scale_y_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = 0:3 / 3, labels = c(&quot;0&quot;, &quot;.33&quot;, &quot;.67&quot;, &quot;1&quot;)) + theme(axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) + facet_wrap(~career) If we’re willing to summarize those fitted lines by their posterior means, we could also make a model-implied version of the stacked area plot from above. # annotation text &lt;- tibble(family_income = c(.25, .5, .75), proportion = c(.2, .5, .8), label = str_c(&quot;career: &quot;, 3:1), color = c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;)) # wrangle rbind(f[, , 1], f[, , 2], f[, , 3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% expand(career = 1:3, family_income)) %&gt;% group_by(family_income) %&gt;% mutate(proportion = Estimate / sum(Estimate), career = factor(career)) %&gt;% # plot! ggplot(aes(x = family_income, y = proportion)) + geom_area(aes(fill = career)) + geom_text(data = text, aes(label = label, color = color), family = &quot;Times&quot;, size = 4.25) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[4:3]) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + theme(legend.position = &quot;none&quot;) For more practice fitting multinomial models with brms, check out my translation of Kruschke’s text, Chapter 22. 10.3.1.2 Multinomial in disguise as Poisson. Here we fit a multinomial likelihood by refactoring it to a series of Poissons. Let’s retrieve the Berkeley data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit rm(UCBadmit) detach(package:rethinking, unload = T) library(brms) Fit the models. # binomial model of overall admission probability b10.binom &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 10, file = &quot;fits/b10.binom&quot;) # Poisson model of overall admission rate and rejection rate b10.pois &lt;- brm(data = d %&gt;% mutate(rej = reject), # &#39;reject&#39; is a reserved word family = poisson, mvbind(admit, rej) ~ 1, prior(normal(0, 100), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 10, file = &quot;fits/b10.pois&quot;) Note, the mvbind() syntax made b10.pois a multivariate Poisson model. Starting with version 2.0.0, brms supports a variety of multivariate models. Anyway, here are the implications of b10.pois. # extract the samples post &lt;- posterior_samples(b10.pois) # wrangle post %&gt;% transmute(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% gather() %&gt;% # plot ggplot(aes(x = value, y = key, fill = key)) + geom_halfeyeh(point_interval = median_qi, .width = .95, color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = c(wes_palette(&quot;Moonrise2&quot;)[1], wes_palette(&quot;Moonrise2&quot;)[2])) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) We might compare the model summaries. summary(b10.binom)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.457936 0.02989252 -0.5167367 -0.3995148 1.002067 1084 1505 summary(b10.pois)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## admit_Intercept 4.984531 0.02447814 4.937252 5.030866 1.0002728 2603 1685 ## rej_Intercept 5.441847 0.01850067 5.405289 5.479288 0.9997181 2526 1769 Here’s the posterior mean for the probability of admission, based on b10.binom. fixef(b10.binom)[ ,&quot;Estimate&quot;] %&gt;% inv_logit_scaled() ## [1] 0.3874756 Happily, we get the same value within simulation error from model b10.pois. k &lt;- fixef(b10.pois) %&gt;% as.numeric() exp(k[1]) / (exp(k[1]) + exp(k[2])) ## [1] 0.3876228 The formula for what we just did in code is \\[p_\\text{admit} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{\\exp (\\alpha_1)}{\\exp (\\alpha_1) + \\exp (\\alpha_2)}.\\] To get a better appreciation on how well the two model types converge on the same solution, we might plot the full poster for admissions probability from each. # wrangle bind_cols( posterior_samples(b10.pois) %&gt;% mutate(`the Poisson` = exp(b_admit_Intercept) / (exp(b_admit_Intercept) + exp(b_rej_Intercept))), posterior_samples(b10.binom) %&gt;% mutate(`the binomial` = inv_logit_scaled(b_Intercept)) ) %&gt;% pivot_longer(starts_with(&quot;the&quot;)) %&gt;% # plot ggplot(aes(x = value, y = name, fill = name)) + geom_halfeyeh(point_interval = median_qi, .width = c(.95, .5), color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = c(wes_palette(&quot;Moonrise2&quot;)[2:1])) + labs(title = &quot;Two models, same marginal posterior&quot;, x = &quot;admissions probability&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2.25)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) 10.3.2 Geometric. Sometimes a count variable is a number of events up until something happened. Call this “something” the terminating event. Often we want to model the probability of that event, a kind of analysis known as event history analysis or survival analysis. When the probability of the terminating event is constant through time (or distance), and the units of time (or distance) are discrete, a common likelihood function is the geometric distribution. This distribution has the form: \\[\\text{Pr} (y | p) = p (1 - p) ^{y - 1}\\] where \\(y\\) is the number of time steps (events) until the terminating event occurred and \\(p\\) is the probability of that event in each time step. This distribution has maximum entropy for unbounded counts with constant expected value. (pp. 327–328) Here we simulate exemplar data. # simulate n &lt;- 100 set.seed(10) x &lt;- runif(n) set.seed(10) y &lt;- rgeom(n, prob = inv_logit_scaled(-1 + 2 * x)) In case you’re curious, here are the data. list(x = x, y = y) %&gt;% as_tibble() %&gt;% ggplot(aes(x = x, y = y)) + geom_point(size = 3/5, alpha = 2/3) We fit the geometric model using family = geometric(link = log). b10.18 &lt;- brm(data = list(y = y, x = x), family = geometric(link = log), y ~ 0 + Intercept + x, prior = c(prior(normal(0, 10), class = b, coef = Intercept), prior(normal(0, 1), class = b, coef = x)), iter = 2500, warmup = 500, chains = 2, cores = 2, seed = 10, file = &quot;fits/b10.18&quot;) Inspect the results. print(b10.18, digits = 2) ## Family: geometric ## Links: mu = log ## Formula: y ~ 0 + Intercept + x ## Data: list(y = y, x = x) (Number of observations: 100) ## Samples: 2 chains, each with iter = 2500; warmup = 500; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.73 0.24 0.27 1.20 1.00 1093 1447 ## x -1.61 0.51 -2.61 -0.65 1.00 1080 1376 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It turns out brms uses a different parameterization for the geometric distribution than rethinking does. It follows the form \\[f(y_i) = {y_i \\choose y_i} \\bigg (\\frac{\\mu_i}{\\mu_i + 1} \\bigg )^{y_i} \\bigg (\\frac{1}{\\mu_i + 1} \\bigg ).\\] Even though the parameters brms yielded look different from those in the text, their predictions describe the data well. Here’s the conditional_effects() plot. conditional_effects(b10.18) %&gt;% plot(points = T, point_args = c(size = 3/5, alpha = 2/3), line_args = c(color = wes_palette(&quot;Moonrise2&quot;)[1], fill = wes_palette(&quot;Moonrise2&quot;)[1])) 10.4 Summary This chapter described some of the most common generalized linear models, those used to model counts. It is important to never convert counts to proportions before analysis, because doing so destroys information about sample size. A fundamental difficulty with these models is that the parameters are on a different scale, typically log-odds (for binomial) or log-rate (for Poisson), than the outcome variable they describe. Therefore computing implied predictions is even more important than before. (pp. 328–329) Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.0.0 tidybayes_2.0.1.9000 GGally_1.4.0 ggExtra_0.9 loo_2.2.0 ## [6] broom_0.5.3 bayesplot_1.7.1 ggthemes_4.2.0 wesanderson_0.3.6 forcats_0.4.0 ## [11] stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [16] tibble_2.1.3 tidyverse_1.3.0 brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ## [21] ggplot2_3.2.1 StanHeaders_2.19.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 base64enc_0.1-3 fs_1.3.1 rstudioapi_0.10 ## [9] listenv_0.8.0 farver_2.0.3 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [17] codetools_0.2-16 bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [21] jsonlite_1.6.1 dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [41] santoku_0.3.0 cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [45] crosstalk_1.0.0 xfun_0.12 globals_0.12.5 ps_1.3.0 ## [49] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 lifecycle_0.1.0 ## [53] gtools_3.8.1 future_1.16.0 MASS_7.3-51.4 zoo_1.8-7 ## [57] scales_1.1.0 colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [61] Brobdingnag_1.2-6 inline_0.3.15 RColorBrewer_1.1-2 shinystan_2.5.0 ## [65] yaml_2.2.1 gridExtra_2.3 reshape_0.8.8 stringi_1.4.5 ## [69] dygraphs_1.1.1.6 checkmate_1.9.4 pkgbuild_1.0.6 rlang_0.4.4 ## [73] pkgconfig_2.0.3 matrixStats_0.55.0 HDInterval_0.2.0 evaluate_0.14 ## [77] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 ## [81] processx_3.4.1 tidyselect_1.0.0 plyr_1.8.5 magrittr_1.5 ## [85] R6_2.4.1 generics_0.0.2 DBI_1.1.0 pillar_1.4.3 ## [89] haven_2.2.0 withr_2.1.2 xts_0.12-0 abind_1.4-5 ## [93] modelr_0.1.5 crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [97] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 callr_3.4.1 ## [101] threejs_0.3.3 reprex_0.3.0 digest_0.6.23 xtable_1.8-4 ## [105] httpuv_1.5.2 stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 "],
["monsters-and-mixtures.html", "11 Monsters and Mixtures 11.1 Ordered categorical outcomes 11.2 Zero-inflated outcomes 11.3 Over-dispersed outcomes Reference Session info", " 11 Monsters and Mixtures [Of these majestic creatures], we’ll consider two common and useful examples. The first type is the ordered categorical model, useful for categorical outcomes with a fixed ordering. This model is built by merging a categorical likelihood function with a special kind of link function, usually a cumulative link. The second type is a family of zero-inflated and zero-augmented models, each of which mixes a binary event within an ordinary GLM likelihood like a Poisson or binomial. Both types of models help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models. (p. 331, emphasis in the original) 11.1 Ordered categorical outcomes It is very common in the social sciences, and occasional in the natural sciences, to have an outcome variable that is discrete, like a count, but in which the values merely indicate different ordered levels along some dimension. For example, if I were to ask you how much you like to eat fish, on a scale from 1 to 7, you might say 5. If I were to ask 100 people the same question, I’d end up with 100 values between 1 and 7. In modeling each outcome value, I’d have to keep in mind that these values are ordered because 7 is greater than 6, which is greater than 5, and so on. But unlike a count, the differences in values are not necessarily equal. In principle, an ordered categorical variable is just a multinomial prediction problem (page 323). But the constraint that the categories be ordered demands special treatment… The conventional solution is to use a cumulative link function. The cumulative probability of a value is the probability of that value or any smaller value. (pp. 331–332, emphasis in the original) 11.1.1 Example: Moral intuition. Let’s get the Trolley data from rethinking (see Cushman, Young &amp; Hauser, 2006). library(rethinking) data(Trolley) d &lt;- Trolley Unload rethinking and load brms. rm(Trolley) detach(package:rethinking, unload = T) library(brms) Use the dplyr::glimpse() to get a sense of the dimensions of the data. library(tidyverse) glimpse(d) ## Observations: 9,930 ## Variables: 12 ## $ case &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbox, fkbur, fkcar, fkspe, fkswi… ## $ response &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 4, 3, 4, 4,… ## $ order &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, 30, 5, 1, 13, 20, 17, 28, 10,… ## $ id &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 9… ## $ age &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1… ## $ male &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ edu &lt;fct&gt; Middle School, Middle School, Middle School, Middle School, Middle School, Middle School,… ## $ action &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,… ## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,… ## $ contact &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ story &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, swi, boa, car, che, sha, swi,… ## $ action2 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,… Though we have 9,930 rows, we only have 331 unique individuals. d %&gt;% distinct(id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 331 11.1.2 Describing an ordered distribution with intercepts. Before we get to plotting, in this chapter we’ll use theme settings and a color palette from the ggthemes package. library(ggthemes) We’ll take our basic theme settings from the theme_hc() function. We’ll use the Green fields color palette, which we can inspect with the canva_pal() function and a little help from scales::show_col(). scales::show_col(canva_pal(&quot;Green fields&quot;)(4)) canva_pal(&quot;Green fields&quot;)(4) ## [1] &quot;#919636&quot; &quot;#524a3a&quot; &quot;#fffae1&quot; &quot;#5a5f37&quot; canva_pal(&quot;Green fields&quot;)(4)[3] ## [1] &quot;#fffae1&quot; Now we’re ready to make our version of the simple Figure 11.1 histogram of our primary variable, response. p1 &lt;- ggplot(data = d, aes(x = response, fill = ..x..)) + geom_histogram(binwidth = 1/4, size = 0) + scale_x_continuous(breaks = 1:7) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) p1 Our cumulative proportion plot, Figure 11.1.b, will require some pre-plot wrangling. p2 &lt;- d %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d), cum_pr_k = cumsum(pr_k)) %&gt;% ggplot(aes(x = response, y = cum_pr_k, fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, color = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1)) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(ylim = c(0, 1)) + theme_hc() + theme(axis.ticks = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) p2 In order to make the next plot, we’ll need McElreath’s logit() function. Here it is, the logarithm of cumulative odds plot, Figure 11.1.c. # McElreath&#39;s convenience function from page 335 logit &lt;- function(x) log(x / (1 - x)) p3 &lt;- d %&gt;% count(response) %&gt;% mutate(cum_pr_k = cumsum(n / nrow(d))) %&gt;% filter(response &lt; 7) %&gt;% # we can do the `logit()` conversion right in ggplot2 ggplot(aes(x = response, y = logit(cum_pr_k), fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_x_continuous(breaks = 1:7) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + coord_cartesian(xlim = c(1, 7)) + ylab(&quot;log-cumulative-odds&quot;) + theme_hc() + theme(axis.ticks = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) p3 Why not combine the three subplots with patchwork? library(patchwork) p1 | p2 | p3 The code for Figure 11.2 is itself something of a monster. d_plot &lt;- d %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d), cum_pr_k = cumsum(n / nrow(d))) ggplot(data = d_plot, aes(x = response, y = cum_pr_k, color = cum_pr_k, fill = cum_pr_k)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + geom_linerange(aes(ymin = 0, ymax = cum_pr_k), alpha = 1/2, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + # there must be more elegant ways to do this part geom_linerange(data = . %&gt;% mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k)), aes(x = response + .025, ymin = ifelse(response == 1, 0, discrete_probability), ymax = cum_pr_k), color = &quot;black&quot;) + geom_text(data = tibble( text = 1:7, response = seq(from = 1.25, to = 7.25, by = 1), cum_pr_k = d_plot$cum_pr_k - .065 ), aes(label = text), size = 4) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + theme_hc() + theme(axis.ticks = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;) McElreath’s convention for this first type of statistical model is \\[\\begin{align*} R_i &amp; \\sim \\text{Ordered} (\\mathbf p) \\\\ \\text{logit} (p_k) &amp; = \\alpha_k \\\\ \\alpha_k &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] The Ordered distribution is really just a categorical distribution that takes a vector \\(\\mathbf p = {p_1, p_2, p_3, p_4, p_5, p_6}\\) of probabilities of each response value below the maximum response (7 in this example). Each response value \\(k\\) in this vector is defined by its link to an intercept parameter, \\(\\alpha_k\\). Finally, some weakly regularizing priors are placed on these intercepts. (p. 335) Whereas in rethinking::map() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(phi , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s how to fit the intercepts-only model. # define the start values inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits) b11.1 &lt;- brm(data = d, family = cumulative, response ~ 1, prior(normal(0, 10), class = Intercept), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = inits_list, # here we add our start values seed = 11, file = &quot;fits/b11.01&quot;) McElreath needed to include the depth=2 argument in the rethinking::precis() function to show the threshold parameters from his m11.1stan model (R code 11.8). With a brm() fit, we just use print() or summary() as usual. print(b11.1) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.92 0.03 -1.98 -1.86 1.00 1412 1559 ## Intercept[2] -1.27 0.02 -1.32 -1.22 1.00 2077 1577 ## Intercept[3] -0.72 0.02 -0.76 -0.68 1.01 2327 1790 ## Intercept[4] 0.25 0.02 0.21 0.29 1.01 2480 1849 ## Intercept[5] 0.89 0.02 0.85 0.93 1.00 2317 1619 ## Intercept[6] 1.77 0.03 1.72 1.83 1.00 2268 1700 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). What McElreath’s m11.1stan summary termed cutpoints[k], our brms summary termed Intercept[k]. In both cases, these are the \\(\\alpha_k\\) parameters from the equations, above (i.e., the thresholds). The summaries look like those in the text, the \\(\\hat R\\) values are great, and both measures of effective sample size are reasonably high. The model looks good. Recall we use the brms::inv_logit_scaled() function in place of McElreath’s logistic() function to get these into the probability metric. b11.1 %&gt;% fixef() %&gt;% inv_logit_scaled() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept[1] 0.1282391 0.5074704 0.1218269 0.1347796 ## Intercept[2] 0.2197094 0.5060889 0.2112961 0.2275291 ## Intercept[3] 0.3276483 0.5053775 0.3182959 0.3365989 ## Intercept[4] 0.5616142 0.5051448 0.5521704 0.5714020 ## Intercept[5] 0.7089929 0.5055609 0.7002765 0.7177224 ## Intercept[6] 0.8544314 0.5070249 0.8477435 0.8612121 But recall that the posterior \\(SD\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the posterior_samples(). posterior_samples(b11.1) %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) ## # A tibble: 6 x 5 ## key mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] 0.128 0.00334 0.122 0.135 ## 2 b_Intercept[2] 0.220 0.00417 0.211 0.228 ## 3 b_Intercept[3] 0.328 0.00474 0.318 0.337 ## 4 b_Intercept[4] 0.562 0.00507 0.552 0.571 ## 5 b_Intercept[5] 0.709 0.00459 0.700 0.718 ## 6 b_Intercept[6] 0.854 0.00349 0.848 0.861 And just to confirm, those posterior means are centered right around the cum_pr_k we computed for Figure 11.2. d_plot %&gt;% select(response, cum_pr_k) ## # A tibble: 7 x 2 ## response cum_pr_k ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.128 ## 2 2 0.220 ## 3 3 0.328 ## 4 4 0.562 ## 5 5 0.709 ## 6 6 0.854 ## 7 7 1 11.1.3 Adding predictor variables. Now we define the linear model as \\(\\phi_i = \\beta x_i\\). Accordingly, the formula for our cumulative logit model becomes \\[\\begin{align*} \\text{log} \\frac{\\text{Pr} (y_i \\leq k)}{1 - \\text{Pr} (y_i \\leq k)} &amp; = \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta x_i. \\end{align*}\\] I’m not aware that brms has an equivalent to the rethinking::dordlogit() function. So here we’ll make it by hand. The code comes from McElreath’s GitHub page. logistic &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) p &lt;- ifelse(x == Inf, 1, p) p } # now we get down to it dordlogit &lt;- function(x, phi, a, log = FALSE) { a &lt;- c(as.numeric(a), Inf) p &lt;- logistic(a[x] - phi) na &lt;- c(-Inf, a) np &lt;- logistic(na[x] - phi) p &lt;- p - np if (log == TRUE) p &lt;- log(p) p } The dordlogit() function works like this: (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1])) ## [1] 0.12823907 0.09147031 0.10793896 0.23396581 0.14737872 0.14543856 0.14556857 Note the slight difference in how we used dordlogit() with a brm() fit summarized by fixef() than the way McElreath did with a map2stan() fit summarized by coef(). McElreath just put coef(m11.1) into dordlogit(). We, however, more specifically placed fixef(b11.1)[, 1] into the function. With the [, 1] part, we specified that we were working with the posterior means (i.e., Estimate) and neglecting the other summaries (i.e., the posterior SDs and 95% intervals). If you forget to subset, chaos ensues. Next, as McElreath further noted on page 338, “these probabilities imply an average outcome of:” sum(pk * (1:7)) ## [1] 4.199365 I found that a bit abstract. Here’s the thing in a more elaborate tibble format. ( explicit_example &lt;- tibble(probability_of_a_response = pk) %&gt;% mutate(the_response = 1:7) %&gt;% mutate(their_product = probability_of_a_response * the_response) ) ## # A tibble: 7 x 3 ## probability_of_a_response the_response their_product ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.128 1 0.128 ## 2 0.0915 2 0.183 ## 3 0.108 3 0.324 ## 4 0.234 4 0.936 ## 5 0.147 5 0.737 ## 6 0.145 6 0.873 ## 7 0.146 7 1.02 explicit_example %&gt;% summarise(average_outcome_value = sum(their_product)) ## # A tibble: 1 x 1 ## average_outcome_value ## &lt;dbl&gt; ## 1 4.20 Aside This made me wonder how this would compare if we were lazy and ignored the categorical nature of the response. Here we refit the model with the typical Gaussian likelihood. b11.1b &lt;- brm(data = d, family = gaussian, response ~ 1, # in this case, 4 (i.e., the middle response) seems to be the conservative place to put the mean prior = c(prior(normal(4, 10), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.01b&quot;) Check the summary. print(b11.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.20 0.02 4.16 4.24 1.00 2633 2369 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.91 0.01 1.88 1.93 1.00 3077 2479 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This yielded a posterior mean of 4.2, much like our average_outcome_value, above. However, the lazy Gaussian model now has a \\(\\sigma\\) parameter, whereas \\(\\sigma\\) is a function of the probability estimate in the more-appropriate b11.2 model (see section 11.3). If you only care about mean estimates, this won’t matter much if you have a lot of data and the mean is far from the boundary. But when your posterior means get close to the upper or lower boundary, the lazy Gaussian model will yield silly estimates for the 95% intervals. Beware the lazy Gaussian model with data like this. End aside Now we’ll try it by subtracting .5 from each. # the probabilities of a given response (pk &lt;- dordlogit(1:7, 0, fixef(b11.1)[, 1] - .5)) ## [1] 0.08191416 0.06395669 0.08226970 0.20912076 0.15914120 0.18430412 0.21929337 # the average rating sum(pk * (1:7)) ## [1] 4.729704 So the rule is we subtract the linear model from each interecept. “This way, a positive \\(\\beta\\) value indicates that an increase in the predictor variable \\(x\\) results in an increase in the average response” (p. 338). Let’s fit our multivariable models. # start values for b11.2 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0) b11.2 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + intention + contact, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = list(inits, inits), seed = 11, file = &quot;fits/b11.02&quot;) # start values for b11.3 inits &lt;- list(`Intercept[1]` = -1.9, `Intercept[2]` = -1.2, `Intercept[3]` = -0.7, `Intercept[4]` = 0.2, `Intercept[5]` = 0.9, `Intercept[6]` = 1.8, action = 0, intention = 0, contact = 0, `action:intention` = 0, `contact:intention` = 0) b11.3 &lt;- update(b11.2, formula = response ~ 1 + action + intention + contact + action:intention + contact:intention, iter = 2000, warmup = 1000, cores = 2, chains = 2, inits = list(inits, inits), seed = 11, file = &quot;fits/b11.03&quot;) We don’t have a coeftab() function in brms like for rethinking. But as we did for Chapter 6, we can reproduce it with help from the broom package and a bit of data wrangling. library(broom) tibble(model = str_c(&quot;b11.&quot;, 1:3)) %&gt;% mutate(fit = purrr::map(model, get)) %&gt;% mutate(tidy = purrr::map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% select(model, term, estimate) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% complete(term, model) %&gt;% mutate(estimate = round(estimate, digits = 2)) %&gt;% spread(key = model, value = estimate) %&gt;% # this last step isn&#39;t necessary, but it orders the rows to match the text slice(c(6:11, 1, 4, 3, 2, 5)) ## # A tibble: 11 x 4 ## term b11.1 b11.2 b11.3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] -1.92 -2.84 -2.64 ## 2 b_Intercept[2] -1.27 -2.16 -1.94 ## 3 b_Intercept[3] -0.72 -1.57 -1.35 ## 4 b_Intercept[4] 0.25 -0.55 -0.31 ## 5 b_Intercept[5] 0.89 0.12 0.36 ## 6 b_Intercept[6] 1.77 1.02 1.27 ## 7 b_action NA -0.71 -0.47 ## 8 b_intention NA -0.72 -0.28 ## 9 b_contact NA -0.96 -0.34 ## 10 b_action:intention NA NA -0.44 ## 11 b_intention:contact NA NA -1.27 If you really wanted that last nobs row at the bottom, you could elaborate on this code: b11.1$data %&gt;% count(). Also, if you want a proper coeftab() function for brms, McElreath’s code lives here. Give it a whirl. Here we compute the WAIC. b11.1 &lt;- add_criterion(b11.1, &quot;waic&quot;) b11.2 &lt;- add_criterion(b11.2, &quot;waic&quot;) b11.3 &lt;- add_criterion(b11.3, &quot;waic&quot;) Now compare the models. loo_compare(b11.1, b11.2, b11.3, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b11.3 0.0 0.0 -18464.7 40.6 11.1 0.1 36929.4 81.2 ## b11.2 -80.2 12.8 -18544.9 38.1 9.0 0.0 37089.8 76.3 ## b11.1 -462.5 31.3 -18927.1 28.8 5.9 0.0 37854.3 57.7 Here are the WAIC weights. model_weights(b11.1, b11.2, b11.3, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b11.1 b11.2 b11.3 ## 0 0 1 McElreath made Figure 11.3 by extracting the samples of his m11.3, saving them as post, and working some hairy base R plot() code. We’ll take a different route and use brms::fitted(). This will take substantial data wrangling, but hopefully it’ll be instructive. Let’s first take a look at the initial fitted() output for the beginnings of Figure 11.3.a. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% glimpse() ## Observations: 100 ## Variables: 14 ## $ `1.1` &lt;dbl&gt; 0.07162247, 0.06338779, 0.07068255, 0.07083774, 0.07092480, 0.06631221, 0.06689340, 0.0678351… ## $ `2.1` &lt;dbl&gt; 0.08488440, 0.07925890, 0.09404759, 0.09429185, 0.09002287, 0.08973713, 0.08273952, 0.0932086… ## $ `1.2` &lt;dbl&gt; 0.06341534, 0.05809328, 0.05970624, 0.06010018, 0.05854282, 0.05727328, 0.05999789, 0.0566324… ## $ `2.2` &lt;dbl&gt; 0.07315865, 0.07031587, 0.07583498, 0.07634476, 0.07156504, 0.07395810, 0.07185583, 0.0740154… ## $ `1.3` &lt;dbl&gt; 0.08606638, 0.08590155, 0.07723122, 0.07938579, 0.07972300, 0.07696115, 0.08352846, 0.0766265… ## $ `2.3` &lt;dbl&gt; 0.09641533, 0.10012087, 0.09353790, 0.09606813, 0.09370160, 0.09457815, 0.09652008, 0.0950633… ## $ `1.4` &lt;dbl&gt; 0.2229344, 0.2161689, 0.2134367, 0.2148205, 0.2121816, 0.2172969, 0.2131465, 0.2147282, 0.211… ## $ `2.4` &lt;dbl&gt; 0.2354135, 0.2333929, 0.2347367, 0.2357648, 0.2302323, 0.2407984, 0.2292882, 0.2390650, 0.230… ## $ `1.5` &lt;dbl&gt; 0.1648621, 0.1698099, 0.1677542, 0.1665207, 0.1651815, 0.1641270, 0.1651834, 0.1606315, 0.169… ## $ `2.5` &lt;dbl&gt; 0.1619287, 0.1667702, 0.1633723, 0.1618094, 0.1621795, 0.1599161, 0.1626227, 0.1564664, 0.166… ## $ `1.6` &lt;dbl&gt; 0.1870092, 0.1931539, 0.1908156, 0.1913794, 0.1949742, 0.1852376, 0.2037272, 0.1902218, 0.195… ## $ `2.6` &lt;dbl&gt; 0.1724180, 0.1742718, 0.1669028, 0.1670487, 0.1748612, 0.1616290, 0.1847066, 0.1649165, 0.173… ## $ `1.7` &lt;dbl&gt; 0.2040901, 0.2134846, 0.2203735, 0.2169556, 0.2184722, 0.2327919, 0.2075232, 0.2333245, 0.214… ## $ `2.7` &lt;dbl&gt; 0.1757815, 0.1758695, 0.1715677, 0.1686724, 0.1774375, 0.1793831, 0.1722670, 0.1772646, 0.171… Hopefully by now it’s clear why we needed the nd tibble, which we made use of in the newdata = nd argument. Because we set summary = F, we get draws from the posterior instead of summaries. With max_iter, we controlled how many of those posterior draws we wanted. McElreath used 100, which he indicated at the top of page 341, so we followed suit. It took me a minute to wrap my head around the meaning of the 14 vectors, which were named by brms::fitted() default. Notice how each column is named by two numerals, separated by a period. That first numeral indicates which if the two intention values the draw is based on (i.e., 1 stands for intention == 0, 2, stands for intention == 1). The numbers on the right of the decimals are the seven response options for response. For each posterior draw, you get one of those for each value of intention. Finally, it might not be immediately apparent, but the values are in the probability scale, just like pk on page 338. Now we know what we have in hand, it’s just a matter of careful wrangling to get those probabilities into a more useful format to feed into ggplot2. I’ve extensively annotated the code, below. If you lose track of what happens in a given step, just run the code up till that point. Go step by step. nd &lt;- tibble(action = 0, contact = 0, intention = 0:1) max_iter &lt;- 100 fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% # we need an variable to index which posterior iteration we&#39;re working with mutate(iter = 1:max_iter) %&gt;% # convert the data to the long format gather(key, pk, -iter) %&gt;% # extract the `intention` and `response` information out of the `key` vector and # spread it into two vectors. separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% # that step produced two character vectors. they’ll be more useful as numbers mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% # here we convert `intention` into its proper 0:1 metric mutate(intention = intention -1) %&gt;% # this step is based on McElreath&#39;s R code 11.10 on page 338 mutate(`pk:rating` = pk * rating) %&gt;% # I’m not sure how to succinctly explain this. you’re just going to have to trust me group_by(iter, intention) %&gt;% # this is very important for the next step. arrange(iter, intention, rating) %&gt;% # here we take our `pk` values and make culmulative sums. why? take a long hard look at Figure 11.2. mutate(probability = cumsum(pk)) %&gt;% # `rating == 7` is unnecessary. these `probability` values are by definition 1 filter(rating &lt; 7) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + # note how we made a new data object for `geom_text()` geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;), legend.position = &quot;none&quot;, axis.ticks = element_blank()) Boom! Okay, that pile of code is a bit of a mess and you’re not going to want to repeatedly cut and paste all that. Let’s condense it into a homemade function, make_Figure_11.3_data(). make_Figure_11.3_data &lt;- function(action, contact, max_iter) { nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% mutate(iter = 1:max_iter) %&gt;% gather(key, pk, -iter) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% arrange(iter, intention, rating) %&gt;% mutate(probability = cumsum(pk)) %&gt;% filter(rating &lt; 7) } Now we’ll use our sweet homemade function to make our plots. # Figure 11.3.a p1 &lt;- make_Figure_11.3_data(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.05, .12, .20, .35, .53, .71, .87)), aes(label = text), size = 3) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + labs(subtitle = &quot;action = 0,\\ncontact = 0&quot;) + theme_hc() + theme(axis.ticks = element_blank(), legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) # Figure 11.3.b p2 &lt;- make_Figure_11.3_data(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.12, .24, .35, .50, .68, .80, .92)), aes(label = text), size = 3) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + labs(subtitle = &quot;action = 1,\\ncontact = 0&quot;) + theme_hc() + theme(axis.ticks = element_blank(), legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) # Figure 11.3.c p3 &lt;- make_Figure_11.3_data(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = probability, color = probability)) + geom_line(aes(group = interaction(iter, rating)), alpha = 1/10) + geom_text(data = tibble(text = 1:7, intention = seq(from = .9, to = .1, length.out = 7), probability = c(.15, .34, .44, .56, .695, .8, .92)), aes(label = text), size = 3) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + labs(subtitle = &quot;action = 0,\\ncontact = 1&quot;) + theme_hc() + theme(axis.ticks = element_blank(), legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) # here we combine them with patchwork p1 + p2 + p3 + plot_annotation(title = &quot;Posterior predictions of the interaction model:&quot;, subtitle = &quot;Here we&#39;re focusing on the parameters.&quot;, theme = theme(plot.background = element_rect(fill = &quot;grey92&quot;))) If you’d like to learn more about using cumulative probabilities to model ordinal data in brms, check out Bürkner and Vuorre’s Ordinal Regression Models in Psychology: A Tutorial and its repository on the Open Science Framework. Also check out Chapter 23 of my sister project Doing Bayesian Data Analysis in brms and the tidyverse were we model ordinal data with a series of cumulative probit models. 11.1.4 Bonus: Figure 11.3 alternative. I have a lot of respect for McElreath. But man, Figure 11.3 is the worst. I’m in clinical psychology and there’s no way a working therapist is going to look at a figure like that and have any sense of what’s going on. Happily, we can go further. Look back at McElreath’s R code 11.10 on page 338. See how he multiplied the elements of pk by their respective response values and then just summed them up to get an average outcome value? With just a little amendment to our custom make_Figure_11.3_data() function, we can wrangle our fitted() output to express average response values for each of our conditions of interest. Here’s the adjusted function: make_alternative_data &lt;- function(action, contact, max_iter) { nd &lt;- tibble(action = action, contact = contact, intention = 0:1) max_iter &lt;- max_iter fitted(b11.3, newdata = nd, subset = 1:max_iter, summary = F) %&gt;% as_tibble() %&gt;% mutate(iter = 1:max_iter) %&gt;% gather(key, pk, -iter) %&gt;% separate(key, into = c(&quot;intention&quot;, &quot;rating&quot;)) %&gt;% mutate(intention = intention %&gt;% as.double(), rating = rating %&gt;% as.double()) %&gt;% mutate(intention = intention -1) %&gt;% mutate(`pk:rating` = pk * rating) %&gt;% group_by(iter, intention) %&gt;% # everything above this point is identical to the previous custom function. # all we do is replace the last few lines with this one line of code. summarise(mean_rating = sum(`pk:rating`)) } Our handy homemade make_alternative_data() function works very much like its predecessor. Before we put it to work, we might simplify our upcoming ggplot2 code. Did you notice how the last three plots, those three subplots for Figure 11.3, were all basically the same with slightly different input? Instead of copy/pasting all that plot code, we can wrap the bulk of it into a custom plotting function we’ll call geom_figure11.3(). geom_figure11.3 &lt;- function(subtitle, ...) { list( geom_line(alpha = 1/10, color = canva_pal(&quot;Green fields&quot;)(4)[1]), scale_x_continuous(&quot;intention&quot;, breaks = 0:1), scale_y_continuous(&quot;response&quot;, breaks = 1:7, limits = c(1, 7)), labs(subtitle = subtitle), theme_hc(), theme(axis.ticks = element_blank(), legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) ) } Now we’ll use our two custom functions, make_alternative_data() to make the data and geom_figure11.3() to specify most of the ggplot2 components, to plot our alternative to Figure 11.3. # alternative to Figure 11.3.a p1 &lt;- make_alternative_data(action = 0, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_figure11.3(subtitle = &quot;action = 0,\\ncontact = 0&quot;) # alternative to Figure 11.3.b p2 &lt;- make_alternative_data(action = 1, contact = 0, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_figure11.3(subtitle = &quot;action = 1,\\ncontact = 0&quot;) # alternative to Figure 11.3.c p3 &lt;- make_alternative_data(action = 0, contact = 1, max_iter = 100) %&gt;% ggplot(aes(x = intention, y = mean_rating, group = iter)) + geom_figure11.3(subtitle = &quot;action = 0,\\ncontact = 1&quot;) # here we combine them with patchwork p1 + p2 + p3 + plot_annotation(title = &quot;Posterior predictions of the interaction model:&quot;, subtitle = &quot;This time we&#39;ll focus on the means.&quot;, theme = theme(plot.background = element_rect(fill = &quot;grey92&quot;))) Finally; now those are plots I can sell in a clinical psychology journal! We did the right thing and used a sophisticated statistical model to treat the data appropriately, but we summarized the results in a way a substantive audience might understand them in. If your new to making functions with ggplot2 components, check out Chapter 16 of Wickham’s ggplot2: Elegant Graphics for Data Analysis. He’ll walk you through all the steps. 11.1.4.1 Rethinking: Staring into the abyss. “The plotting code for ordered logistic models is complicated, compared to that of models from previous chapters. But as models become more monstrous, so too does the code needed to compute predictions and display them” (p. 342). I’ll just add that I have found models and plots like this get easier with time. Just keep chipping away. You’ll get it! 11.2 Zero-inflated outcomes Very often, the things we can measure are not emissions from any pure process. Instead, they are mixtures of multiple processes. Whenever there are different causes for the same observation, then a mixture model may be useful. A mixture model uses more than one simple probability distribution to model a mixture of causes. In effect, these models use more than one likelihood for the same outcome variable. Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A “zero” means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started. (p. 342, emphasis in the original) 11.2.0.1 Rethinking: Breaking the law. McElreath discussed how advances in computing have made it possible for working scientists to define their own data generating models. If you’d like to dive deeper into the topic, check out Bürkner’s vignette, Define Custom Response Distributions with brms. We’ll even make use of it a little further down. 11.2.1 Example: Zero-inflated Poisson. Do you remember the monk data from back in Chapter 10? Here we simulate some more. This time we’ll work in a little alcohol. # define parameters prob_drink &lt;- 0.2 # 20% of days rate_work &lt;- 1 # average 1 manuscript per day # sample one year of production n &lt;- 365 # simulate days monks drink set.seed(11) drink &lt;- rbinom(n, 1, prob_drink) # simulate manuscripts completed y &lt;- (1 - drink) * rpois(n, rate_work) We’ll put those data in a tidy tibble before plotting. d &lt;- tibble(Y = y) %&gt;% arrange(Y) %&gt;% mutate(zeros = c(rep(&quot;zeros_drink&quot;, times = sum(drink)), rep(&quot;zeros_work&quot;, times = sum(y == 0 &amp; drink == 0)), rep(&quot;nope&quot;, times = n - sum(y == 0))) ) ggplot(data = d, aes(x = Y)) + geom_histogram(aes(fill = zeros), binwidth = 1, size = 1/10, color = &quot;grey92&quot;) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[1], canva_pal(&quot;Green fields&quot;)(4)[2], canva_pal(&quot;Green fields&quot;)(4)[1])) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) With these data, the likelihood of observing zero on y, (i.e., the likelihood zero manuscripts were completed on a given occasion) is \\[\\begin{align*} \\text{Pr} (0 | p, \\lambda) &amp; = \\text{Pr} (\\text{drink} | p) + \\text{Pr} (\\text{work} | p) \\times \\text{Pr} (0 | \\lambda) \\\\ &amp; = p + (1 - p) \\text{ exp} (- \\lambda). \\end{align*}\\] And since the Poisson likelihood of \\(y\\) is \\(\\text{Pr} (y | \\lambda) = \\lambda^y \\text{exp} (- \\lambda) / y!\\), the likelihood of \\(y = 0\\) is just \\(\\text{exp} (- \\lambda)\\). The above is just the mathematics for: The probability of observing a zero is the probability that the monks didn’t drink OR (\\(+\\)) the probability that the monks worked AND (\\(\\times\\)) failed to finish anything. And the likelihood of a non-zero value \\(y\\) is: \\[\\begin{align*} \\text{Pr} (y | p, \\lambda) &amp; = \\text{Pr} (\\text{drink} | p) (0) + \\text{Pr} (\\text{work} | p) \\text{Pr} (y | \\lambda) \\\\ &amp; = (1 - p) \\frac {\\lambda^y \\text{ exp} (- \\lambda)}{y!} \\end{align*}\\] Since drinking monks never produce \\(y &gt; 0\\), the expression above is just the chance the monks both work \\(1 - p\\), and finish \\(y\\) manuscripts. (p. 344, emphasis in the original) So letting \\(p\\) be the probability \\(y\\) is zero and \\(\\lambda\\) be the shape of the distribution, the zero-inflated Poisson (ZIPoisson) regression model takes the basic form \\[\\begin{align*} y_i &amp; \\sim \\text{ZIPoisson} (p_i, \\lambda_i)\\\\ \\text{logit} (p_i) &amp; = \\alpha_p + \\beta_p x_i \\\\ \\text{log} (\\lambda_i) &amp; = \\alpha_\\lambda + \\beta_\\lambda x_i. \\end{align*}\\] One last thing to note is that in brms, \\(p_i\\) is denoted zi. To fit a zero-inflated Poisson model with brms, make sure to specify the correct likelihood with family = zero_inflated_poisson. To use a non-default prior for zi, make sure to indicate class = zi within the prior() function. b11.4 &lt;- brm(data = d, family = zero_inflated_poisson, Y ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(beta(2, 2), class = zi)), # the brms default is beta(1, 1) cores = 4, seed = 11, file = &quot;fits/b11.04&quot;) print(b11.4) ## Family: zero_inflated_poisson ## Links: mu = log; zi = identity ## Formula: Y ~ 1 ## Data: d (Number of observations: 365) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.10 0.08 -0.07 0.25 1.00 1259 1712 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## zi 0.24 0.05 0.14 0.33 1.00 1551 1874 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The zero-inflated Poisson is parameterized in brms a little differently than it is in rethinking. The different parameterization did not influence the estimate for the Intercept, \\(\\lambda\\). In both here and in the text, \\(\\lambda\\) was about zero. However, it did influence the summary of zi. Note how McElreath’s logistic(-1.39) yielded 0.1994078. Seems rather close to our zi estimate of 0.236. First off, because he didn’t set his seed in the text before simulating, we couldn’t exactly reproduce his simulated drunk monk data. So our results will vary a little due to that alone. But after accounting for simulation variance, hopefully it’s clear that zi in brms is already in the probability metric. There’s no need to convert it. In the prior argument, we used beta(2, 2) for zi and also mentioned in the margin that the brms default is beta(1, 1). The beta distribution ranges from 0 to 1, making it a natural distribution to use for priors on probabilities. To give you a sense of what those two versions of the beta look like, let’s plot them. tibble(`zi prior`= seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(`beta(1, 1)` = dbeta(`zi prior`, 1, 1), `beta(2, 2)` = dbeta(`zi prior`, 2, 2)) %&gt;% gather(prior, density, -`zi prior`) %&gt;% ggplot(aes(x = `zi prior`, ymin = 0, ymax = density)) + geom_ribbon(aes(fill = prior)) + scale_fill_manual(values = c(canva_pal(&quot;Green fields&quot;)(4)[4], canva_pal(&quot;Green fields&quot;)(4)[2])) + scale_x_continuous(&quot;prior for zi&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme_hc() + theme(legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) + facet_wrap(~prior) Hopefully this clarifies that the brms default is flat, whereas our prior regularized a bit toward .5. Anyway, here’s that exponentiated \\(\\lambda\\). fixef(b11.4)[1, ] %&gt;% exp() ## Estimate Est.Error Q2.5 Q97.5 ## 1.1009233 1.0860754 0.9299058 1.2864349 11.2.1.1 Overthinking: Zero-inflated Poisson distribution function. Define the dzip() function. dzip &lt;- function(x, p, lambda, log = TRUE) { ll &lt;- ifelse( x == 0, p + (1 - p) * exp(-lambda), (1 - p) * dpois(x, lambda, log = FALSE) ) if (log == TRUE) ll &lt;- log(ll) return(ll) } We can use McElreath’s dzip() to do a posterior predictive check for our model. To work with our estimates for \\(p\\) and \\(\\lambda\\) directly, we’ll set log = F. p_b11.4 &lt;- posterior_summary(b11.4)[2, 1] lambda_b11.4 &lt;- posterior_summary(b11.4)[1, 1] %&gt;% exp() tibble(x = 0:4) %&gt;% mutate(density = dzip(x = x, p = p_b11.4, lambda = lambda_b11.4, log = F)) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + xlab(&quot;Manuscripts completed&quot;) + theme_hc() + theme(axis.ticks = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;)) If you look up to the histogram we made at the beginning of this section, you’ll see this isn’t a terrible approximation. We can do something similar with the brms::pp_check() function. By setting type = bars, we’ll get back a series of model-based simulations summarized by mean and error bars superimposed atop a histogram of the original data. With the nsamples argument, we indicated we wanted those mean and error bars to be based on 100 simulations. # this helps us set our custom color scheme bayesplot::color_scheme_set(canva_pal(&quot;Green fields&quot;)(4)[c(1, 1, 1, 1, 1, 4)]) set.seed(11) pp_check(b11.4, type = &quot;bars&quot;, nsamples = 100) + scale_x_continuous(breaks = 0:7) + theme_hc() + theme(axis.ticks = element_blank(), legend.position = c(.91, .842), legend.background = element_rect(fill = &quot;transparent&quot;), plot.background = element_rect(fill = &quot;grey92&quot;)) Those mean and error bars suggest the model did a good job simulating data that resemble the original data. 11.3 Over-dispersed outcomes All statistical models omit something. The question is only whether that something is necessary for making useful inferences. One symptom that something important has been omitted from a count model is over-dispersion. The variance of a variable is sometimes called its dispersion. For a counting process like a binomial, the variance is a function of the same parameters as the expected value. For example, the expected value of a binomial is \\(np\\) and its variance is \\(np (1 - p)\\). When the observed variance exceeds this amount–after conditioning on all the predictor variables–this implies that some omitted variable is producing additional dispersion in the observed counts. What could go wrong, if we ignore the over-dispersion? Ignoring it can lead to all of the same problems as ignoring any predictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or producing spurious inferences. (p, 346, emphasis in the original) In this chapter we’ll cope with the problem using continuous mixture models–first the beta-binomial and then the gamma-Poisson (a.k.a. negative binomial). 11.3.1 Beta-binomial. A beta-binomial model assumes that each binomial count observation has its own probability of success. The model estimates the distribution of probabilities of success across cases, instead of a single probability of success. And predictor variables change the shape of this distribution, instead of directly determining the probability of each success. (p, 347, emphasis in the original) Unfortunately, we need to digress. As it turns out, there are multiple ways to parameterize the beta distribution and we’ve run square into two. In the text, McElreath wrote the beta distribution has two parameters, an average probability \\(\\bar p\\) and a shape parameter \\(\\theta\\). In his R code 11.24, which we’ll reproduce in a bit, he demonstrated that parameterization with the rethinking::dbeta2() function. The nice thing about this parameterization is how intuitive the pbar parameter is. If you want a beta with an average of .2, you set pbar = .2. If you want the distribution to be more or less certain, make the theta argument more or less large, respectively. However, the beta density is often defined in terms of \\(\\alpha\\) and \\(\\beta\\). If you denote the data as \\(y\\), this follows the form \\[\\text{Beta} (y | \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{\\text B (\\alpha, \\beta)},\\] which you can verify in the Continuous Distributions on [0, 1] section of the Stan Functions Reference. In the formula, \\(\\text B\\) stands for the Beta function, which computes a normalizing constant, which you can learn about in the Mathematical Functions of the Stan reference manual. This is all important to be aware of because when we defined that beta prior for zi in the last model, it was using this parameterization. Also, if you look at the base R dbeta() function, you’ll learn it takes two parameters, shape1 and shape2. Those uncreatively-named parameters are the same \\(\\alpha\\) and \\(\\beta\\) from the density, above. They do not correspond to the pbar and theta parameters of McEreath’s rethinking::dbeta2() function. McElreath had good reason for using dbeta2(). Beta’s typical \\(\\alpha\\) and \\(\\beta\\) parameters aren’t the most intuitive to use; the parameters in McElreath’s dbeta2() are much nicer. But if you’re willing to dive deeper, it turns out you can find the mean of a beta distribution in terms of \\(\\alpha\\) and \\(\\beta\\) like this \\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\] We can talk about the spread of the distribution, sometimes called \\(\\kappa\\), in terms \\(\\alpha\\) and \\(\\beta\\) like this \\[\\kappa = \\alpha + \\beta\\] With \\(\\mu\\) and \\(\\kappa\\) in hand, we can even find the \\(SD\\) of a beta distribution with \\[\\sigma = \\sqrt{\\mu (1 - \\mu) / (\\kappa + 1)}\\] I explicate all this because McElreath’s pbar is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) and his theta is \\(\\kappa = \\alpha + \\beta\\). This is great news because it means that we can understand what McElreath did with his beta2() function in terms of base R’s dbeta() function. Which also means that we can understand the distribution of the beta parameters used in brms::brm(). To demonstrate, let’s walk through McElreath’s R code 11.25. pbar &lt;- 0.5 theta &lt;- 5 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01)), aes(x = x, ymin = 0, ymax = rethinking::dbeta2(x, pbar, theta))) + geom_ribbon(fill = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(The~beta~distribution), subtitle = expression(&quot;Defined in terms of &quot;*mu*&quot; (i.e., pbar) and &quot;*kappa*&quot; (i.e., theta)&quot;)) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) In his 2015 text, Doing Bayesian Data Analysis, Kruschke provided code for a convenience function that will take pbar and theta as inputs and return the corresponding \\(\\alpha\\) and \\(\\beta\\) values. Here’s the function: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now we can use Kruschke’s betaABfromMeanKappa() to find the \\(\\alpha\\) and \\(\\beta\\) values corresponding to pbar and theta. betaABfromMeanKappa(mean = pbar, kappa = theta) ## $a ## [1] 2.5 ## ## $b ## [1] 2.5 And finally, we can double check that all of this works. Here’s the same distribution but defined in terms of \\(\\alpha\\) and \\(\\beta\\). ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01)), aes(x = x, ymin = 0, ymax = dbeta(x, 2.5, 2.5))) + geom_ribbon(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(The~beta~distribution), subtitle = expression(&quot;This time defined in terms of &quot;*alpha*&quot; and &quot;*beta)) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) McElreath encouraged us to “explore different values for pbar and theta” (p. 348). Here’s a grid of plots with pbar = c(.25, .5, .75) and theta = c(5, 10, 15). # data crossing(pbar = c(.25, .5, .75), theta = c(5, 15, 30)) %&gt;% expand(nesting(pbar, theta), x = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(density = rethinking::dbeta2(x, pbar, theta), mu = str_c(&quot;mu == &quot;, pbar %&gt;% str_remove(., &quot;0&quot;)), kappa = factor(str_c(&quot;kappa == &quot;, theta), levels = c(&quot;kappa == 30&quot;, &quot;kappa == 15&quot;, &quot;kappa == 5&quot;))) %&gt;% # plot ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, labels = NULL) + theme_hc() + theme(axis.ticks.y = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;)) + facet_grid(kappa ~ mu, labeller = label_parsed) If you’d like to see how to make a similar plot in terms of \\(\\alpha\\) and \\(\\beta\\), see Chapter 6 of my project recoding Kruschke’s text into tidyverse and brms code. But remember, we’re not fitting a beta model. We’re using the beta-binomial. “We’re going to bind our linear model to \\(\\bar p\\), so that changes in predictor variables change the central tendency of the distribution” (p. 348). The statistical model we’ll be fitting follows the form \\[ \\begin{align*} \\text{admit}_i &amp; \\sim \\operatorname{BetaBinomial} (n_i, \\bar p_i, \\theta)\\\\ \\text{logit} (\\bar p_i) &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\operatorname{Normal} (0, 2) \\\\ \\theta &amp; \\sim \\operatorname{Exponential} (1). \\end{align*} \\] Here the size \\(n = \\text{applications}\\). In case you’re confused, yes, our statistical model is not the one McElreath presented at the top of page 348 in the text. If you look closely, the statistical formula he presented does not match up with the one implied by his R code 11.26. Our statistical formula and the brm() model we’ll be fitting, below, correspond to his R code 11.26. Before we fit, we have an additional complication. The beta-binomial likelihood is not implemented in brms at this time. However, brms versions 2.2.0 and above allow users to define custom distributions. You can find the handy Define Custom Response Distributions with brms vignette here. Happily, Bürkner even used the beta-binomial distribution as the exemplar in the vignette. Before we get carried away, let’s load the data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Unload rethinking and load brms. rm(UCBadmit) detach(package:rethinking, unload = T) library(brms) I’m not going to go into great detail explaining the ins and outs of making custom distributions for brm(). You’ve got Bürkner’s vignette for that. For our purposes, we need two preparatory steps. First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some functions for Stan which are not defined in Stan itself. We’ll save them as stan_funs. Third, we’ll make a stanvar() statement which will allow us to pass our stan_funs to brm(). beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 0), type = &quot;int&quot;, vars = &quot;vint1[n]&quot; ) stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int T) { return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int T) { return beta_binomial_rng(T, mu * phi, (1 - mu) * phi); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) With that out of the way, we’re almost ready to test this baby out. Before we do, we should clarify two points: First, what McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar p\\), Bürkner simply called \\(\\mu\\). Second, we’ve become accustomed to using the y | trials() ~ ... syntax when defining our formula arguments for binomial models. Here we are replacing trials() with vint(). From Bürkner’s Define Custom Response Distributions with brms vignette, we read: To provide information about the number of trials (an integer variable), we are going to use the addition argument vint(), which can only be used in custom families. Simiarily, if we needed to include additional vectors of real data, we would use vreal(). Actually, for this particular example, we could more elegantly apply the addition argument trials() instead of vint() as in the basic binomial model. However, since the present vignette is ment to give a general overview of the topic, we will go with the more general method. We now have all components together to fit our custom beta-binomial model: b11.5 &lt;- brm(data = d, family = beta_binomial2, # here&#39;s our custom likelihood admit | vint(applications) ~ 1, prior = c(prior(normal(0, 2), class = Intercept), prior(exponential(1), class = phi)), iter = 4000, warmup = 1000, cores = 2, chains = 2, stanvars = stanvars, # note our `stanvars` seed = 11, file = &quot;fits/b11.05&quot;) Success, our results look a lot like those in the text! print(b11.5) ## Family: beta_binomial2 ## Links: mu = logit; phi = identity ## Formula: admit | vint(applications) ~ 1 ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.38 0.30 -0.97 0.21 1.00 3840 3408 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## phi 2.79 0.94 1.31 4.99 1.00 4068 3517 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s what the corresponding posterior_samples() data object looks like. post &lt;- posterior_samples(b11.5) head(post) ## b_Intercept phi lp__ ## 1 -0.36233302 3.701785 -70.39182 ## 2 -0.64609608 1.772836 -71.37820 ## 3 -0.11839922 2.667552 -70.43501 ## 4 0.53363769 2.024830 -74.18534 ## 5 -0.02124876 2.860503 -70.85063 ## 6 -0.25357651 2.138199 -70.37938 Here’s our median and percentile-based 95% interval. post %&gt;% tidybayes::median_qi(inv_logit_scaled(b_Intercept)) %&gt;% mutate_if(is.double, round, digits = 3) ## inv_logit_scaled(b_Intercept) .lower .upper .width .point .interval ## 1 0.406 0.275 0.552 0.95 median qi To stay within the tidyverse while making the many thin lines in Figure 11.5.a, we’re going to need to do a bit of data processing. First, we’ll want a variable to index the rows in post (i.e., to index the posterior draws). And we’ll want to convert the b_Intercept to the \\(\\bar p\\) metric with the inv_logit_scaled() function. Then we’ll use sample_n() to randomly draw a subset of the posterior draws. Then with the expand() function, we’ll insert a tightly-spaced sequence of x values ranging between 0 and 1–the parameter space of beta distribution. Finally, we’ll use pmap_dbl() to compute the density values for the rethinking::dbeta2 distribution corresponding to the unique combination of x, p_bar, and phi values in each row. set.seed(11) lines &lt;- post %&gt;% mutate(iter = 1:n(), p_bar = inv_logit_scaled(b_Intercept)) %&gt;% sample_n(size = 100) %&gt;% expand(nesting(iter, p_bar, phi), x = seq(from = 0, to = 1, by = .005)) %&gt;% mutate(density = pmap_dbl(list(x, p_bar, phi), rethinking::dbeta2)) str(lines) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 20100 obs. of 5 variables: ## $ iter : int 6 6 6 6 6 6 6 6 6 6 ... ## $ p_bar : num 0.437 0.437 0.437 0.437 0.437 ... ## $ phi : num 2.14 2.14 2.14 2.14 2.14 ... ## $ x : num 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 ... ## $ density: num Inf 1.58 1.51 1.47 1.44 ... All that was just for the thin lines. To make the thicker line for the posterior mean, we’ll get tricky with stat_function(). lines %&gt;% ggplot(aes(x = x, y = density)) + stat_function(fun = rethinking::dbeta2, args = list(prob = mean(inv_logit_scaled(post[, 1])), theta = mean(post[, 2])), size = 1.5, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_line(aes(group = iter), alpha = .2, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 3)) + xlab(&quot;probability admit&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) There are other ways to do this. For ideas, check out my blog post, Make rotated Gaussians, Kruschke style. Before we can do our variant of Figure 11.5.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and posterior_predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, pp_expect_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it. expose_functions(b11.5, vectorize = TRUE) # required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, draws) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi N &lt;- draws$data$trials[i] y &lt;- draws$data$Y[i] beta_binomial2_lpmf(y, mu, phi, N) } posterior_predict_beta_binomial2 &lt;- function(i, draws, ...) { mu &lt;- draws$dpars$mu[, i] phi &lt;- draws$dpars$phi trials &lt;- draws$data$vint1[i] beta_binomial2_rng(mu, phi, trials) } # required to use `fitted()` pp_expect_beta_binomial2 &lt;- function(draws) { mu &lt;- draws$dpars$mu trials &lt;- draws$data$vint1 trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } With those intermediary steps out of the way, we’re ready to make Figure 11.5.b. # the prediction intervals predict(b11.5) %&gt;% as_tibble() %&gt;% transmute(ll = Q2.5, ul = Q97.5) %&gt;% # the fitted intervals bind_cols(fitted(b11.5) %&gt;% as_tibble()) %&gt;% # the original data used to fit the model bind_cols(b11.5$data) %&gt;% mutate(case = 1:12) %&gt;% # plot! ggplot(aes(x = case)) + geom_linerange(aes(ymin = ll / applications, ymax = ul / applications), color = canva_pal(&quot;Green fields&quot;)(4)[1], size = 2.5, alpha = 1/4) + geom_pointrange(aes(ymin = Q2.5 / applications, ymax = Q97.5 / applications, y = Estimate/applications), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1/2, shape = 1) + geom_point(aes(y = admit/applications), color = canva_pal(&quot;Green fields&quot;)(4)[2], size = 2) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;Posterior validation check&quot;, y = &quot;Admittance probability&quot;) + theme_hc() + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;grey92&quot;)) As in the text, the raw data are consistent with the prediction intervals. But those intervals are so incredibly wide, they’re hardly an endorsement of the model. Once we learn about hierarchical models, we’ll be able to do much better. 11.3.2 Negative-binomial or gamma-Poisson. Recall the Poisson distribution presumes \\(\\sigma^2\\) scales with \\(\\mu\\). The negative binomial distribution relaxes this assumption and presumes “each Poisson count observation has its own rate. It estimates the shape of a gamma distribution to describe the Poisson rates across cases” (p. 350). Here’s a look at the \\(\\gamma\\) distribution. mu &lt;- 3 theta &lt;- 1 ggplot(data = tibble(x = seq(from = 0, to = 12, by = .01)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = rethinking::dgamma2(x, mu, theta)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_vline(xintercept = mu, linetype = 3, color = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(NULL, breaks = c(0, mu, 10)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(paste(&quot;Our sweet &quot;, gamma, &quot;(3, 1)&quot;))) + coord_cartesian(xlim = 0:10) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) McElreath encouraged us to fool around with plotting gamma distributions with different combinations of \\(\\mu\\) and \\(\\theta\\). Here’s a small attempt with different combinations of each taking on 1, 5, and 9. # data crossing(mu = c(1, 5, 9), theta = c(1, 5, 9)) %&gt;% expand(nesting(mu, theta), x = seq(from = 0, to = 27, length.out = 100)) %&gt;% mutate(density = rethinking::dgamma2(x, mu, theta), mu_char = str_c(&quot;mu==&quot;, mu), theta_char = str_c(&quot;theta==&quot;, theta)) %&gt;% # plot ggplot() + geom_ribbon(aes(x = x, ymin = 0, ymax = density), fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_vline(aes(xintercept = mu), color = canva_pal(&quot;Green fields&quot;)(4)[2], linetype = 3) + scale_y_continuous(NULL, labels = NULL) + labs(title = &quot;Gamma can take many shapes&quot;, subtitle = &quot;(dotted vertical lines mark off the means)&quot;, x = &quot;parameter space&quot;) + coord_cartesian(xlim = 0:25) + theme_hc() + theme(axis.ticks.y = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;)) + facet_grid(theta_char~mu_char, labeller = label_parsed) 11.3.2.1 Bonus: Let’s fit a negative-binomial model. McElreath didn’t give an example of negative-binomial regression in the text. This can be a useful but somewhat perplexing model type, so let’s take a detour for an extended example with the UCBadmit data. As in other examples, we will be modeling the admit rates. We’ll keep things simple and leave out predictor variables. b11.6 &lt;- brm(data = d, family = negbinomial, admit ~ 1, prior = c(prior(normal(0, 10), class = Intercept), prior(gamma(0.01, 0.01), class = shape)), # this is the brms default iter = 4000, warmup = 1000, cores = 2, chains = 2, seed = 11, file = &quot;fits/b11.06&quot;) You may have noticed we used the brms default prior(gamma(0.01, 0.01), class = shape) for the shape parameter. Here’s what that prior looks like. ggplot(data = tibble(x = seq(from = 0, to = 60, by = .1)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dgamma(x, 0.01, 0.01)), color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:50) + ggtitle(expression(brms~default~gamma(0.01*&quot;, &quot;*0.01)~shape~prior)) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) Before we even look at the summary of the model parameters, we’ll use the brms::predict() function to return random samples of the poster predictive distribution. This will help us get a sense of what this model is. Because we want random samples instead of summary values, we will specify summary = F. Let’s take a look of what this returns. p &lt;- predict(b11.6, summary = F) p %&gt;% str() ## num [1:6000, 1:12] 52 23 23 139 366 106 206 75 48 270 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL Because we have 6,000 posterior iterations, we also get back 6,000 rows. We have 12 columns, which correspond to the 12 rows (i.e., cases) in the original data. In the next block, we’ll put convert that output to a data frame and wrangle a little before plotting the results. p %&gt;% data.frame() %&gt;% set_names(c(str_c(0, 1:9), 10:12)) %&gt;% gather(case, lambda) %&gt;% mutate(case = str_c(&quot;case: &quot;, case)) %&gt;% ggplot(aes(x = lambda)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(expression(lambda[&quot;[case]&quot;]), breaks = c(0, 200, 400)) + coord_cartesian(xlim = 0:500) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) + facet_wrap(~case, nrow = 2) Because this model had no predictors, we have similar posterior-predictive distributions for each case in the data. It’s important, however, to be very clear of what these posterior-predictive distributions are of. They are not of the data, per se. Let’s look back at the text: A negative-binomial model, more usefully called a gamma-Poisson model, assumes that each Poisson count observation has its own rate. It estimates the shape of the gamma distribution to describe the Poisson rates across cases. (p. 350, emphasis in the original) As a reminder, the “rate” for the Poisson distribution is just another word for the mean, also called \\(\\lambda\\). So unlike a simple Poisson model where we use the individual cases to estimate one overall \\(\\lambda\\), here we’re presuming each case has it’s own \\(\\lambda_i\\). So there are 12 \\(\\lambda_i\\) values that generated our data and if we look at those \\(\\lambda_i\\) values on the whole, their distribution can be described with a gamma distribution. And again, this is not a gamma distribution for our data. This is a gamma distribution of the \\(\\lambda_i\\) values from the 12 separate Poisson distributions that presumably made our data. It turns out there are several ways to parameterize the gamma distribution. When we have the mean and shape parameters, we can define the gamma density for some variable \\(y\\) as \\[\\text{Gamma} (y | \\mu, \\alpha) = \\frac{(\\frac{\\alpha}{\\mu})^\\alpha}{\\Gamma (\\alpha)} y^{\\alpha - 1} \\exp (- \\frac{\\alpha y}{\\mu}),\\] where \\(\\mu\\) is the mean, \\(\\alpha\\) is the shape, and \\(\\Gamma\\) is the gamma function. If you look through Bürkner’s Parameterization of Response Distributions in brms vignette, you’ll see this is the way the gamma likelihood is parameterized in brms. Now let’s finally take a look at the model output for b11.6. print(b11.6) ## Family: negbinomial ## Links: mu = log; shape = identity ## Formula: admit ~ 1 ## Data: d (Number of observations: 12) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 5.02 0.30 4.47 5.67 1.00 3355 2428 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 1.12 0.42 0.47 2.11 1.00 3124 3455 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We have a shape (i.e., \\(\\alpha\\)) and an intercept (i.e., \\(\\log \\mu\\)). After exponentiating the intercept parameter, here are the posterior distributions for those two gamma parameters. post &lt;- posterior_samples(b11.6) post %&gt;% transmute(mu = exp(b_Intercept), alpha = shape) %&gt;% gather(parameter, value) %&gt;% ggplot(aes(x = value)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;posterior&quot;) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) + facet_wrap(~parameter, scales = &quot;free&quot;, labeller = label_parsed) There is a difficulty, however, if we would like to use McElreath’s rethinking::dgamma2() to visualize what gamma distributions based on those parameters might look like. The dgamma2() function has arguments for mu and the scale, not the shape. As it turns out, we can express the mean of the gamma density in terms of the shape and scale with the equation \\[\\mu = \\alpha \\theta,\\] where the scale is termed \\(\\theta\\). By simple algebra, then, we can solve for the scale with \\[\\theta = \\frac{\\mu}{\\alpha}.\\] Using that formulation, here are the posterior means for \\(\\mu\\) and \\(\\theta\\). (mu &lt;- mean(exp(post$b_Intercept))) ## [1] 158.4089 (theta &lt;- mean(exp(post$b_Intercept) / post$shape)) ## [1] 166.0334 Much like we did in Figure 11.5 for the beta binomial model, here is the plot of that posterior mean gamma distribution accompanied by an additional 100 combinations of \\(\\mu\\) and \\(\\theta\\) values sampled from the posterior. # this makes `sample_n()` reproducible set.seed(11) # wrangle to get the 100 draws post %&gt;% mutate(iter = 1:n(), mu = exp(b_Intercept), theta = exp(b_Intercept) / shape) %&gt;% sample_n(size = 100) %&gt;% expand(nesting(iter, mu, theta), x = 1:550) %&gt;% mutate(density = rethinking::dgamma2(x, mu, theta)) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_line(aes(group = iter), alpha = .1, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + stat_function(fun = rethinking::dgamma2, args = list(mu = mu, scale = theta), size = 1.5, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(distribution~of~lambda~values)) + coord_cartesian(xlim = 0:500, ylim = c(0, 0.01)) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) But what if you wanted to use the base R dgamma() function instead of McElreath’s rethinking::dgamma2()? Well, the dgamma() function parameterizes the gamma density in terms of shape and scale, following the equation \\[\\text{Gamma} (y | \\alpha, \\theta) = \\frac{y^{\\alpha - 1} e^{-x /\\theta}}{\\theta^\\alpha \\Gamma (\\alpha)},\\] where \\(\\alpha\\) is the shape, \\(\\theta\\) is the scale, \\(e\\) is base of the natural logarithm, and \\(\\Gamma\\) is the gamma function. And just in case you were worried you life wasn’t complicated enough, the base R dgamma() function will also allow you to define the gamma density with a rate parameter, rather than with scale. As it turns out, the “rate” in this context is just the inverse of the scale. When one parameterizes gamma this way, it follows the formula \\[\\text{Gamma} (y | \\alpha, \\beta) = \\frac{\\beta^\\alpha y^{\\alpha - 1} e^{-\\beta y}}{\\Gamma (\\alpha)},\\] where \\(\\alpha\\), \\(e\\), and \\(\\Gamma\\) are all as they were before and \\(\\beta\\) is the rate parameter. Let’s say we wanted to use the \\(\\text{Gamma} (y | \\alpha, \\theta)\\) parameterization with the dgamma() function. Here are the posterior means for our \\(\\alpha\\) and \\(\\theta\\) parameters. (alpha &lt;- mean(post$shape)) ## [1] 1.12113 (theta &lt;- mean(exp(post$b_Intercept) / post$shape)) ## [1] 166.0334 And here’s a remake of the plot from above, but with the \\(\\alpha\\) and \\(\\theta\\) parameterization. set.seed(11) # wrangle to get the 100 draws post %&gt;% mutate(iter = 1:n(), alpha = shape, theta = exp(b_Intercept) / shape) %&gt;% sample_n(size = 100) %&gt;% expand(nesting(iter, alpha, theta), x = 1:550) %&gt;% mutate(density = dgamma(x, shape = alpha, scale = theta)) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_line(aes(group = iter), alpha = .1, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + stat_function(fun = dgamma, args = list(shape = alpha, scale = theta), size = 1.5, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(distribution~of~lambda~values)) + coord_cartesian(xlim = 0:500, ylim = c(0, 0.01)) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) It turns out it’s possible to get the variance of a gamma distribution with \\(\\alpha \\theta^2\\). Here’s that posterior. post %&gt;% transmute(var = shape * (exp(b_Intercept) / shape)^2) %&gt;% ggplot(aes(x = var)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(&quot;the variance (i.e., &quot;*alpha*theta^2*&quot;)&quot;)) + coord_cartesian(xlim = 0:150000) + theme_hc() + theme(plot.background = element_rect(fill = &quot;grey92&quot;)) Here’s the posterior mean for the gamma variance. post %&gt;% transmute(var = shape * (exp(b_Intercept) / shape)^2) %&gt;% summarise(mean = mean(var)) ## mean ## 1 30207.08 And just to round things out, here are the mean and variance values for those 12 distributions of draws from the predict() output. p %&gt;% data.frame() %&gt;% set_names(c(str_c(0, 1:9), 10:12)) %&gt;% gather(case, lambda) %&gt;% group_by(case) %&gt;% summarise(mean = mean(lambda), variance = var(lambda)) ## # A tibble: 12 x 3 ## case mean variance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 153. 29359. ## 2 02 157. 32376. ## 3 03 157. 35294. ## 4 04 156. 32113. ## 5 05 160. 33441. ## 6 06 160. 35589. ## 7 07 158. 32927. ## 8 08 161. 33267. ## 9 09 159. 30264. ## 10 10 160. 32045. ## 11 11 160. 32683. ## 12 12 159. 33647. Finally, here are the mean and variance for the admit data. d %&gt;% summarise(mean = mean(admit), variance = var(admit)) ## mean variance ## 1 146.25 22037.11 If you’d like more practice with the negative-binomial model and some of the others models for categorical data we covered in this chapter and in Chapter 10, Alan Agresti covered them extensively in his (2015) text, Foundations of linear and generalized linear models. 11.3.3 Over-dispersion, entropy, and information criteria. Both the beta-binomial and the gamma-Poisson models are maximum entropy for the same constraints as the regular binomial and Poisson. They just try to account for unobserved heterogeneity in probabilities and rates. So while they can be a lot harder to fit to data, they can be usefully conceptualized much like ordinary binomial and Poisson GLMs. So in terms of model comparison using information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson (negative-binomial) is a Poisson model. (pp. 350–351) Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.5.3 patchwork_1.0.0 ggthemes_4.2.0 forcats_0.4.0 stringr_1.4.0 ## [6] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [11] tidyverse_1.3.0 brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ggplot2_3.2.1 ## [16] StanHeaders_2.19.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 rsconnect_0.8.16 ## [5] markdown_1.1 rethinking_1.59 base64enc_0.1-3 fs_1.3.1 ## [9] rstudioapi_0.10 farver_2.0.3 svUnit_0.7-12 DT_0.11 ## [13] fansi_0.4.1 mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [17] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 bayesplot_1.7.1 ## [21] jsonlite_1.6.1 dbplyr_1.4.2 shiny_1.4.0 compiler_3.6.2 ## [25] httr_1.4.1 backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [29] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 later_1.0.0 ## [33] htmltools_0.4.0 prettyunits_1.1.1 tools_3.6.2 igraph_1.2.4.2 ## [37] coda_0.19-3 gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [41] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 crosstalk_1.0.0 ## [45] xfun_0.12 ps_1.3.0 rvest_0.3.5 mime_0.8 ## [49] miniUI_0.1.1.1 lifecycle_0.1.0 tidybayes_2.0.1.9000 gtools_3.8.1 ## [53] MASS_7.3-51.4 zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [57] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 inline_0.3.15 ## [61] shinystan_2.5.0 yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [65] stringi_1.4.5 dygraphs_1.1.1.6 pkgbuild_1.0.6 rlang_0.4.4 ## [69] pkgconfig_2.0.3 matrixStats_0.55.0 evaluate_0.14 lattice_0.20-38 ## [73] rstantools_2.0.0 htmlwidgets_1.5.1 labeling_0.3 processx_3.4.1 ## [77] tidyselect_1.0.0 plyr_1.8.5 magrittr_1.5 R6_2.4.1 ## [81] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 haven_2.2.0 ## [85] withr_2.1.2 xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [89] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_2.0 ## [93] grid_3.6.2 readxl_1.3.1 callr_3.4.1 threejs_0.3.3 ## [97] reprex_0.3.0 digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [101] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 "],
["multilevel-models.html", "12 Multilevel Models 12.1 Example: Multilevel tadpoles 12.2 Varying effects and the underfitting/overfitting trade-off 12.3 More than one type of cluster 12.4 Multilevel posterior predictions 12.5 Summary Bonus: Put your random effects to work Reference Session info", " 12 Multilevel Models Multilevel models… remember features of each cluster in the data as they learn about all of the clusters. Depending upon the variation among clusters, which is learned from the data as well, the model pools information across clusters. This pooling tends to improve estimates about each cluster. This improved estimation leads to several, more pragmatic sounding, benefits of the multilevel approach. (p. 356) These benefits include: improved estimates for repeated sampling (i.e., in longitudinal data) improved estimates when there are imbalances among subsamples estimates of the variation across subsamples avoiding simplistic averaging by retaining variation across subsamples All of these benefits flow out of the same strategy and model structure. You learn one basic design and you get all of this for free. When it comes to regression, multilevel regression deserves to be the default approach. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous. It is better to begin to build a multilevel analysis, and then realize it’s unnecessary, than to overlook it. And once you grasp the basic multilevel stragety, it becomes much easier to incorporate related tricks such as allowing for measurement error in the data and even model missing data itself (Chapter 14). (p. 356) I’m totally on board with this. After learning about the multilevel model, I see it everywhere. For more on the sentiment it should be the default, check out McElreath’s blog post, Multilevel Regression as Default. 12.1 Example: Multilevel tadpoles Let’s load the reedfrogs data (see Vonesh &amp; Bolker, 2005). library(rethinking) data(reedfrogs) d &lt;- reedfrogs Detach rethinking and load brms. rm(reedfrogs) detach(package:rethinking, unload = T) library(brms) Go ahead and acquaint yourself with the reedfrogs. library(tidyverse) d %&gt;% glimpse() ## Observations: 48 ## Variables: 5 ## $ density &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25, 25, 25, … ## $ pred &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pred, pred, pred, pred, … ## $ size &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, big, big, small, small… ## $ surv &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23, 22, 25, 23, 23, 23, 2… ## $ propsurv &lt;dbl&gt; 0.9000000, 1.0000000, 0.7000000, 1.0000000, 0.9000000, 0.9000000, 1.0000000, 0.9… Making the tank cluster variable is easy. d &lt;- d %&gt;% mutate(tank = 1:nrow(d)) Here’s the formula for the un-pooled model in which each tank gets its own intercept: \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim \\text{Normal} (0, 5), \\end{align*}\\] where \\(n_i\\) is indexed by the density column. It’s values are distributed like so: d %&gt;% count(density) ## # A tibble: 3 x 2 ## density n ## &lt;int&gt; &lt;int&gt; ## 1 10 16 ## 2 25 16 ## 3 35 16 Now fit this simple aggregated binomial model much like we practiced in Chapter 10. b12.1 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 0 + factor(tank), prior(normal(0, 5), class = b), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 12, file = &quot;fits/b12.01&quot;) We don’t need a depth=2 argument to discover we have 48 different intercepts. Just good old print() will do. print(b12.1) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 0 + factor(tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## factortank1 2.51 1.17 0.63 5.26 1.00 9234 3174 ## factortank2 5.75 2.84 1.66 12.53 1.00 7253 3949 ## factortank3 0.94 0.75 -0.43 2.48 1.00 14660 3992 ## factortank4 5.68 2.75 1.72 12.13 1.00 7200 3530 ## factortank5 2.50 1.14 0.65 5.10 1.00 10773 3863 ## factortank6 2.53 1.23 0.62 5.52 1.00 9481 3031 ## factortank7 5.73 2.75 1.79 12.50 1.00 7129 3851 ## factortank8 2.53 1.21 0.58 5.27 1.00 10268 3662 ## factortank9 -0.43 0.67 -1.80 0.86 1.00 13317 4377 ## factortank10 2.51 1.18 0.61 5.23 1.00 10319 3894 ## factortank11 0.93 0.74 -0.44 2.48 1.00 10923 4261 ## factortank12 0.43 0.67 -0.82 1.79 1.00 12542 4160 ## factortank13 0.93 0.73 -0.39 2.48 1.00 11452 3979 ## factortank14 0.00 0.67 -1.31 1.33 1.00 12844 3648 ## factortank15 2.52 1.18 0.65 5.21 1.00 10889 4142 ## factortank16 2.49 1.18 0.67 5.16 1.00 9097 4170 ## factortank17 3.49 1.11 1.76 6.08 1.00 8862 3772 ## factortank18 2.61 0.79 1.30 4.39 1.00 10176 3459 ## factortank19 2.11 0.66 0.99 3.53 1.00 10781 3708 ## factortank20 6.41 2.65 2.65 12.61 1.00 7678 3762 ## factortank21 2.61 0.79 1.29 4.38 1.00 11531 3656 ## factortank22 2.63 0.82 1.27 4.49 1.00 11484 3771 ## factortank23 2.60 0.76 1.29 4.32 1.00 10216 4006 ## factortank24 1.73 0.56 0.73 2.94 1.00 13135 3775 ## factortank25 -1.20 0.50 -2.25 -0.28 1.00 12764 4193 ## factortank26 0.08 0.40 -0.70 0.89 1.00 12670 3990 ## factortank27 -1.74 0.57 -2.94 -0.70 1.00 13363 4140 ## factortank28 -0.60 0.43 -1.47 0.23 1.00 13634 3825 ## factortank29 0.08 0.41 -0.73 0.92 1.00 14428 4105 ## factortank30 1.45 0.52 0.49 2.53 1.00 13856 3853 ## factortank31 -0.78 0.43 -1.66 0.04 1.00 13487 4263 ## factortank32 -0.43 0.42 -1.29 0.40 1.00 12758 3302 ## factortank33 3.85 1.13 2.12 6.51 1.00 8754 3073 ## factortank34 2.98 0.79 1.68 4.80 1.00 10321 3734 ## factortank35 2.96 0.76 1.68 4.70 1.00 11989 3889 ## factortank36 2.13 0.54 1.16 3.29 1.00 12107 3739 ## factortank37 2.13 0.53 1.18 3.28 1.00 12712 4205 ## factortank38 6.63 2.56 2.95 12.72 1.00 7054 3781 ## factortank39 2.98 0.79 1.68 4.78 1.00 11935 3867 ## factortank40 2.47 0.63 1.37 3.87 1.00 11859 4026 ## factortank41 -2.14 0.56 -3.33 -1.16 1.00 12879 4155 ## factortank42 -0.66 0.36 -1.37 0.02 1.00 12973 3883 ## factortank43 -0.54 0.36 -1.27 0.15 1.00 14390 4483 ## factortank44 -0.41 0.34 -1.10 0.25 1.00 13749 4497 ## factortank45 0.53 0.36 -0.16 1.25 1.00 13663 3927 ## factortank46 -0.67 0.36 -1.40 0.02 1.00 11467 4454 ## factortank47 2.13 0.55 1.15 3.28 1.00 13573 3806 ## factortank48 -0.06 0.33 -0.72 0.59 1.00 14525 4148 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Do you remember the dummy variables models from back in Chapter 5? This model is like one of those on steroids. It’ll be instructive to take a look at their distributions in density plots. We’ll plot them in both their log-odds and probability metrics. For kicks and giggles, let’s use a FiveThirtyEight-like theme for this chapter’s plots. An easy way to do so is with help from the ggthemes package. library(ggthemes) tibble(estimate = fixef(b12.1)[, 1]) %&gt;% mutate(p = inv_logit_scaled(estimate)) %&gt;% gather() %&gt;% mutate(key = if_else(key == &quot;p&quot;, &quot;expected survival probability&quot;, &quot;expected survival log-odds&quot;)) %&gt;% ggplot(aes(x = value, fill = key)) + geom_density(size = 0) + scale_fill_manual(values = c(&quot;orange1&quot;, &quot;orange4&quot;)) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Tank-level intercepts from the no-pooling model&quot;, subtitle = &quot;Notice now inspecting the distributions of the posterior means can offer insights you\\nmight not get if you looked at them one at a time.&quot;) + theme_fivethirtyeight() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) Even though it seems like we can derive important insights from how the tank-level intercepts are distributed, that information is not explicitly encoded in the statistical model. Keep that in mind as we now consider the multilevel alternative. Its formula is \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{tank}_i} \\\\ \\alpha_{\\text{tank}} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1). \\end{align*}\\] The Gaussian distribution with mean \\(\\alpha\\) and standard deviation \\(\\sigma\\) is the prior for each tank’s intercept. But that prior itself has priors for \\(\\alpha\\) and \\(\\sigma\\). So there are two levels in the model, each resembling a simpler model. (p. 359, emphasis in the original) You specify the corresponding multilevel model like this. b12.2 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 1 + (1 | tank), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 12, file = &quot;fits/b12.02&quot;) The syntax for the varying effects follows the lme4 style, ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(cauchy(0, 1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. For technical reasons we won’t really get into until Chapter 13, Stan parameterizes this as a standard deviation. Let’s do the WAIC comparisons. b12.1 &lt;- add_criterion(b12.1, &quot;waic&quot;) b12.2 &lt;- add_criterion(b12.2, &quot;waic&quot;) w &lt;- loo_compare(b12.1, b12.2, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b12.2 0.0 0.0 -100.4 3.6 21.2 0.8 200.7 7.2 ## b12.1 -0.5 2.2 -100.9 4.7 22.8 0.7 201.7 9.4 The se_diff is large relative to the elpd_diff. If we convert the \\(\\text{elpd}\\) difference to the WAIC metric, the message stays the same. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b12.2 0.000000 0.000000 ## b12.1 1.032079 4.472925 Here are the WAIC weights. model_weights(b12.1, b12.2, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b12.1 b12.2 ## 0.37 0.63 I’m not going to show it here, but if you’d like a challenge, try comparing the models with the LOO. You’ll learn all about high pareto_k values, kfold() recommendations, and challenges implementing those kfold() recommendations. If you’re interested, pour yourself a calming adult beverage, execute the code below, and check out the Kfold(): “Error: New factor levels are not allowed” thread in the Stan forums. b12.1 &lt;- add_criterion(b12.1, &quot;loo&quot;) b12.2 &lt;- add_criterion(b12.2, &quot;loo&quot;) But back on track, let’s check he model summary. print(b12.2) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.62 0.21 1.26 2.09 1.00 3183 5643 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.29 0.25 0.82 1.80 1.00 2361 4922 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time we don’t get a list of 48 separate tank-level parameters. However, we do get a description of their distribution interns of a mean (i.e., Intercept) and standard deviation (i.e., sd(Intercept)). If you’d like the actual tank-level parameters, don’t worry; they’re coming in Figure 12.1. We’ll need to do a little prep work, though. post &lt;- posterior_samples(b12.2, add_chain = T) post_mdn &lt;- coef(b12.2, robust = T)$tank[, , ] %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(post_mdn = inv_logit_scaled(Estimate)) post_mdn ## # A tibble: 48 x 11 ## Estimate Est.Error Q2.5 Q97.5 density pred size surv propsurv tank post_mdn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2.06 0.850 0.582 4.03 10 no big 9 0.9 1 0.887 ## 2 2.93 1.06 1.17 5.53 10 no big 10 1 2 0.949 ## 3 0.971 0.655 -0.268 2.40 10 no big 7 0.7 3 0.725 ## 4 2.95 1.07 1.15 5.56 10 no big 10 1 4 0.950 ## 5 2.06 0.859 0.583 4.04 10 no small 9 0.9 5 0.887 ## 6 2.07 0.856 0.586 3.99 10 no small 9 0.9 6 0.888 ## 7 2.96 1.09 1.19 5.51 10 no small 10 1 7 0.951 ## 8 2.07 0.832 0.586 4.03 10 no small 9 0.9 8 0.888 ## 9 -0.182 0.612 -1.45 1.05 10 pred big 4 0.4 9 0.455 ## 10 2.06 0.849 0.592 4.01 10 pred big 9 0.9 10 0.887 ## # … with 38 more rows Here’s the ggplot2 code to reproduce Figure 12.1. post_mdn %&gt;% ggplot(aes(x = tank)) + geom_hline(yintercept = inv_logit_scaled(median(post$b_Intercept)), linetype = 2, size = 1/4) + geom_vline(xintercept = c(16.5, 32.5), size = 1/4) + geom_point(aes(y = propsurv), color = &quot;orange2&quot;) + geom_point(aes(y = post_mdn), shape = 1) + annotate(geom = &quot;text&quot;, x = c(8, 16 + 8, 32 + 8), y = 0, label = c(&quot;small tanks&quot;, &quot;medium tanks&quot;, &quot;large tanks&quot;)) + scale_x_continuous(breaks = c(1, 16, 32, 48)) + labs(title = &quot;Multilevel shrinkage!&quot;, subtitle = &quot;The empirical proportions are in orange while the model-\\nimplied proportions are the black circles. The dashed line is\\nthe model-implied average survival proportion.&quot;) + coord_cartesian(ylim = c(0, 1)) + theme_fivethirtyeight() + theme(panel.grid = element_blank()) Here is the code for our version of Figure 12.2.a, where we visualize the model-implied population distribution of log-odds survival (i.e., the population distribution yielding all the tank-level intercepts). # this makes the output of `sample_n()` reproducible set.seed(12) p1 &lt;- post %&gt;% sample_n(100) %&gt;% expand(nesting(iter, b_Intercept, sd_tank__Intercept), x = seq(from = -4, to = 5, length.out = 100)) %&gt;% ggplot(aes(x = x, group = iter)) + geom_line(aes(y = dnorm(x, b_Intercept, sd_tank__Intercept)), alpha = .2, color = &quot;orange2&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Population survival distribution&quot;, subtitle = &quot;log-odds scale&quot;) + coord_cartesian(xlim = c(-3, 4)) Now we make our Figure 12.2.b and then bind the two subplots with patchwork. p2 &lt;- ggplot(data = post, aes(x = rnorm(n = nrow(post), mean = b_Intercept, sd = sd_tank__Intercept) %&gt;% inv_logit_scaled())) + geom_density(size = 0, fill = &quot;orange2&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Probability of survival&quot;, subtitle = &quot;transformed by the inverse-logit function&quot;) library(patchwork) (p1 + p2) &amp; theme_fivethirtyeight() &amp; theme(plot.title = element_text(size = 12), plot.subtitle = element_text(size = 10)) In the left plot, notice the uncertainty in terms of both location \\(\\alpha\\) and scale \\(\\sigma\\). For both pots, note how we sampled 12,000 imaginary tanks rather than McElreath’s 8,000. This is because we had 12,000 HMC iterations (i.e., execute nrow(post)). The aes() code in that last block was a bit much. To get a sense of how it worked, consider this. set.seed(12) rnorm(n = 1, mean = post$b_Intercept, sd = post$sd_tank__Intercept) %&gt;% inv_logit_scaled() ## [1] 0.2294144 First, we took one random draw from a normal distribution with a mean of the first row in post$b_Intercept and a standard deviation of the value from the first row in post$sd_tank__Intercept, and passed it through the inv_logit_scaled() function. By replacing the 1 with nrow(post), we do this nrow(post) times (i.e., 12,000). Our orange density, then, is the summary of that process. 12.1.0.1 Rethinking: Varying intercepts as over-dispersion. In the previous chapter (page 346), the beta-binomial and gamma-Poisson models were presented as ways for coping with over-dispersion of count data. Varying intercepts accomplish the same thing, allowing count outcomes to be over-dispersed. They accomplish this, because when each observed count gets its own unique intercept, but these intercepts are pooled through a common distribution, the predictions expect over-dispersion just like a beta-binomial or gamma-Poisson model would. Compared to a beta-binomial or gamma-Poisson model, a binomial or Poisson model with a varying intercept on every observed outcome will often be easier to estimate and easier to extend. (p. 363, emphasis in the original) 12.1.0.2 Overthinking: Prior for variance components. Yep, you can use the exponential distribution for your priors in brms, too. Here it is for model b12.2. b12.2b &lt;- update(b12.2, prior = c(prior(normal(0, 1), class = Intercept), prior(exponential(1), class = sd)), seed = 12, file = &quot;fits/b12.02b&quot;) The model summary: print(b12.2b) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.61 0.21 1.25 2.09 1.00 3345 5951 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.30 0.25 0.80 1.80 1.00 2532 4195 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you’re curious how the exponential prior compares to the posterior, you might just plot. tibble(x = seq(from = 0, to = 6, by = .01)) %&gt;% ggplot() + # the prior geom_ribbon(aes(x = x, ymin = 0, ymax = dexp(x, rate = 1)), fill = &quot;orange2&quot;, alpha = 1/3) + # the posterior geom_density(data = posterior_samples(b12.2b), aes(x = sd_tank__Intercept), fill = &quot;orange2&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Bonus prior/posterior plot\\nfor sd_tank__Intercept&quot;, subtitle = &quot;The prior is the semitransparent ramp in the\\nbackground. The posterior is the solid orange\\nmound.&quot;) + coord_cartesian(xlim = c(0, 5)) + theme_fivethirtyeight() 12.2 Varying effects and the underfitting/overfitting trade-off Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp… A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provides better estimates is that they do a better job trading off underfitting and overfitting. (p. 364) In this section, we explicate this by contrasting three perspectives: Complete pooling (i.e., a single-\\(\\alpha\\) model) No pooling (i.e., the single-level \\(\\alpha_{\\text{tank}_i}\\) model) Partial pooling (i.e., the multilevel model for which \\(\\alpha_{\\text{tank}} \\sim \\text{Normal} (\\alpha, \\sigma)\\)) To demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 365) 12.2.1 The model. The simulation formula should look familiar. \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{pond}_i} \\\\ \\alpha_{\\text{pond}} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\end{align*}\\] 12.2.2 Assign values to the parameters. Here we follow along with McElreath and “assign specific values representative of the actual tadpole data” (p. 366). However, our values will differ a little from his. Because he did not show a set.seed() line, we don’t know what seed was used to generate pseudo random draws from the rnorm() function. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(12) dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) head(dsim) ## # A tibble: 6 x 3 ## pond ni true_a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 -0.821 ## 2 2 5 3.77 ## 3 3 5 -0.0351 ## 4 4 5 0.0200 ## 5 5 5 -1.60 ## 6 6 5 0.992 McElreath twice urged us to inspect the contents of ths simulation. In addition to looking at the data with head(), we might well plot. library(tidybayes) dsim %&gt;% ggplot(aes(x = true_a, y = ni)) + geom_halfeyeh(fill = &quot;orange2&quot;, .width = c(.5, .95)) + scale_y_continuous(breaks = c(5, 10, 25, 35)) + ggtitle(&quot;Log-odds varying by # tadpoles per pond&quot;) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14)) 12.2.3 Sumulate survivors. Each pond \\(i\\) has \\(n_i\\) potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival \\(p_i\\). This probability \\(p_i\\) is implied by the model definition, and is equal to: \\[p_i = \\frac{\\exp (\\alpha_i)}{1 + \\exp (\\alpha_i)}\\] The model uses a logit link, and so the probability is defined by the [inv_logit_scaled()] function. (p. 367) set.seed(12) ( dsim &lt;- dsim %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) ) ## # A tibble: 60 x 4 ## pond ni true_a si ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 5 -0.821 0 ## 2 2 5 3.77 5 ## 3 3 5 -0.0351 4 ## 4 4 5 0.0200 3 ## 5 5 5 -1.60 0 ## 6 6 5 0.992 5 ## 7 7 5 0.927 5 ## 8 8 5 0.458 3 ## 9 9 5 1.24 5 ## 10 10 5 2.04 5 ## # … with 50 more rows 12.2.4 Compute the no-pooling estimates. The no-pooling estimates (i.e., \\(\\alpha_{\\text{tank}_i}\\)) are the results of simple algebra. ( dsim &lt;- dsim %&gt;% mutate(p_nopool = si / ni) ) ## # A tibble: 60 x 5 ## pond ni true_a si p_nopool ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 -0.821 0 0 ## 2 2 5 3.77 5 1 ## 3 3 5 -0.0351 4 0.8 ## 4 4 5 0.0200 3 0.6 ## 5 5 5 -1.60 0 0 ## 6 6 5 0.992 5 1 ## 7 7 5 0.927 5 1 ## 8 8 5 0.458 3 0.6 ## 9 9 5 1.24 5 1 ## 10 10 5 2.04 5 1 ## # … with 50 more rows “These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induce no regularization” (p. 367). That is, these are the same kinds of estimates we got back when we fit b12.1. 12.2.5 Compute the partial-pooling estimates. To follow along with McElreath, set chains = 1, cores = 1 to fit with one chain. b12.3 &lt;- brm(data = dsim, family = binomial, si | trials(ni) ~ 1 + (1 | pond), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 10000, warmup = 1000, chains = 1, cores = 1, seed = 12, file = &quot;fits/b12.03&quot;) Here’s our standard brms summary. print(b12.3) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.30 0.18 0.97 1.70 1.00 3022 5232 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.27 0.20 0.89 1.67 1.00 2424 4114 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not aware that you can use McElreath’s depth=2 trick in brms for summary() or print(). But can get most of that information and more with the Stan-like summary using the $fit syntax. b12.3$fit ## Inference for Stan model: 539fb9cd42fc10350850bbd19a1a802f. ## 1 chains, each with iter=10000; warmup=1000; thin=1; ## post-warmup draws per chain=9000, total post-warmup draws=9000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 1.27 0.00 0.20 0.89 1.13 1.27 1.40 1.67 2425 1 ## sd_pond__Intercept 1.30 0.00 0.18 0.97 1.17 1.29 1.42 1.70 3048 1 ## r_pond[1,Intercept] -2.34 0.01 0.90 -4.19 -2.92 -2.29 -1.72 -0.67 10674 1 ## r_pond[2,Intercept] 1.03 0.01 1.00 -0.72 0.33 0.96 1.66 3.22 13253 1 ## r_pond[3,Intercept] 0.17 0.01 0.88 -1.45 -0.43 0.13 0.74 2.02 13091 1 ## r_pond[4,Intercept] -0.50 0.01 0.82 -2.06 -1.05 -0.52 0.04 1.16 12019 1 ## r_pond[5,Intercept] -2.35 0.01 0.92 -4.31 -2.92 -2.31 -1.71 -0.67 10963 1 ## r_pond[6,Intercept] 1.03 0.01 1.00 -0.74 0.34 0.97 1.65 3.18 9844 1 ## r_pond[7,Intercept] 1.03 0.01 1.02 -0.81 0.32 0.96 1.67 3.18 12114 1 ## r_pond[8,Intercept] -0.52 0.01 0.80 -2.05 -1.05 -0.53 -0.01 1.10 14648 1 ## r_pond[9,Intercept] 1.02 0.01 1.00 -0.77 0.32 0.96 1.67 3.18 11229 1 ## r_pond[10,Intercept] 1.03 0.01 0.99 -0.74 0.35 0.96 1.65 3.18 12385 1 ## r_pond[11,Intercept] -0.50 0.01 0.81 -2.06 -1.05 -0.51 0.03 1.13 12747 1 ## r_pond[12,Intercept] -0.51 0.01 0.80 -2.03 -1.04 -0.53 0.01 1.13 13542 1 ## r_pond[13,Intercept] -0.49 0.01 0.81 -2.04 -1.04 -0.51 0.04 1.15 14097 1 ## r_pond[14,Intercept] 0.17 0.01 0.90 -1.48 -0.45 0.14 0.74 2.00 14364 1 ## r_pond[15,Intercept] 0.19 0.01 0.89 -1.46 -0.42 0.15 0.76 2.06 13900 1 ## r_pond[16,Intercept] -0.64 0.01 0.63 -1.85 -1.07 -0.65 -0.22 0.63 11340 1 ## r_pond[17,Intercept] 1.41 0.01 0.93 -0.23 0.77 1.35 1.99 3.41 9969 1 ## r_pond[18,Intercept] 0.72 0.01 0.77 -0.68 0.18 0.67 1.21 2.37 12195 1 ## r_pond[19,Intercept] 0.73 0.01 0.79 -0.69 0.18 0.69 1.23 2.40 11864 1 ## r_pond[20,Intercept] 0.71 0.01 0.78 -0.70 0.19 0.67 1.19 2.40 12721 1 ## r_pond[21,Intercept] 0.73 0.01 0.80 -0.71 0.16 0.68 1.24 2.37 12174 1 ## r_pond[22,Intercept] 1.42 0.01 0.91 -0.23 0.79 1.36 1.97 3.38 11659 1 ## r_pond[23,Intercept] 1.43 0.01 0.95 -0.25 0.78 1.36 2.02 3.49 11853 1 ## r_pond[24,Intercept] -0.64 0.01 0.64 -1.84 -1.08 -0.66 -0.22 0.64 10635 1 ## r_pond[25,Intercept] -1.69 0.01 0.62 -2.96 -2.09 -1.67 -1.27 -0.51 11684 1 ## r_pond[26,Intercept] 0.18 0.01 0.69 -1.08 -0.29 0.15 0.61 1.59 11883 1 ## r_pond[27,Intercept] 0.19 0.01 0.71 -1.11 -0.29 0.16 0.66 1.69 11358 1 ## r_pond[28,Intercept] 0.72 0.01 0.78 -0.70 0.18 0.68 1.22 2.36 12185 1 ## r_pond[29,Intercept] 0.72 0.01 0.78 -0.70 0.18 0.68 1.21 2.38 11013 1 ## r_pond[30,Intercept] 0.18 0.01 0.71 -1.11 -0.32 0.15 0.64 1.64 10947 1 ## r_pond[31,Intercept] 1.43 0.01 0.71 0.15 0.94 1.38 1.87 2.96 11154 1 ## r_pond[32,Intercept] 1.44 0.01 0.72 0.16 0.94 1.38 1.88 3.03 9315 1 ## r_pond[33,Intercept] -1.07 0.00 0.43 -1.92 -1.36 -1.08 -0.79 -0.21 7822 1 ## r_pond[34,Intercept] -1.07 0.01 0.43 -1.92 -1.37 -1.07 -0.78 -0.23 6603 1 ## r_pond[35,Intercept] -0.43 0.01 0.46 -1.32 -0.74 -0.44 -0.13 0.47 7820 1 ## r_pond[36,Intercept] -1.08 0.01 0.43 -1.91 -1.37 -1.07 -0.79 -0.22 7396 1 ## r_pond[37,Intercept] 0.40 0.01 0.53 -0.59 0.04 0.38 0.74 1.50 9807 1 ## r_pond[38,Intercept] -0.92 0.01 0.44 -1.77 -1.21 -0.92 -0.63 -0.05 6691 1 ## r_pond[39,Intercept] 1.44 0.01 0.72 0.16 0.94 1.38 1.89 2.99 10544 1 ## r_pond[40,Intercept] -1.38 0.01 0.44 -2.25 -1.67 -1.37 -1.09 -0.53 7110 1 ## r_pond[41,Intercept] 0.16 0.01 0.51 -0.79 -0.19 0.15 0.50 1.18 10171 1 ## r_pond[42,Intercept] -1.37 0.00 0.43 -2.23 -1.67 -1.36 -1.08 -0.54 7755 1 ## r_pond[43,Intercept] -0.05 0.01 0.47 -0.95 -0.38 -0.06 0.26 0.91 8597 1 ## r_pond[44,Intercept] -2.36 0.01 0.49 -3.38 -2.67 -2.35 -2.03 -1.47 8637 1 ## r_pond[45,Intercept] -0.60 0.01 0.44 -1.45 -0.90 -0.61 -0.31 0.27 7724 1 ## r_pond[46,Intercept] 1.31 0.01 0.61 0.22 0.88 1.28 1.70 2.58 9103 1 ## r_pond[47,Intercept] -1.35 0.00 0.38 -2.11 -1.60 -1.34 -1.09 -0.61 6400 1 ## r_pond[48,Intercept] 1.31 0.01 0.60 0.20 0.89 1.30 1.69 2.60 8836 1 ## r_pond[49,Intercept] 1.69 0.01 0.69 0.47 1.21 1.65 2.11 3.20 11398 1 ## r_pond[50,Intercept] -0.79 0.00 0.39 -1.54 -1.05 -0.79 -0.53 -0.02 6454 1 ## r_pond[51,Intercept] 0.15 0.01 0.44 -0.69 -0.15 0.14 0.44 1.04 7750 1 ## r_pond[52,Intercept] 0.15 0.01 0.45 -0.67 -0.15 0.14 0.44 1.07 7146 1 ## r_pond[53,Intercept] 1.00 0.01 0.55 -0.01 0.62 0.97 1.36 2.14 8765 1 ## r_pond[54,Intercept] 2.22 0.01 0.81 0.84 1.64 2.15 2.71 3.99 10468 1 ## r_pond[55,Intercept] -1.34 0.00 0.38 -2.10 -1.59 -1.34 -1.09 -0.60 6458 1 ## r_pond[56,Intercept] 0.32 0.00 0.46 -0.54 0.01 0.31 0.62 1.27 8571 1 ## r_pond[57,Intercept] 0.74 0.01 0.53 -0.21 0.38 0.72 1.08 1.86 9308 1 ## r_pond[58,Intercept] -1.68 0.00 0.39 -2.45 -1.94 -1.67 -1.41 -0.92 6500 1 ## r_pond[59,Intercept] -0.30 0.01 0.41 -1.08 -0.57 -0.30 -0.03 0.54 6431 1 ## r_pond[60,Intercept] 0.15 0.01 0.45 -0.68 -0.16 0.14 0.43 1.06 7103 1 ## lp__ -189.91 0.17 7.53 -205.74 -194.89 -189.44 -184.70 -176.19 2038 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Feb 26 10:21:14 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). As an aside, notice how this summary still reports the old-style n_eff values, rather than the updated Bulk_ESS and Tail_ESS values. I suspect this will change sometime soon. In the meantime, here’s a thread on the Stan Forums featuring members of the Stan team discussing how. Let’s get ready for the diagnostic plot of Figure 12.3. First we add the partially-pooled estimates, as summarized by their posterior means, to the dsim data. Then we compute error values. # we could have included this step in the block of code below, if we wanted to p_partpool &lt;- coef(b12.3)$pond[, , ] %&gt;% as_tibble() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) dsim &lt;- dsim %&gt;% bind_cols(p_partpool) %&gt;% mutate(p_true = inv_logit_scaled(true_a)) %&gt;% mutate(nopool_error = abs(p_nopool - p_true), partpool_error = abs(p_partpool - p_true)) dsim %&gt;% glimpse() ## Observations: 60 ## Variables: 9 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 1… ## $ true_a &lt;dbl&gt; -0.82085139, 3.76575421, -0.03511672, 0.01999213, -1.59646315, 0.99155593,… ## $ si &lt;int&gt; 0, 5, 4, 3, 0, 5, 5, 3, 5, 5, 3, 3, 3, 4, 4, 6, 10, 9, 9, 9, 9, 10, 10, 6,… ## $ p_nopool &lt;dbl&gt; 0.00, 1.00, 0.80, 0.60, 0.00, 1.00, 1.00, 0.60, 1.00, 1.00, 0.60, 0.60, 0.… ## $ p_partpool &lt;dbl&gt; 0.2547150, 0.9088810, 0.8086646, 0.6834865, 0.2530613, 0.9086626, 0.908728… ## $ p_true &lt;dbl&gt; 0.3055830, 0.9773737, 0.4912217, 0.5049979, 0.1684765, 0.7293951, 0.716461… ## $ nopool_error &lt;dbl&gt; 0.305582963, 0.022626343, 0.308778278, 0.095002134, 0.168476520, 0.2706048… ## $ partpool_error &lt;dbl&gt; 0.050867959, 0.068492635, 0.317442919, 0.178488609, 0.084584786, 0.1792674… Here is our code for Figure 12.3. The extra data processing for dfline is how we get the values necessary for the horizontal summary lines. dfline &lt;- dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key, ni) %&gt;% summarise(mean_error = mean(value)) %&gt;% mutate(x = c( 1, 16, 31, 46), xend = c(15, 30, 45, 60)) dsim %&gt;% ggplot(aes(x = pond)) + geom_vline(xintercept = c(15.5, 30.5, 45.4), color = &quot;white&quot;, size = 2/3) + geom_point(aes(y = nopool_error), color = &quot;orange2&quot;) + geom_point(aes(y = partpool_error), shape = 1) + geom_segment(data = dfline, aes(x = x, xend = xend, y = mean_error, yend = mean_error), color = rep(c(&quot;orange2&quot;, &quot;black&quot;), each = 4), linetype = rep(1:2, each = 4)) + annotate(&quot;text&quot;, x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, label = c(&quot;tiny (5)&quot;, &quot;small (10)&quot;, &quot;medium (25)&quot;, &quot;large (35)&quot;)) + scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) + labs(title = &quot;Estimate error by model type&quot;, subtitle = &quot;The horizontal axis displays pond number. The vertical axis measures\\nthe absolute error in the predicted proportion of survivors, compared to\\nthe true value used in the simulation. The higher the point, the worse\\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\\nThe orange and dashed black lines show the average error for each kind\\nof estimate, across each initial density of tadpoles (pond size). Smaller\\nponds produce more error, but the partial pooling estimates are better\\non average, especially in smaller ponds.&quot;, y = &quot;absolute error&quot;) + theme_fivethirtyeight() + theme(panel.grid = element_blank(), plot.subtitle = element_text(size = 10)) If you wanted to quantify the difference in simple summaries, you might do something like this: dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% gather(key, value, -ni) %&gt;% group_by(key) %&gt;% summarise(mean_error = mean(value) %&gt;% round(digits = 3), median_error = median(value) %&gt;% round(digits = 3)) ## # A tibble: 2 x 3 ## key mean_error median_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nopool_error 0.078 0.05 ## 2 partpool_error 0.067 0.051 I originally learned about the multilevel in order to work with longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, took me some time to wrap my head around. If you’re struggling with this, be patient and keep chipping away. When McElreath lectured on this topic in 2015, he traced partial pooling to statistician Charles M. Stein. In 1977, Efron and Morris wrote the now classic paper, Stein’s Paradox in Statistics, which does a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you’d like more practice seeing how partial pooling works–or if you just like baseball–, check out my blog post, Stein’s Paradox and What Partial Pooling Can Do For You. 12.2.5.1 Overthinking: Repeating the pond simulation. Within the brms workflow, we can reuse a compiled model with update(). But first, we’ll simulate new data. a &lt;- 1.4 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(1999) # for new data, set a new seed new_dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a, sd = sigma)) %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) %&gt;% mutate(p_nopool = si / ni) glimpse(new_dsim) ## Observations: 60 ## Variables: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10,… ## $ true_a &lt;dbl&gt; 2.4990087, 1.3432554, 3.2045137, 3.6047030, 1.6005354, 2.1797409, 0.5759270, -0.… ## $ si &lt;int&gt; 4, 4, 5, 4, 4, 4, 2, 4, 3, 5, 4, 5, 2, 2, 5, 10, 7, 10, 10, 8, 10, 9, 5, 10, 10,… ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 1.00, 0.80, 0.80, 0.80, 0.40, 0.80, 0.60, 1.00, 0.80, 1.00, 0.40, 0.… Fit the new model. b12.3_new &lt;- update(b12.3, newdata = new_dsim, seed = 12, file = &quot;fits/b12.03_new&quot;) print(b12.3_new) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: new_dsim (Number of observations: 60) ## Samples: 1 chains, each with iter = 10000; warmup = 1000; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.26 0.18 0.95 1.65 1.00 3266 4835 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.52 0.20 1.15 1.91 1.00 3105 5010 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Why not plot the first simulation versus the second one? bind_rows(posterior_samples(b12.3), posterior_samples(b12.3_new)) %&gt;% mutate(model = rep(c(&quot;b12.3&quot;, &quot;b12.3_new&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = sd_pond__Intercept)) + stat_density_2d(geom = &quot;raster&quot;, aes(fill = stat(density)), contour = F, n = 200) + geom_vline(xintercept = a, color = &quot;orange3&quot;, linetype = 3) + geom_hline(yintercept = sigma, color = &quot;orange3&quot;, linetype = 3) + scale_fill_gradient(low = &quot;grey25&quot;, high = &quot;orange3&quot;) + ggtitle(&quot;Our simulation posteriors contrast a bit&quot;, subtitle = expression(alpha*&quot; is on the x and &quot;*sigma*&quot; is on the y, both in log-odds. The dotted lines intersect at the true values.&quot;)) + coord_cartesian(xlim = c(.7, 2), ylim = c(.8, 1.9)) + theme_fivethirtyeight() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~model, ncol = 2) If you’d like the stanfit portion of your brm() object, subset with $fit. Take b12.3, for example. You might check out its structure via b12.3$fit %&gt;% str(). Here’s the actual Stan code. b12.3$fit@stanmodel ## S4 class stanmodel &#39;539fb9cd42fc10350850bbd19a1a802f&#39; coded as follows: ## // generated with brms 2.12.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // number of observations ## int Y[N]; // response variable ## int trials[N]; // number of trials ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; N_1; // number of grouping levels ## int&lt;lower=1&gt; M_1; // number of coefficients per level ## int&lt;lower=1&gt; J_1[N]; // grouping indicator per observation ## // group-level predictor values ## vector[N] Z_1_1; ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## vector[N_1] z_1[M_1]; // standardized group-level effects ## } ## transformed parameters { ## vector[N_1] r_1_1; // actual group-level effects ## r_1_1 = (sd_1[1] * (z_1[1])); ## } ## model { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0, N); ## for (n in 1:N) { ## // add more terms to the linear predictor ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; ## } ## // priors including all constants ## target += normal_lpdf(Intercept | 0, 1); ## target += cauchy_lpdf(sd_1 | 0, 1) ## - 1 * cauchy_lccdf(0 | 0, 1); ## target += normal_lpdf(z_1[1] | 0, 1); ## // likelihood including all constants ## if (!prior_only) { ## target += binomial_logit_lpmf(Y | trials, mu); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } ## And you can get the data of a given brm() fit object like so. b12.3$data %&gt;% head() ## si ni pond ## 1 0 5 1 ## 2 5 5 2 ## 3 4 5 3 ## 4 3 5 4 ## 5 0 5 5 ## 6 5 5 6 12.3 More than one type of cluster “We can use and often should use more than one type of cluster in the same model” (p. 370). 12.3.0.1 Rethinking: Cross-classification and hierarchy. The kind of data structure in data(chimpanzees) is usually called a cross-classified multilevel model. It is cross-classified, because actors are not nested within unique blocks. If each chimpanzee had instead done all of his or her pulls on a single day, within a single block, then the data structure would instead be hierarchical. (p. 371, emphasis in the original) 12.3.1 Multilevel chimpanzees. The initial multilevel update from model b10.4 from Chapter 10 follows the statistical formula \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1). \\end{align*}\\] Notice that \\(\\alpha\\) is inside the linear model, not inside the Gaussian prior for \\(\\alpha_\\text{actor}\\). This is mathematically equivalent to what [we] did with the tadpoles earlier in the chapter. You can always take the mean out of a Gaussian distribution and treat that distribution as a constant plus a Gaussian distribution centered on zero. This might seem a little weird at first, so it might help train your intuition by experimenting in R. (p. 371) Behold our two identical Gaussians in a tidy tibble. set.seed(12) two_gaussians &lt;- tibble(y1 = rnorm(n = 1e4, mean = 10, sd = 1), y2 = 10 + rnorm(n = 1e4, mean = 0, sd = 1)) Let’s follow McElreath’s advice to make sure they are same by superimposing the density of one on the other. two_gaussians %&gt;% ggplot() + geom_density(aes(x = y1), size = 0, fill = &quot;orange1&quot;, alpha = 1/3) + geom_density(aes(x = y2), size = 0, fill = &quot;orange4&quot;, alpha = 1/3) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Our simulated Gaussians&quot;) + theme_fivethirtyeight() Yep, those Gaussians look about the same. Let’s get the chimpanzees data from rethinking. library(rethinking) data(chimpanzees) d &lt;- chimpanzees Detach rethinking and reload brms. rm(chimpanzees) detach(package:rethinking, unload = T) library(brms) For our brms model with varying intercepts for actor but not block, we employ the pulled_left ~ 1 + ... + (1 | actor) syntax, specifically omitting a (1 | block) section. b12.4 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), # I&#39;m using 4 cores, instead of the `cores=3` in McElreath&#39;s code iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12, file = &quot;fits/b12.04&quot;) The initial solutions came with a few divergent transitions. Increasing adapt_delta to 0.95 solved the problem. McElreath encouraged us to inspect the trace plots. Here they are. library(bayesplot) color_scheme_set(&quot;orange&quot;) post &lt;- posterior_samples(b12.4, add_chain = T) post %&gt;% select(-lp__, -iter) %&gt;% mcmc_trace(facet_args = list(ncol = 4)) + scale_x_continuous(breaks = c(0, 2500, 5000)) + theme_fivethirtyeight() + theme(legend.direction = &quot;vertical&quot;, legend.key.size = unit(0.5, &quot;cm&quot;), legend.position = c(.96, .1)) They look great. Here’s the posterior distribution for \\(\\sigma_\\text{actor}\\). posterior_samples(b12.4) %&gt;% ggplot(aes(x = sd_actor__Intercept, y = 0)) + geom_halfeyeh(fill = &quot;orange2&quot;, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(sigma[actor])) + theme_fivethirtyeight() We can inspect the \\(\\hat R\\) and effective sample size values for all parameters with the bayesplot::rhat() function. Here we’ll put it in a data frame for easy viewing. data.frame(rhat = rhat(b12.4)) ## rhat ## b_Intercept 1.0019775 ## b_prosoc_left 1.0003011 ## b_prosoc_left:condition 1.0001307 ## sd_actor__Intercept 1.0003201 ## r_actor[1,Intercept] 1.0019991 ## r_actor[2,Intercept] 0.9998247 ## r_actor[3,Intercept] 1.0020363 ## r_actor[4,Intercept] 1.0018960 ## r_actor[5,Intercept] 1.0017907 ## r_actor[6,Intercept] 1.0017260 ## r_actor[7,Intercept] 1.0017929 ## lp__ 1.0009881 I’m now aware that any software packages currently provide convenience functions to compute the Bulk_ESS or Tail_ESS values for arbitrary combinations of parameters. However, it appears the Stan team is working on that. For more, see this discussion among the bayesplot team and this Stan Forum comment from Vehtari. McElreath pointed out it’s important to understand the actor-level parameters, as in the summary above, are deviations from the grand mean. Here’s one way to add them together. post %&gt;% select(starts_with(&quot;r_actor&quot;)) %&gt;% gather() %&gt;% # this is how we might add the grand mean to the actor-level deviations mutate(value = value + post$b_Intercept) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value) %&gt;% round(digits = 2)) ## # A tibble: 7 x 2 ## key mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 r_actor[1,Intercept] -0.71 ## 2 r_actor[2,Intercept] 4.61 ## 3 r_actor[3,Intercept] -1.02 ## 4 r_actor[4,Intercept] -1.02 ## 5 r_actor[5,Intercept] -0.71 ## 6 r_actor[6,Intercept] 0.23 ## 7 r_actor[7,Intercept] 1.75 Here’s another way to get at the same information, this time using coef() and a little formatting help from the stringr::str_c() function. Just for kicks, we’ll throw in the 95% intervals, too. coef(b12.4)$actor[, c(1, 3:4), 1] %&gt;% as_tibble() %&gt;% round(digits = 2) %&gt;% # here we put the credible intervals in an APA-6-style format mutate(`95% CIs` = str_c(&quot;[&quot;, Q2.5, &quot;, &quot;, Q97.5, &quot;]&quot;), actor = str_c(&quot;chimp #&quot;, 1:7)) %&gt;% rename(mean = Estimate) %&gt;% select(actor, mean, `95% CIs`) %&gt;% knitr::kable() actor mean 95% CIs chimp #1 -0.71 [-1.25, -0.19] chimp #2 4.61 [2.55, 8.66] chimp #3 -1.02 [-1.59, -0.48] chimp #4 -1.02 [-1.58, -0.48] chimp #5 -0.71 [-1.25, -0.19] chimp #6 0.23 [-0.3, 0.74] chimp #7 1.75 [1.05, 2.56] If you prefer the posterior median to the mean, just add a robust = T argument inside the coef() function. 12.3.2 Two types of cluster. The full cross-classified statistical model follows the form \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\text{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block}_i} + (\\beta_1 + \\beta_2 \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha_{\\text{block}} &amp; \\sim \\text{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_{\\text{block}} &amp; \\sim \\text{HalfCauchy} (0, 1). \\end{align*}\\] Our brms model with varying intercepts for both actor and block now employs the ... (1 | actor) + (1 | block) syntax. b12.5 &lt;- update(b12.4, newdata = d, formula = pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) + (1 | block), iter = 6000, warmup = 1000, cores = 4, chains = 4, control = list(adapt_delta = 0.99), seed = 12, file = &quot;fits/b12.05&quot;) This time we increased adapt_delta to 0.99 to avoid divergent transitions. We can look at the primary coefficients with print(). McElreath urged us again to inspect the trace plots. post &lt;- posterior_samples(b12.5, add_chain = T) post %&gt;% select(-lp__, -iter) %&gt;% mcmc_trace(facet_args = list(ncol = 4)) + scale_x_continuous(breaks = c(0, 2500, 5000)) + theme_fivethirtyeight() + theme(legend.position = c(.75, .06)) The trace plots look great. Here are the other diagnostics along with the parameter summaries. b12.5$fit ## Inference for Stan model: 1845efd85a4a0959106eaad58bc19025. ## 4 chains, each with iter=6000; warmup=1000; thin=1; ## post-warmup draws per chain=5000, total post-warmup draws=20000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 0.44 0.02 0.97 -1.42 -0.14 0.41 0.99 2.45 3790 1 ## b_prosoc_left 0.83 0.00 0.26 0.32 0.65 0.83 1.00 1.34 14928 1 ## b_prosoc_left:condition -0.14 0.00 0.30 -0.73 -0.34 -0.14 0.06 0.44 15605 1 ## sd_actor__Intercept 2.26 0.01 0.93 1.13 1.64 2.05 2.62 4.62 5208 1 ## sd_block__Intercept 0.22 0.00 0.18 0.01 0.09 0.18 0.31 0.68 7479 1 ## r_actor[1,Intercept] -1.16 0.02 0.97 -3.20 -1.70 -1.13 -0.57 0.68 3927 1 ## r_actor[2,Intercept] 4.18 0.02 1.69 1.84 3.10 3.91 4.94 8.03 7572 1 ## r_actor[3,Intercept] -1.47 0.02 0.98 -3.51 -2.02 -1.44 -0.88 0.40 3887 1 ## r_actor[4,Intercept] -1.47 0.02 0.98 -3.50 -2.01 -1.44 -0.88 0.39 3827 1 ## r_actor[5,Intercept] -1.16 0.02 0.98 -3.19 -1.71 -1.12 -0.57 0.71 3835 1 ## r_actor[6,Intercept] -0.21 0.02 0.98 -2.23 -0.77 -0.18 0.38 1.68 3817 1 ## r_actor[7,Intercept] 1.33 0.02 1.00 -0.71 0.76 1.35 1.93 3.28 4114 1 ## r_block[1,Intercept] -0.18 0.00 0.23 -0.74 -0.30 -0.12 -0.01 0.14 11446 1 ## r_block[2,Intercept] 0.04 0.00 0.19 -0.33 -0.05 0.02 0.12 0.46 17148 1 ## r_block[3,Intercept] 0.06 0.00 0.19 -0.30 -0.04 0.03 0.14 0.50 18141 1 ## r_block[4,Intercept] 0.01 0.00 0.18 -0.38 -0.08 0.00 0.09 0.41 19547 1 ## r_block[5,Intercept] -0.03 0.00 0.19 -0.45 -0.12 -0.01 0.06 0.34 19093 1 ## r_block[6,Intercept] 0.12 0.00 0.20 -0.20 -0.01 0.07 0.22 0.60 14510 1 ## lp__ -292.89 0.06 3.73 -301.09 -295.17 -292.58 -290.24 -286.52 4361 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Feb 26 10:32:39 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Happily, our n_eff values are generally quite higher than those McElreath presented in the text. The good folks at Stan have been working hard to improve the software. If we’d like to restrict our focus to the parameter summaries, we can also use brms::ranef() to get those depth=2-type estimates. With ranef(), you get the group-specific estimates in a deviance metric. The coef() function, in contrast, yields the group-specific estimates in what you might call the natural metric. We’ll get more language for this in the next chapter. ranef(b12.5)$actor[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.16 0.97 -3.20 0.68 ## 2 4.18 1.69 1.84 8.03 ## 3 -1.47 0.98 -3.51 0.40 ## 4 -1.47 0.98 -3.50 0.39 ## 5 -1.16 0.98 -3.19 0.71 ## 6 -0.21 0.98 -2.23 1.68 ## 7 1.33 1.00 -0.71 3.28 ranef(b12.5)$block[, , &quot;Intercept&quot;] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.18 0.23 -0.74 0.14 ## 2 0.04 0.19 -0.33 0.46 ## 3 0.06 0.19 -0.30 0.50 ## 4 0.01 0.18 -0.38 0.41 ## 5 -0.03 0.19 -0.45 0.34 ## 6 0.12 0.20 -0.20 0.60 We might make the coefficient plot of Figure 12.4.a with mcmc_plot(). mcmc_plot(b12.5, pars = c(&quot;^r_&quot;, &quot;^b_&quot;, &quot;^sd_&quot;)) + theme_fivethirtyeight() + theme(axis.text.y = element_text(hjust = 0)) Once we get the posterior samples, it’s easy to compare the random variances as in Figure 12.4.b. post %&gt;% ggplot(aes(x = sd_actor__Intercept)) + geom_density(size = 0, fill = &quot;orange1&quot;, alpha = 3/4) + geom_density(aes(x = sd_block__Intercept), size = 0, fill = &quot;orange4&quot;, alpha = 3/4) + annotate(geom = &quot;text&quot;, x = 2/3, y = 2, label = &quot;block&quot;, color = &quot;orange4&quot;) + annotate(geom = &quot;text&quot;, x = 2, y = 3/4, label = &quot;actor&quot;, color = &quot;orange1&quot;) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(sigma[&quot;[x]&quot;])) + coord_cartesian(xlim = c(0, 4)) + theme_fivethirtyeight() We might compare our models by their PSIS-LOO values. b12.4 &lt;- add_criterion(b12.4, &quot;loo&quot;) b12.5 &lt;- add_criterion(b12.5, &quot;loo&quot;) loo_compare(b12.4, b12.5) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b12.4 0.0 0.0 -265.8 9.7 8.2 0.4 531.6 19.5 ## b12.5 -0.6 0.9 -266.4 9.9 10.4 0.5 532.7 19.7 model_weights(b12.4, b12.5, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b12.4 b12.5 ## 0.64 0.36 The two models yield nearly-equivalent information criteria values. Yet recall what McElreath wrote: “There is nothing to gain here by selecting either model. The comparison of the two models tells a richer story” (p. 367). 12.3.3 Even more clusters. Adding more types of clusters proceeds the same way. At some point the model may become too complex to reliably fit to data. But Hamiltonian Monte Carlo is very capable with varying effects. It can easily handle tens of thousands of varying effect parameters. Sampling will be slow in such cases, but it will work. So don’t be shy–if you have a good theoretical reason to include a cluster variable, then you also have a good theoretical reason toe partially pool its parameters. (p. 376) This section just hints at a historical software difficulty. In short, it’s not uncommon to have a theory-based model that includes multiple sources of clustering (i.e., requiring many ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ) parts in the model formula). This can make for all kinds of computational difficulties and result in software error messages, inadmissible solutions, and so on. One of the practical solutions to difficulties like these has been to simplify the statistical models by removing some of the clustering terms. Even though such simpler models were not the theory-based ones, at least they yielded solutions. Nowadays, Stan (via brms or otherwise) is making it easier to fit the full theoretically-based model. To learn more about this topic, check out this nice blog post by Michael Frank, Mixed effects models: Is it time to go Bayesian by default?. Make sure to check out the discussion in the comments section, which includes all-stars like Bürkner and Douglas Bates. You can get more context for the issue from this (2013) paper by Barr, Levy, Scheepers, and Tily. 12.4 Multilevel posterior predictions Producing implied predictions from a fit model, is very helpful for understanding what the model means. Every model is a merger of sense and nonsense. When we understand a model, we can find its sense and control its nonsense. But as models get more complex, it is very difficult to impossible to understand them just by inspecting tables of posterior means and intervals. Exploring implied posterior predictions helps much more… …The introduction of varying effects does introduce nuance, however. First, we should no longer expect the model to exactly retrodict the sample, because adaptive regularization has as its goal to trade off poorer fit in sample for better inference and hopefully better fit out of sample. This is what shrinkage does for us… Second, “prediction” in the context of a multilevel model requires additional choices. If we wish to validate a model against the specific clusters used to fit the model, that is one thing. But if we instead wish to compute predictions for new clusters, other than the one observed in the sample, that is quite another. We’ll consider each of these in turn, continuing to use the chimpanzees model from the previous section. (p. 376) 12.4.1 Posterior prediction for same clusters. Like McElreath did in the text, we’ll do this two ways. Recall we use brms::fitted() in place of rethinking::link(). chimp &lt;- 2 nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1), actor = chimp) ( chimp_2_fitted &lt;- fitted(b12.4, newdata = nd) %&gt;% as_tibble() %&gt;% mutate(condition = factor(c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 5 ## Estimate Est.Error Q2.5 Q97.5 condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.980 0.0196 0.927 1.00 0/0 ## 2 0.991 0.00952 0.965 1.00 1/0 ## 3 0.980 0.0196 0.927 1.00 0/1 ## 4 0.990 0.0107 0.962 1.00 1/1 ( chimp_2_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = str_c(prosoc_left, &quot;/&quot;, condition)) %&gt;% mutate(condition = factor(condition, levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 1 ## 2 0 0/1 1 ## 3 1 1/0 1 ## 4 1 1/1 1 McElreath didn’t show the corresponding plot in the text. It might look like this. chimp_2_fitted %&gt;% # if you want to use `geom_line()` or `geom_ribbon()` with a factor on the x axis, # you need to code something like `group = 1` in `aes()` ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_2_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #2&quot;, subtitle = &quot;The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.&quot;) + coord_cartesian(ylim = c(.75, 1)) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Do note how severely we’ve restricted the y-axis range. But okay, now let’s do things by hand. We’ll need to extract the posterior samples and look at the structure of the data. post &lt;- posterior_samples(b12.4) glimpse(post) ## Observations: 16,000 ## Variables: 12 ## $ b_Intercept &lt;dbl&gt; 0.23547946, 0.03497523, -0.03777228, 0.56790171, 0.73010098, 0.… ## $ b_prosoc_left &lt;dbl&gt; 1.1453911, 0.6267454, 0.6751499, 1.2895758, 1.2899931, 1.126489… ## $ `b_prosoc_left:condition` &lt;dbl&gt; -0.248333615, -0.294051100, -0.282770864, -0.388776230, -0.2590… ## $ sd_actor__Intercept &lt;dbl&gt; 2.002346, 1.701811, 1.577582, 2.419690, 2.395834, 1.964988, 2.0… ## $ `r_actor[1,Intercept]` &lt;dbl&gt; -0.7305592, -0.8645669, -0.9064857, -1.8322334, -1.9834019, -1.… ## $ `r_actor[2,Intercept]` &lt;dbl&gt; 4.183711, 5.467217, 5.153922, 2.317675, 2.064236, 6.140877, 1.9… ## $ `r_actor[3,Intercept]` &lt;dbl&gt; -1.210496677, -1.001792285, -1.093242186, -2.025558647, -2.1725… ## $ `r_actor[4,Intercept]` &lt;dbl&gt; -0.93728036, -1.23363060, -0.68325735, -1.54222507, -2.26634557… ## $ `r_actor[5,Intercept]` &lt;dbl&gt; -0.8926356, -0.7082916, -0.8901743, -1.7444680, -1.4919088, -1.… ## $ `r_actor[6,Intercept]` &lt;dbl&gt; -0.361511125, 0.560052254, 0.266428495, -0.613563055, -0.965595… ## $ `r_actor[7,Intercept]` &lt;dbl&gt; 1.0714660, 2.1202229, 1.9114253, 1.0014093, 1.8935331, 0.987593… ## $ lp__ &lt;dbl&gt; -282.8272, -283.9986, -284.1550, -282.6171, -286.0210, -283.770… McElreath didn’t show what his R code 12.29 dens( post$a_actor[,5] ) would look like. But here’s our analogue. post %&gt;% transmute(actor_5 = `r_actor[5,Intercept]`) %&gt;% ggplot(aes(x = actor_5)) + geom_density(size = 0, fill = &quot;blue&quot;) + scale_y_continuous(breaks = NULL) + ggtitle(&quot;Chimp #5&#39;s density&quot;) + theme_fivethirtyeight() And because we made the density only using the r_actor[5,Intercept] values (i.e., we didn’t add b_Intercept to them), the density is in a deviance-score metric. McElreath built his own link() function in R code 12.30. Here we’ll build an alternative to fitted(). # our hand-made `brms::fitted()` alternative my_fitted &lt;- function(prosoc_left, condition) { post %&gt;% transmute(fitted = (b_Intercept + `r_actor[5,Intercept]` + b_prosoc_left * prosoc_left + `b_prosoc_left:condition` * prosoc_left * condition) %&gt;% inv_logit_scaled()) } # the posterior summaries ( chimp_5_my_fitted &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) %&gt;% mutate(post = map2(prosoc_left, condition, my_fitted)) %&gt;% unnest(post) %&gt;% mutate(condition = factor(str_c(prosoc_left, &quot;/&quot;, condition), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(condition) %&gt;% tidybayes::mean_qi(fitted) ) ## # A tibble: 4 x 7 ## condition fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0/0 0.332 0.224 0.452 0.95 mean qi ## 2 1/0 0.526 0.380 0.667 0.95 mean qi ## 3 0/1 0.332 0.224 0.452 0.95 mean qi ## 4 1/1 0.495 0.348 0.640 0.95 mean qi # the empirical summaries chimp &lt;- 5 ( chimp_5_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(prosoc_left, condition) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(condition = factor(str_c(prosoc_left, &quot;/&quot;, condition), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 3 ## prosoc_left condition prob ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0/0 0.333 ## 2 0 0/1 0.278 ## 3 1 1/0 0.556 ## 4 1 1/1 0.5 Okay, let’s see how good we are at retrodicting the pulled_left probabilities for actor == 5. chimp_5_my_fitted %&gt;% ggplot(aes(x = condition, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_5_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #5&quot;, subtitle = &quot;This plot is like the last except\\nwe did more by hand.&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.subtitle = element_text(size = 10)) Not bad. 12.4.2 Posterior prediction for new clusters. By average actor, McElreath referred to a chimp with an intercept exactly at the population mean \\(\\alpha\\). So this time we’ll only be working with the population parameters, or what are also sometimes called the fixed effects. When using brms::posterior_samples() output, this would mean working with columns beginning with the b_ prefix (i.e., b_Intercept, b_prosoc_left, and b_prosoc_left:condition). post_average_actor &lt;- post %&gt;% # here we use the linear regression formula to get the log_odds for the 4 conditions transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% # with `mutate_all()` we can convert the estimates to probabilities in one fell swoop mutate_all(inv_logit_scaled) %&gt;% # putting the data in the long format and grouping by condition (i.e., `key`) gather() %&gt;% mutate(key = factor(key, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% group_by(key) %&gt;% # here we get the summary values for the plot summarise(m = mean(value), # note we&#39;re using 80% intervals ll = quantile(value, probs = .1), ul = quantile(value, probs = .9)) post_average_actor ## # A tibble: 4 x 4 ## key m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.590 0.346 0.820 ## 2 1/0 0.746 0.545 0.914 ## 3 0/1 0.590 0.346 0.820 ## 4 1/1 0.723 0.505 0.904 Figure 12.5.a. p1 &lt;- post_average_actor %&gt;% ggplot(aes(x = key, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p1 If we want to depict the variability across the chimps, we need to include sd_actor__Intercept into the calculations. In the first block of code, below, we simulate a bundle of new intercepts defined by \\[\\alpha_\\text{actor} \\sim \\operatorname{Normal} (0, \\sigma_\\text{actor}).\\] # the random effects set.seed(12.42) ran_ef &lt;- tibble(random_effect = rnorm(n = 1000, mean = 0, sd = post$sd_actor__Intercept)) %&gt;% # with the `., ., ., .` syntax, we quadruple the previous line bind_rows(., ., ., .) # the fixed effects (i.e., the population parameters) fix_ef &lt;- post %&gt;% slice(1:1000) %&gt;% transmute(`0/0` = b_Intercept, `1/0` = b_Intercept + b_prosoc_left, `0/1` = b_Intercept, `1/1` = b_Intercept + b_prosoc_left + `b_prosoc_left:condition`) %&gt;% gather() %&gt;% rename(condition = key, fixed_effect = value) %&gt;% mutate(condition = factor(condition, level = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) # combine them ran_and_fix_ef &lt;- bind_cols(ran_ef, fix_ef) %&gt;% mutate(intercept = fixed_effect + random_effect) %&gt;% mutate(prob = inv_logit_scaled(intercept)) # to simplify things, we&#39;ll reduce them to summaries ( marginal_effects &lt;- ran_and_fix_ef %&gt;% group_by(condition) %&gt;% summarise(m = mean(prob), ll = quantile(prob, probs = .1), ul = quantile(prob, probs = .9)) ) ## # A tibble: 4 x 4 ## condition m ll ul ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0/0 0.544 0.0611 0.964 ## 2 1/0 0.658 0.132 0.984 ## 3 0/1 0.544 0.0611 0.964 ## 4 1/1 0.637 0.108 0.982 Behold Figure 12.5.b. p2 &lt;- marginal_effects %&gt;% ggplot(aes(x = condition, y = m, group = 1)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p2 Figure 12.5.c just takes a tiny bit more wrangling. p3 &lt;- ran_and_fix_ef %&gt;% mutate(iter = rep(1:1000, times = 4)) %&gt;% filter(iter %in% c(1:50)) %&gt;% ggplot(aes(x = condition, y = prob, group = iter)) + geom_line(alpha = 1/2, color = &quot;orange3&quot;) + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p3 For the finale, we’ll stitch the three plots together. p1 | p2 | p3 12.4.2.1 Bonus: Let’s use fitted() this time. We just made those plots using various wrangled versions of post, the data frame returned by posterior_samples(b.12.4). If you followed along closely, part of what made that a great exercise is that it forced you to consider what the various vectors in post meant with respect to the model formula. But it’s also handy to see how to do that from a different perspective. So in this section, we’ll repeat that process by relying on the fitted() function, instead. We’ll go in the same order, starting with the average actor. nd &lt;- tibble(prosoc_left = c(0, 1, 0, 1), condition = c(0, 0, 1, 1)) ( f &lt;- fitted(b12.4, newdata = nd, re_formula = NA, probs = c(.1, .9)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = factor(str_c(prosoc_left, &quot;/&quot;, condition), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.590 0.182 0.346 0.820 0 0/0 ## 2 0.746 0.152 0.545 0.914 1 1/0 ## 3 0.590 0.182 0.346 0.820 0 0/1 ## 4 0.723 0.159 0.505 0.904 1 1/1 You should notice a few things. Since b12.4 is a multilevel model, it had three predictors: prosoc_left, condition, and actor. However, our nd data only included the first two of those predictors. The reason fitted() permitted that was because we set re_formula = NA. When you do that, you tell fitted() to ignore group-level effects (i.e., focus only on the fixed effects). This was our fitted() version of ignoring the r_ vectors returned by posterior_samples(). Here’s the plot. p4 &lt;- f %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p4 For marginal of actor, we can continue using the same nd data. This time we’ll be sticking with the default re_formula setting, which will accommodate the multilevel nature of the model. However, we’ll also be adding allow_new_levels = T and sample_new_levels = &quot;gaussian&quot;. The former will allow us to marginalize across the specific actors in our data and the latter will instruct fitted() to use the multivariate normal distribution implied by the random effects. It’ll make more sense why I say multivariate normal by the end of the next chapter. For now, just go with it. ( f &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% mutate(condition = factor(str_c(prosoc_left, &quot;/&quot;, condition), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) ) ## # A tibble: 4 x 6 ## Estimate Est.Error Q10 Q90 prosoc_left condition ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.558 0.329 0.0689 0.971 0 0/0 ## 2 0.666 0.311 0.145 0.987 1 1/0 ## 3 0.558 0.329 0.0689 0.971 0 0/1 ## 4 0.650 0.316 0.127 0.986 1 1/1 Here’s our fitted()-based marginal of actor plot. p5 &lt;- f %&gt;% ggplot(aes(x = condition, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p5 For the simulated actors plot, we’ll just amend our process from the last one. This time we set summary = F to keep the iteration-specific results and set nsamples = n_sim to indicate the number of actors we’d like to simulate (i.e., 50, as in the text). # how many simulated actors would you like? n_sim &lt;- 50 ( f &lt;- fitted(b12.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;, summary = F, nsamples = n_sim) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n_sim) %&gt;% gather(key, value, -iter) %&gt;% bind_cols(nd %&gt;% transmute(condition = factor(str_c(prosoc_left, &quot;/&quot;, condition), levels = c(&quot;0/0&quot;, &quot;1/0&quot;, &quot;0/1&quot;, &quot;1/1&quot;))) %&gt;% expand(condition, iter = 1:n_sim)) ) ## # A tibble: 200 x 5 ## iter key value condition iter1 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 V1 0.0442 0/0 1 ## 2 2 V1 0.558 0/0 2 ## 3 3 V1 0.809 0/0 3 ## 4 4 V1 0.929 0/0 4 ## 5 5 V1 0.820 0/0 5 ## 6 6 V1 0.370 0/0 6 ## 7 7 V1 0.347 0/0 7 ## 8 8 V1 0.0993 0/0 8 ## 9 9 V1 0.847 0/0 9 ## 10 10 V1 0.0179 0/0 10 ## # … with 190 more rows p6 &lt;- f %&gt;% ggplot(aes(x = condition, y = value, group = iter)) + geom_line(alpha = 1/2, color = &quot;blue&quot;) + ggtitle(&quot;50 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 14, hjust = .5)) p6 Here they are altogether. p4 | p5 | p6 12.4.3 Focus and multilevel prediction. First, let’s load the Kline data. # prep data library(rethinking) data(Kline) d &lt;- Kline Switch out the packages, once again. detach(package:rethinking, unload = T) library(brms) rm(Kline) The statistical formula for our multilevel count model is \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\operatorname{Poisson} (\\mu_i) \\\\ \\text{log} (\\mu_i) &amp; = \\alpha + \\alpha_{\\text{culture}_i} + \\beta \\text{log} (\\text{population}_i) \\\\ \\alpha &amp; \\sim \\operatorname{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\operatorname{Normal} (0, 1) \\\\ \\alpha_{\\text{culture}} &amp; \\sim \\operatorname{Normal} (0, \\sigma_{\\text{culture}}) \\\\ \\sigma_{\\text{culture}} &amp; \\sim \\operatorname{HalfCauchy} (0, 1). \\\\ \\end{align*}\\] With brms, we don’t actually need to make the logpop or society variables. We’re ready to fit the multilevel Kline model with the data in hand. b12.6 &lt;- brm(data = d, family = poisson, total_tools ~ 0 + Intercept + log(population) + (1 | culture), prior = c(prior(normal(0, 10), class = b, coef = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 3, chains = 3, seed = 12, file = &quot;fits/b12.06&quot;) Note how we used the special 0 + intercept syntax rather than using the default Intercept. This is because our predictor variable was not mean centered. For more info, see brms GitHub issue #114. Though we used the 0 + intercept syntax for the fixed effect, it was not necessary for the random effect. Both ways work. Here is the data-processing work for our variant of Figure 12.6. nd &lt;- tibble(population = seq(from = 1000, to = 400000, by = 5000), # to &quot;simulate counterfactual societies, using the hyper-parameters&quot; (p. 383), # we&#39;ll plug a new island into the `culture` variable culture = &quot;my_island&quot;) p &lt;- predict(b12.6, # this allows us to simulate values for our counterfactual island, &quot;my_island&quot; allow_new_levels = T, # here we explicitly tell brms we want to include the group-level effects re_formula = ~ (1 | culture), # from the brms manual, this uses the &quot;(multivariate) normal distribution implied by # the group-level standard deviations and correlations&quot;, which appears to be # what McElreath did in the text. sample_new_levels = &quot;gaussian&quot;, newdata = nd, probs = c(.015, .055, .165, .835, .945, .985)) %&gt;% as_tibble() %&gt;% bind_cols(nd) p %&gt;% glimpse() ## Observations: 80 ## Variables: 10 ## $ Estimate &lt;dbl&gt; 19.85467, 31.01778, 36.26311, 39.99256, 43.04722, 45.49378, 47.70667, 49.76333… ## $ Est.Error &lt;dbl&gt; 9.765832, 13.065952, 14.943993, 16.476664, 17.912137, 19.180434, 20.110882, 21… ## $ Q1.5 &lt;dbl&gt; 5.000, 11.000, 12.000, 13.985, 15.000, 16.000, 16.985, 17.985, 18.000, 18.000,… ## $ Q5.5 &lt;dbl&gt; 8.000, 15.000, 18.000, 19.000, 21.000, 22.000, 23.000, 25.000, 25.000, 25.000,… ## $ Q16.5 &lt;dbl&gt; 12, 20, 24, 26, 28, 30, 31, 33, 33, 34, 35, 36, 37, 37, 38, 39, 39, 40, 40, 41… ## $ Q83.5 &lt;dbl&gt; 27.000, 41.000, 48.000, 53.000, 57.000, 59.000, 62.000, 65.000, 68.000, 70.000… ## $ Q94.5 &lt;dbl&gt; 36.000, 53.000, 61.000, 67.000, 72.000, 77.000, 80.055, 85.000, 89.000, 91.000… ## $ Q98.5 &lt;dbl&gt; 47.000, 68.000, 79.000, 88.000, 96.000, 101.000, 108.000, 112.000, 116.000, 12… ## $ population &lt;dbl&gt; 1000, 6000, 11000, 16000, 21000, 26000, 31000, 36000, 41000, 46000, 51000, 560… ## $ culture &lt;chr&gt; &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, &quot;my_island&quot;, … For a detailed discussion on this way of using brms::predict(), see Andrew MacDonald’s great blogpost on this very figure. Here’s what we’ve been working for. p %&gt;% ggplot(aes(x = log(population), y = Estimate)) + geom_ribbon(aes(ymin = Q1.5, ymax = Q98.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q5.5, ymax = Q94.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_ribbon(aes(ymin = Q16.5, ymax = Q83.5), fill = &quot;orange2&quot;, alpha = 1/3) + geom_line(color = &quot;orange4&quot;) + geom_text(data = d, aes(y = total_tools, label = culture), size = 2.33, color = &quot;blue&quot;) + ggtitle(&quot;Total tools as a function of log(population)&quot;) + coord_cartesian(ylim = range(d$total_tools)) + theme_fivethirtyeight() + theme(plot.title = element_text(size = 12, hjust = .5)) Glorious. The envelope of predictions is a lot wider here than it was back in Chapter 10. This is a consequene of the varying intercepts, combined with the fact that there is much more variation in the data than a pure-Poisson model anticipates. (p. 384) 12.5 Summary Bonus: Put your random effects to work A big part of this chapter, both what McElreath focused on in the text and even our plotting digression a bit above, focused on how to combine the fixed effects of a multilevel with the group-level. Given some binomial variable, \\(\\text{criterion}\\), and some group term, \\(\\text{grouping variable}\\), we’ve learned the simple multilevel model follows a form like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\operatorname{Binomial} (n_i \\geq 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{grouping variable}_i}\\\\ \\alpha &amp; \\sim \\operatorname{Normal} (0, 1) \\\\ \\alpha_{\\text{grouping variable}} &amp; \\sim \\operatorname{Normal} (0, \\sigma_{\\text{grouping variable}}) \\\\ \\sigma_{\\text{grouping variable}} &amp; \\sim \\operatorname{HalfCauchy} (0, 1), \\end{align*}\\] and we’ve been grappling with the relation between the grand mean \\(\\alpha\\) and the group-level deviations \\(\\alpha_{\\text{grouping variable}}\\). For situations where we have the brms::brm() model fit in hand, we’ve been playing with various ways to use the iterations, particularly with either the posterior_samples() method and the fitted()/predict() method. Both are great. But (a) we have other options, which I’d like to share, and (b) if you’re like me, you probably need more practice than following along with the examples in the text. In this bonus section, we are going to introduce two simplified models and then practice working with combining the grand mean various combinations of the random effects. For our first step, we’ll introduce the models. 12.5.1 Intercepts-only models with one or two grouping variables. If you recall, b12.4 was our first multilevel model with the chimps data. We can retrieve the model formula like so. b12.4$formula ## pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) In addition to the model intercept and random effects for the individual chimps (i.e., actor), we also included fixed effects for the study conditions. For our bonus section, it’ll be easier if we reduce this to a simple intercepts-only model with the sole actor grouping factor. That model will follow the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\operatorname{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i}\\\\ \\alpha &amp; \\sim \\operatorname{Normal} (0, 10) \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\operatorname{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\operatorname{HalfCauchy} (0, 1). \\end{align*}\\] Before we fit the model, you might recall that (a) we’ve already removed the chimpanzees data after saving the data as d and (b) we subsequently reassigned the Kline data to d. Instead of reloading the rethinking package to retrieve the chimpanzees data, we might also acknowledge that the data has also been saved within our b12.4 fit object. [It’s easy to forget such things.] b12.4$data %&gt;% glimpse() ## Observations: 504 ## Variables: 4 ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ actor &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… So there’s no need to reload anything. Everything we need is already at hand. Let’s fit the intercepts-only model. b12.7 &lt;- brm(data = b12.4$data, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12, file = &quot;fits/b12.07&quot;) Here’s the model summary. print(b12.7) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) ## Data: b12.4$data (Number of observations: 504) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.21 0.94 1.09 4.64 1.00 2581 3553 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.79 0.95 -1.07 2.82 1.00 2492 2462 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now recall that our competing cross-classified model, b12.5, added random effects for the trial blocks. Here was that formula. b12.5$formula ## pulled_left | trials(1) ~ prosoc_left + (1 | actor) + (1 | block) + prosoc_left:condition And, of course, we can retrieve the data from that model, too. b12.5$data %&gt;% glimpse() ## Observations: 504 ## Variables: 5 ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ actor &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ block &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5,… It’s the same data we used from the b12.4 model, but with the addition of the block index. With those data in hand, we can fit the intercepts-only version of our cross-classified model. This model formula follows the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\operatorname{Binomial} (n_i = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block}_i}\\\\ \\alpha &amp; \\sim \\operatorname{Normal} (0, 10) \\\\ \\alpha_{\\text{actor}} &amp; \\sim \\operatorname{Normal} (0, \\sigma_{\\text{actor}}) \\\\ \\alpha_{\\text{block}} &amp; \\sim \\operatorname{Normal} (0, \\sigma_{\\text{block}}) \\\\ \\sigma_{\\text{actor}} &amp; \\sim \\operatorname{HalfCauchy} (0, 1) \\\\ \\sigma_{\\text{block}} &amp; \\sim \\operatorname{HalfCauchy} (0, 1). \\end{align*}\\] Fit the model. b12.8 &lt;- brm(data = b12.5$data, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95), seed = 12, file = &quot;fits/b12.08&quot;) Here’s the summary. print(b12.8) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) ## Data: b12.5$data (Number of observations: 504) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.22 0.92 1.09 4.56 1.00 4627 6606 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.22 0.18 0.01 0.66 1.00 5973 7861 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.78 0.94 -1.01 2.80 1.00 4150 6049 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ve fit our two intercepts-only models, let’s get to the heart of this section. We are going to practice four methods for working with the posterior samples. Each method will revolve around a different primary function. In order, they are brms::posterior_samples(), brms::coef(), brms::fitted(), and tidybayes::spread_draws(). We’ve already had some practice with the first three, but I hope this section will make them even more clear. The tidybayes::spread_draws() method will be new, to us. I think you’ll find it’s a handy alternative. With each of the four methods, we’ll practice three different model summaries: getting the posterior draws for the actor-level estimates from the b12.7 model; getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block; and getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1. So to be clear, our goal is to accomplish those three tasks with four methods, each of which should yield equivalent results. 12.5.2 brms::posterior_samples(). To warm up, let’s take a look at the structure of the posterior_samples() output for the simple b12.7 model. posterior_samples(b12.7) %&gt;% str() ## &#39;data.frame&#39;: 16000 obs. of 10 variables: ## $ b_Intercept : num -0.76 -0.64 -0.786 -0.274 -0.367 ... ## $ sd_actor__Intercept : num 1.89 1.5 2.35 2.1 1.98 ... ## $ r_actor[1,Intercept]: num 0.191 0.311 0.328 0.123 -0.167 ... ## $ r_actor[2,Intercept]: num 5.19 3.86 5.27 4.98 4.32 ... ## $ r_actor[3,Intercept]: num -0.0268 0.2526 0.3228 -0.5455 0.0647 ... ## $ r_actor[4,Intercept]: num 0.294 0.296 -0.18 -0.315 -0.166 ... ## $ r_actor[5,Intercept]: num 0.35 0.658 0.294 0.164 0.099 ... ## $ r_actor[6,Intercept]: num 1.46 0.951 1.713 1.044 0.772 ... ## $ r_actor[7,Intercept]: num 3.17 2.41 3.59 2.77 1.73 ... ## $ lp__ : num -282 -285 -283 -280 -282 ... The b_Intercept vector corresponds to the \\(\\alpha\\) term in the statistical model. The second vector, sd_actor__Intercept, corresponds to the \\(\\sigma_{\\text{actor}}\\) term. And the next 7 vectors beginning with the r_actor suffix are the \\(\\alpha_{\\text{actor}}\\) deviations from the grand mean, \\(\\alpha\\). Thus if we wanted to get the model-implied probability for our first chimp, we’d add b_Intercept to r_actor[1,Intercept] and then take the inverse logit. posterior_samples(b12.7) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = (b_Intercept + `r_actor[1,Intercept]`) %&gt;% inv_logit_scaled()) %&gt;% head() ## chimp 1&#39;s average probability of pulling left ## 1 0.3615339 ## 2 0.4185300 ## 3 0.3876797 ## 4 0.4621443 ## 5 0.3696150 ## 6 0.3832393 To complete our first task, then, of getting the posterior draws for the actor-level estimates from the b12.7 model, we can do that in bulk. p1 &lt;- posterior_samples(b12.7) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p1) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.362 0.419 0.388 0.462 0.37 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.988 0.962 0.989 0.991 0.981 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.313 0.404 0.386 0.306 0.425 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.386 0.415 0.276 0.357 0.37 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.399 0.504 0.379 0.472 0.433 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.668 0.577 0.717 0.683 0.6 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.918 0.854 0.943 0.924 0.796 ... One of the things I really like about this method is the b_Intercept + r_actor[i,Intercept] part of the code makes it very clear, to me, how the porterior_samples() columns correspond to the statistical model, \\(\\text{logit} (p_i) = \\alpha + \\alpha_{\\text{actor}_i}\\). This method easily extends to our next task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. In fact, other than switching out b12.7 for b12.8, the method is identical. p2 &lt;- posterior_samples(b12.8) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p2) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.343 0.459 0.522 0.338 0.481 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.984 0.99 0.995 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.382 0.317 0.394 0.359 0.295 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.257 0.438 0.299 0.318 0.367 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.502 0.358 0.399 0.411 0.374 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.746 0.641 0.702 0.625 0.671 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.907 0.873 0.891 0.909 0.923 ... The reason we can still get away with this is because the grand mean in the b12.8 model is the grand mean across all levels of actor and block. AND it’s the case that the r_actor and r_block vectors returned by posterior_samples(b12.8) are all in deviation metrics–execute posterior_samples(b12.8) %&gt;% glimpse() if it will help you follow along. So if we simply leave out the r_block vectors, we are ignoring the specific block-level deviations, effectively averaging over them. Now for our third task, we’ve decided we wanted to retrieve the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1. To get the chimp-specific estimates for the first block, we simply add + r_block[1,Intercept] to the end of each formula. p3 &lt;- posterior_samples(b12.8) %&gt;% transmute(`chimp 1&#39;s average probability of pulling left` = b_Intercept + `r_actor[1,Intercept]` + `r_block[1,Intercept]`, `chimp 2&#39;s average probability of pulling left` = b_Intercept + `r_actor[2,Intercept]` + `r_block[1,Intercept]`, `chimp 3&#39;s average probability of pulling left` = b_Intercept + `r_actor[3,Intercept]` + `r_block[1,Intercept]`, `chimp 4&#39;s average probability of pulling left` = b_Intercept + `r_actor[4,Intercept]` + `r_block[1,Intercept]`, `chimp 5&#39;s average probability of pulling left` = b_Intercept + `r_actor[5,Intercept]` + `r_block[1,Intercept]`, `chimp 6&#39;s average probability of pulling left` = b_Intercept + `r_actor[6,Intercept]` + `r_block[1,Intercept]`, `chimp 7&#39;s average probability of pulling left` = b_Intercept + `r_actor[7,Intercept]` + `r_block[1,Intercept]`) %&gt;% mutate_all(inv_logit_scaled) str(p3) ## &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.334 0.464 0.372 0.304 0.45 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.971 0.988 0.994 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.373 0.321 0.261 0.324 0.27 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.25 0.443 0.188 0.286 0.338 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.492 0.362 0.265 0.374 0.346 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.739 0.646 0.561 0.588 0.643 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.903 0.875 0.817 0.895 0.913 ... Again, I like this method because of how close the wrangling code within transmute() is to the statistical model formula. I wrote a lot of code like this in my early days of working with these kinds of models, and I think the pedagogical insights were helpful. But this method has its limitations. It works fine if you’re working with some small number of groups. But that’s a lot of repetitious code and it would be utterly un-scalable to situations where you have 50 or 500 levels in your grouping variable. We need alternatives. 12.5.3 brms::coef(). First, let’s review what the coef() function returns. coef(b12.7) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.3251634 0.2398378 -0.8051655 0.1367131 ## 2 4.8694192 1.5994943 2.8433952 8.8152085 ## 3 -0.6150724 0.2479640 -1.1074610 -0.1390830 ## 4 -0.6156542 0.2464290 -1.1052761 -0.1422099 ## 5 -0.3245951 0.2389499 -0.7966241 0.1476559 ## 6 0.5815257 0.2410089 0.1218875 1.0735606 ## 7 2.0743520 0.3692175 1.3982606 2.8434187 By default, we get the familiar summaries for mean performances for each of our seven chimps. These, of course, are in the log-odds metric and simply tacking on inv_logit_scaled() isn’t going to fully get the job done if we want posterior standard deviations. So to get things in the probability metric, we’ll want to first set summary = F in order to work directly with un-summarized samples and then wrangle quite a bit. Part of the wrangling challenge is because coef() returns a list, rather than a data frame. With that in mind, the code for our first task of getting the posterior draws for the actor-level estimates from the b12.7 model looks like so. c1 &lt;- coef(b12.7, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), # we need an iteration index for `spread()` to work properly iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.362 0.419 0.388 0.462 0.37 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.988 0.962 0.989 0.991 0.981 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.313 0.404 0.386 0.306 0.425 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.386 0.415 0.276 0.357 0.37 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.399 0.504 0.379 0.472 0.433 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.668 0.577 0.717 0.683 0.6 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.918 0.854 0.943 0.924 0.796 ... So with this method, you get a little practice with three-dimensional indexing, which is a good skill to have. Now let’s extend it to our second task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. c2 &lt;- coef(b12.8, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.343 0.459 0.522 0.338 0.481 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.984 0.99 0.995 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.382 0.317 0.394 0.359 0.295 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.257 0.438 0.299 0.318 0.367 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.502 0.358 0.399 0.411 0.374 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.746 0.641 0.702 0.625 0.671 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.907 0.873 0.891 0.909 0.923 ... As with our posterior_samples() method, this code was near identical to the block, above. All we did was switch out b12.7 for b12.8. [Okay, we removed a line of annotations. But that doesn’t really count.] We should point something out, though. Consider what coef() yields when working with a cross-classified model. coef(b12.8) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.3272676 0.2680123 -0.85099713 0.19654934 ## 2 4.8766907 1.5481365 2.82952309 8.70212556 ## 3 -0.6239696 0.2756037 -1.16300948 -0.07969651 ## 4 -0.6215705 0.2774583 -1.17011895 -0.08768968 ## 5 -0.3225477 0.2693271 -0.85050319 0.20408084 ## 6 0.5874763 0.2726324 0.06104288 1.13988240 ## 7 2.0898762 0.3909642 1.37507305 2.89198477 ## ## ## $block ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.5937507 0.9537021 -1.2417019 2.624202 ## 2 0.8347826 0.9441717 -0.9620358 2.874522 ## 3 0.8344098 0.9418143 -0.9589585 2.895764 ## 4 0.7613565 0.9409664 -1.0331506 2.780971 ## 5 0.7624919 0.9422556 -1.0428255 2.796276 ## 6 0.8834536 0.9454122 -0.9048657 2.938581 Now we have a list of two elements, one for actor and one for block. What might not be immediately obvious is that the summaries returned by one grouping level are based off of averaging over the other. Although this made our second task easy, it provides a challenge for our third task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1. To accomplish that, we’ll need to bring in ranef(). Let’s review what that returns. ranef(b12.8) ## $actor ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.1050136 0.9549622 -3.151913 0.6921519 ## 2 4.0989448 1.6100072 1.759933 7.9470579 ## 3 -1.4017156 0.9570086 -3.460878 0.3851861 ## 4 -1.3993164 0.9578458 -3.468069 0.4029023 ## 5 -1.1002937 0.9543137 -3.129181 0.7170250 ## 6 -0.1902697 0.9523662 -2.207368 1.6210836 ## 7 1.3121302 0.9776822 -0.727854 3.1851834 ## ## ## $block ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.18399525 0.2286488 -0.7543671 0.1203472 ## 2 0.05703663 0.1869749 -0.2997584 0.4958983 ## 3 0.05666392 0.1877072 -0.2928721 0.4955291 ## 4 -0.01638938 0.1821434 -0.4248867 0.3555710 ## 5 -0.01525401 0.1827458 -0.4140817 0.3666592 ## 6 0.10570764 0.1961558 -0.2181223 0.5745469 The format of the ranef() output is identical to that from coef(). However, the summaries are in the deviance metric. They’re all centered around zero, which corresponds to the part of the statistical model that specifies how \\(\\alpha_{\\text{block}} \\sim \\text{Normal} (0, \\sigma_{\\text{block}})\\). So then, if we want to continue using our coef() method, we’ll need to augment it with ranef() to accomplish our last task. c3 &lt;- coef(b12.8, summary = F)$actor[, , ] %&gt;% as_tibble() %&gt;% gather() %&gt;% # here we add in the `block == 1` deviations from the grand mean mutate(value = value + ranef(b12.8, summary = F)$block[, 1, ] %&gt;% rep(., times = 7)) %&gt;% mutate(key = str_c(&quot;chimp &quot;, key, &quot;&#39;s average probability of pulling left&quot;), value = inv_logit_scaled(value), iter = rep(1:16000, times = 7)) %&gt;% spread(key = key, value = value) str(c3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.334 0.464 0.372 0.304 0.45 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.971 0.988 0.994 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.373 0.321 0.261 0.324 0.27 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.25 0.443 0.188 0.286 0.338 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.492 0.362 0.265 0.374 0.346 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.739 0.646 0.561 0.588 0.643 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.903 0.875 0.817 0.895 0.913 ... One of the nicest things about the coef() method is how is scales well. This code is no more burdensome for 5 group levels than it is for 5000. It’s also a post-processing version of the distinction McElreath made on page 372 between the two equivalent ways you might define a Gaussian: \\[\\operatorname{Normal}(10, 1)\\] and \\[10 + \\operatorname{Normal}(0, 1).\\] Conversely, it can be a little abstract. Let’s keep expanding our options. 12.5.4 brms::fitted(). As is often the case, we’re going to want to define our predictor values for fitted(). (nd &lt;- b12.7$data %&gt;% distinct(actor)) ## actor ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 Now we have our new data, nd, here’s how we might use fitted() to accomplish our first task, getting the posterior draws for the actor-level estimates from the b12.7 model. f1 &lt;- fitted(b12.7, newdata = nd, summary = F, # within `fitted()`, this line does the same work that # `inv_logit_scaled()` did with the other two methods scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.362 0.419 0.388 0.462 0.37 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.988 0.962 0.989 0.991 0.981 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.313 0.404 0.386 0.306 0.425 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.386 0.415 0.276 0.357 0.37 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.399 0.504 0.379 0.472 0.433 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.668 0.577 0.717 0.683 0.6 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.918 0.854 0.943 0.924 0.796 ... This scales reasonably well. But might not work well if the vectors you wanted to rename didn’t follow a serial order, like ours. If you’re willing to pay with a few more lines of wrangling code, this method is more general, but still scalable. f1 &lt;- fitted(b12.7, newdata = nd, summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% # you&#39;ll need this line to make the `spread()` line work properly mutate(iter = 1:n()) %&gt;% gather(key, value, -iter) %&gt;% mutate(key = str_replace(key, &quot;V&quot;, &quot;chimp &quot;)) %&gt;% mutate(key = str_c(key, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(key = key, value = value) str(f1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ iter : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.362 0.419 0.388 0.462 0.37 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.988 0.962 0.989 0.991 0.981 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.313 0.404 0.386 0.306 0.425 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.386 0.415 0.276 0.357 0.37 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.399 0.504 0.379 0.472 0.433 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.668 0.577 0.717 0.683 0.6 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.918 0.854 0.943 0.924 0.796 ... Now unlike with the previous two methods, our fitted() method will not allow us to simply switch out b12.7 for b12.8 to accomplish our second task of getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. This is because when we use fitted() in combination with its newdata argument, the function expects us to define values for all the predictor variables in the formula. Because the b12.8 model has both actor and block grouping variables as predictors, the default requires we include both in our new data. But if we were to specify a value for block in the nd data, we would no longer be averaging over the levels of block anymore; we’d be selecting one of the levels of block in particular, which we don’t yet want to do. Happily, brms::fitted() has a re_formula argument. If we would like to average out block, we simply drop it from the formula. Here’s how to do so. f2 &lt;- fitted(b12.8, newdata = nd, # this line allows us to average over the levels of `block` re_formula = pulled_left ~ 1 + (1 | actor), summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.343 0.459 0.522 0.338 0.481 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.984 0.99 0.995 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.382 0.317 0.394 0.359 0.295 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.257 0.438 0.299 0.318 0.367 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.502 0.358 0.399 0.411 0.374 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.746 0.641 0.702 0.625 0.671 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.907 0.873 0.891 0.909 0.923 ... If we want to use fitted() for our third task of getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, based on block == 1, we’ll need to augment our nd data. ( nd &lt;- b12.8$data %&gt;% distinct(actor) %&gt;% mutate(block = 1) ) ## actor block ## 1 1 1 ## 2 2 1 ## 3 3 1 ## 4 4 1 ## 5 5 1 ## 6 6 1 ## 7 7 1 This time, we no longer need that re_formula argument. f3 &lt;- fitted(b12.8, newdata = nd, summary = F, scale = &quot;response&quot;) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;chimp &quot;, 1:7, &quot;&#39;s average probability of pulling left&quot;)) str(f3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 7 variables: ## $ chimp 1&#39;s average probability of pulling left: num 0.334 0.464 0.372 0.304 0.45 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.971 0.988 0.994 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.373 0.321 0.261 0.324 0.27 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.25 0.443 0.188 0.286 0.338 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.492 0.362 0.265 0.374 0.346 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.739 0.646 0.561 0.588 0.643 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.903 0.875 0.817 0.895 0.913 ... Let’s learn one more option. 12.5.5 tidybayes::spread_draws(). Up till this point, we’ve really only used the tidybayes package for plotting (e.g., with geom_halfeyeh()) and summarizing (e.g., with median_qi()). But tidybayes is more general; it offers a handful of convenience functions for wrangling posterior draws from a tidyverse perspective. One such function is spread_draws(), which you can learn all about in Matthew Kay’s vignette, Extracting and visualizing tidy draws from brms models. Let’s take a look at how we’ll be using it. b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) ## # A tibble: 112,000 x 6 ## # Groups: actor [7] ## .chain .iteration .draw b_Intercept actor r_actor ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 -0.760 1 0.191 ## 2 1 1 1 -0.760 2 5.19 ## 3 1 1 1 -0.760 3 -0.0268 ## 4 1 1 1 -0.760 4 0.294 ## 5 1 1 1 -0.760 5 0.350 ## 6 1 1 1 -0.760 6 1.46 ## 7 1 1 1 -0.760 7 3.17 ## 8 1 2 2 -0.640 1 0.311 ## 9 1 2 2 -0.640 2 3.86 ## 10 1 2 2 -0.640 3 0.253 ## # … with 111,990 more rows First, notice tidybayes::spread_draws() took the model fit itself, b12.7, as input. No need for posterior_samples(). Now, notice we fed it two additional arguments. By the first argument, we that requested spead_draws() extract the posterior samples for the b_Intercept. By the second argument, r_actor[actor,], we instructed spead_draws() to extract all the random effects for the actor variable. Also notice how within the brackets [] we specified actor, which then became the name of the column in the output that indexed the levels of the grouping variable actor. By default, the code returns the posterior samples for all the levels of actor. However, had we only wanted those from chimps #1 and #3, we might use typical tidyverse-style indexing. E.g., b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% filter(actor %in% c(1, 3)) ## # A tibble: 32,000 x 6 ## # Groups: actor [2] ## .chain .iteration .draw b_Intercept actor r_actor ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 -0.760 1 0.191 ## 2 1 1 1 -0.760 3 -0.0268 ## 3 1 2 2 -0.640 1 0.311 ## 4 1 2 2 -0.640 3 0.253 ## 5 1 3 3 -0.786 1 0.328 ## 6 1 3 3 -0.786 3 0.323 ## 7 1 4 4 -0.274 1 0.123 ## 8 1 4 4 -0.274 3 -0.546 ## 9 1 5 5 -0.367 1 -0.167 ## 10 1 5 5 -0.367 3 0.0647 ## # … with 31,990 more rows Also notice those first three columns. By default, spread_draws() extracted information about which Markov chain a given draw was from, which iteration a given draw was within a given chain, and which draw from an overall standpoint. If it helps to keep track of which vector indexed what, consider this. b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% ungroup() %&gt;% select(.chain:.draw) %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(min = min(value), max = max(value)) ## # A tibble: 3 x 3 ## key min max ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 .chain 1 4 ## 2 .draw 1 16000 ## 3 .iteration 1 4000 Above we simply summarized each of the three variables by their minimum and maximum values. If you recall that we fit b12.7 with four Markov chains, each with 4,000 post-warmup iterations, hopefully it’ll make sense what each of those three variables index. Now we’ve done a little clarification, let’s use spread_draws() to accomplish our first task, getting the posterior draws for the actor-level estimates from the b12.7 model. s1 &lt;- b12.7 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% mutate(p = (b_Intercept + r_actor) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) str(s1) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.362 0.419 0.388 0.462 0.37 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.988 0.962 0.989 0.991 0.981 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.313 0.404 0.386 0.306 0.425 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.386 0.415 0.276 0.357 0.37 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.399 0.504 0.379 0.472 0.433 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.668 0.577 0.717 0.683 0.6 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.918 0.854 0.943 0.924 0.796 ... The method remains essentially the same for accomplishing our second task, getting the posterior draws for the actor-level estimates from the cross-classified b12.8 model, averaging over the levels of block. s2 &lt;- b12.8 %&gt;% spread_draws(b_Intercept, r_actor[actor,]) %&gt;% mutate(p = (b_Intercept + r_actor) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) str(s2) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 8 variables: ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.343 0.459 0.522 0.338 0.481 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.984 0.99 0.995 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.382 0.317 0.394 0.359 0.295 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.257 0.438 0.299 0.318 0.367 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.502 0.358 0.399 0.411 0.374 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.746 0.641 0.702 0.625 0.671 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.907 0.873 0.891 0.909 0.923 ... To accomplish our third task, we augment the spread_draws() and first mutate() lines, and add a filter() line between them. s3 &lt;- b12.8 %&gt;% spread_draws(b_Intercept, r_actor[actor,], r_block[block,]) %&gt;% filter(block == 1) %&gt;% mutate(p = (b_Intercept + r_actor + r_block) %&gt;% inv_logit_scaled()) %&gt;% select(.draw, actor, p) %&gt;% ungroup() %&gt;% mutate(actor = str_c(&quot;chimp &quot;, actor, &quot;&#39;s average probability of pulling left&quot;)) %&gt;% spread(value = p, key = actor) str(s3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 16000 obs. of 9 variables: ## $ block : int 1 1 1 1 1 1 1 1 1 1 ... ## $ .draw : int 1 2 3 4 5 6 7 8 9 10 ... ## $ chimp 1&#39;s average probability of pulling left: num 0.334 0.464 0.372 0.304 0.45 ... ## $ chimp 2&#39;s average probability of pulling left: num 0.998 0.975 0.971 0.988 0.994 ... ## $ chimp 3&#39;s average probability of pulling left: num 0.373 0.321 0.261 0.324 0.27 ... ## $ chimp 4&#39;s average probability of pulling left: num 0.25 0.443 0.188 0.286 0.338 ... ## $ chimp 5&#39;s average probability of pulling left: num 0.492 0.362 0.265 0.374 0.346 ... ## $ chimp 6&#39;s average probability of pulling left: num 0.739 0.646 0.561 0.588 0.643 ... ## $ chimp 7&#39;s average probability of pulling left: num 0.903 0.875 0.817 0.895 0.913 ... Hopefully working through these examples gave you some insight on the relation between fixed and random effects within multilevel models, and perhaps added to your posterior-iteration-wrangling toolkit. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.7.1 tidybayes_2.0.1.9000 patchwork_1.0.0 ggthemes_4.2.0 ## [5] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 purrr_0.3.3 ## [9] readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 tidyverse_1.3.0 ## [13] brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ggplot2_3.2.1 ## [17] StanHeaders_2.19.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] jsonlite_1.6.1 broom_0.5.3 dbplyr_1.4.2 ## [22] shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 htmltools_0.4.0 prettyunits_1.1.1 ## [34] tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [43] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [46] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 ## [52] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 loo_2.2.0 stringi_1.4.5 ## [64] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [67] rlang_0.4.4 pkgconfig_2.0.3 matrixStats_0.55.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5.1 labeling_0.3 processx_3.4.1 ## [76] tidyselect_1.0.0 plyr_1.8.5 magrittr_1.5 ## [79] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [82] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [85] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [94] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [97] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [100] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 "],
["adventures-in-covariance.html", "13 Adventures in Covariance 13.1 Varying slopes by construction 13.2 Example: Admission decisions and gender 13.3 Example: Cross-classified chimpanzees with varying slopes 13.4 Continuous categories and the Gaussian process 13.5 Summary Bonus: Another Berkley-admissions-data-like example. Reference Session info", " 13 Adventures in Covariance In this chapter, you’ll see how to… specify varying slopes in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. It will also improve estimates of intercepts, by borrowing information across parameter types. Essentially, varying slopes models are massive interaction machines. They allow every unit in the data to have its own unique response to any treatment or exposure or event, while also improving estimates via pooling. When the variation in slopes is large, the average slope is of less interest. Sometimes, the pattern of variation in slopes provides hints about omitted variables that explain why some units respond more or less. We’ll see an example in this chapter. The machinery that makes such complex varying effects possible will be used later in the chapter to extend the varying effects strategy to more subtle model types, including the use of continuous categories, using Gaussian process. (p. 388, emphasis in the original) 13.1 Varying slopes by construction How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 389) 13.1.0.1 Rethinking: Why Gaussian? McElreath discussed how researchers might use other multivariate distributions to model multiple random effects. The only one he named as an alternative to the Gaussian was the multivariate Student’s \\(t\\). As it turns out, brms does currently allow users to use multivariate Student’s \\(t\\) in this way. For details, check out this discussion from the brms GitHub repository. Bürkner’s exemplar syntax from his comment on May 13, 2018, was y ~ x + (x | gr(g, dist = &quot;student&quot;)). I haven’t experimented with this, but if you do, do consider sharing how it went. 13.1.1 Simulate the population. If you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done some nice work along these lines, I have a blog series on Bayesian power analysis, and Kruschke covered the topic in Chapter 13 of his text. a &lt;- 3.5 # average morning wait time b &lt;- -1 # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- -.7 # correlation between intercepts and slopes # the next three lines of code simply combine the terms, above mu &lt;- c(a, b) cov_ab &lt;- sigma_a * sigma_b * rho sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) If you haven’t used matirx() before, you might get a sense of the elements like so. matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 This next block of code will finally yield our café data. library(tidyverse) sigmas &lt;- c(sigma_a, sigma_b) # standard deviations rho &lt;- matrix(c(1, rho, # correlation matrix rho, 1), nrow = 2) # now matrix multiply to get covariance matrix sigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas) # how many cafes would you like? n_cafes &lt;- 20 set.seed(13) # used to replicate example vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) %&gt;% data.frame() %&gt;% set_names(&quot;a_cafe&quot;, &quot;b_cafe&quot;) head(vary_effects) ## a_cafe b_cafe ## 1 2.917639 -0.8649154 ## 2 3.552770 -1.6814372 ## 3 1.694390 -0.4168858 ## 4 3.442417 -0.6011724 ## 5 2.289988 -0.7461953 ## 6 3.069283 -0.8839639 Let’s make sure we’re keeping this all straight. a_cafe = our café-specific intercepts; b_cafe = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask What’s the distribution of a_cafe and b_cafe? Our variant of Figure 13.2 contains the answer. For our plots in this chapter, we’ll make our own custom ggplot2 theme. The color palette will come from the “pearl_earring” palette of the dutchmasters package. You can learn more about the original painting, Vermeer’s Girl with a Pearl Earring, here. # devtools::install_github(&quot;EdwinTh/dutchmasters&quot;) library(dutchmasters) dutchmasters$pearl_earring ## red(lips) skin blue(scarf1) blue(scarf2) white(colar) ## &quot;#A65141&quot; &quot;#E7CDC2&quot; &quot;#80A0C7&quot; &quot;#394165&quot; &quot;#FCF9F0&quot; ## gold(dress) gold(dress2) black(background) grey(scarf3) yellow(scarf4) ## &quot;#B1934A&quot; &quot;#DCA258&quot; &quot;#100F14&quot; &quot;#8B9DAF&quot; &quot;#EEDA9D&quot; ## ## &quot;#E8DCCF&quot; We’ll name our custom theme theme_pearl_earring(). I cobbled together this approach to defining a custom ggplot2 theme with help from Chapter 16 of Wichkam’s ggplot2: Elegant Graphics for Data Analysis; Chapter 4.6 of Peng, Kross, and Anderson’s Mastering Software Development in R; Lea Waniek’s blog post, Custom themes in ggplot2, and Joey Stanley’s blog post Custom Themes in ggplot2. theme_pearl_earring &lt;- function(light_color = &quot;#E8DCCF&quot;, dark_color = &quot;#100F14&quot;, my_family = &quot;Courier&quot;, ...) { theme(line = element_line(color = light_color), text = element_text(color = light_color, family = my_family), strip.text = element_text(color = light_color, family = my_family), axis.text = element_text(color = light_color), axis.ticks = element_line(color = light_color), axis.line = element_blank(), legend.background = element_rect(fill = dark_color, color = &quot;transparent&quot;), legend.key = element_rect(fill = dark_color, color = &quot;transparent&quot;), panel.background = element_rect(fill = dark_color, color = light_color), panel.grid = element_blank(), plot.background = element_rect(fill = dark_color, color = dark_color), strip.background = element_rect(fill = dark_color, color = &quot;transparent&quot;), ...) } Note how our custom theme_pearl_earing() function has a few adjustable parameters. Feel free to play around with alternative settings to see how they work. If we just use the defaults as we have defined them, here is our Figure 13.2. vary_effects %&gt;% ggplot(aes(x = a_cafe, y = b_cafe)) + geom_point(color = &quot;#80A0C7&quot;) + geom_rug(color = &quot;#8B9DAF&quot;, size = 1/7) + theme_pearl_earring() Again, these are not “data.” Figure 13.2 shows a distribution of parameters. Here’s their Pearson’s correlation coefficient. cor(vary_effects$a_cafe, vary_effects$b_cafe) ## [1] -0.7281604 13.1.2 Simulate observations. Here we put those simulated parameters to use and simulate actual data from them. n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes set.seed(13) # used to replicate example d &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = a_cafe + b_cafe * afternoon) %&gt;% mutate(wait = rnorm(n = n(), mean = mu, sd = sigma)) We might peek at the data. d %&gt;% head() ## # A tibble: 6 x 7 ## cafe a_cafe b_cafe visit afternoon mu wait ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.92 -0.865 1 0 2.92 3.19 ## 2 1 2.92 -0.865 2 1 2.05 1.91 ## 3 1 2.92 -0.865 3 0 2.92 3.81 ## 4 1 2.92 -0.865 4 1 2.05 2.15 ## 5 1 2.92 -0.865 5 0 2.92 3.49 ## 6 1 2.92 -0.865 6 1 2.05 2.26 Now we’ve finally simulated our data, we are ready to make our version of Figure 13.1, from way back on page 388. d %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;M&quot;, &quot;A&quot;), day = rep(rep(1:5, each = 2), times = n_cafes)) %&gt;% filter(cafe %in% c(3, 5)) %&gt;% mutate(cafe = ifelse(cafe == 3, &quot;cafe #3&quot;, &quot;cafe #5&quot;)) %&gt;% ggplot(aes(x = visit, y = wait, group = day)) + geom_point(aes(color = afternoon), size = 2) + geom_line(color = &quot;#8B9DAF&quot;) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(NULL, breaks = 1:10, labels = rep(c(&quot;M&quot;, &quot;A&quot;), times = 5)) + scale_y_continuous(&quot;wait time in minutes&quot;, limits = c(0, 4)) + theme_pearl_earring(legend.position = &quot;none&quot;, axis.ticks.x = element_blank()) + facet_wrap(~cafe, ncol = 1) 13.1.3 The varying slopes model. The statistical formula for our varying-slopes model follows the form \\[\\begin{align*} \\text{wait}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{cafe}_i} + \\beta_{\\text{cafe}_i} \\text{afternoon}_i \\\\ \\begin{bmatrix} \\alpha_\\text{cafe} \\\\ \\beta_\\text{cafe} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\alpha &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\sigma_\\beta &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\mathbf R &amp; \\sim \\text{LKJcorr} (2), \\end{align*}\\] where \\(\\mathbf S\\) is the covariance matrix and \\(\\mathbf R\\) is the corresponding correlation matrix, which we might more fully express as \\[\\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix}.\\] And according to our prior, \\(\\mathbf R\\) is distributed as \\(\\text{LKJcorr} (2)\\). We’ll use rethinking::rlkjcorr() to get a better sense of what that even is. library(rethinking) n_sim &lt;- 1e5 set.seed(13) r_1 &lt;- rlkjcorr(n_sim, K = 2, eta = 1) %&gt;% as_tibble() set.seed(13) r_2 &lt;- rlkjcorr(n_sim, K = 2, eta = 2) %&gt;% as_tibble() set.seed(13) r_4 &lt;- rlkjcorr(n_sim, K = 2, eta = 4) %&gt;% as_tibble() Here are the \\(\\text{LKJcorr}\\) distributions of Figure 13.3. ggplot(data = r_1, aes(x = V2)) + geom_density(color = &quot;transparent&quot;, fill = &quot;#DCA258&quot;, alpha = 2/3) + geom_density(data = r_2, color = &quot;transparent&quot;, fill = &quot;#FCF9F0&quot;, alpha = 2/3) + geom_density(data = r_4, color = &quot;transparent&quot;, fill = &quot;#394165&quot;, alpha = 2/3) + geom_text(data = tibble(x = c(.83, .62, .46), y = c(.54, .74, 1), label = c(&quot;eta = 1&quot;, &quot;eta = 2&quot;, &quot;eta = 4&quot;)), aes(x = x, y = y, label = label), color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring() As it turns out, the shape of the LKJ is sensitive to both \\(\\eta\\) and the \\(K\\) dimensions of the correlation matrix. Our simulations only considered the shapes for when \\(K = 2\\). We can use a combination of the parse_dist() and stat_dist_halfeyeh() functions from the tidybayes package to derive analytic solutions for different combinations of \\(\\eta\\) and \\(K\\). library(tidybayes) crossing(k = 2:5, eta = 1:4) %&gt;% mutate(prior = str_c(&quot;lkjcorr_marginal(&quot;, k, &quot;, &quot;, eta, &quot;)&quot;), strip = str_c(&quot;K==&quot;, k)) %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = eta, dist = .dist, args = .args)) + stat_dist_halfeyeh(.width = c(.5, .95), color = &quot;#FCF9F0&quot;, fill = &quot;#A65141&quot;) + scale_x_continuous(expression(rho), limits = c(-1, 1), breaks = c(-1, -.5, 0, .5, 1), labels = c(&quot;-1&quot;, &quot;-.5&quot;, &quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(expression(eta), breaks = 1:4) + ggtitle(expression(&quot;Marginal correlation for the LKJ prior relative to K and &quot;*eta)) + theme_pearl_earring() + facet_wrap(~strip, labeller = label_parsed, ncol = 4) To learn more about this method, check out Kay’s Marginal distribution of a single correlation from an LKJ distribution. Okay, let’s get ready to model and switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) As defined above, our first model has both varying intercepts and afternoon slopes. I should point out that the (1 + afternoon | cafe) syntax specifies that we’d like brm() to fit the random effects for 1 (i.e., the intercept) and the afternoon slope as correlated. Had we wanted to fit a model in which they were orthogonal, we’d have coded (1 + afternoon || cafe). b13.1 &lt;- brm(data = d, family = gaussian, wait ~ 1 + afternoon + (1 + afternoon | cafe), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sd), prior(cauchy(0, 2), class = sigma), prior(lkj(2), class = cor)), iter = 5000, warmup = 2000, chains = 2, cores = 2, seed = 13, file = &quot;fits/b13.01&quot;) With Figure 13.4, we assess how the posterior for the correlation of the random effects compares to its prior. post &lt;- posterior_samples(b13.1) post %&gt;% ggplot() + geom_density(data = r_2, aes(x = V2), color = &quot;transparent&quot;, fill = &quot;#EEDA9D&quot;, alpha = 3/4) + geom_density(aes(x = cor_cafe__Intercept__afternoon), color = &quot;transparent&quot;, fill = &quot;#A65141&quot;, alpha = 9/10) + annotate(geom = &quot;text&quot;, x = -0.35, y = 2.2, label = &quot;posterior&quot;, color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + annotate(geom = &quot;text&quot;, x = 0, y = 0.9, label = &quot;prior&quot;, color = &quot;#EEDA9D&quot;, alpha = 2/3, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;correlation&quot;) + theme_pearl_earring() McElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the cafe-specific intercepts and afternoon slopes with coef(), which returns a three-dimensional list. # coef(b13.1) %&gt;% glimpse() coef(b13.1) ## $cafe ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 3.257478 0.2035942 2.852822 3.656762 ## 2 3.165662 0.2087660 2.763492 3.579353 ## 3 1.615157 0.2060121 1.222783 2.016879 ## 4 3.335387 0.1964793 2.949353 3.717637 ## 5 2.296397 0.2063222 1.887291 2.698056 ## 6 3.148887 0.2017595 2.740449 3.536169 ## 7 2.648014 0.2090663 2.241426 3.053719 ## 8 3.642193 0.1988416 3.245539 4.027335 ## 9 4.117556 0.2028759 3.724034 4.515102 ## 10 2.322228 0.2080664 1.910779 2.728812 ## 11 4.444412 0.2075830 4.037092 4.841307 ## 12 2.833985 0.2002205 2.439787 3.228190 ## 13 4.653762 0.2050913 4.249860 5.054741 ## 14 5.535245 0.2214844 5.115992 5.971740 ## 15 3.561178 0.1994760 3.170220 3.949092 ## 16 3.485721 0.1980807 3.093399 3.871864 ## 17 2.341366 0.1987429 1.943998 2.730567 ## 18 3.959822 0.2078971 3.560317 4.367699 ## 19 3.500621 0.1999719 3.104086 3.895698 ## 20 3.168557 0.2026318 2.764338 3.566792 ## ## , , afternoon ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.9075166 0.2096445 -1.3618266 -0.511335490 ## 2 -1.0466650 0.2429569 -1.5658734 -0.631545880 ## 3 -0.4292542 0.2328915 -0.8855643 0.024001165 ## 4 -0.7421363 0.2071748 -1.1324862 -0.311491345 ## 5 -0.5481318 0.2163228 -0.9617509 -0.120817780 ## 6 -0.7952144 0.2086028 -1.2129496 -0.368283468 ## 7 -0.4905239 0.2285836 -0.8967930 -0.007445871 ## 8 -0.8224418 0.2090859 -1.2171772 -0.390551849 ## 9 -1.0802684 0.2129073 -1.5259491 -0.678236588 ## 10 -0.4258400 0.2287290 -0.8410538 0.062666711 ## 11 -1.0932314 0.2210538 -1.5331957 -0.660440952 ## 12 -0.7616165 0.2098765 -1.1865653 -0.360890306 ## 13 -1.2126147 0.2234728 -1.6645061 -0.787262410 ## 14 -1.5344024 0.2712118 -2.0737942 -1.023297217 ## 15 -0.9242634 0.2080521 -1.3465731 -0.521195971 ## 16 -0.7348618 0.2107951 -1.1228306 -0.296388693 ## 17 -0.5667775 0.2138634 -0.9935640 -0.132124851 ## 18 -1.0093023 0.2111856 -1.4437750 -0.605782458 ## 19 -0.7444063 0.2116041 -1.1260144 -0.291720917 ## 20 -0.7314524 0.2081339 -1.1390389 -0.296033163 Here’s the code to extract the relevant elements from the coef() list, convert them to a tibble, and add the cafe index. partially_pooled_params &lt;- # with this line we select each of the 20 cafe&#39;s posterior mean (i.e., Estimate) # for both `Intercept` and `afternoon` coef(b13.1)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% # convert the two vectors to a tibble rename(Slope = afternoon) %&gt;% mutate(cafe = 1:nrow(.)) %&gt;% # add the `cafe` index select(cafe, everything()) # simply moving `cafe` to the leftmost position Like McElreath, we’ll compute the unpooled estimates directly from the data. # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% # with these two lines, we compute the mean value for each cafe&#39;s wait time # in the morning and then the afternoon group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% ungroup() %&gt;% # ungrouping allows us to alter afternoon, one of the grouping variables mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% spread(key = afternoon, value = mean) %&gt;% # use `spread()` just as in the previous block mutate(Slope = Slope - Intercept) # finally, here&#39;s our slope! # here we combine the partially-pooled and unpooled means into a single data object, # which will make plotting easier. params &lt;- # `bind_rows()` will stack the second tibble below the first bind_rows(partially_pooled_params, un_pooled_params) %&gt;% # index whether the estimates are pooled mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = nrow(.)/2)) # here&#39;s a glimpse at what we&#39;ve been working for params %&gt;% slice(c(1:5, 36:40)) ## # A tibble: 10 x 4 ## cafe Intercept Slope pooled ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 3.26 -0.908 partially ## 2 2 3.17 -1.05 partially ## 3 3 1.62 -0.429 partially ## 4 4 3.34 -0.742 partially ## 5 5 2.30 -0.548 partially ## 6 16 3.38 -0.465 not ## 7 17 2.29 -0.531 not ## 8 18 4.01 -1.07 not ## 9 19 3.39 -0.484 not ## 10 20 3.12 -0.617 not Finally, here’s our code for Figure 13.5.a, showing shrinkage in two dimensions. p1 &lt;- ggplot(data = params, aes(x = Intercept, y = Slope)) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 1/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 2/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 3/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 4/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 5/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 6/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 7/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 8/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 9/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = .99, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(params$Intercept), ylim = range(params$Slope)) + theme_pearl_earring() p1 Learn more about stat_ellipse(), here. Let’s prep for Figure 13.5.b. # retrieve the partially-pooled estimates with `coef()` partially_pooled_estimates &lt;- coef(b13.1)$cafe[ , 1, 1:2] %&gt;% # convert the two vectors to a tibble as_tibble() %&gt;% # the Intercept is the wait time for morning (i.e., `afternoon == 0`) rename(morning = Intercept) %&gt;% # `afternoon` wait time is the `morning` wait time plus the afternoon slope mutate(afternoon = morning + afternoon, cafe = 1:n()) %&gt;% # add the `cafe` index select(cafe, everything()) # compute unpooled estimates directly from data un_pooled_estimates &lt;- d %&gt;% # as above, with these two lines, we compute each cafe&#39;s mean wait value by time of day group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% # ungrouping allows us to alter the grouping variable, afternoon ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;morning&quot;, &quot;afternoon&quot;)) %&gt;% # this seperates out the values into morning and afternoon columns spread(key = afternoon, value = mean) estimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) The code for Figure 13.5.b. p2 &lt;- ggplot(data = estimates, aes(x = morning, y = afternoon)) + # nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` # functions in the previous plot mapply(function(level) { stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, # enter the levels here level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(x = &quot;morning wait (mins)&quot;, y = &quot;afternoon wait (mins)&quot;) + coord_cartesian(xlim = range(estimates$morning), ylim = range(estimates$afternoon)) + theme_pearl_earring() Here we bind the two subplots together with patchwork syntax. library(patchwork) (p1 + theme(legend.position = &quot;none&quot;)) + p2 + plot_annotation(title = &quot;Shrinkage in two dimensions&quot;, theme = theme_pearl_earring()) 13.2 Example: Admission decisions and gender Let’s revisit the infamous UCB admissions data. library(rethinking) data(UCBadmit) d &lt;- UCBadmit Here we detach rethinking, reload brms, and augment the data a bit. detach(package:rethinking, unload = T) library(brms) rm(UCBadmit) d &lt;- d %&gt;% mutate(male = ifelse(applicant.gender == &quot;male&quot;, 1, 0), dept_id = rep(1:6, each = 2)) 13.2.1 Varying intercepts. The statistical formula for our varying-intercepts logistic regression model follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept_id}_i} + \\beta \\text{male}_i \\\\ \\alpha_\\text{dept_id} &amp; \\sim \\text{Normal} (\\alpha, \\sigma) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 1) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 2). \\\\ \\end{align*}\\] Since there’s only one left-hand term in our (1 | dept_id) code, there’s only one random effect. b13.2 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd)), iter = 4500, warmup = 500, chains = 3, cores = 3, seed = 13, control = list(adapt_delta = 0.99), file = &quot;fits/b13.02&quot;) Since we don’t have a depth=2 argument in brms::summary(), we’ll have to get creative. One way to look at the parameters is with b13.2$fit: b13.2$fit ## Inference for Stan model: 4366f95d50e83541509caa319e56f393. ## 3 chains, each with iter=4500; warmup=500; thin=1; ## post-warmup draws per chain=4000, total post-warmup draws=12000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept -0.58 0.01 0.64 -1.83 -0.94 -0.59 -0.24 0.72 1872 1 ## b_male -0.09 0.00 0.08 -0.25 -0.15 -0.09 -0.04 0.06 4081 1 ## sd_dept_id__Intercept 1.47 0.01 0.58 0.76 1.08 1.33 1.69 2.95 2210 1 ## r_dept_id[1,Intercept] 1.26 0.01 0.64 -0.04 0.91 1.27 1.61 2.51 1891 1 ## r_dept_id[2,Intercept] 1.21 0.01 0.64 -0.09 0.87 1.22 1.57 2.47 1892 1 ## r_dept_id[3,Intercept] 0.00 0.01 0.64 -1.30 -0.34 0.01 0.36 1.24 1883 1 ## r_dept_id[4,Intercept] -0.03 0.01 0.64 -1.32 -0.38 -0.02 0.33 1.21 1873 1 ## r_dept_id[5,Intercept] -0.48 0.01 0.64 -1.76 -0.83 -0.46 -0.12 0.76 1897 1 ## r_dept_id[6,Intercept] -2.02 0.01 0.65 -3.36 -2.37 -2.01 -1.65 -0.79 1941 1 ## lp__ -61.90 0.06 2.62 -68.04 -63.40 -61.56 -60.00 -57.89 2169 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Feb 26 14:25:37 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). However, notice that the group-specific parameters don’t match up with those in the text. Though our r_dept_id[1,Intercept] had a posterior mean of 1.25, the number for a_dept[1] in the text is 0.67. This is because the brms package presented the random effects in the non-centered metric. The rethinking package, in contrast, presented the random effects in the centered metric. On page 399, McElreath wrote: Remember, the values above are the \\(\\alpha_\\text{DEPT}\\) estimates, and so they are deviations from the global mean \\(\\alpha\\), which in this case has posterior mean -0.58. So department A, “[1]” in the table, has the highest average admission rate. Department F, “[6]” in the table, has the lowest. Here’s another fun fact: # numbers taken from the mean column on page 399 in the text c(0.67, 0.63, -0.59, -0.62, -1.06, -2.61) %&gt;% mean() ## [1] -0.5966667 The average of the rethinking-based centered random effects is within rounding error of the global mean, -0.58. If you want the random effects in the centered metric from brms, you can use the coef() function: coef(b13.2) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6739699 0.09865218 0.4796662 0.8702509 ## 2 0.6287080 0.11513963 0.4060330 0.8563216 ## 3 -0.5838133 0.07485234 -0.7308554 -0.4377947 ## 4 -0.6168501 0.08527233 -0.7854734 -0.4523313 ## 5 -1.0594978 0.09986883 -1.2558990 -0.8672611 ## 6 -2.6072163 0.15622511 -2.9222397 -2.3116200 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.09457215 0.07960447 -0.2520517 0.06096449 ## 2 -0.09457215 0.07960447 -0.2520517 0.06096449 ## 3 -0.09457215 0.07960447 -0.2520517 0.06096449 ## 4 -0.09457215 0.07960447 -0.2520517 0.06096449 ## 5 -0.09457215 0.07960447 -0.2520517 0.06096449 ## 6 -0.09457215 0.07960447 -0.2520517 0.06096449 And just to confirm, the average of the posterior means of the Intercept random effects with brms::coef() is also the global mean within rounding error: mean(coef(b13.2)$dept_id[ , &quot;Estimate&quot;, &quot;Intercept&quot;]) ## [1] -0.5941166 Note how coef() returned a three-dimensional list. coef(b13.2) %&gt;% str() ## List of 1 ## $ dept_id: num [1:6, 1:4, 1:2] 0.674 0.629 -0.584 -0.617 -1.059 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## .. ..$ : chr [1:2] &quot;Intercept&quot; &quot;male&quot; If you just want the parameter summaries for the random intercepts, you have to use three-dimensional indexing. coef(b13.2)$dept_id[ , , &quot;Intercept&quot;] # this also works: coef(b13.2)$dept_id[ , , 1] ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.6739699 0.09865218 0.4796662 0.8702509 ## 2 0.6287080 0.11513963 0.4060330 0.8563216 ## 3 -0.5838133 0.07485234 -0.7308554 -0.4377947 ## 4 -0.6168501 0.08527233 -0.7854734 -0.4523313 ## 5 -1.0594978 0.09986883 -1.2558990 -0.8672611 ## 6 -2.6072163 0.15622511 -2.9222397 -2.3116200 So to get our brms summaries in a similar format to those in the text, we’ll have to combine coef() with fixef() and VarCorr(). rbind(coef(b13.2)$dept_id[, , &quot;Intercept&quot;], fixef(b13.2), VarCorr(b13.2)$dept_id$sd) ## Estimate Est.Error Q2.5 Q97.5 ## 1 0.67396988 0.09865218 0.4796662 0.87025090 ## 2 0.62870803 0.11513963 0.4060330 0.85632158 ## 3 -0.58381331 0.07485234 -0.7308554 -0.43779468 ## 4 -0.61685012 0.08527233 -0.7854734 -0.45233135 ## 5 -1.05949779 0.09986883 -1.2558990 -0.86726107 ## 6 -2.60721632 0.15622511 -2.9222397 -2.31161996 ## Intercept -0.58403450 0.64009342 -1.8311351 0.72038226 ## male -0.09457215 0.07960447 -0.2520517 0.06096449 ## Intercept 1.46514016 0.58060788 0.7617361 2.95225343 A little more data wrangling will make the summaries easier to read. rbind(coef(b13.2)$dept_id[, , &quot;Intercept&quot;], fixef(b13.2), VarCorr(b13.2)$dept_id$sd) %&gt;% as_tibble() %&gt;% mutate(parameter = c(str_c(&quot;Intercept[&quot;, 1:6, &quot;]&quot;), &quot;Intercept&quot;, &quot;male&quot;, &quot;sigma&quot;)) %&gt;% select(parameter, everything()) %&gt;% mutate_if(is_double, round, digits = 2) ## # A tibble: 9 x 5 ## parameter Estimate Est.Error Q2.5 Q97.5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Intercept[1] 0.67 0.1 0.48 0.87 ## 2 Intercept[2] 0.63 0.12 0.41 0.86 ## 3 Intercept[3] -0.580 0.07 -0.73 -0.44 ## 4 Intercept[4] -0.62 0.09 -0.79 -0.45 ## 5 Intercept[5] -1.06 0.1 -1.26 -0.87 ## 6 Intercept[6] -2.61 0.16 -2.92 -2.31 ## 7 Intercept -0.580 0.64 -1.83 0.72 ## 8 male -0.09 0.08 -0.25 0.06 ## 9 sigma 1.47 0.580 0.76 2.95 I’m not aware of a slick and easy way to get the n_eff and Rhat summaries into the mix. But if you’re fine with working with the brms-default non-centered parameterization, b13.2$fit gets you those just fine. One last thing. The broom package offers a very handy way to get those brms random effects. Just throw the model brm() fit into the tidy() function. library(broom) tidy(b13.2) %&gt;% mutate_if(is.numeric, round, digits = 2) # this line just rounds the output ## term estimate std.error lower upper ## 1 b_Intercept -0.58 0.64 -1.58 0.43 ## 2 b_male -0.09 0.08 -0.22 0.04 ## 3 sd_dept_id__Intercept 1.47 0.58 0.83 2.56 ## 4 r_dept_id[1,Intercept] 1.26 0.64 0.25 2.25 ## 5 r_dept_id[2,Intercept] 1.21 0.64 0.20 2.22 ## 6 r_dept_id[3,Intercept] 0.00 0.64 -1.01 1.01 ## 7 r_dept_id[4,Intercept] -0.03 0.64 -1.04 0.97 ## 8 r_dept_id[5,Intercept] -0.48 0.64 -1.50 0.52 ## 9 r_dept_id[6,Intercept] -2.02 0.65 -3.06 -1.01 ## 10 lp__ -61.90 2.62 -66.67 -58.32 But note how, just as with b13.2$fit, this approach summarizes the posterior with the non-centered parameterization. Which is a fine parameterization. It’s just a little different from what you’ll get when using precis( m13.2 , depth=2 ), as in the text. 13.2.2 Varying effects of being male. Now we’re ready to allow our male dummy to varies, too, the statistical model follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\text{Binomial} (n_i, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_{\\text{dept_id}_i} + \\beta_{\\text{dept_id}_i} \\text{male}_i \\\\ \\begin{bmatrix} \\alpha_\\text{dept_id} \\\\ \\beta_\\text{dept_id} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\bigg (\\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf{S} \\bigg ) \\\\ \\mathbf S &amp; = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\mathbf R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta &amp; \\sim \\text{Normal} (0, 1) \\\\ (\\sigma_\\alpha, \\sigma_\\beta) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R &amp; \\sim \\text{LKJcorr} (2). \\end{align*}\\] Fit the model. b13.3 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + male + (1 + male | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(2), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = .99, max_treedepth = 12), file = &quot;fits/b13.03&quot;) McElreath encouraged us to make sure the chains look good. Instead of relying on convenience functions, let’s do it by hand. post &lt;- posterior_samples(b13.3, add_chain = T) post %&gt;% select(-lp__) %&gt;% gather(key, value, -chain, -iter) %&gt;% mutate(chain = as.character(chain)) %&gt;% ggplot(aes(x = iter, y = value, group = chain, color = chain)) + geom_line(size = 1/15) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#B1934A&quot;, &quot;#A65141&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(NULL, breaks = c(1001, 5000)) + ylab(NULL) + theme_pearl_earring(legend.position = c(.825, .06), legend.direction = &quot;horizontal&quot;) + facet_wrap(~key, ncol = 3, scales = &quot;free_y&quot;) Our chains look great. While we’re at it, let’s examine the \\(\\hat R\\) vales in a handmade plot, too. rhat(b13.3) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% set_names(&quot;parameter&quot;, &quot;rhat&quot;) %&gt;% filter(parameter != &quot;lp__&quot;) %&gt;% ggplot(aes(x = rhat, y = reorder(parameter, rhat))) + geom_segment(aes(xend = 1, yend = parameter), color = &quot;#EEDA9D&quot;) + geom_point(aes(color = rhat &gt; 1), size = 2) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(x = NULL, y = NULL) + theme_pearl_earring(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) Them are some respectable \\(\\hat R\\) values. The plot accentuates their differences, but they’re all basically 1 (e.g., see what happens is you set coord_cartesian(xlim = c(0.99, 1.01))). Here are the random effects in the centered metric. coef(b13.3) ## $dept_id ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.3066385 0.25479242 0.8153333 1.8072484 ## 2 0.7448623 0.32317068 0.1137833 1.4035408 ## 3 -0.6466957 0.08564769 -0.8156532 -0.4819434 ## 4 -0.6173676 0.10379096 -0.8209649 -0.4169028 ## 5 -1.1323554 0.11564908 -1.3594885 -0.9085594 ## 6 -2.6012371 0.20266348 -3.0151291 -2.2200890 ## ## , , male ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -0.79468026 0.2682620 -1.3266356 -0.2679886 ## 2 -0.21413173 0.3247101 -0.8736885 0.4214672 ## 3 0.08266285 0.1393503 -0.1904788 0.3572314 ## 4 -0.09255217 0.1391145 -0.3691793 0.1835969 ## 5 0.12199224 0.1875870 -0.2383589 0.4924885 ## 6 -0.12352441 0.2696998 -0.6658091 0.4051199 We may as well keep our doing-things-by-hand kick going. Instead relying on bayesplog::mcmc_intervals() or tidybayes::pointintervalh() to make our coefficient plot, we’ll combine geom_pointrange() and coord_flip(). But we will need to wrangle a bit to get those brms-based centered random effects into a usefully-formatted tidy tibble. # as far as I can tell, because `coef()` yields a list, you have to take out the two # random effects one at a time and then bind them together to get them ready for a tibble rbind(coef(b13.3)$dept_id[, , 1], coef(b13.3)$dept_id[, , 2]) %&gt;% as_tibble() %&gt;% mutate(param = c(str_c(&quot;Intercept &quot;, 1:6), str_c(&quot;male &quot;, 1:6)), reorder = c(6:1, 12:7)) %&gt;% # plot ggplot(aes(x = reorder(param, reorder))) + geom_hline(yintercept = 0, linetype = 3, color = &quot;#8B9DAF&quot;) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5, y = Estimate, color = reorder &lt; 7), shape = 20, size = 3/4) + scale_color_manual(values = c(&quot;#394165&quot;, &quot;#A65141&quot;)) + xlab(NULL) + coord_flip() + theme_pearl_earring(axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) Just like in the text, our male slopes are much less dispersed than our intercepts. 13.2.3 Shrinkage. Figure 13.6.a depicts the correlation between the full UCB model’s varying intercepts and slopes. post &lt;- posterior_samples(b13.3) post %&gt;% ggplot(aes(x = cor_dept_id__Intercept__male, y = 0)) + geom_halfeyeh(point_interval = median_qi, .width = .95, fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;) + scale_x_continuous(breaks = c(-1, median(post$cor_dept_id__Intercept__male), 1), labels = c(-1, &quot;-.35&quot;, 1), limits = c(-1, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The dot is at the median; the\\nhorizontal bar is the 95% CI.&quot;, x = &quot;correlation&quot;) + theme_pearl_earring() Much like for Figure 13.5.b, above, it’ll take a little data processing before we’re ready to reproduce Figure 13.6.b. # here we put the partially-pooled estimate summaries in a tibble partially_pooled_params &lt;- coef(b13.3)$dept_id[ , 1, ] %&gt;% as_tibble() %&gt;% set_names(&quot;intercept&quot;, &quot;slope&quot;) %&gt;% mutate(dept = 1:n()) %&gt;% select(dept, everything()) # in order to calculate the unpooled estimates from the data, we&#39;ll need a function that # can convert probabilities into the logit metric. if you do the algebra, this is just # a transformation of the `inv_logit_scaled()` function. prob_to_logit &lt;- function(x) { -log((1 / x) -1) } # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% group_by(male, dept_id) %&gt;% summarise(prob_admit = mean(admit / applications)) %&gt;% ungroup() %&gt;% mutate(male = ifelse(male == 0, &quot;intercept&quot;, &quot;slope&quot;)) %&gt;% spread(key = male, value = prob_admit) %&gt;% rename(dept = dept_id) %&gt;% # here we put our `prob_to_logit()` function to work mutate(intercept = prob_to_logit(intercept), slope = prob_to_logit(slope)) %&gt;% mutate(slope = slope - intercept) # here we combine the partially-pooled and unpooled means into a single data object params &lt;- bind_rows(partially_pooled_params, un_pooled_params) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) %&gt;% mutate(dept_letter = rep(LETTERS[1:6], times = 2)) # this will help with plotting params ## # A tibble: 12 x 5 ## dept intercept slope pooled dept_letter ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1.31 -0.795 partially A ## 2 2 0.745 -0.214 partially B ## 3 3 -0.647 0.0827 partially C ## 4 4 -0.617 -0.0926 partially D ## 5 5 -1.13 0.122 partially E ## 6 6 -2.60 -0.124 partially F ## 7 1 1.54 -1.05 not A ## 8 2 0.754 -0.220 not B ## 9 3 -0.660 0.125 not C ## 10 4 -0.622 -0.0820 not D ## 11 5 -1.16 0.200 not E ## 12 6 -2.58 -0.189 not F Here’s our version of Figure 13.6.b, depicting two-dimensional shrinkage for the partially-pooled multilevel estimates (posterior means) relative to the unpooled coefficients, calculated from the data. The ggrepel::geom_text_repel() function will help us with the in-plot labels. library(ggrepel) ggplot(data = params, aes(x = intercept, y = slope)) + mapply(function(level) { stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, level = c(seq(from = 1/10, to = 9/10, by = 1/10), .99)) + geom_point(aes(group = dept, color = pooled)) + geom_line(aes(group = dept), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + geom_text_repel(data = params %&gt;% filter(pooled == &quot;partially&quot;), aes(label = dept_letter), color = &quot;#E8DCCF&quot;, size = 4, family = &quot;Courier&quot;, seed = 13.6) + labs(x = expression(&quot;intercept (&quot;*alpha[dept_id]*&quot;)&quot;), y = expression(&quot;slope (&quot;*beta[dept_id]*&quot;)&quot;)) + coord_cartesian(xlim = range(params$intercept), ylim = range(params$slope)) + theme_pearl_earring() 13.2.4 Model comparison. Fit the no-gender model. b13.4 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1 + (1 | dept_id), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 2), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = .99, max_treedepth = 12), file = &quot;fits/b13.04&quot;) Compare the three models by the WAIC. b13.2 &lt;- add_criterion(b13.2, &quot;waic&quot;) b13.3 &lt;- add_criterion(b13.3, &quot;waic&quot;) b13.4 &lt;- add_criterion(b13.4, &quot;waic&quot;) loo_compare(b13.2, b13.3, b13.4, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.3 0.0 0.0 -45.4 2.3 6.7 1.4 90.9 4.7 ## b13.4 -7.1 7.6 -52.5 9.0 6.5 2.3 105.0 18.0 ## b13.2 -8.8 6.6 -54.2 8.2 9.4 3.0 108.4 16.4 In terms of the WAIC estimates and \\(\\text{elpd}\\) differences, the models are similar. The story changes when we look at the WAIC weights. model_weights(b13.2, b13.3, b13.4, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b13.2 b13.3 b13.4 ## 0.000 0.999 0.001 The varying slopes model, [b13.3], dominates [the other two]. This is despite the fact that the average slope in [b13.3] is nearly zero. The average isn’t what matters, however. It is the individual slopes, one for each department, that matter. If we wish to generalize to new departments, the variation in slopes suggest that it’ll be worth paying attention to gender, even if the average slope is nearly zero in the population. (pp. 402–403, emphasis in the original) 13.2.5 More slopes. The varying slopes strategy generalizes to as many slopes as you like, within practical limits. All that happens is that each new predictor you want to construct varying slopes for adds one more dimension to the covariance matrix of the varying effects prior. So this means one more standard deviation parameter and one more dimension to the correlation matrix. (p. 403) 13.3 Example: Cross-classified chimpanzees with varying slopes Retrieve the chimpanzees data. library(rethinking) data(chimpanzees) d &lt;- chimpanzees detach(package:rethinking, unload = T) library(brms) rm(chimpanzees) d &lt;- d %&gt;% select(-recipient) %&gt;% mutate(block_id = block) My maths aren’t the best. But if I’m following along correctly, here’s a fuller statistical expression of our cross-classified model. \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\text{Binomial} (n = 1, p_i) \\\\ \\text{logit} (p_i) &amp; = \\alpha_i + (\\beta_{1i} + \\beta_{2i} \\text{condition}_i) \\text{prosoc_left}_i \\\\ \\alpha_i &amp; = \\alpha + \\alpha_{\\text{actor}_i} + \\alpha_{\\text{block_id}_i} \\\\ \\beta_{1i} &amp; = \\beta_1 + \\beta_{1, \\text{actor}_i} + \\beta_{1, \\text{block_id}_i} \\\\ \\beta_{2i} &amp; = \\beta_2 + \\beta_{2, \\text{actor}_i} + \\beta_{2, \\text{block_id}_i} \\\\ \\begin{bmatrix} \\alpha_\\text{actor} \\\\ \\beta_{1, \\text{actor}} \\\\ \\beta_{2, \\text{actor}} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{actor} \\end{pmatrix} \\\\ \\begin{bmatrix} \\alpha_\\text{block_id} \\\\ \\beta_{1, \\text{block_id}} \\\\ \\beta_{2, \\text{block_id}} \\end{bmatrix} &amp; \\sim \\text{MVNormal} \\begin{pmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} , \\mathbf{S}_\\text{block_id} \\end{pmatrix} \\\\ \\mathbf S_\\text{actor} &amp; = \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\mathbf R_\\text{actor} \\begin{pmatrix} \\sigma_{\\alpha_\\text{actor}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{actor}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{actor}}} \\end{pmatrix} \\\\ \\mathbf S_\\text{block_id} &amp; = \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\mathbf R_\\text{block_id} \\begin{pmatrix} \\sigma_{\\alpha_\\text{block_id}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{1_\\text{block_id}}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\beta_{2_\\text{block_id}}} \\end{pmatrix} \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 1) \\\\ (\\sigma_{\\alpha_\\text{actor}}, \\sigma_{\\beta_{1_\\text{actor}}}, \\sigma_{\\beta_{2_\\text{actor}}}) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ (\\sigma_{\\alpha_\\text{block_id}}, \\sigma_{\\beta_{1_\\text{block_id}}}, \\sigma_{\\beta_{2_\\text{block_id}}}) &amp; \\sim \\text{HalfCauchy} (0, 2) \\\\ \\mathbf R_\\text{actor} &amp; \\sim \\text{LKJcorr} (4) \\\\ \\mathbf R_\\text{block_id} &amp; \\sim \\text{LKJcorr} (4), \\end{align*}\\] where each \\(\\mathbf R\\) is a \\(3 \\times 3\\) correlation matrix. Let’s fit this beast. b13.6 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + condition:prosoc_left + (1 + prosoc_left + condition:prosoc_left | actor) + (1 + prosoc_left + condition:prosoc_left | block_id), prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 2), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 3, cores = 3, seed = 13, file = &quot;fits/b13.06&quot;) Even though it’s not apparent in the syntax, our model b13.6 was already fit using the non-centered parameterization. Behind the scenes, Bürkner has brms do this automatically. It’s been that way all along. It’s a little janky, but we can compute the number of effective samples (bulk NES, anyways) for all our parameters like so. ratios_cp &lt;- neff_ratio(b13.6) neff &lt;- ratios_cp %&gt;% as_tibble %&gt;% rename(neff_ratio = value) %&gt;% mutate(neff = neff_ratio * 12000) head(neff) ## # A tibble: 6 x 2 ## neff_ratio neff ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.297 3569. ## 2 0.617 7400. ## 3 0.658 7897. ## 4 0.368 4410. ## 5 0.455 5454. ## 6 0.493 5921. Now we’re ready for our variant of Figure 13.7. The handy ggbeeswarm package and its geom_quasirandom() function will give a better sense of the distribution. library(ggbeeswarm) neff %&gt;% ggplot(aes(x = factor(0), y = neff)) + geom_boxplot(fill = &quot;#394165&quot;, color = &quot;#8B9DAF&quot;) + geom_quasirandom(method = &quot;tukeyDense&quot;, size = 2/3, color = &quot;#EEDA9D&quot;, alpha = 2/3) + scale_x_discrete(NULL, breaks = NULL, expand = c(.75, .75)) + scale_y_continuous(&quot;effective samples&quot;, breaks = c(0, 6000, 12000)) + labs(subtitle = &quot;The non-centered\\nparameterization is the\\nbrms default. No fancy\\ncoding required.&quot;) + coord_cartesian(ylim = 0:15000) + theme_pearl_earring() McElreath reported this model only has about 18 parameters. Let’s compute the WAIC and check the p_waic. b13.6 &lt;- add_criterion(b13.6, &quot;waic&quot;) ## Automatically saving the model object in &#39;fits/b13.06.rds&#39; b13.6$criteria$waic ## ## Computed from 12000 by 504 log-likelihood matrix ## ## Estimate SE ## elpd_waic -267.3 9.9 ## p_waic 18.2 0.9 ## waic 534.6 19.9 Yep, only about 18. Here are our standard deviation parameters. tidy(b13.6) %&gt;% filter(str_detect(term , &quot;sd_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 sd_actor__Intercept 2.33 0.89 1.29 4.01 ## 2 sd_actor__prosoc_left 0.45 0.36 0.03 1.11 ## 3 sd_actor__prosoc_left:condition 0.51 0.47 0.03 1.41 ## 4 sd_block_id__Intercept 0.23 0.20 0.02 0.61 ## 5 sd_block_id__prosoc_left 0.57 0.39 0.07 1.28 ## 6 sd_block_id__prosoc_left:condition 0.52 0.42 0.04 1.32 McElreath discussed rethinking::link() in the middle of page 407. He showed how his link(m13.6NC) code returned a list of four matrices, of which the p matrix was of primary interest. The brms::fitted() function doesn’t work quite the same way, here. fitted(b13.6, summary = F, nsamples = 1000) %&gt;% str() ## num [1:1000, 1:504] 0.413 0.39 0.308 0.357 0.273 ... First off, recall that fitted() returns summary values, by default. If we want individual values, set summary = FALSE. It’s also the fitted() default to use all posterior iterations, which is 12,000 in this case. To match the text, we need to set nsamples = 1000. But those are just details. The main point is that fitted() only returns one matrix, which is the analogue to the p matrix in the text. Moving forward, before we can follow along with McElreath’s R code 13.27, we need to refit the simpler model from way back in Chapter 12. b12.5 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + prosoc_left + prosoc_left:condition + (1 | actor) + (1 | block), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sd)), iter = 6000, warmup = 1000, cores = 4, chains = 4, control = list(adapt_delta = 0.99), seed = 12, file = &quot;fits/b12.05&quot;) Now we can compare them by the WAIC. b12.5 &lt;- add_criterion(b12.5, &quot;waic&quot;) loo_compare(b13.6, b12.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b12.5 0.0 0.0 -266.3 9.9 10.4 0.5 532.7 19.7 ## b13.6 -1.0 2.0 -267.3 9.9 18.2 0.9 534.6 19.9 Here are the WAIC weights. model_weights(b13.6, b12.5, weights = &quot;waic&quot;) ## b13.6 b12.5 ## 0.2736897 0.7263103 In this example, no matter which varying effect structure you use, you’ll find that actors vary a lot in their baseline preference for the left-hand lever. Everything else is much less important. But using the most complex model, [b13.6], tells the correct story. Because the varying slopes are adaptively regularized, the model hasn’t overfit much, relative to the simpler model that contains only the important intercept variation. (p. 408) 13.4 Continuous categories and the Gaussian process There is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works. We’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 410, emphasis in the original) 13.4.1 Example: Spatial autocorrelation in Oceanic tools. We start by loading the matrix of geographic distances. # load the distance matrix library(rethinking) data(islandsDistMatrix) # display short column names, so fits on screen d_mat &lt;- islandsDistMatrix colnames(d_mat) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) round(d_mat, 1) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 If you wanted to use color to more effectively visualize the values in the matirx, you might do something like this. d_mat %&gt;% data.frame() %&gt;% rownames_to_column(&quot;row&quot;) %&gt;% gather(column, distance, -row) %&gt;% mutate(column = factor(column, levels = colnames(d_mat)), row = factor(row, levels = rownames(d_mat)) %&gt;% fct_rev()) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = distance)) + geom_text(aes(label = round(distance, digits = 1)), size = 3, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme_pearl_earring(axis.text.y = element_text(hjust = 0)) + theme(axis.ticks = element_blank()) Figure 13.8 shows the “shape of the function relating distance to the covariance \\(\\mathbf K_{ij}\\).” tibble(x = seq(from = 0, to = 4, by = .01), linear = exp(-1 * x), squared = exp(-1 * x^2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = linear), color = &quot;#B1934A&quot;, linetype = 2) + geom_line(aes(y = squared), color = &quot;#DCA258&quot;) + scale_x_continuous(&quot;distance&quot;, expand = c(0, 0)) + scale_y_continuous(&quot;correlation&quot;, breaks = c(0, .5, 1), labels = c(0, &quot;.5&quot;, 1)) + theme_pearl_earring() Now load the primary data. data(Kline2) # load the ordinary data, now with coordinates d &lt;- Kline2 %&gt;% mutate(society = 1:10) rm(Kline2) d %&gt;% glimpse() ## Observations: 10 ## Variables: 10 ## $ culture &lt;fct&gt; Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tonga,… ## $ population &lt;int&gt; 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000 ## $ contact &lt;fct&gt; low, low, low, high, high, high, high, low, high, low ## $ total_tools &lt;int&gt; 13, 22, 24, 43, 33, 19, 40, 28, 55, 71 ## $ mean_TU &lt;dbl&gt; 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6 ## $ lat &lt;dbl&gt; -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9 ## $ lon &lt;dbl&gt; 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6 ## $ lon2 &lt;dbl&gt; -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4 ## $ logpop &lt;dbl&gt; 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.47270… ## $ society &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) Okay, it appears this is going to be a bit of a ride. It’s not entirely clear to me if we can fit a Gaussian process model in brms that’s a direct equivalent to what McElreath did with rethinking. But we can try. First, note our use of the gp() syntax in the brm() function, below. We’re attempting to tell brms that we would like to include latitude and longitude (i.e., lat and long2, respectively) in a Gaussian process. Also note how our priors are a little different than those in the text. I’ll explain, below. Let’s just move ahead and fit the model. b13.7 &lt;- brm(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b, coef = logpop), prior(inv_gamma(2.874624, 0.393695), class = lscale, coef = gplatlon2), prior(cauchy(0, 1), class = sdgp)), iter = 1e4, warmup = 2000, chains = 4, cores = 4, seed = 13, control = list(adapt_delta = 0.999, max_treedepth = 12), file = &quot;fits/b13.07&quot;) Here’s the model summary. posterior_summary(b13.7) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.45 1.12 -0.79 3.76 ## b_logpop 0.23 0.11 0.02 0.45 ## sdgp_gplatlon2 0.53 0.37 0.16 1.46 ## lscale_gplatlon2 0.23 0.13 0.07 0.57 ## zgp_gplatlon2[1] -0.60 0.79 -2.18 0.93 ## zgp_gplatlon2[2] 0.45 0.85 -1.24 2.09 ## zgp_gplatlon2[3] -0.62 0.70 -1.97 0.88 ## zgp_gplatlon2[4] 0.87 0.70 -0.47 2.29 ## zgp_gplatlon2[5] 0.26 0.76 -1.24 1.75 ## zgp_gplatlon2[6] -1.00 0.79 -2.56 0.59 ## zgp_gplatlon2[7] 0.13 0.73 -1.42 1.52 ## zgp_gplatlon2[8] -0.19 0.87 -1.88 1.59 ## zgp_gplatlon2[9] 0.40 0.93 -1.55 2.13 ## zgp_gplatlon2[10] -0.30 0.82 -1.94 1.32 ## lp__ -51.55 3.19 -58.81 -46.38 Our Gaussian process parameters are different than McElreath’s. From the gp section of the brms reference manual, we learn the brms parameterization follows the form \\[k(x_{i},x_{j}) = sdgp^2 \\exp \\big (-||x_i - x_j||^2 / (2 lscale^2) \\big ).\\] What McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^2\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp_gplatlon2 before it’s on the same scale as etasq in the text. Here it is. posterior_samples(b13.7) %&gt;% transmute(sdgp_squared = sdgp_gplatlon2^2) %&gt;% mean_hdi(sdgp_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## sdgp_squared .lower .upper .width .point .interval ## 1 0.411 0 0.782 0.89 mean hdi Now we’re in the ballpark. In our model brm() code, above, we just went with the flow and kept the cauchy(0, 1) prior on sdgp. Now look at the denominator of the inner part of Bürkner equation, \\(2 lscale^2\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^2\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^2\\) directly as rhosq. If I’m doing the algebra correctly–and that may well be a big if–, we might expect \\[\\rho^2 = 1/(2 \\cdot lscale^2).\\] But that doesn’t appear to be the case. Sigh. posterior_samples(b13.7) %&gt;% transmute(rho_squared = 1 / (2 * lscale_gplatlon2^2)) %&gt;% mean_hdi(rho_squared, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## rho_squared .lower .upper .width .point .interval ## 1 21.901 0.355 47.042 0.89 mean hdi Oh man, that isn’t even close to the 2.67 McElreath reported in the text. The plot deepens. If you look back, you’ll see we used a very different prior for \\(lscale\\). Here is it: inv_gamma(2.874624, 0.393695). Use get_prior() to discover where that came from. get_prior(data = d, family = poisson, total_tools ~ 1 + gp(lat, lon2) + logpop) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b logpop ## 3 student_t(3, 3, 10) Intercept ## 4 lscale ## 5 inv_gamma(2.874624, 0.393695) lscale gplatlon2 ## 6 student_t(3, 0, 10) sdgp ## 7 sdgp gplatlon2 That is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt’s vignette on the topic. We can use the dinvgamma() function from the well-named invgamma package to get a sense of what that prior looks like. tibble(x = seq(from = 0, to = 2, by = 0.01)) %&gt;% mutate(density = invgamma::dinvgamma(x, 2.874624, 0.393695)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = density)) + geom_ribbon(size = 0, fill = &quot;#EEDA9D&quot;) + annotate(geom = &quot;text&quot;, x = 1.1, y = 5.5, label = &quot;inverse gamma(2.874624, 0.393695)&quot;, color = &quot;#EEDA9D&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme_pearl_earring() Though it isn’t included in this document, I also ran the model with the cauchy(0, 1) prior and the results were quite similar. So the big discrepancy between our model and the one in the text isn’t based on that prior. Now that we’ve hopped on the comparison train, we may as well keep going down the track. Let’s reproduce McElreath’s model with rethinking. Switch out brms for rethinking. detach(package:brms, unload = T) library(rethinking) Now fit the rethinking::map2stan() model. m13.7 &lt;- map2stan( alist( total_tools ~ dpois(lambda), log(lambda) &lt;- a + g[society] + bp*logpop, g[society] ~ GPL2( Dmat , etasq , rhosq , 0.01 ), a ~ dnorm(0,10), bp ~ dnorm(0,1), etasq ~ dcauchy(0,1), rhosq ~ dcauchy(0,1) ), data=list( total_tools=d$total_tools, logpop=d$logpop, society=d$society, Dmat=islandsDistMatrix), warmup=2000 , iter=1e4 , chains=4) Alright, now we’ll work directly with the posteriors to make some visual comparisons. # rethinking-based posterior post_m13.7 &lt;- rethinking::extract.samples(m13.7)[2:5] %&gt;% as_tibble() detach(package:rethinking, unload = T) library(brms) # brms-based posterior post_b13.7 &lt;- posterior_samples(b13.7) Here’s the model intercept posterior, by package. post_m13.7[, &quot;a&quot;] %&gt;% bind_rows(post_b13.7%&gt;% transmute(a = b_Intercept)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = a, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Not identical, but pretty close&quot;, x = &quot;intercept&quot;) + theme_pearl_earring() Now check the slopes. post_m13.7[, &quot;bp&quot;] %&gt;% bind_rows(post_b13.7 %&gt;% transmute(bp = b_logpop)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = bp, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Again, pretty close&quot;, x = &quot;slope&quot;) + theme_pearl_earring() This one, \\(\\eta^2\\), required a little transformation. post_m13.7[, &quot;etasq&quot;] %&gt;% bind_rows(post_b13.7 %&gt;% transmute(etasq = sdgp_gplatlon2^2)) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = etasq, fill = package)) + geom_density(size = 0, alpha = 1/2) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Still in the same ballpark&quot;, x = expression(eta^2)) + coord_cartesian(xlim = 0:3) + theme_pearl_earring() \\(\\rho^2\\) required more extensive transformation of the brms posterior: post_m13.7[, &quot;rhosq&quot;] %&gt;% bind_rows(post_b13.7%&gt;% transmute(rhosq = 1 / (2 * (lscale_gplatlon2^2)))) %&gt;% mutate(package = rep(c(&quot;rethinking&quot;, &quot;brms&quot;), each = nrow(post_m13.7))) %&gt;% ggplot(aes(x = rhosq, fill = package)) + geom_density(size = 0) + scale_fill_manual(NULL, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(title = &quot;Holy smokes are those not the same!&quot;, subtitle = &quot;Notice how differently the y axes got scaled. Also, that brms density is\\nright skewed for days.&quot;, x = expression(rho^2)) + coord_cartesian(xlim = 0:50) + theme_pearl_earring(legend.position = &quot;none&quot;) + facet_wrap(~package, scales = &quot;free_y&quot;) I’m in clinical psychology. Folks in my field don’t tend to use Gaussian processes, so getting to the bottom of this is low on my to-do list. Perhaps one of y’all are more experienced with Gaussian processes and see a flaw somewhere in my code. Please hit me up if you do. Anyways, here’s our brms + ggplot2 version of Figure 13.9. # for `sample_n()` set.seed(13) # wrangle post_b13.7 %&gt;% transmute(iter = 1:n(), etasq = sdgp_gplatlon2^2, rhosq = lscale_gplatlon2^2 * .5) %&gt;% sample_n(100) %&gt;% expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 55, by = 1)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% # plot ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, alpha = 1/4, color = &quot;#EEDA9D&quot;) + stat_function(fun = function(x) median(post_b13.7$sdgp_gplatlon2)^2 * exp(-median(post_b13.7$lscale_gplatlon2)^2 *.5 * x^2), color = &quot;#EEDA9D&quot;, size = 1.1) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = seq(from = 0, to = 50, by = 10)) + coord_cartesian(xlim = 0:50, ylim = 0:1) + theme_pearl_earring() Do note the scale on which we placed our x axis. Our brms parameterization resulted in a gentler decline in spatial covariance. Let’s finish this up and “push the parameters back through the function for \\(\\mathbf{K}\\), the covariance matrix” (p. 415). # compute posterior median covariance among societies k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(post_b13.7$sdgp_gplatlon2^2) * exp(-median(post_b13.7$lscale_gplatlon2^2) * islandsDistMatrix[i, j]^2) diag(k) &lt;- median(post_b13.7$sdgp_gplatlon2^2) + 0.01 k %&gt;% round(2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.20 0.18 0.18 0.09 0.18 0.16 0.12 0.14 0.16 0.05 ## [2,] 0.18 0.20 0.19 0.09 0.18 0.16 0.13 0.14 0.16 0.06 ## [3,] 0.18 0.19 0.20 0.10 0.17 0.17 0.14 0.15 0.15 0.06 ## [4,] 0.09 0.09 0.10 0.20 0.06 0.15 0.17 0.17 0.04 0.02 ## [5,] 0.18 0.18 0.17 0.06 0.20 0.12 0.10 0.10 0.18 0.07 ## [6,] 0.16 0.16 0.17 0.15 0.12 0.20 0.16 0.18 0.10 0.03 ## [7,] 0.12 0.13 0.14 0.17 0.10 0.16 0.20 0.18 0.07 0.05 ## [8,] 0.14 0.14 0.15 0.17 0.10 0.18 0.18 0.20 0.08 0.03 ## [9,] 0.16 0.16 0.15 0.04 0.18 0.10 0.07 0.08 0.20 0.07 ## [10,] 0.05 0.06 0.06 0.02 0.07 0.03 0.05 0.03 0.07 0.20 And we’ll continue to follow suit and change these to a correlation matrix. # convert to correlation matrix rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;,&quot;Ti&quot;,&quot;SC&quot;,&quot;Ya&quot;,&quot;Fi&quot;,&quot;Tr&quot;,&quot;Ch&quot;,&quot;Mn&quot;,&quot;To&quot;,&quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.94 0.93 0.44 0.89 0.80 0.63 0.69 0.83 0.26 ## Ti 0.94 1.00 0.95 0.47 0.89 0.81 0.68 0.71 0.81 0.31 ## SC 0.93 0.95 1.00 0.52 0.86 0.84 0.73 0.76 0.77 0.30 ## Ya 0.44 0.47 0.52 1.00 0.30 0.74 0.86 0.85 0.21 0.12 ## Fi 0.89 0.89 0.86 0.30 1.00 0.63 0.50 0.52 0.93 0.37 ## Tr 0.80 0.81 0.84 0.74 0.63 1.00 0.83 0.92 0.52 0.16 ## Ch 0.63 0.68 0.73 0.86 0.50 0.83 1.00 0.89 0.38 0.25 ## Mn 0.69 0.71 0.76 0.85 0.52 0.92 0.89 1.00 0.40 0.16 ## To 0.83 0.81 0.77 0.21 0.93 0.52 0.38 0.40 1.00 0.34 ## Ha 0.26 0.31 0.30 0.12 0.37 0.16 0.25 0.16 0.34 1.00 The correlations in our rho matrix look a little higher than those in the text. Before we get see them in a plot, let’s consider psize. If you really want to scale the points in Figure 13.10.a like McElreath did, you can make the psize variable in a tidyverse sort of way as follows. However, if you compare the psize method and the default ggplot2 method using just logpop, you’ll see the difference is negligible. In that light, I’m going to be lazy and just use logpop in my plots. d %&gt;% transmute(psize = logpop / max(logpop)) %&gt;% transmute(psize = exp(psize * 1.5) - 2) ## psize ## 1 0.3134090 ## 2 0.4009582 ## 3 0.6663711 ## 4 0.7592196 ## 5 0.9066890 ## 6 0.9339560 ## 7 0.9834797 ## 8 1.1096138 ## 9 1.2223112 ## 10 2.4816891 As far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so. tidy_rho &lt;- rho %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(d %&gt;% select(culture, logpop, total_tools, lon2, lat)) %&gt;% gather(colname, correlation, -rowname, -culture, -logpop, -total_tools, -lon2, -lat) %&gt;% mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) %&gt;% select(rowname, colname, group, culture, everything()) head(tidy_rho) ## rowname colname group culture logpop total_tools lon2 lat correlation ## 1 Ml Ml MlMl Malekula 7.003065 13 -12.5 -16.3 1.00 ## 2 Ti Ml MlTi Tikopia 7.313220 22 -11.2 -12.3 0.94 ## 3 SC Ml MlSC Santa Cruz 8.188689 24 -14.0 -10.7 0.93 ## 4 Ya Ml MlYa Yap 8.474494 43 -41.9 9.5 0.44 ## 5 Fi Ml FiMl Lau Fiji 8.909235 33 -1.9 -17.7 0.89 ## 6 Tr Ml MlTr Trobriand 8.987197 19 -29.1 -8.7 0.80 Okay, here’s the code for our version of Figure 13.10.a. p1 &lt;- tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_line(aes(group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Among societies in geographic space\\n&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(d$lon2), ylim = range(d$lat)) + theme_pearl_earring(legend.position = &quot;none&quot;) Here’s our the code for our version of Figure 13.10.b. # new data for `fitted()` nd &lt;- tibble(logpop = seq(from = 6, to = 14, length.out = 30), lat = median(d$lat), lon2 = median(d$lon2)) # `fitted()` f &lt;- fitted(b13.7, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # plot p2 &lt;- tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#394165&quot;, color = &quot;#100F14&quot;, alpha = .5, size = 1.1) + geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = &quot;#80A0C7&quot;) + geom_point(data = d, aes(y = total_tools, size = logpop), color = &quot;#DCA258&quot;) + geom_text_repel(data = d, aes(y = total_tools, label = culture), seed = 0, point.padding = .3, size = 3, color = &quot;#FCF9F0&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Shown against the relation between\\ntotal tools and log pop&quot;, x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(d$logpop), ylim = range(d$total_tools)) + theme_pearl_earring(legend.position = &quot;none&quot;) Now we combine them to make the full version of Figure 13.10. p1 + p2 + plot_annotation(title = &quot;Posterior median correlations&quot;, theme = theme_pearl_earring()) Yep, as expressed by the intensity of the colors of the connecting lines, those correlations are more pronounced than those in the text. Our higher correlations make for a more intensely-webbed plot. To learn more on Bürkner’s thoughts on this model in brms, check out the thread on this GitHub issue. 13.5 Summary Bonus: Another Berkley-admissions-data-like example. McElreath uploaded recordings of him teaching out of his text for a graduate course during the 2017/2018 fall semester. In the beginning of lecture 13 from week 7, he discussed a paper from van der Lee and Ellemers (2015) published an article in PNAS. Their paper suggested male researchers were more likely than female researchers to get research funding in the Netherlands. In their initial analysis (p. 12350) they provided a simple \\(\\chi^2\\) test to test the null hypothesis there was no difference in success for male versus female researchers, for which they reported \\(\\chi_{df = 1}^2 = 4.01, p = .045\\). Happily, van der Lee and Ellemers provided their data values in their supplemental material (i.e., Table S1.), which McElreath also displayed in his video. Their data follows the same structure as the Berkley admissions data. In his lecture, McElreath suggested their \\(\\chi^2\\) test is an example of Simpson’s paradox, just as with the Berkley data. He isn’t the first person to raise this criticism (see Volker and SteenBeek’s critique, which McElreath also pointed to in the lecture). Here are the data: funding &lt;- tibble( discipline = rep(c(&quot;Chemical sciences&quot;, &quot;Physical sciences&quot;, &quot;Physics&quot;, &quot;Humanities&quot;, &quot;Technical sciences&quot;, &quot;Interdisciplinary&quot;, &quot;Earth/life sciences&quot;, &quot;Social sciences&quot;, &quot;Medical sciences&quot;), each = 2), gender = rep(c(&quot;m&quot;, &quot;f&quot;), times = 9), applications = c(83, 39, 135, 39, 67, 9, 230, 166, 189, 62, 105, 78, 156, 126, 425, 409, 245, 260) %&gt;% as.integer(), awards = c(22, 10, 26, 9, 18, 2, 33, 32, 30, 13, 12, 17, 38, 18, 65, 47, 46, 29) %&gt;% as.integer(), rejects = c(61, 29, 109, 30, 49, 7, 197, 134, 159, 49, 93, 61, 118, 108, 360, 362, 199, 231) %&gt;% as.integer(), male = ifelse(gender == &quot;f&quot;, 0, 1) %&gt;% as.integer() ) funding ## # A tibble: 18 x 6 ## discipline gender applications awards rejects male ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Chemical sciences m 83 22 61 1 ## 2 Chemical sciences f 39 10 29 0 ## 3 Physical sciences m 135 26 109 1 ## 4 Physical sciences f 39 9 30 0 ## 5 Physics m 67 18 49 1 ## 6 Physics f 9 2 7 0 ## 7 Humanities m 230 33 197 1 ## 8 Humanities f 166 32 134 0 ## 9 Technical sciences m 189 30 159 1 ## 10 Technical sciences f 62 13 49 0 ## 11 Interdisciplinary m 105 12 93 1 ## 12 Interdisciplinary f 78 17 61 0 ## 13 Earth/life sciences m 156 38 118 1 ## 14 Earth/life sciences f 126 18 108 0 ## 15 Social sciences m 425 65 360 1 ## 16 Social sciences f 409 47 362 0 ## 17 Medical sciences m 245 46 199 1 ## 18 Medical sciences f 260 29 231 0 Let’s fit a few models. First, we’ll fit an analogue to the initial van der Lee and Ellemers \\(\\chi^2\\) test. Since we’re Bayesian modelers, we’ll use a simple logistic regression, using male (dummy coded 0 = female, 1 = male) to predict admission (i.e., awards). b13.8 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male, # note our continued use of weakly-regularizing priors prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b)), iter = 5000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.08&quot;) If you inspect them, the chains look great. We’ll jump straight to the posterior summaries. tidy(b13.8) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.75 0.08 -1.88 -1.61 ## 2 b_male 0.21 0.10 0.04 0.38 Yep, the 95% intervals for male dummy exclude zero. If you wanted a one-sided Bayesian \\(p\\)-value, you might do something like this. posterior_samples(b13.8) %&gt;% summarise(one_sided_Bayesian_p_value = mean(b_male &lt;= 0)) ## one_sided_Bayesian_p_value ## 1 0.02225 Pretty small. But recall how Simpson’s paradox helped us understand the Berkley data. [If you need a refresher on Simpson’s paradox, go here.] Different departments in Berkley had different acceptance rates AND different ratios of male and female applicants. Similarly, different academic disciplines in the Netherlands might have different award rates for funding AND different ratios of male and female applications. Just like in section 13.2, let’s fit two more models. The first model will allow intercepts to vary by discipline. The second model will allow intercepts and the male dummy slopes to vary by discipline. b13.9 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99), seed = 13, file = &quot;fits/b13.09&quot;) b13.10 &lt;- brm(data = funding, family = binomial, awards | trials(applications) ~ 1 + male + (1 + male | discipline), prior = c(prior(normal(0, 4), class = Intercept), prior(normal(0, 4), class = b), prior(cauchy(0, 1), class = sd), prior(lkj(4), class = cor)), iter = 5000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .99), seed = 13, file = &quot;fits/b13.10&quot;) We’ll compare the models with information criteria. b13.8 &lt;- add_criterion(b13.8, &quot;waic&quot;) b13.9 &lt;- add_criterion(b13.9, &quot;waic&quot;) b13.10 &lt;- add_criterion(b13.10, &quot;waic&quot;) loo_compare(b13.8, b13.9, b13.10, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.10 0.0 0.0 -58.3 2.8 8.8 1.0 116.6 5.6 ## b13.9 -4.6 1.4 -62.9 3.7 10.1 1.5 125.9 7.3 ## b13.8 -6.6 2.8 -64.9 4.5 4.8 1.4 129.8 8.9 The WAIC suggests the varying intercepts/varying slopes model made the best sense of the data. The WAIC weights tell a similar story. model_weights(b13.8, b13.9, b13.10, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b13.8 b13.9 b13.10 ## 0.00 0.01 0.99 Here we’ll practice our tidybayes::spread_draws() skills from the end of the last chapter to see what the random intercepts from the full model look like in a coefficient plot. b13.10 %&gt;% spread_draws(b_male, r_discipline[discipline,term]) %&gt;% filter(term == &quot;male&quot;) %&gt;% ungroup() %&gt;% mutate(effect = b_male + r_discipline, discipline = str_replace(discipline, &quot;[.]&quot;, &quot; &quot;)) %&gt;% ggplot(aes(x = effect, y = reorder(discipline, effect))) + geom_vline(xintercept = 0, color = &quot;#E8DCCF&quot;, alpha = 1/2) + geom_halfeyeh(.width = .95, size = .9, scale = .9, color = &quot;#80A0C7&quot;, fill = &quot;#394165&quot;) + labs(title = &quot;Random slopes for the male dummy&quot;, subtitle = &quot;The dots and horizontal lines are the posterior means and percentile-based\\n95% intervals, respectively. The values are on the log scale.&quot;, x = NULL, y = NULL) + coord_cartesian(xlim = c(-1.5, 1.5)) + theme_pearl_earring(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Note how the 95% intervals for all the random male slopes contain zero within their bounds. Here are the fixed effects. tidy(b13.10) %&gt;% filter(str_detect(term , &quot;b_&quot;)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.63 0.14 -1.85 -1.38 ## 2 b_male 0.15 0.17 -0.13 0.42 And if you wanted a one-sided Bayesian \\(p\\)-value for the male dummy for the full model, you execute something like this. posterior_samples(b13.10) %&gt;% summarise(one_sided_Bayesian_p_value = mean(b_male &lt;= 0)) ## one_sided_Bayesian_p_value ## 1 0.1725625 Here’s a fuller look at the posterior. posterior_samples(b13.10) %&gt;% ggplot(aes(x = b_male, y = 0)) + geom_vline(xintercept = 0, color = &quot;#E8DCCF&quot;, alpha = 1/2) + stat_halfeyeh(.width = c(.5, .95), color = &quot;#80A0C7&quot;, fill = &quot;#394165&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;b_male (i.e., the population estimate for gender bias)&quot;) + coord_cartesian(xlim = c(-1.5, 1.5)) + theme_pearl_earring() So, the estimate of the gender bias is small and consistent with the null hypothesis. Which is good! We want gender equality for things like funding success. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 ggbeeswarm_0.6.0 ggrepel_0.8.1 broom_0.5.3 ## [5] patchwork_1.0.0 Rcpp_1.0.3 tidybayes_2.0.1.9000 rstan_2.19.2 ## [9] StanHeaders_2.19.0 dutchmasters_0.1.0 forcats_0.4.0 stringr_1.4.0 ## [13] dplyr_0.8.4 purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 ## [17] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 dbplyr_1.4.2 ## [22] shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 htmltools_0.4.0 prettyunits_1.1.1 ## [34] tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [43] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [46] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 nleqslv_3.3.2 ## [52] MASS_7.3-51.4 zoo_1.8-7 scales_1.1.0 ## [55] colourpicker_1.0 hms_0.5.3 promises_1.1.0 ## [58] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.1 gridExtra_2.3 loo_2.2.0 ## [64] stringi_1.4.5 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [67] rlang_0.4.4 pkgconfig_2.0.3 matrixStats_0.55.0 ## [70] HDInterval_0.2.0 invgamma_1.1 evaluate_0.14 ## [73] lattice_0.20-38 rstantools_2.0.0 htmlwidgets_1.5.1 ## [76] labeling_0.3 processx_3.4.1 tidyselect_1.0.0 ## [79] plyr_1.8.5 magrittr_1.5 R6_2.4.1 ## [82] generics_0.0.2 DBI_1.1.0 pillar_1.4.3 ## [85] haven_2.2.0 withr_2.1.2 xts_0.12-0 ## [88] abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## [91] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_2.0 ## [94] grid_3.6.2 readxl_1.3.1 callr_3.4.1 ## [97] threejs_0.3.3 reprex_0.3.0 digest_0.6.23 ## [100] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.2 ## [103] munsell_0.5.0 beeswarm_0.2.3 vipor_0.4.5 ## [106] shinyjs_1.1 "],
["missing-data-and-other-opportunities.html", "14 Missing Data and Other Opportunities 14.1 Measurement error 14.2 Missing data 14.3 Summary Bonus: Meta-analysis Reference Session info", " 14 Missing Data and Other Opportunities For the opening example, we’re playing with the conditional probability \\[ \\text{Pr(burnt down | burnt up)} = \\frac{\\text{Pr(burnt up, burnt down)}}{\\text{Pr(burnt up)}}. \\] It works out that \\[ \\text{Pr(burnt down | burnt up)} = \\frac{1/3}{1/2} = \\frac{2}{3}. \\] We might express the math in the middle of page 423 in tibble form like this. library(tidyverse) p_pancake &lt;- 1/3 ( d &lt;- tibble(pancake = c(&quot;BB&quot;, &quot;BU&quot;, &quot;UU&quot;), p_burnt = c(1, .5, 0)) %&gt;% mutate(p_burnt_up = p_burnt * p_pancake) ) ## # A tibble: 3 x 3 ## pancake p_burnt p_burnt_up ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BB 1 0.333 ## 2 BU 0.5 0.167 ## 3 UU 0 0 d %&gt;% summarise(`p (burnt_down | burnt_up)` = p_pancake / sum(p_burnt_up)) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.667 I understood McElreath’s simulation better after breaking it apart. The first part of sim_pancake() takes one random draw from the integers 1, 2, and 3. It just so happens that if we set set.seed(1), the code returns a 1. set.seed(1) sample(x = 1:3, size = 1) ## [1] 1 So here’s what it looks like if we use seeds 2:11. take_sample &lt;- function(seed) { set.seed(seed) sample(x = 1:3, size = 1) } tibble(seed = 2:11) %&gt;% mutate(value_returned = map_dbl(seed, take_sample)) ## # A tibble: 10 x 2 ## seed value_returned ## &lt;int&gt; &lt;dbl&gt; ## 1 2 1 ## 2 3 1 ## 3 4 3 ## 4 5 2 ## 5 6 1 ## 6 7 2 ## 7 8 3 ## 8 9 3 ## 9 10 3 ## 10 11 2 Each of those value_returned values stands for one of the three pancakes: 1 = BB, 2 = BU, and 3 = UU. In the next line, McElreath made slick use of a matrix to specify that. Here’s what the matrix looks like. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] 1 0 0 See how the three columns are identified as [,1], [,2], and [,3]? If, say, we wanted to subset the values in the second column, we’d execute matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] ## [1] 1 0 which returns a numeric vector. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] %&gt;% str() ## num [1:2] 1 0 And that 1 0 corresponds to the pancake with one burnt (i.e., 1) and one unburnt (i.e., 0) side. So when McElreath then executed sample(sides), he randomly sampled from one of those two values. In the case of pancake == 2, he randomly sampled one the pancake with one burnt and one unburnt side. Had he sampled from pancake == 1, he would have sampled from the pancake with both sides burnt. Going forward, let’s amend McElreath’s sim_pancake() function so it will take a seed argument, which will allow us to make the output reproducible. # simulate a `pancake` and return randomly ordered `sides` sim_pancake &lt;- function(seed) { set.seed(seed) pancake &lt;- sample(x = 1:3, size = 1) sides &lt;- matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, pancake] sample(sides) } Let’s take this baby for a whirl. # how many simulations would you like? n_sim &lt;- 1e4 d &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(burnt = map(seed, sim_pancake)) %&gt;% unnest(burnt) %&gt;% mutate(side = rep(c(&quot;up&quot;, &quot;down&quot;), times = n() / 2)) Take a look at what we’ve done. head(d, n = 10) ## # A tibble: 10 x 3 ## seed burnt side ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 up ## 2 1 1 down ## 3 2 1 up ## 4 2 1 down ## 5 3 1 up ## 6 3 1 down ## 7 4 0 up ## 8 4 0 down ## 9 5 1 up ## 10 5 0 down And now we’ll spread() and summarise() to get the value we’ve been working for. d %&gt;% spread(key = side, value = burnt) %&gt;% summarise(`p (burnt_down | burnt_up)` = sum(up == 1 &amp; down == 1) / (sum(up))) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.658 The results are within rounding error of the ideal 2/3. Probability theory is not difficult mathematically. It’s just counting. But it is hard to interpret and apply. Doing so often seems to require some cleverness, and authors have an incentive to solve problems in clever ways, just to show off. But we don’t need that cleverness, if we ruthlessly apply conditional probability… In this chapter, [we’ll] meet two commonplace applications of this assume-and-deduce strategy. The first is the incorporation of measurement error into our models. The second is the estimation of missing data through Bayesian imputation… In neither application do [we] have to intuit the consequences of measurement errors nor the implications of missing values in order to design the models. All [we] have to do is state [the] information about the error or about the variables with missing values. Logic does the rest. (p. 424) 14.1 Measurement error Let’s grab those WaffleDivorce data from back in Chapter 5. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce rm(WaffleDivorce) Switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) The brms package currently supports theme_black(), which changes the default ggplot2 theme to a black background with white lines, text, and so forth. The origins of this version of the code are from this post on Jon Lefcheck’s website. Though I like the idea of brms including theme_black(), I’m not a fan of some of the default settings (e.g., it includes gridlines). Happily, data scientist Tyler Rinker has some nice alternative theme_black() code you can find here. The version of theme_black() used for this chapter is based on his version, with a few amendments of my own. theme_black &lt;- function(base_size=12, base_family=&quot;&quot;) { theme_grey(base_size=base_size, base_family=base_family) %+replace% theme( # specify axis options axis.line=element_blank(), # all text colors used to be &quot;grey55&quot; axis.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9, vjust=1), axis.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, lineheight=0.9,hjust=1), axis.ticks=element_line(color=&quot;grey55&quot;, size = 0.2), axis.title.x=element_text(size=base_size, color=&quot;grey85&quot;, vjust=1, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;)), axis.title.y=element_text(size=base_size, color=&quot;grey85&quot;, angle=90, margin=ggplot2::margin(.5, 0, 0, 0, &quot;lines&quot;), vjust=0.5), axis.ticks.length=grid::unit(0.3, &quot;lines&quot;), # specify legend options legend.background=element_rect(color=NA, fill=&quot;black&quot;), legend.key=element_rect(color=&quot;grey55&quot;, fill=&quot;black&quot;), legend.key.size=grid::unit(1.2, &quot;lines&quot;), legend.key.height=NULL, legend.key.width=NULL, legend.text=element_text(size=base_size*0.8, color=&quot;grey85&quot;), legend.title=element_text(size=base_size*0.8, face=&quot;bold&quot;,hjust=0, color=&quot;grey85&quot;), # legend.position=&quot;right&quot;, legend.position = &quot;none&quot;, legend.text.align=NULL, legend.title.align=NULL, legend.direction=&quot;vertical&quot;, legend.box=NULL, # specify panel options panel.background=element_rect(fill=&quot;black&quot;, color = NA), panel.border=element_rect(fill=NA, color=&quot;grey55&quot;), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.spacing=grid::unit(0.25,&quot;lines&quot;), # specify facetting options strip.background=element_rect(fill = &quot;black&quot;, color=&quot;grey10&quot;), # fill=&quot;grey30&quot; strip.text.x=element_text(size=base_size*0.8, color=&quot;grey85&quot;), strip.text.y=element_text(size=base_size*0.8, color=&quot;grey85&quot;, angle=-90), # specify plot options plot.background=element_rect(color=&quot;black&quot;, fill=&quot;black&quot;), plot.title=element_text(size=base_size*1.2, color=&quot;grey85&quot;, hjust = 0), # added hjust = 0 plot.subtitle=element_text(size=base_size*.9, color=&quot;grey85&quot;, hjust = 0), # added line # plot.margin=grid::unit(c(1, 1, 0.5, 0.5), &quot;lines&quot;) plot.margin=grid::unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;) ) } One way to use our theme_black() is to make it part of the code for an individual plot, such as ggplot() + geom_point() + theme_back(). Another way is to make theme_black() the default setting with ggplot2::theme_set(). That’s the method we’ll use. theme_set(theme_black()) # to reset the default ggplot2 theme to its default parameters, # execute `theme_set(theme_default())` In the brms reference manual, Bürkner recommended complimenting theme_black() with color scheme “C” from the viridis package, which provides a variety of colorblind-safe color palettes. # install.packages(&quot;viridis&quot;) library(viridis) The viridis_pal() function gives a list of colors within a given palette. The colors in each palette fall on a spectrum. Within viridis_pal(), the option argument allows one to select a given spectrum, “C”, in our case. The final parentheses, (), allows one to determine how many discrete colors one would like to break the spectrum up by. We’ll choose 7. viridis_pal(option = &quot;C&quot;)(7) ## [1] &quot;#0D0887FF&quot; &quot;#5D01A6FF&quot; &quot;#9C179EFF&quot; &quot;#CC4678FF&quot; &quot;#ED7953FF&quot; &quot;#FDB32FFF&quot; &quot;#F0F921FF&quot; With a little data wrangling, we can put the colors of our palette in a tibble and display them in a plot. tibble(number = 1:7, color_number = str_c(1:7, &quot;. &quot;, viridis_pal(option = &quot;C&quot;)(7))) %&gt;% ggplot(aes(x = factor(0), y = reorder(color_number, number))) + geom_tile(aes(fill = factor(number))) + geom_text(aes(color = factor(number), label = color_number)) + scale_color_manual(values = c(rep(&quot;black&quot;, times = 4), rep(&quot;white&quot;, times = 3))) + scale_fill_viridis(option = &quot;C&quot;, discrete = T, direction = -1) + scale_x_discrete(NULL, breaks = NULL) + scale_y_discrete(NULL, breaks = NULL) + ggtitle(&quot;Behold: viridis C!&quot;) Now, let’s make use of our custom theme and reproduce/reimagine Figure 14.1.a. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] p1 &lt;- d %&gt;% ggplot(aes(x = MedianAgeMarriage, y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate&quot;) Notice how viridis_pal(option = &quot;C&quot;)(7)[7] called the seventh color in the color scheme, &quot;#F0F921FF&quot;. For Figure 14.1.b, we’ll select the sixth color in the palette by coding viridis_pal(option = &quot;C&quot;)(7)[6]. We’ll then combine the two subplots with patchwork. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6] p2 &lt;- d %&gt;% ggplot(aes(x = log(Population), y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;log population&quot;) library(patchwork) p1 | p2 Just like in the text, our plot shows states with larger populations tend to have smaller measurement error. The relation between measurement error and MedianAgeMarriage is less apparent. 14.1.1 Error on the outcome. To get a better sense of what we’re about to do, imagine for a moment that each state’s divorce rate is normally distributed with a mean of Divorce and standard deviation Divorce.SE. Those distributions would be: d %&gt;% mutate(Divorce_distribution = str_c(&quot;Divorce ~ Normal(&quot;, Divorce, &quot;, &quot;, Divorce.SE, &quot;)&quot;)) %&gt;% select(Loc, Divorce_distribution) %&gt;% head() ## Loc Divorce_distribution ## 1 AL Divorce ~ Normal(12.7, 0.79) ## 2 AK Divorce ~ Normal(12.5, 2.05) ## 3 AZ Divorce ~ Normal(10.8, 0.74) ## 4 AR Divorce ~ Normal(13.5, 1.22) ## 5 CA Divorce ~ Normal(8, 0.24) ## 6 CO Divorce ~ Normal(11.6, 0.94) As in the text, in [the following] example we’ll use a Gaussian distribution with mean equal to the observed value and standard deviation equal to the measurement’s standard error. This is the logical choice, because if all we know about the error is its standard deviation, then the maximum entropy distribution for it will be Gaussian… Here’s how to define the distribution for each divorce rate. For each observed value \\(D_{\\text{OBS},i}\\), there will be one parameter, \\(D_{\\text{EST},i}\\), defined by: \\[D_{\\text{OBS},i} \\sim \\text{Normal} (D_{\\text{EST},i}, D_{\\text{SE},i})\\] All this does is define the measurement \\(D_{\\text{OBS},i}\\) as having the specified Gaussian distribution centered on the unknown parameter \\(D_{\\text{EST},i}\\). So the above defines a probability for each State \\(i\\)’s observed divorce rate, given a known measurement error. (pp. 426–427) Now we’re ready to fit some models. In brms, there are at least two ways to accommodate measurement error in the criterion. The first way uses the se() syntax, following the form &lt;response&gt; | se(&lt;se_response&gt;, sigma = TRUE). With this syntax, se stands for standard error, the loose frequentist analogue to the Bayesian posterior \\(SD\\). Unless you’re fitting a meta-analysis on summary information, which we’ll be doing at the end of this chapter, make sure to specify sigma = TRUE. Without that you’ll have no posterior for \\(\\sigma\\)! For more information on the se() method, go to the brms reference manual and find the Additional response information subsection of the brmsformula section. The second way uses the mi() syntax, following the form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the newer missing data capabilities for brms. We will cover that in more detail in the second half of this chapter. We’ll start off using both methods. Our first model, b14.1_se, will follow the se() syntax; the second model, b14.1_mi, will follow the mi() syntax. # put the data into a `list()` dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, R = d$Marriage, A = d$MedianAgeMarriage) # here we specify the initial (i.e., starting) values inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # fit the models b14.1_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + Intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), inits = inits_list, file = &quot;fits/b14.01_se&quot;) b14.1_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + Intercept + R + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, # note this line for the `mi()` model inits = inits_list, file = &quot;fits/b14.01_mi&quot;) Before we dive into the model summaries, notice how the starting values (i.e., inits) differ by model. Even though we coded inits = inits_list for both models, the differ by fit@inits. b14.1_se$fit@inits ## [[1]] ## [[1]]$b ## [1] 0.6133048 -1.9171497 1.7551789 ## ## [[1]]$sigma ## [1] 0.4668127 ## ## ## [[2]] ## [[2]]$b ## [1] 0.9114156 1.2512265 -0.4276127 ## ## [[2]]$sigma ## [1] 1.906943 b14.1_mi$fit@inits ## [[1]] ## [[1]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[1]]$b ## [1] -0.5034648 1.1693530 -1.0539336 ## ## [[1]]$sigma ## [1] 1.281562 ## ## ## [[2]] ## [[2]]$Yl ## [1] 12.7 12.5 10.8 13.5 8.0 11.6 6.7 8.9 6.3 8.5 11.5 8.3 7.7 8.0 11.0 10.2 10.6 12.6 11.0 ## [20] 13.0 8.8 7.8 9.2 7.4 11.1 9.5 9.1 8.8 10.1 6.1 10.2 6.6 9.9 8.0 9.5 12.8 10.4 7.7 ## [39] 9.4 8.1 10.9 11.4 10.0 10.2 9.6 8.9 10.0 10.9 8.3 10.3 ## ## [[2]]$b ## [1] -0.1543955 1.1642108 -0.4231833 ## ## [[2]]$sigma ## [1] 4.802142 As we explore further, it should become apparent why. Here are the primary model summaries. print(b14.1_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + Intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.21 6.62 7.81 33.76 1.00 1903 2405 ## R 0.13 0.08 -0.02 0.28 1.00 2242 2765 ## A -0.55 0.21 -0.94 -0.11 1.00 2055 2639 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.13 0.21 0.77 1.57 1.00 3136 3721 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.1_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + Intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 21.37 6.52 8.50 33.82 1.00 3865 5142 ## R 0.13 0.08 -0.02 0.27 1.00 4487 5820 ## A -0.55 0.21 -0.95 -0.13 1.00 3963 5029 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.13 0.21 0.76 1.56 1.00 3133 4489 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Based on the print()/summary() information, the main parameters for the models are about the same. However, the plot deepens when we summarize the models with the broom::tidy() method. library(broom) tidy(b14.1_se) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept 21.21 6.62 10.19 31.85 ## 2 b_R 0.13 0.08 0.00 0.26 ## 3 b_A -0.55 0.21 -0.89 -0.19 ## 4 sigma 1.13 0.21 0.82 1.49 ## 5 lp__ -105.41 1.42 -108.18 -103.71 tidy(b14.1_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept 21.37 6.52 10.68 31.79 ## 2 b_R 0.13 0.08 0.00 0.25 ## 3 b_A -0.55 0.21 -0.89 -0.20 ## 4 sigma 1.13 0.21 0.80 1.48 ## 5 Yl[1] 11.79 0.68 10.70 12.92 ## 6 Yl[2] 11.20 1.04 9.53 12.92 ## 7 Yl[3] 10.47 0.63 9.44 11.51 ## 8 Yl[4] 12.31 0.85 10.94 13.74 ## 9 Yl[5] 8.05 0.24 7.66 8.44 ## 10 Yl[6] 11.02 0.74 9.82 12.25 ## 11 Yl[7] 7.23 0.64 6.17 8.27 ## 12 Yl[8] 9.35 0.91 7.84 10.82 ## 13 Yl[9] 7.00 1.09 5.22 8.76 ## 14 Yl[10] 8.54 0.30 8.04 9.04 ## 15 Yl[11] 11.15 0.54 10.27 12.04 ## 16 Yl[12] 9.10 0.90 7.60 10.57 ## 17 Yl[13] 9.69 0.91 8.15 11.14 ## 18 Yl[14] 8.11 0.42 7.43 8.79 ## 19 Yl[15] 10.68 0.54 9.79 11.57 ## 20 Yl[16] 10.17 0.71 9.02 11.32 ## 21 Yl[17] 10.50 0.78 9.22 11.80 ## 22 Yl[18] 11.95 0.63 10.94 12.98 ## 23 Yl[19] 10.50 0.69 9.39 11.63 ## 24 Yl[20] 10.18 1.02 8.53 11.90 ## 25 Yl[21] 8.76 0.59 7.78 9.74 ## 26 Yl[22] 7.77 0.48 6.98 8.54 ## 27 Yl[23] 9.14 0.47 8.36 9.92 ## 28 Yl[24] 7.74 0.54 6.84 8.62 ## 29 Yl[25] 10.43 0.76 9.20 11.70 ## 30 Yl[26] 9.54 0.58 8.58 10.51 ## 31 Yl[27] 9.42 0.96 7.84 11.01 ## 32 Yl[28] 9.26 0.74 8.05 10.42 ## 33 Yl[29] 9.17 0.96 7.62 10.75 ## 34 Yl[30] 6.38 0.44 5.64 7.11 ## 35 Yl[31] 9.98 0.79 8.66 11.29 ## 36 Yl[32] 6.69 0.30 6.21 7.17 ## 37 Yl[33] 9.88 0.44 9.17 10.61 ## 38 Yl[34] 9.77 0.96 8.15 11.29 ## 39 Yl[35] 9.43 0.42 8.75 10.13 ## 40 Yl[36] 11.97 0.79 10.70 13.26 ## 41 Yl[37] 10.07 0.66 9.00 11.17 ## 42 Yl[38] 7.80 0.40 7.14 8.45 ## 43 Yl[39] 8.21 1.00 6.62 9.91 ## 44 Yl[40] 8.40 0.59 7.41 9.37 ## 45 Yl[41] 10.01 1.04 8.30 11.75 ## 46 Yl[42] 10.94 0.64 9.88 11.99 ## 47 Yl[43] 10.02 0.34 9.46 10.57 ## 48 Yl[44] 11.08 0.78 9.79 12.38 ## 49 Yl[45] 8.90 0.99 7.31 10.54 ## 50 Yl[46] 9.01 0.46 8.24 9.78 ## 51 Yl[47] 9.96 0.56 9.04 10.87 ## 52 Yl[48] 10.61 0.89 9.16 12.07 ## 53 Yl[49] 8.46 0.51 7.63 9.30 ## 54 Yl[50] 11.53 1.09 9.67 13.26 ## 55 lp__ -152.51 6.58 -163.76 -141.95 # you can get similar output with `b14.1_mi$fit` Again, from b_Intercept to sigma, the output is about the same. But model b14.1_mi, based on the mi() syntax, contained posterior summaries for all 50 of the criterion values. The se() method gave us similar model result, but no posterior summaries for the 50 criterion values. The rethinking package indexed those additional 50 as div_est[i]; with the mi() method, brms indexed them as Yl[i]–no big deal. So while both brms methods accommodated measurement error, the mi() method appears to be the brms analogue to what McElreath did with his model m14.1 in the text. Thus, it’s our b14.1_mi model that follows the form \\[\\begin{align*} \\text{Divorce}_{\\text{estimated}, i} &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu &amp; = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\text R_i \\\\ \\text{Divorce}_{\\text{observed}, i} &amp; \\sim \\text{Normal} (\\text{Divorce}_{\\text{estimated}, i}, \\text{Divorce}_{\\text{standard error}, i}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 2.5). \\end{align*}\\] Note. The normal(0, 10) prior McElreath used was quite informative and can lead to discrepancies between the rethinking and brms results if you’re not careful. A large issue is the default way brms handles intercept priors. From the hyperlink, Bürkner wrote: The formula for the original intercept is b_intercept = temp_intercept - dot_product(means_X, b), where means_X is the vector of means of the predictor variables and b is the vector of regression coefficients (fixed effects). That is, when transforming a prior on the intercept to an “equivalent” prior on the temporary intercept, you have to take the means of the predictors and well as the priors on the other coefficients into account. If this seems confusing, you have an alternative. The 0 + intercept part of the brm formula kept the intercept in the metric of the untransformed data, leading to similar results to those from rethinking. When your priors are vague, this might not be much of an issue. And since many of the models in Statistical Rethinking use only weakly-regularizing priors, this hasn’t been much of an issue up to this point. But this model is quite sensitive to the intercept syntax. My general recommendation for applied data analysis is this: If your predictors aren’t mean centered, default to the 0 + intercept syntax for the formula argument when using brms::brm(). Otherwise, your priors might not be doing what you think they’re doing. Anyway, since our mi()-syntax b14.1_mi model appears to be the analogue to McElreath’s m14.1, we’ll use that one for our plots. Here’s the code for our Figure 14.2.a. data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5] p1 &lt;- data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) + labs(x = &quot;Observed standard error for divorce&quot;, y = &quot;Divorce (estimate - observed)&quot;) Before we make Figure 14.2.b, we need to fit a model that ignores measurement error. b14.1b &lt;- brm(data = dlist, family = gaussian, div_obs ~ 0 + Intercept + R + A, prior = c(prior(normal(0, 50), class = b, coef = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), chains = 2, iter = 5000, warmup = 1000, cores = 2, seed = 14, control = list(adapt_delta = 0.95), file = &quot;fits/b14.01b&quot;) print(b14.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs ~ 0 + Intercept + R + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 36.06 7.71 20.55 50.81 1.00 1828 2347 ## R -0.05 0.08 -0.21 0.12 1.00 1926 2642 ## A -0.97 0.25 -1.45 -0.48 1.00 1923 2597 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.51 0.16 1.24 1.88 1.00 3286 3607 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With the ignore-measurement-error fit in hand, we’re ready for Figure 14.2.b. nd &lt;- tibble(R = mean(d$Marriage), A = seq(from = 22, to = 30.2, length.out = 30), div_sd = mean(d$Divorce.SE)) # red line f_error &lt;- fitted(b14.1_mi, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # yellow line f_no_error &lt;- fitted(b14.1b, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) # white dots data_error &lt;- fitted(b14.1_mi) %&gt;% as_tibble() %&gt;% bind_cols(b14.1_mi$data) color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_r &lt;- viridis_pal(option = &quot;C&quot;)(7)[4] # plot p2 &lt;- f_no_error %&gt;% ggplot(aes(x = A, y = Estimate)) + # `f_no_error` geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = color_y, color = color_y, alpha = 1/4, size = 1/2, linetype = 2) + # `f_error` geom_smooth(data = f_error, aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = color_r, color = color_r, alpha = 1/3, size = 1/2, linetype = 1) + geom_pointrange(data = data_error, aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), color = &quot;white&quot;, shape = 20, alpha = 1/2) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(xlim = range(data_error$A), ylim = c(4, 15)) p1 | p2 In our plot on the right, it’s the reddish regression line that accounts for measurement error. 14.1.2 Error on both outcome and predictor. In brms, you can specify error on predictors with an me() statement in the form of me(predictor, sd_predictor) where sd_predictor is a vector in the data denoting the size of the measurement error, presumed to be in a standard-deviation metric. # the data dlist &lt;- list( div_obs = d$Divorce, div_sd = d$Divorce.SE, mar_obs = d$Marriage, mar_sd = d$Marriage.SE, A = d$MedianAgeMarriage) # the `inits` inits &lt;- list(Yl = dlist$div_obs) inits_list &lt;- list(inits, inits) # the models b14.2_se &lt;- brm(data = dlist, family = gaussian, div_obs | se(div_sd, sigma = TRUE) ~ 0 + Intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, chains = 3, cores = 3, seed = 14, control = list(adapt_delta = 0.95), save_mevars = TRUE, # note the lack if `inits` file = &quot;fits/b14.02_se&quot;) b14.2_mi &lt;- brm(data = dlist, family = gaussian, div_obs | mi(div_sd) ~ 0 + Intercept + me(mar_obs, mar_sd) + A, prior = c(prior(normal(0, 10), class = b), prior(cauchy(0, 2.5), class = sigma)), iter = 5000, warmup = 1000, cores = 2, chains = 2, seed = 14, control = list(adapt_delta = 0.99, max_treedepth = 12), save_mevars = TRUE, inits = inits_list, file = &quot;fits/b14.02_mi&quot;) We already know including inits values for our Yl[i] estimates is a waste of time for our se() model. But note how we still defined our inits values as inits &lt;- list(Yl = dlist$div_obs) for the mi() model. Although it’s easy in brms to set the starting values for our Yl[i] estimates, much the way McElreath did, that is not the case when you have measurement error on the predictors. The brms package uses a non-centered parameterization for these, which requires users to have a deeper understanding of the underlying Stan code. This is where I get off the train, but if you want to go further, execute stancode(b14.2_mi). Here are the two versions of the model. print(b14.2_se) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | se(div_sd, sigma = TRUE) ~ 0 + Intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 3 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 15.84 6.68 2.50 28.55 1.00 4694 7844 ## A -0.45 0.20 -0.83 -0.05 1.00 5398 7807 ## memar_obsmar_sd 0.27 0.10 0.07 0.48 1.00 5457 7956 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.00 0.21 0.61 1.44 1.00 12263 8222 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b14.2_mi) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: div_obs | mi(div_sd) ~ 0 + Intercept + me(mar_obs, mar_sd) + A ## Data: dlist (Number of observations: 50) ## Samples: 2 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 15.74 6.54 2.60 28.45 1.00 2228 3709 ## A -0.44 0.20 -0.83 -0.05 1.00 2435 4325 ## memar_obsmar_sd 0.27 0.10 0.07 0.48 1.00 2199 3696 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.00 0.21 0.62 1.45 1.00 1755 2472 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll use broom::tidy(), again, to get a sense of depth=2 summaries. tidy(b14.2_se) %&gt;% mutate_if(is.numeric, round, digits = 2) tidy(b14.2_mi) %&gt;% mutate_if(is.numeric, round, digits = 2) Due to space concerns, I’m not going to show the results, here. You can do that on your own. Both methods yielded the posteriors for Xme_memar_obs[1], but only the b14.2_mi model based on the mi() syntax yielded posteriors for the criterion, the Yl[i] summaries. Note that you’ll need to specify save_mevars = TRUE in the brm() function in order to save the posterior samples of error-adjusted variables obtained by using the me() argument. Without doing so, functions like predict() may give you trouble. Here is the code for Figure 14.3.a. data_error &lt;- fitted(b14.2_mi) %&gt;% as_tibble() %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[3] p1 &lt;- data_error %&gt;% ggplot(aes(x = Divorce.SE, y = Estimate - Divorce)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, size = 2, color = color) + labs(x = &quot;Observed standard error for marriage rate&quot;, y = &quot;Marriage rate (estimate - observed)&quot;) To get the posterior samples for error-adjusted Marriage rate, we’ll use posterior_samples. If you examine the object with glimpse(), you’ll notice 50 Xme_memar_obsmar_sd[i] vectors, with \\(i\\) ranging from 1 to 50, each corresponding to one of the 50 states. With a little data wrangling, you can get the mean of each to put in a plot. Once we have those summaries, we can make our version of Figure 14.4.b. color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_p &lt;- viridis_pal(option = &quot;C&quot;)(7)[2] p2 &lt;- posterior_samples(b14.2_mi) %&gt;% select(starts_with(&quot;Xme&quot;)) %&gt;% gather() %&gt;% # this extracts the numerals from the otherwise cumbersome names in `key` and saves them as integers mutate(key = str_extract(key, &quot;\\\\d+&quot;) %&gt;% as.integer()) %&gt;% group_by(key) %&gt;% summarise(mean = mean(value)) %&gt;% bind_cols(data_error) %&gt;% ggplot(aes(x = mean, y = Estimate)) + geom_segment(aes(xend = Marriage, yend = Divorce), color = &quot;white&quot;, size = 1/4) + geom_point(size = 2, alpha = 2/3, color = color_y) + geom_point(aes(x = Marriage, y = Divorce), size = 2, alpha = 2/3, color = color_p) + scale_y_continuous(breaks = seq(from = 4, to = 14, by = 2)) + labs(x = &quot;Marriage rate (posterior)&quot; , y = &quot;Divorce rate (posterior)&quot;) + coord_cartesian(ylim = c(4, 14.5)) p1 | p2 In the right panel, the yellow points are model-implied; the purple ones are of the original data. It turns out our brms model regularized more aggressively than McElreath’s rethinking model. I’m unsure of why. If you understand the difference, please share with the rest of the class. Anyway, the big take home point for this section is that when you have a distribution of values, don’t reduce it down to a single value to use in a regression. Instead, use the entire distribution. Anytime we use an average value, discarding the uncertainty around that average, we risk overconfidence and spurious inference. This doesn’t only apply to measurement error, but also to cases which data are averaged before analysis. Do not average. Instead, model. (p. 431) 14.2 Missing data Starting with version 2.2.0 brms now supports Bayesian missing data imputation using adaptations of the multivariate syntax. Bürkner’s Handle Missing Values with brms vignette is quite helpful. 14.2.1 Imputing neocortex Once again, here are the milk data. library(rethinking) data(milk) d &lt;- milk d &lt;- d %&gt;% mutate(neocortex.prop = neocortex.perc / 100, logmass = log(mass)) Now we’ll switch out rethinking for brms and do a little data wrangling. detach(package:rethinking, unload = T) library(brms) rm(milk) # prep data data_list &lt;- list(kcal = d$kcal.per.g, neocortex = d$neocortex.prop, logmass = d$logmass) Here’s the structure of our data list. data_list ## $kcal ## [1] 0.49 0.51 0.46 0.48 0.60 0.47 0.56 0.89 0.91 0.92 0.80 0.46 0.71 0.71 0.73 0.68 0.72 0.97 0.79 ## [20] 0.84 0.48 0.62 0.51 0.54 0.49 0.53 0.48 0.55 0.71 ## ## $neocortex ## [1] 0.5516 NA NA NA NA 0.6454 0.6454 0.6764 NA 0.6885 0.5885 0.6169 0.6032 ## [14] NA NA 0.6997 NA 0.7041 NA 0.7340 NA 0.6753 NA 0.7126 0.7260 NA ## [27] 0.7024 0.7630 0.7549 ## ## $logmass ## [1] 0.6678294 0.7371641 0.9202828 0.4824261 0.7839015 1.6582281 1.6808279 0.9202828 ## [9] -0.3424903 -0.3856625 -2.1202635 -0.7550226 -1.1394343 -0.5108256 1.2441546 0.4382549 ## [17] 1.9572739 1.1755733 2.0719133 2.5095993 2.0268316 1.6808279 2.3721112 3.5689692 ## [25] 4.3748761 4.5821062 3.7072104 3.4998354 4.0064237 Our statistical model follows the form \\[\\begin{align*} \\text{kcal}_i &amp; \\sim \\text{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex}_i + \\beta_2 \\text{logmass}_i \\\\ \\text{neocortex}_i &amp; \\sim \\text{Normal} (\\nu, \\sigma_\\text{neocortex}) \\\\ \\alpha &amp; \\sim \\text{Normal} (0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\beta_2 &amp; \\sim \\text{Normal} (0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy} (0, 1) \\\\ \\nu &amp; \\sim \\text{Normal} (0.5, 1) \\\\ \\sigma_\\text{neocortex} &amp; \\sim \\text{HalfCauchy} (0, 1). \\end{align*}\\] If you look closely, you’ll discover the prior McElreath reported in the model equation for the intercept, \\(\\alpha \\sim \\text{Normal} (0, 10)\\), does not match up with the prior he used in R code 14.7, a ~ dnorm(0,100). Here we use the latter. When writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things get cluttered in a hurry. b_model &lt;- # here&#39;s the primary `kcal` model bf(kcal ~ 1 + mi(neocortex) + logmass) + # here&#39;s the model for the missing `neocortex` data bf(neocortex | mi() ~ 1) + # here we set the residual correlations for the two models to zero set_rescor(FALSE) Note the mi(neocortex) syntax in the kcal model. This indicates that the predictor, neocortex, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model, use the get_prior() function. get_prior(data = data_list, family = gaussian, b_model) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 Intercept ## 3 b kcal ## 4 b logmass kcal ## 5 b mineocortex kcal ## 6 student_t(3, 1, 10) Intercept kcal ## 7 student_t(3, 0, 10) sigma kcal ## 8 student_t(3, 1, 10) Intercept neocortex ## 9 student_t(3, 0, 10) sigma neocortex With the one-step Bayesian imputation procedure in brms, you might need to use the resp argument when specifying non-defaut priors. Anyway, here we fit the model. b14.3 &lt;- brm(data = data_list, family = gaussian, b_model, # here we insert the model prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2, seed = 14, file = &quot;fits/b14.03&quot;) The imputed neocortex values are indexed by occasion number from the original data. tidy(b14.3) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.53 0.47 -1.30 0.25 ## 2 b_neocortex_Intercept 0.67 0.01 0.65 0.69 ## 3 b_kcal_logmass -0.07 0.02 -0.11 -0.03 ## 4 bsp_kcal_mineocortex 1.89 0.74 0.68 3.09 ## 5 sigma_kcal 0.13 0.02 0.10 0.18 ## 6 sigma_neocortex 0.06 0.01 0.05 0.08 ## 7 Ymi_neocortex[2] 0.63 0.05 0.55 0.72 ## 8 Ymi_neocortex[3] 0.62 0.05 0.54 0.71 ## 9 Ymi_neocortex[4] 0.62 0.05 0.54 0.71 ## 10 Ymi_neocortex[5] 0.65 0.05 0.58 0.73 ## 11 Ymi_neocortex[9] 0.70 0.05 0.62 0.79 ## 12 Ymi_neocortex[14] 0.66 0.05 0.58 0.74 ## 13 Ymi_neocortex[15] 0.69 0.05 0.61 0.76 ## 14 Ymi_neocortex[17] 0.70 0.05 0.61 0.77 ## 15 Ymi_neocortex[19] 0.71 0.05 0.63 0.79 ## 16 Ymi_neocortex[21] 0.65 0.05 0.57 0.73 ## 17 Ymi_neocortex[23] 0.66 0.05 0.58 0.74 ## 18 Ymi_neocortex[26] 0.70 0.05 0.61 0.78 ## 19 lp__ 40.46 4.36 32.48 46.68 Here’s the model that drops the cases with NAs on neocortex. b14.3cc &lt;- brm(data = data_list, family = gaussian, kcal ~ 1 + neocortex + logmass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 1e4, chains = 2, cores = 2, seed = 14, file = &quot;fits/b14.03cc&quot;) The parameters: tidy(b14.3cc) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_Intercept -1.07 0.58 -2.00 -0.13 ## 2 b_neocortex 2.77 0.90 1.31 4.23 ## 3 b_logmass -0.10 0.03 -0.14 -0.05 ## 4 sigma 0.14 0.03 0.10 0.19 ## 5 lp__ -4.26 1.67 -7.44 -2.36 In order to make our versions of Figure 14.4, we’ll need to do a little data wrangling with fitted(). nd &lt;- tibble(neocortex = seq(from = .5, to = .85, length.out = 30), logmass = median(data_list$logmass)) f_b14.3 &lt;- fitted(b14.3, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.3 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.3312196, 0.3540894, 0.3769593, 0.3998291, 0.4226989, 0.4455688, 0.4… ## $ Est.Error.kcal &lt;dbl&gt; 0.12585397, 0.11720996, 0.10860587, 0.10005203, 0.09156252, 0.0831570… ## $ Q2.5.kcal &lt;dbl&gt; 0.08679786, 0.12619653, 0.16488722, 0.20378873, 0.24274987, 0.2828499… ## $ Q97.5.kcal &lt;dbl&gt; 0.5848699, 0.5902992, 0.5960885, 0.6013202, 0.6069732, 0.6133748, 0.6… ## $ Estimate.neocortex &lt;dbl&gt; 0.6714736, 0.6714736, 0.6714736, 0.6714736, 0.6714736, 0.6714736, 0.6… ## $ Est.Error.neocortex &lt;dbl&gt; 0.01368433, 0.01368433, 0.01368433, 0.01368433, 0.01368433, 0.0136843… ## $ Q2.5.neocortex &lt;dbl&gt; 0.6446126, 0.6446126, 0.6446126, 0.6446126, 0.6446126, 0.6446126, 0.6… ## $ Q97.5.neocortex &lt;dbl&gt; 0.6980734, 0.6980734, 0.6980734, 0.6980734, 0.6980734, 0.6980734, 0.6… ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0.5… ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155,… To include the imputed neocortex values in the plot, we’ll extract the information from broom::tidy(). f_b14.3_mi &lt;- tidy(b14.3) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols(data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex))) f_b14.3_mi %&gt;% head() ## term estimate std.error lower upper kcal neocortex logmass ## 1 Ymi_neocortex[2] 0.6332440 0.05112357 0.5529586 0.7185007 0.51 NA 0.7371641 ## 2 Ymi_neocortex[3] 0.6245501 0.05149849 0.5412463 0.7101070 0.46 NA 0.9202828 ## 3 Ymi_neocortex[4] 0.6225438 0.05143366 0.5405524 0.7084048 0.48 NA 0.4824261 ## 4 Ymi_neocortex[5] 0.6525390 0.04812462 0.5761665 0.7331180 0.60 NA 0.7839015 ## 5 Ymi_neocortex[9] 0.7009097 0.04967649 0.6227489 0.7852700 0.91 NA -0.3424903 ## 6 Ymi_neocortex[14] 0.6567370 0.05031958 0.5766811 0.7398577 0.71 NA -0.5108256 Data wrangling done–here’s our code for Figure 14.4.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] p1 &lt;- f_b14.3 %&gt;% ggplot(aes(x = neocortex)) + geom_smooth(aes(y = Estimate.kcal, ymin = Q2.5.kcal, ymax = Q97.5.kcal), stat = &quot;identity&quot;, fill = color, color = color, alpha = 1/3, size = 1/2) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.3_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.3_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + labs(subtitle = &quot;Note: For the regression line in this plot,\\nlog(mass) has been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) Here we make Figure 14.4.b, combine it with Figure 14.4.a, and plot. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] p2 &lt;- data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.3_mi, aes(y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(&quot;log(mass)&quot;, breaks = -2:4) + ylab(&quot;neocortex proportion&quot;) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) p1 | p2 14.2.2 Improving the imputation model Like McElreath, we’ll update the imputation line of our statistical model to: \\[\\begin{align*} \\text{neocortex}_i &amp; \\sim \\text{Normal} (\\nu_i, \\sigma_\\text{neocortex}) \\\\ \\nu_i &amp; = \\alpha_\\text{neocortex} + \\gamma_1 \\text{logmass}_i, \\end{align*}\\] which includes the updated priors \\[\\begin{align*} \\alpha_\\text{neocortex} &amp; \\sim \\text{Normal} (0.5, 1) \\\\ \\gamma_1 &amp; \\sim \\text{Normal} (0, 10). \\end{align*}\\] As far as the brms code goes, adding logmass as a predictor to the neocortex submodel is pretty simple. # define the model b_model &lt;- bf(kcal ~ 1 + mi(neocortex) + logmass) + bf(neocortex | mi() ~ 1 + logmass) + # here&#39;s the big difference set_rescor(FALSE) # fit the model b14.4 &lt;- brm(data = data_list, family = gaussian, b_model, prior = c(prior(normal(0, 100), class = Intercept, resp = kcal), prior(normal(0.5, 1), class = Intercept, resp = neocortex), prior(normal(0, 10), class = b, resp = kcal), prior(normal(0, 10), class = b, resp = neocortex), prior(cauchy(0, 1), class = sigma, resp = kcal), prior(cauchy(0, 1), class = sigma, resp = neocortex)), iter = 1e4, chains = 2, cores = 2, seed = 14, file = &quot;fits/b14.04&quot;) Behold the parameter estimates. tidy(b14.4) %&gt;% mutate_if(is.numeric, round, digits = 2) ## term estimate std.error lower upper ## 1 b_kcal_Intercept -0.88 0.48 -1.64 -0.07 ## 2 b_neocortex_Intercept 0.64 0.01 0.62 0.66 ## 3 b_kcal_logmass -0.09 0.02 -0.13 -0.05 ## 4 b_neocortex_logmass 0.02 0.01 0.01 0.03 ## 5 bsp_kcal_mineocortex 2.46 0.75 1.19 3.65 ## 6 sigma_kcal 0.13 0.02 0.10 0.17 ## 7 sigma_neocortex 0.04 0.01 0.03 0.06 ## 8 Ymi_neocortex[2] 0.63 0.03 0.57 0.69 ## 9 Ymi_neocortex[3] 0.63 0.04 0.57 0.69 ## 10 Ymi_neocortex[4] 0.62 0.04 0.56 0.68 ## 11 Ymi_neocortex[5] 0.65 0.03 0.59 0.70 ## 12 Ymi_neocortex[9] 0.66 0.04 0.60 0.72 ## 13 Ymi_neocortex[14] 0.63 0.03 0.57 0.68 ## 14 Ymi_neocortex[15] 0.68 0.03 0.62 0.74 ## 15 Ymi_neocortex[17] 0.70 0.03 0.64 0.75 ## 16 Ymi_neocortex[19] 0.71 0.03 0.65 0.77 ## 17 Ymi_neocortex[21] 0.66 0.03 0.61 0.72 ## 18 Ymi_neocortex[23] 0.68 0.03 0.62 0.73 ## 19 Ymi_neocortex[26] 0.74 0.04 0.68 0.80 ## 20 lp__ 48.90 4.05 41.43 54.75 Here’s our pre-Figure 14.5 data wrangling. f_b14.4 &lt;- fitted(b14.4, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) f_b14.4_mi &lt;- tidy(b14.4) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% bind_cols( data_list %&gt;% as_tibble() %&gt;% filter(is.na(neocortex)) ) f_b14.4 %&gt;% glimpse() ## Observations: 30 ## Variables: 10 ## $ Estimate.kcal &lt;dbl&gt; 0.2380801, 0.2677612, 0.2974423, 0.3271235, 0.3568046, 0.3864857, 0.4… ## $ Est.Error.kcal &lt;dbl&gt; 0.12785556, 0.11906079, 0.11030292, 0.10159151, 0.09293962, 0.0843655… ## $ Q2.5.kcal &lt;dbl&gt; -0.009206759, 0.037348106, 0.083427138, 0.129856800, 0.176586987, 0.2… ## $ Q97.5.kcal &lt;dbl&gt; 0.5025289, 0.5142660, 0.5262648, 0.5377310, 0.5487249, 0.5612721, 0.5… ## $ Estimate.neocortex &lt;dbl&gt; 0.6670467, 0.6670467, 0.6670467, 0.6670467, 0.6670467, 0.6670467, 0.6… ## $ Est.Error.neocortex &lt;dbl&gt; 0.009622526, 0.009622526, 0.009622526, 0.009622526, 0.009622526, 0.00… ## $ Q2.5.neocortex &lt;dbl&gt; 0.6478488, 0.6478488, 0.6478488, 0.6478488, 0.6478488, 0.6478488, 0.6… ## $ Q97.5.neocortex &lt;dbl&gt; 0.685581, 0.685581, 0.685581, 0.685581, 0.685581, 0.685581, 0.685581,… ## $ neocortex &lt;dbl&gt; 0.5000000, 0.5120690, 0.5241379, 0.5362069, 0.5482759, 0.5603448, 0.5… ## $ logmass &lt;dbl&gt; 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155, 1.244155,… f_b14.4_mi %&gt;% glimpse() ## Observations: 12 ## Variables: 8 ## $ term &lt;chr&gt; &quot;Ymi_neocortex[2]&quot;, &quot;Ymi_neocortex[3]&quot;, &quot;Ymi_neocortex[4]&quot;, &quot;Ymi_neocortex[5]&quot;,… ## $ estimate &lt;dbl&gt; 0.6310163, 0.6284698, 0.6195025, 0.6463966, 0.6629660, 0.6273373, 0.6796101, 0.… ## $ std.error &lt;dbl&gt; 0.03443509, 0.03515450, 0.03533123, 0.03347496, 0.03580270, 0.03456625, 0.03450… ## $ lower &lt;dbl&gt; 0.5741997, 0.5712126, 0.5617213, 0.5924678, 0.6040286, 0.5711475, 0.6242119, 0.… ## $ upper &lt;dbl&gt; 0.6875889, 0.6862857, 0.6765639, 0.7024188, 0.7213913, 0.6839601, 0.7362796, 0.… ## $ kcal &lt;dbl&gt; 0.51, 0.46, 0.48, 0.60, 0.91, 0.71, 0.73, 0.72, 0.79, 0.48, 0.51, 0.53 ## $ neocortex &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ## $ logmass &lt;dbl&gt; 0.7371641, 0.9202828, 0.4824261, 0.7839015, -0.3424903, -0.5108256, 1.2441546, … For our final plots, let’s play around with colors from viridis_pal(option = &quot;D&quot;). Here’s the code for Figure 14.5.a. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] p1 &lt;- f_b14.4 %&gt;% ggplot(aes(x = neocortex)) + geom_smooth(aes(y = Estimate.kcal, ymin = Q2.5.kcal, ymax = Q97.5.kcal), stat = &quot;identity&quot;, fill = color, color = color, alpha = 1/2, size = 1/2) + geom_point(data = data_list %&gt;% as_tibble(), aes(y = kcal), color = &quot;white&quot;) + geom_point(data = f_b14.4_mi, aes(x = estimate, y = kcal), color = color, shape = 1) + geom_segment(data = f_b14.4_mi, aes(x = lower, xend = upper, y = kcal, yend = kcal), color = color, size = 1/4) + labs(subtitle = &quot;Note: For the regression line in this plot,\\nlog(mass) has been set to its median, 1.244.&quot;, x = &quot;neocortex proportion&quot;, y = &quot;kcal per gram&quot;) + coord_cartesian(xlim = c(.55, .8), ylim = range(data_list$kcal, na.rm = T)) Make the code for Figure 14.5.b, combine it with Figure 14.5.a, and plot. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] p2 &lt;- data_list %&gt;% as_tibble() %&gt;% ggplot(aes(x = logmass, y = neocortex)) + geom_point(color = &quot;white&quot;) + geom_pointrange(data = f_b14.4_mi, aes(y = estimate, ymin = lower, ymax = upper), color = color, size = 1/3, shape = 1) + scale_x_continuous(&quot;log(mass)&quot;, breaks = -2:4) + ylab(&quot;neocortex proportion&quot;) + coord_cartesian(xlim = range(data_list$logmass, na.rm = T), ylim = c(.55, .8)) p1 | p2 If modern missing data methods are new to you, you might also check out van Burren’s great online text, Flexible Imputation of Missing Data. Second Edition. I’m also a fan of Enders’s Applied Missing Data Analysis, for which you can find a free sample chapter here. I’ll also quickly mention that brms accommodates multiple imputation, too. 14.3 Summary Bonus: Meta-analysis If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too. Before we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Meta-analysis is a special case of Bayesian multilevel modeling. And since McElreath’s text doesn’t directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s Bayesian data analysis, Third edition. We’ll let Gelman and colleagues introduce the topic: Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis… The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125—126, emphasis in the original) The basic version of a Bayesian meta-analysis follows the form \\[y_i \\sim \\text{Normal}(\\theta_i, \\sigma_i),\\] where \\(y_i\\) = the point estimate for the effect size of a single study, \\(i\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_i\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. Which we do. The standard error from study \\(i\\) is specified \\(\\sigma_i\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_i\\), here. Those values we take directly from the original studies. Building on the model, we further presume that study \\(i\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such. we presume \\(\\theta_i\\) itself has a distribution following the form \\[\\theta_i \\sim \\text{Normal} (\\mu, \\tau),\\] where \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\). Since there’s no example of a meta-analysis in the text, we’ll have to get our data elsewhere. We’ll focus on Gershoff and Grogan-Kaylor’s (2016) paper, Spanking and child outcomes: Old controversies and new meta-analyses. From their introduction, we read: Around the world, most children (80%) are spanked or otherwise physically punished by their parents (UNICEF, 2014). The question of whether parents should spank their children to correct misbehaviors sits at a nexus of arguments from ethical, religious, and human rights perspectives both in the U.S. and around the world (Gershoff, 2013). Several hundred studies have been conducted on the associations between parents’ use of spanking or physical punishment and children’s behavioral, emotional, cognitive, and physical outcomes, making spanking one of the most studied aspects of parenting. What has been learned from these hundreds of studies? (p. 453) Our goal will be to learn Bayesian meta-analysis by answering part of that question. I’ve transcribed the values directly from Gershoff and Grogan-Kaylor’s paper and saved them as a file called spank.xlsx. You can find the data in this project’s GitHub repository. Let’s load them and glimpse(). spank &lt;- readxl::read_excel(&quot;spank.xlsx&quot;) glimpse(spank) ## Observations: 111 ## Variables: 8 ## $ study &lt;chr&gt; &quot;Bean and Roberts (1981)&quot;, &quot;Day and Roberts (1983)&quot;, &quot;Minton, Kagan, and Levine (… ## $ year &lt;dbl&gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1986, 2012, 1979, 200… ## $ outcome &lt;chr&gt; &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defi… ## $ between &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ within &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, … ## $ d &lt;dbl&gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18, 1.18, 0.70, 0.63, … ## $ ll &lt;dbl&gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -0.49, 0.15, 0.35, -… ## $ ul &lt;dbl&gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.22, 1.05, 1.71, 0.2… In this paper, the effect size of interest is a Cohen’s \\(d\\), derived from the formula \\[d = \\frac{\\mu_\\text{treatment} - \\mu_\\text{comparison}}{\\sigma_\\text{pooled}},\\] where \\[\\sigma_\\text{pooled} = \\sqrt{\\frac{((n_1 - 1) \\sigma_1^2) + ((n_2 - 1) \\sigma_2^2)}{n_1 + n_2 -2}}.\\] To help make the equation for \\(d\\) clearer for our example, we might re-express it as \\[d = \\frac{\\mu_\\text{spanked} - \\mu_\\text{not spanked}}{\\sigma_\\text{pooled}}.\\] McElreath didn’t really focus on effect sizes in his text. If you need a refresher, you might check out Kelley and Preacher’s On effect size. But in words, Cohen’s \\(d\\) is a standardized mean difference between two groups. So if you look back up at the results of glimpse(spank) you’ll notice the column d, which is indeed a vector of Cohen’s \\(d\\) effect sizes. The last two columns, ll and ul, are the lower and upper limits of the associated 95% frequentist confidence intervals. But we don’t want confidence intervals for our d-values; we want their standard errors. Fortunately, we can compute those with the following formula \\[SE = \\frac{\\text{upper limit} - \\text{lower limit}}{3.92}.\\] Here it is in code. spank &lt;- spank %&gt;% mutate(se = (ul - ll) / 3.92) glimpse(spank) ## Observations: 111 ## Variables: 9 ## $ study &lt;chr&gt; &quot;Bean and Roberts (1981)&quot;, &quot;Day and Roberts (1983)&quot;, &quot;Minton, Kagan, and Levine (… ## $ year &lt;dbl&gt; 1981, 1983, 1971, 1988, 1990, 1961, 1962, 1990, 2002, 2005, 1986, 2012, 1979, 200… ## $ outcome &lt;chr&gt; &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defiance&quot;, &quot;Immediate defi… ## $ between &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, … ## $ within &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, … ## $ d &lt;dbl&gt; -0.74, 0.36, 0.34, -0.08, 0.10, 0.63, 0.19, 0.47, 0.14, -0.18, 1.18, 0.70, 0.63, … ## $ ll &lt;dbl&gt; -1.76, -1.04, -0.09, -1.01, -0.82, 0.16, -0.14, 0.20, -0.42, -0.49, 0.15, 0.35, -… ## $ ul &lt;dbl&gt; 0.28, 1.77, 0.76, 0.84, 1.03, 1.10, 0.53, 0.74, 0.70, 0.13, 2.22, 1.05, 1.71, 0.2… ## $ se &lt;dbl&gt; 0.52040816, 0.71683673, 0.21683673, 0.47193878, 0.47193878, 0.23979592, 0.1709183… Now are data are ready, we can express our first Bayesian meta-analysis with the formula \\[\\begin{align*} \\text{d}_i &amp; \\sim \\text{Normal}(\\theta_i, \\sigma_i = \\text{se}_i) \\\\ \\theta_i &amp; \\sim \\text{Normal} (\\mu, \\tau) \\\\ \\mu &amp; \\sim \\text{Normal} (0, 1) \\\\ \\tau &amp; \\sim \\text{HalfCauchy} (0, 1). \\end{align*}\\] The last two lines, of course, spell out our priors. In psychology, it’s pretty rare to see Cohen’s \\(d\\)-values greater than the absolute value of \\(\\pm 1\\). So in the absence of more specific domain knowledge–which I don’t have–, it seems like \\(\\text{Normal} (0, 1)\\) is a reasonable place to start. And just like McElreath used \\(\\text{HalfCauchy} (0, 1)\\) as the default prior for the group-level standard deviations, it makes sense to use it here for our meta-analytic \\(\\tau\\) parameter. Here’s the code for the first model. b14.5 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14, file = &quot;fits/b14.05&quot;) One thing you might notice is our se(se) function excluded the sigma argument. If you recall from section 14.1, we specified sigma = T in our measurement-error models. The brms default is that within se(), sigma = FALSE. As such, we have no estimate for sigma the way we would if we were doing this analysis with the raw data from the studies. Hopefully this makes sense. The uncertainty around the d-value for each study \\(i\\) has already been encoded in the data as se. This brings us to another point. We typically perform meta-analyses on data summaries. In my field and perhaps in yours, this is due to the historical accident that it has not been the norm among researchers to make their data publically available. So effect size summaries were the best we typically had. However, times are changing (e.g., here, here). If the raw data from all the studies for your meta-analysis are available, you can just fit a multilevel model in which the data are nested in the studies. Heck, you could even allow the studies to vary by \\(\\sigma\\) by taking the distributional modeling approach and specify something like sigma ~ 0 + study or even sigma ~ 1 + (1 | study). But enough technical talk. Let’s look at the model results. print(b14.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.26 0.03 0.21 0.33 1.00 2470 3979 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.38 0.04 0.30 0.45 1.00 1349 2609 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Thus, in our simple Bayesian meta-analysis, we have a population Cohen’s \\(d\\) of about 0.38. Our estimate for \\(\\tau\\), 0.26, suggests we have quite a bit of between-study variability. One question you might ask is: What exactly are these Cohen’s \\(d\\)s measuring, anyways? We’ve encoded that in the outcome vector of the spank data. spank %&gt;% distinct(outcome) %&gt;% knitr::kable() outcome Immediate defiance Low moral internalization Child aggression Child antisocial behavior Child externalizing behavior problems Child internalizing behavior problems Child mental health problems Child alcohol or substance abuse Negative parent–child relationship Impaired cognitive ability Low self-esteem Low self-regulation Victim of physical abuse Adult antisocial behavior Adult mental health problems Adult alcohol or substance abuse Adult support for physical punishment There are a few things to note. First, with the possible exception of Adult support for physical punishment, all of the outcomes are negative. We prefer conditions associated with lower values for things like Child aggression and Adult mental health problems. Second, the way the data are coded, larger effect sizes are interpreted as more negative outcomes associated with children having been spanked. That is, our analysis suggests spanking children is associated with worse outcomes. What might not be immediately apparent is that even though there are 111 cases in the data, there are only 76 distinct studies. spank %&gt;% distinct(study) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 76 In other words, some studies have multiple outcomes. In order to better accommodate the study- and outcome-level variances, let’s fit a cross-classified Bayesian meta-analysis reminiscent of the cross-classified chimp model from Chapter 13. b14.6 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 1 + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = Intercept), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14, file = &quot;fits/b14.06&quot;) print(b14.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 1 + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.08 0.03 0.04 0.14 1.00 2800 5177 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.25 0.03 0.20 0.32 1.00 2295 4532 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.36 0.04 0.28 0.44 1.00 1823 3737 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we have two \\(\\tau\\) parameters. We might plot them to get a sense of where the variance is at. # we&#39;ll want this to label the plot label &lt;- tibble(tau = c(.12, .3), y = c(15, 10), label = c(&quot;sigma[&#39;outcome&#39;]&quot;, &quot;sigma[&#39;study&#39;]&quot;)) # wrangle posterior_samples(b14.6) %&gt;% select(starts_with(&quot;sd&quot;)) %&gt;% gather(key, tau) %&gt;% mutate(key = str_remove(key, &quot;sd_&quot;) %&gt;% str_remove(., &quot;__Intercept&quot;)) %&gt;% # plot ggplot(aes(x = tau)) + geom_density(aes(fill = key), color = &quot;transparent&quot;) + geom_text(data = label, aes(y = y, label = label, color = label), size = 5, parse = T) + scale_fill_viridis_d(NULL, option = &quot;B&quot;, begin = .5) + scale_color_viridis_d(NULL, option = &quot;B&quot;, begin = .5) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(tau)) + theme(panel.grid = element_blank()) So at this point, the big story is there’s more variability between the studies than there is the outcomes. But I still want to get a sense of the individual outcomes. Here we’ll use tidybayes::geom_halfeyeh() to help us make our version of a forest plot and tidybayes::spread_draws() to help with the initial wrangling. library(tidybayes) b14.6 %&gt;% spread_draws(b_Intercept, r_outcome[outcome,]) %&gt;% # add the grand mean to the group-specific deviations mutate(mu = b_Intercept + r_outcome) %&gt;% ungroup() %&gt;% mutate(outcome = str_replace_all(outcome, &quot;[.]&quot;, &quot; &quot;)) %&gt;% # plot ggplot(aes(x = mu, y = reorder(outcome, mu), fill = reorder(outcome, mu))) + geom_vline(xintercept = fixef(b14.6)[1, 1], color = &quot;grey33&quot;, size = 1) + geom_vline(xintercept = fixef(b14.6)[1, 3:4], color = &quot;grey33&quot;, linetype = 2) + geom_halfeyeh(.width = .95, size = 2/3, color = &quot;white&quot;) + scale_fill_viridis_d(option = &quot;B&quot;, begin = .2) + labs(x = expression(italic(&quot;Cohen&#39;s d&quot;)), y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) The solid and dashed vertical white lines in the background mark off the grand mean (i.e., the meta-analytic effect) and its 95% intervals. But anyway, there’s not a lot of variability across the outcomes. Let’s go one step further with the model. Doubling back to Gelman and colleagues, we read: When assuming exchangeability we assume there are no important covariates that might form the basis of a more complex model, and this assumption (perhaps misguidedly) is widely adopted in meta-analysis. What if other information (in addition to the data \\((n, y)\\)) is available to distinguish among the \\(J\\) studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates, for example using a hierarchical regression model. (p. 126) One important covariate Gershoff and Grogan-Kaylor addressed in their meta-analysis was the type of study. The 76 papers they based their meta-analysis on contained both between- and within-participants designs. In the spank data, we’ve dummy coded that information with the between and within vectors. Both are dummy variables and \\(\\text{within} = 1 - \\text{between}\\). Here are the counts. spank %&gt;% count(between) ## # A tibble: 2 x 2 ## between n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 71 ## 2 1 40 When I use dummies in my models, I prefer to have the majority group stand as the reference category. As such, I typically name those variables by the minority group. In this case, most occasions are based on within-participant designs. Thus, we’ll go ahead and add the between variable to the model. While we’re at it, we’ll practice using the 0 + intercept syntax. b14.7 &lt;- brm(data = spank, family = gaussian, d | se(se) ~ 0 + Intercept + between + (1 | study) + (1 | outcome), prior = c(prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sd)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 14, file = &quot;fits/b14.07&quot;) Behold the summary. print(b14.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d | se(se) ~ 0 + intercept + between + (1 | study) + (1 | outcome) ## Data: spank (Number of observations: 111) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~outcome (Number of levels: 17) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.08 0.02 0.04 0.14 1.00 4802 6993 ## ## ~study (Number of levels: 76) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.25 0.03 0.20 0.32 1.00 3800 6117 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## intercept 0.38 0.05 0.29 0.48 1.00 2989 5161 ## between -0.07 0.07 -0.22 0.07 1.00 3128 4663 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s take a closer look at the b_between parameter. color &lt;- viridis_pal(option = &quot;B&quot;)(7)[5] posterior_samples(b14.7) %&gt;% ggplot(aes(x = b_between, y = 0)) + geom_halfeyeh(.width = c(.5, .95), color = &quot;white&quot;, fill = color) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;Overall difference for between- vs within-participant designs&quot;) + theme(panel.grid = element_blank()) That difference isn’t as large I’d expect it to be. But then again, I’m no spanking researcher. So what do I know? There are other things you might do with these data. For example, you might check for trends by year or, as the authors did in their manuscript, distinguish among different severities of corporal punishment. But I think we’ve gone far enough to get you started. If you’d like to learn more about these methods, do check out Vourre’s Meta-analysis is a special case of Bayesian multilevel modeling. From his blog, you’ll learn additional tricks, like making a more traditional-looking forest plot with the brmstools::forest() function and how our Bayesian brms method compares with frequentist meta-analyses via the metafor package. You might also check out Williams, Rast, and Bürkner’s manuscript, Bayesian Meta-Analysis with Weakly Informative Prior Distributions to give you an empirical justification for using a half-Cauchy prior for your meta-analysis \\(\\tau\\) parameters. Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.0.1.9000 broom_0.5.3 patchwork_1.0.0 viridis_0.5.1 ## [5] viridisLite_0.3.0 brms_2.12.0 Rcpp_1.0.3 rstan_2.19.2 ## [9] StanHeaders_2.19.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.4 ## [13] purrr_0.3.3 readr_1.3.1 tidyr_1.0.2 tibble_2.1.3 ## [17] ggplot2_3.2.1 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.2 ## [4] rsconnect_0.8.16 markdown_1.1 base64enc_0.1-3 ## [7] fs_1.3.1 rstudioapi_0.10 farver_2.0.3 ## [10] svUnit_0.7-12 DT_0.11 fansi_0.4.1 ## [13] mvtnorm_1.0-12 lubridate_1.7.4 xml2_1.2.2 ## [16] bridgesampling_0.8-1 knitr_1.26 shinythemes_1.1.2 ## [19] bayesplot_1.7.1 jsonlite_1.6.1 dbplyr_1.4.2 ## [22] shiny_1.4.0 compiler_3.6.2 httr_1.4.1 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-18 ## [28] fastmap_1.0.1 lazyeval_0.2.2 cli_2.0.1 ## [31] later_1.0.0 htmltools_0.4.0 prettyunits_1.1.1 ## [34] tools_3.6.2 igraph_1.2.4.2 coda_0.19-3 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.2.2 nlme_3.1-142 ## [43] crosstalk_1.0.0 xfun_0.12 ps_1.3.0 ## [46] rvest_0.3.5 mime_0.8 miniUI_0.1.1.1 ## [49] lifecycle_0.1.0 gtools_3.8.1 MASS_7.3-51.4 ## [52] zoo_1.8-7 scales_1.1.0 colourpicker_1.0 ## [55] hms_0.5.3 promises_1.1.0 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.1 ## [61] gridExtra_2.3 loo_2.2.0 stringi_1.4.5 ## [64] highr_0.8 dygraphs_1.1.1.6 pkgbuild_1.0.6 ## [67] rlang_0.4.4 pkgconfig_2.0.3 matrixStats_0.55.0 ## [70] evaluate_0.14 lattice_0.20-38 labeling_0.3 ## [73] rstantools_2.0.0 htmlwidgets_1.5.1 processx_3.4.1 ## [76] tidyselect_1.0.0 plyr_1.8.5 magrittr_1.5 ## [79] R6_2.4.1 generics_0.0.2 DBI_1.1.0 ## [82] pillar_1.4.3 haven_2.2.0 withr_2.1.2 ## [85] xts_0.12-0 abind_1.4-5 modelr_0.1.5 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_2.0 grid_3.6.2 readxl_1.3.1 ## [94] callr_3.4.1 threejs_0.3.3 reprex_0.3.0 ## [97] digest_0.6.23 xtable_1.8-4 httpuv_1.5.2 ## [100] stats4_3.6.2 munsell_0.5.0 shinyjs_1.1 "],
["horoscopes-insights.html", "15 Horoscopes Insights 15.1 Use R Notebooks 15.2 Save your model fits 15.3 Build your models slowly 15.4 Look at your data 15.5 Use the 0 + Intercept syntax 15.6 Annotate your workflow 15.7 Annotate your code 15.8 Break up your workflow 15.9 Code in public 15.10 Read Gelman’s blog 15.11 Check out other social media, too 15.12 Parting wisdom Reference Session info", " 15 Horoscopes Insights Statistical inference is indeed critically important. But only as much as every other part of research. Scientific discovery is not an additive process, in which sin in one part can be atoned by virtue in another. Everything interacts. So equally when science works as intended as when it does not, every part of the process deserves attention. (p. 441) In this final chapter, there are no models for us to fit and no figures for use to reimagine. McElreath took the opportunity to comment more broadly on the scientific process. He made a handful of great points, some of which I’ll quote in a bit. But for the bulk of this chapter, I’d like to take the opportunity to pass on a few of my own insights about workflow. I hope they’re of use. 15.1 Use R Notebooks OMG I first started using R in the winter of 2015/2016. Right from the start, I learned how to code from within the R Studio environment. But within R Studio I was using simple scripts. No longer. I now use R Notebooks for just about everything, including my scientific projects, this bookdown project, and even my academic webpage and blog. Nathan Stephens wrote a nice blog on Why I love R Notebooks. I agree. This has fundamentally changed my workflow as a scientist. I only wish I’d learned about this before starting my dissertation project. So it goes… Do yourself a favor, adopt R Notebooks into your workflow. Do it today. If you prefer to learn with videos, here’s a nice intro by Kristine Yu and another one by JJ Allaire. Try it out for like one afternoon and you’ll be hooked. 15.2 Save your model fits It’s embarrassing how long it took for this to dawn on me. Unlike classical statistics, Bayesian models using MCMC take a while to compute. Most of the simple models in McElreath’s text take 30 seconds up to a couple minutes. If your data are small, well-behaved and of a simple structure, you might have a lot of wait times in that range in your future. It hasn’t been that way, for me. Most of my data have a complicated multilevel structure and often aren’t very well behaved. It’s normal for my models to take an hour or several to fit. Once you start measuring your model fit times in hours, you do not want to fit these things more than once. So, it’s not enough to document my code in a nice R Notebook file. I need to save my brm() fit objects in external files. Consider this model. It’s taken from Bürkner’s vignette, Estimating Multivariate Models with brms. It took about five minutes for my several-year-old laptop to fit. library(brms) data(&quot;BTdata&quot;, package = &quot;MCMCglmm&quot;) b15.1 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex + hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15) Five minutes isn’t terribly long to wait, but still. I’d prefer to never have to wait for another five minutes, again. Sure, if I save my code in a document like this, I will always be able to fit the model again. But I can work smarter. Here I’ll save my b15.1 object outside of R with the save() function. save(b15.1, file = &quot;fits/b15.01.rda&quot;) Hopefully y’all are savvy Bayesian R users and find this insultingly remedial. But if it’s new to you like it was me, you can learn more about .rda files here. Now b15.1 is saved outside of R, I can safely remove it and then reload it. rm(b15.1) load(&quot;fits/b15.01.rda&quot;) The file took a fraction of a second to reload. Once reloaded, I can perform typical operations, like examine summaries of the model parameters or refreshing my memory on what data I used. print(b15.1) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Group-Level Effects: ## ~dam (Number of levels: 106) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(tarsus_Intercept) 0.48 0.05 0.39 0.58 1.00 1114 1456 ## sd(back_Intercept) 0.24 0.07 0.10 0.38 1.00 455 910 ## cor(tarsus_Intercept,back_Intercept) -0.52 0.21 -0.92 -0.10 1.00 618 777 ## ## ~fosternest (Number of levels: 104) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(tarsus_Intercept) 0.27 0.05 0.16 0.38 1.00 843 1113 ## sd(back_Intercept) 0.35 0.06 0.25 0.47 1.00 666 1510 ## cor(tarsus_Intercept,back_Intercept) 0.68 0.20 0.20 0.98 1.01 214 571 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## tarsus_Intercept -0.41 0.07 -0.54 -0.28 1.00 1571 1442 ## back_Intercept -0.02 0.07 -0.15 0.11 1.00 2728 1378 ## tarsus_sexMale 0.77 0.06 0.66 0.88 1.00 3265 1639 ## tarsus_sexUNK 0.23 0.13 -0.03 0.49 1.00 3258 1461 ## tarsus_hatchdate -0.04 0.06 -0.16 0.07 1.00 1640 1334 ## back_sexMale 0.01 0.07 -0.12 0.14 1.00 4477 1403 ## back_sexUNK 0.15 0.15 -0.15 0.43 1.00 3499 1231 ## back_hatchdate -0.09 0.05 -0.20 0.01 1.00 2248 1643 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_tarsus 0.76 0.02 0.72 0.80 1.00 1942 1259 ## sigma_back 0.90 0.02 0.86 0.95 1.00 2151 1509 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(tarsus,back) -0.05 0.04 -0.13 0.02 1.00 2758 1584 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). head(b15.1$data) ## tarsus sex hatchdate fosternest dam back ## 1 -1.89229718 Fem -0.6874021 F2102 R187557 1.1464212 ## 2 1.13610981 Male -0.6874021 F1902 R187559 -0.7596521 ## 3 0.98468946 Male -0.4279814 A602 R187568 0.1449373 ## 4 0.37900806 Male -1.4656641 A1302 R187518 0.2555847 ## 5 -0.07525299 Fem -1.4656641 A2602 R187528 -0.3006992 ## 6 -1.13519543 Fem 0.3502805 C2302 R187945 1.5577219 The other option, which we’ve been using extensively throughout the earlier chapters, is to use file argument within the brms::brm() function. You can read about the origins of the argument in issue #472 on the brms GitHub repo. To make use of the file argument, specify a character string. brm() will then save your fitted model object in an external .rds file via the saveRDS() function. Let’s give it a whirl, this time with an interaction. b15.2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex*hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15, file = &quot;fits/b15.02&quot;) Now b15.2 is saved outside of R, I can safely remove it and then reload it. rm(b15.2) We might load b15.2 with the readRDS() function. b15.2 &lt;- readRDS(&quot;fits/b15.02.rds&quot;) Now we can work with b15.2 as desired. fixef(b15.2) ## Estimate Est.Error Q2.5 Q97.5 ## tarsus_Intercept -0.409058221 0.07000115 -0.5465915 -0.26824807 ## back_Intercept -0.010715306 0.06687225 -0.1435934 0.11329108 ## tarsus_sexMale 0.770596137 0.05698090 0.6584964 0.88406317 ## tarsus_sexUNK 0.194499540 0.14690185 -0.1037126 0.48480535 ## tarsus_hatchdate -0.053765823 0.06776875 -0.1862419 0.07744207 ## tarsus_sexMale:hatchdate 0.013002442 0.05763197 -0.1005761 0.12532968 ## tarsus_sexUNK:hatchdate 0.058184772 0.12220525 -0.1739590 0.29863949 ## back_sexMale 0.004504031 0.06873231 -0.1280008 0.13826869 ## back_sexUNK 0.144949811 0.17049854 -0.1904839 0.47229684 ## back_hatchdate -0.052088249 0.06068786 -0.1726243 0.06116698 ## back_sexMale:hatchdate -0.078568190 0.06879491 -0.2087758 0.05303484 ## back_sexUNK:hatchdate -0.033029373 0.14295086 -0.3210653 0.25294364 The file method has another handy feature. Let’s remove b15.2 one more time to see. rm(b15.2) If you’ve fit a brm() model once and saved the results with file, executing the same brm() code will not re-fit the model. Rather, it will just load and return the model from the .rds file. b15.2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex*hatchdate + (1|p|fosternest) + (1|q|dam), chains = 2, cores = 2, seed = 15, file = &quot;fits/b15.02&quot;) It takes just a fraction of a second. Once again, we’re ready to work with b15.2. b15.2$formula ## tarsus ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) And if you’d like to remind yourself what the name of that external file was or what subfolder you saved it in, you can extract it from the brm() fit object. b15.2$file ## [1] &quot;fits/b15.02.rds&quot; Also, see Gavin Simpson’s blog post, A better way of saving and loading objects in R, for a discussion on the distinction between .rda and .rds files. 15.3 Build your models slowly The model from Bürkner’s vignette, b15.1, was no joke. If you wanted to be verbose about it, it was a multilevel, multivariate, multivariable model. It had a cross-classified multilevel structure, two predictors (for each criterion), and two criteria. Not only is that a lot to keep track of, there’s a whole lot of places for things to go wrong. Even if that was the final model I was interested in as a scientist, I still wouldn’t start with it. I’d build up incrementally, just to make sure nothing looked fishy. One place to start would be a simple intercepts-only model. b15.0 &lt;- brm(mvbind(tarsus, back) ~ 1, data = BTdata, chains = 2, cores = 2, file = &quot;fits/b15.00&quot;) plot(b15.0) print(b15.0) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ 1 ## back ~ 1 ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## tarsus_Intercept 0.00 0.04 -0.07 0.07 1.00 2446 1531 ## back_Intercept 0.00 0.04 -0.07 0.07 1.00 2369 1508 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_tarsus 1.00 0.03 0.95 1.05 1.00 2422 1457 ## sigma_back 1.00 0.03 0.95 1.05 1.00 2116 1337 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(tarsus,back) -0.03 0.04 -0.10 0.04 1.00 2146 1445 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If the chains look good and the summary statistics look like what I’d expect, I’m on good footing to keep building up to the model I really care about. The results from this model, for example, suggest that both criteria were standardized (i.e., intercepts at 0 and \\(\\sigma\\)s at 1). If that wasn’t what I intended, I’d rather catch it here than spend five minutes fitting the more complicated b15.1 model, the parameters for which are sufficiently complicated that I may have had trouble telling what scale the data were on. Note, this is not the same as \\(p\\)-hacking or wandering aimlessly down the garden of forking paths. We are not chasing the flashiest model to put in a paper. Rather, this is just good pragmatic data science. If you start off with a theoretically-justified but complicated model and run into computation problems or produce odd-looking estimates, it won’t be clear where things went awry. When you build up, step by step, it’s easier to catch data cleaning failures, coding goofs and the like. So, when I’m working on a project, I fit one or a few simplified models before fitting my complicated model of theoretical interest. This is especially the case when I’m working with model types that are new to me or that I haven’t worked with in a while. I document each step in my R Notebook files and I save the fit objects for each in external files. I have caught surprises this way. Hopefully this will help you catch your mistakes, too. 15.4 Look at your data Relatedly, and perhaps even a precursor, you should always plot your data before fitting a model. There were plenty examples of this in the text, but it’s worth of making explicit. Simple summary statistics are great, but they’re not enough. For an entertaining exposition, check out Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. Though it might make for a great cocktail party story, I’d hate to pollute the scientific literature with a linear model based on a set of dinosaur-shaped data. 15.5 Use the 0 + Intercept syntax We covered this a little in the last couple chapters (e.g., section 12.4.3), but it’s easy to miss. If your real-world model has predictors (i.e., isn’t an intercept-only model), it’s important to keep track of how you have centered those predictors. When you specify a prior for a brms Intercept (i.e., an intercept resulting from the y ~ x or y ~ 1 + x style of syntax), that prior is applied under the presumption all the predictors are mean centered. In the Population-level (‘fixed’) effects subsection of the set_prior section of the brms reference manual (version 2.12.0), we read: Note that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means. To treat the intercept as an ordinary population-level effect and avoid the centering parameterization, use 0 + intercept on the right-hand side of the model formula. (p. 180) We get a little more information from the Parameterization of the population-level intercept subsection of the brmsformula section: This behavior can be avoided by using the reserved (and internally generated) variable Intercept. Instead of y ~ x, you may write y ~ 0 + Intercept + x. This way, priors can be defined on the real intercept, directly. In addition, the intercept is just treated as an ordinary population-level effect and thus priors defined on b will also apply to it. Note that this parameterization may be less efficient than the default parameterization discussed above. (pp. 38–39) We didn’t bother with this for most of the project because our priors on the Intercept were often vague and the predictors were often on small enough scales (e.g., the mean of a dummy variable is close to 0) that it just didn’t matter. But this will not always be the case. Set your Intercept priors with care. There’s also the flip side of the issue. If you have no strong reason not to, consider mean-centering or even standardizing your predictors. Not only will that solve the Intercept prior issue, but it often results in more meaningful parameter estimates. 15.6 Annotate your workflow In a typical model-fitting file, I’ll load my data, perhaps transform the data a bit, fit several models, and examine the output of each with trace plots, model summaries, information criteria, and the like. In my early days, I just figured each of these steps were self-explanatory. Nope. “In every project you have at least one other collaborator; future-you. You don’t want future-you to curse past-you.” My experience was that even a couple weeks between taking a break from a project and restarting it was enough time to make my earlier files confusing. And they were my files. I now start each R Notebook document with an introductory paragraph or two explaining exactly what the purpose of the file is. I separate my major sections by headers and subheaders. My working R Notebook files are peppered with bullets, sentences, and full on paragraphs between code blocks. 15.7 Annotate your code This idea is implicit in McElreath’s text, but it’s easy to miss the message. I know I did, at first. I find this is especially important for data wrangling. I’m a tidyverse guy and, for me, the big-money verbs like mutate(), gather(), select(), filter(), group_by(), and summarise() take care of the bulk of my data wrangling. But every once and a while I need to do something less common, like with str_extract() or case_when(). And when I end up using a new or less familiar function, I typically annotate right in the code and even sometimes leave a hyperlink to some R-bloggers post or stackoverflow question that explained how to use it. 15.8 Break up your workflow I’ve also learned to break up my projects into multiple R Notebook files. If you have a small project for which you just want a quick and dirty plot, fine, do it all in one file. My typical project has: A primary data cleaning file A file with basic descriptive statistics and the like At least one primary analysis file Possible secondary and tertiary analysis files A file or two for my major figures A file explaining and depicting my priors, often accompanied by my posteriors, for comparison Putting all that information in one R Notebook file would be overwhelming. Your workflow might well look different, but hopefully you get the idea. You don’t want working files with thousands of lines of code. And mainly to keep Jenny Bryan from setting my computer on fire, I’m also getting into the habit of organizing all these interconnected files with help from R Studio Projects, which you can learn even more about from this chapter in R4DS. 15.9 Code in public If you would like to improve the code you write for data-wrangling, modeling, and/or visualizing, code in public. Yes, it can be intimidating. Yes, you will probably make mistakes. If you’re lucky, others will point them out and you will revise and learn and grow. You can do this on any number of mediums, such as GitHub (e.g., here), personal blogs (e.g., here), the Open Science Framework (e.g., here), online books (e.g., here), full-blown YouTube lectures (e.g., here), or even in brief Twitter gifs (e.g., here). I’ve found that just the possibility that others might look at my code makes it more likely I’ll slow down, annotate, and try to use more consistent formatting. Hopefully it will benefit you, too. 15.10 Read Gelman’s blog Yes, that Gelman. Actually, I started reading Gelman’s blog around the same time I dove into McElreath’s text. But if this isn’t the case for you, it’s time to correct that evil. My graduate mentor often recalled how transformative his first academic conference was. He was an undergrad at the time and it was his first experience meeting and talking with the people whose names he’d seen in his text books. He learned that science was an ongoing conversation among living scientists and–at that time–the best place to take part in that conversation was at conferences. Times keep changing. Nowadays, the living conversation of science occurs online on social media and in blogs. One of the hottest places to find scientists conversing about Bayesian statistics and related methods is Gelman’s blog. The posts are great. But a lot of the action is in the comments sections, too. 15.11 Check out other social media, too If you’re not on it, consider joining academic twitter. The word on the street is correct. Twitter can be rage-fueled dumpster fire. But if you’re selective about who you follow, it’s a great place to lean from and connect with your academic heroes. If you’re a fan of this project, here’s a list of some of the people you might want to follow: Mara Averick Michael Bentacourt Paul Bürkner Jenny Bryan Frank Harrell Matthew Kay Tristan Mahr Richard McElreath Danielle Navarro Roger Peng Dan Simpson Aki Vehtari Matti Vuorre Hadley Wickham Yihui Xie I’m on twitter, too. If you’re on facebook and in the social sciences, you might check out the Bayesian Inference in Psychology group. It hasn’t been terribly active, as of late. But there are a lot of great folks to connect with, there. I’ve already mentioned Gelman’s blog. McElreath has one, too. He posts infrequently, but it’s usually pretty good when he does. Also, do check out the Stan Forums. They have a special brms tag there, under which you can find all kinds of hot brms talk. But if you’re new to the world of asking for help with your code online, you might acquaint yourself with the notion of a minimally reproducible example. In short, a good minimally reproducible example helps others help you. If you fail to do this, prepare for some snark. 15.12 Parting wisdom Okay, that’s enough from me. Let’s start wrapping this project up with some McElreath. There is an aspect of science that you do personally control: openness. Pre-plan your research together with the statistical analysis. Doing so will improve both the research design and the statistics. Document it in the form of a mock analysis that you would not be ashamed to share with a colleague. Register it publicly, perhaps in a simple repository, like Github or any other. But your webpage will do just fine, as well. Then collect the data. Then analyze the data as planned. If you must change the plan, that’s fine. But document the changes and justify them. Provide all of the data and scripts necessary to repeat your analysis. Do not provide scripts and data “on request,” but rather put them online so reviewers of your paper can access them without your interaction. There are of course cases in which full data cannot be released, due to privacy concerns. But the bulk of science is not of that sort. The data and its analysis are the scientific product. The paper is just an advertisement. If you do your honest best to design, conduct, and document your research, so that others can build directly upon it, you can make a difference. (p. 443) Toward that end, also check out the OSF and their YouTube channel, here. Katie Corker gets the last words: “Open science is stronger because we’re doing this together.” Reference McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.12.0 Rcpp_1.0.3 ## ## loaded via a namespace (and not attached): ## [1] Brobdingnag_1.2-6 gtools_3.8.1 StanHeaders_2.19.0 threejs_0.3.3 shiny_1.4.0 ## [6] assertthat_0.2.1 stats4_3.6.2 yaml_2.2.1 backports_1.1.5 pillar_1.4.3 ## [11] lattice_0.20-38 glue_1.3.1 digest_0.6.23 promises_1.1.0 colorspace_1.4-1 ## [16] htmltools_0.4.0 httpuv_1.5.2 Matrix_1.2-18 plyr_1.8.5 dygraphs_1.1.1.6 ## [21] pkgconfig_2.0.3 rstan_2.19.2 purrr_0.3.3 xtable_1.8-4 mvtnorm_1.0-12 ## [26] scales_1.1.0 processx_3.4.1 later_1.0.0 tibble_2.1.3 farver_2.0.3 ## [31] bayesplot_1.7.1 ggplot2_3.2.1 DT_0.11 shinyjs_1.1 lazyeval_0.2.2 ## [36] cli_2.0.1 magrittr_1.5 crayon_1.3.4 mime_0.8 evaluate_0.14 ## [41] ps_1.3.0 fansi_0.4.1 nlme_3.1-142 xts_0.12-0 pkgbuild_1.0.6 ## [46] colourpicker_1.0 prettyunits_1.1.1 rsconnect_0.8.16 tools_3.6.2 loo_2.2.0 ## [51] lifecycle_0.1.0 matrixStats_0.55.0 stringr_1.4.0 munsell_0.5.0 callr_3.4.1 ## [56] compiler_3.6.2 rlang_0.4.4 grid_3.6.2 ggridges_0.5.2 htmlwidgets_1.5.1 ## [61] crosstalk_1.0.0 igraph_1.2.4.2 miniUI_0.1.1.1 labeling_0.3 base64enc_0.1-3 ## [66] rmarkdown_2.0 gtable_0.3.0 inline_0.3.15 abind_1.4-5 markdown_1.1 ## [71] reshape2_1.4.3 R6_2.4.1 gridExtra_2.3 rstantools_2.0.0 zoo_1.8-7 ## [76] knitr_1.26 bridgesampling_0.8-1 dplyr_0.8.4 fastmap_1.0.1 shinystan_2.5.0 ## [81] shinythemes_1.1.2 stringi_1.4.5 parallel_3.6.2 vctrs_0.2.2 tidyselect_1.0.0 ## [86] xfun_0.12 coda_0.19-3 "]
]
